{"number": "100", "title": "Should AND I AND learn AND Python AND or AND R AND for AND data AND analysis", "results": 8846, "documents": [[{"score": 1968.1794, "uuid": "3980cb99-a719-5e5b-9bc9-efd4e5dda3b5", "index": "cw12", "trec_id": "clueweb12-0800wb-21-10162", "target_hostname": "answers.oreilly.com", "target_uri": "http://answers.oreilly.com/topic/3056-what-programming-language-should-i-learn-to-manipulate-and-clean-up-data-for-analysis/", "page_rank": 1.1935722e-09, "spam_rank": 84, "title": "What programming language <em>should</em> <em>I</em> <em>learn</em> to manipulate and clean up <em>data</em>", "snippet": "<em>I</em> am currently a graduate student in predictive analytics I&#x27;m looking to <em>learn</em> a programming language to manipulate <em>data</em> files so <em>i</em> can clean them up <em>for</em> <em>analysis</em>. <em>I</em> used to program in VB but am looking to <em>learn</em> something more current. <em>Should</em> <em>I</em> <em>learn</em> javascript, Perl, <em>Python</em>, Ruby...?", "explanation": null, "document": "Voting for yourself is not allowed.\nVoting for yourself is not allowed.\nAnswered by mhalverson\nPosted Dec 15 2011 08:46 PM\nI would recommend either Python or Perl. Either one will serve you very well for that sort of task. Both are very powerful languages but are very different as well. Look up some examples on the net of the sort of tasks you will want to accomplish for both languages in order to get a feel for them, and then choose which one you feel more comfortable with. Coming from VB, Python may feel a little more natural to you, but both will be a change. Perl will probably make you feel a bit lost at first and will have a steeper learning curve.\nI've used both of them for data analysis tasks myself, and typically do continue to use both. If you have the time to learn more than one language, it would not hurt you to learn both (plus, the more languages you learn, the easier it will be to learn a new one). You will find that some things are easier to do in Perl (text processing and filtering, due to its strong reliance on regular exp ressions, and quick and dirty one-time processing, due to its reliance on 'default' variables) and other things are easier to do in Python (restructuring and intermediate calculations, due to its large number of variable types and comprehension exp ressions).\nR, a statistical language, may be worth looking at as well.\n"}, {"score": 1846.1644, "uuid": "d52f06c8-7925-55f1-903b-df640e068189", "index": "cw12", "trec_id": "clueweb12-1000tw-36-03099", "target_hostname": "learndataanalysis.com", "target_uri": "http://learndataanalysis.com/mcmc-programming-r-python-java-and-c", "page_rank": 1.1700305e-09, "spam_rank": 89, "title": "MCMC programming in <em>R</em>, <em>Python</em>, Java and C | <em>Learn</em> <em>Data</em> <em>Analysis</em>", "snippet": "One possibility that is often put forward is to prototype in a dynamic language like <em>R</em> <em>or</em> <em>Python</em> and then port (the slow parts) to a statically typed language later.", "explanation": null, "document": "MCMC programming in R, Python, Java and C\n3 votes\nTweet\nSubmitted by Sepehr Akhavan on Sat, 03/17/2012 - 13:08\nMarkov Chain Monte Carlo (MCMC) is a powerful simulation technique for exploring posterior distributions that arise in Bayesian Statistics. In order to do MCMC, you can either use some generic tools including WinBugs and JAGS or you might want to write your own MCMC code in a programing language.\nOften times, MCMC algorithms are run through millions of cycles and therefore it's natural to wonder what programming language should be chosen that includes good scientific library on generating random numbers as well as being fast enough to implement MCMC algorithm.\nIn the post below, Dr. Darren Wilkinson compares MCMC codes under four different languages of R, Python, Java, and C.\nHe thinks:\n\"R and Python are slow, on account of their dynamic type systems, but are quick and easy to develop in. Java and C are much faster. The speed advantages of Java and C can be important in the context of complex MCMC algorithms. One possibility that is often put forward is to prototype in a dynamic language like R or Python and then port (the slow parts) to a statically typed language later. This could be time efficient, as the debugging and re-factoring can take place in the dynamic language where it is easier, then just re-coded fairly directly into the statically typed language once the code is working well. Actually, I haven\u2019t found this to be time efficient for the development of MCMC codes. That could be just me, but it could be a feature of MCMC algorithms in that the dynamic code runs too slowly to be amenable to quick and simple debugging.\"\nTo read the full article, please click here .\nCategories:\n"}, {"score": 1652.3538, "uuid": "3e88e184-530c-5614-ad3f-6ff5b54ce607", "index": "cw12", "trec_id": "clueweb12-1100tw-49-05402", "target_hostname": "www.r-bloggers.com", "target_uri": "http://www.r-bloggers.com/data-analysis-training/", "page_rank": 1.206068e-09, "spam_rank": 85, "title": "<em>Data</em> <em>Analysis</em> Training | (<em>R</em> news &amp; tutorials)", "snippet": "Why <em>I</em> started learning <em>data</em> science and picked <em>R</em> Curriculum <em>for</em> Intro to <em>R</em> (<em>R</em> has steep learning curve. Purpose of this discussion is to get you started) To leave a comment <em>for</em> the author, please follow the link and comment on his blog: Enterprise Software Doesn&#x27;t Have to Suck.", "explanation": null, "document": "(This article was first published on Enterprise Software Doesn't Have to Suck , and kindly contributed to R-bloggers)\nI'm training some of my colleagues on Big'ish data analysis this week. Here's how I'm running the class. Would love your ideas to make it better.\nCLASS OBJECTIVES (LEARNING OUTCOMES)\nAfter completion of the course, you will be able to:\nUnderstand concepts of data science, related processes, tools, techniques and path to building expertise\nUse Unix command line tools for file processing (awk, sort, paste, join, gunzip, gzip)\nUse Excel to do basic analysis\nWrite and understand R code (data structures, functions, packages, etc.)\nExplore a new dataset with ease (visualize it, summarize it, slice/dice it, answer questions related to dataset)\nPlot charts on a dataset\nCLASS PREREQUISITES\nGood knowledge of basic statistics (min, max, avg, sd, variance, factors, quantiles/deciles, etc.)\nFamiliarity with Unix OS\nCLASS TOPICS\nA) Intro to data science\nExplain data science and its importance. Data-driven business functions e.g. MROI, mix optimization, IPL teams / fantasy teams, predictions\nBig data\n- Definition: Data sets that no longer fit on a disk, requiring compute clusters and respective software and algorithms (map/reduce running on Hadoop).\n- Real big data problems: parallel computing, distributed computing, cloud, hadoop, casandra\n- Most analysis isn't Big Data. Business apps often deal with datasets that fit in Excel/Access\nProducts: Desktop tools (Excel (solver, what if), Access, SQL, spss, stata, R, sas, programming languages (ruby, python, java) -- stats libs in these languages, BI tools, etc.\nB) Steps in data science\nAcquire data: \"obtaining the data\"... databases, log files... exports, surveys, web scraping etc.\nVerify data\nCleanse and transform data: outliers, missing values, dedupe, merge\nExplore data:\u00a0The first step when dealing with a new data set needs to be exploratory in nature: what actually is in the data set? Summarize, Visually inspect entire data\n- What does the data look like? summaries, cross-tabulation\n- What does knowing one thing tell me about another? Relationships between data elements\n- What the heck is going on?\nVisualize data\nInteract with data (not covered here): BI tools, custom dashboards, other tools (ggobi etc.)\nArchive data (not covered here)\nC) Skills needed for data science\nStatistics: Concepts, approach, techniques\n"}, {"score": 1409.8422, "uuid": "b2b9f4b7-7f84-57a3-9623-bac808532116", "index": "cw12", "trec_id": "clueweb12-0900tw-50-10644", "target_hostname": "www.r-bloggers.com", "target_uri": "http://www.r-bloggers.com/data-science-learn-the-lessons-of-software/", "page_rank": 1.1892188e-09, "spam_rank": 69, "title": "<em>Data</em> Science \u2013 <em>learn</em> the lessons of software | (<em>R</em> news &amp; tutorials)", "snippet": "to <em>R</em>-bloggers) We&#x27;re starting to see a deluge of companies who businesses are all about making <em>data</em> <em>analysis</em>&#x2F;science&#x2F;insight &quot;easy <em>for</em> the non-expert&quot;.", "explanation": null, "document": "Data Science \u2013 learn the lessons of software\nMarch 13, 2012\nBy alexfarquhar's posterous\n(This article was first published on alexfarquhar's posterous , and kindly contributed to R-bloggers)\nWe're starting to see a deluge of companies who businesses are all about making data analysis/science/insight \"easy for the non-expert\". We've been here before, quite a few times sadly. When I started writing software 12 years ago, there was great excitement in the air - finally we could use tools to design software, then press a button that would create our whole beautiful design in code! Then we could just hire some barely-sentient code monkeys to fill in the 'easy bits' like method definitions and those pesky database access routines.\nIt was a disaster. The fundamental problem was that by the time you'd crafted your beloved design and polished it to a high shine, the world had moved on. What may have worked on day 1 of the project was now hopelessly inadequate. We should always remember the maxim \"no plan survives contact with the enemy\", the enemy here being the shifting reality of what your software needs to deliver.\nAnother major problem with this approach was the proliferation of so-called Software Architects, beings of such insight and experience that they didn't even need to code anymore! Since they didn't code, they couldn't experience the grinding pain of trying to jam their grandiose designs into a reality-shaped hole.\nFast-forward to today - data is big, Data Science is even bigger (as a buzzword anyway), and we're all short of the right people. The answer, however, is not to make tools that hide the complex, ever-shifting reality of the analytical process. It's to make people better at doing this stuff. And there'll be no magic off-the-shelf solution that can achieve this, any more than giving a terrible golfer great clubs will make them win The Masters.\nPermalink | Leave a comment\u00a0\u00a0\u00bb\nTo leave a comment for the author, please follow the link and comment on his blog: alexfarquhar's posterous .\n"}, {"score": 1415.7408, "uuid": "1ac42f7c-f95f-5034-bd46-ec8466831614", "index": "cw12", "trec_id": "clueweb12-0400wb-22-16393", "target_hostname": "stommel.tamu.edu", "target_uri": "http://stommel.tamu.edu/%7Ebaum/graphics-analysis.html", "page_rank": 1.2907447e-09, "spam_rank": 91, "title": "Graphics and <em>Data</em> <em>Analysis</em> - <em>Analysis</em>", "snippet": "Using numarray, it is possible to write many efficient numerical <em>data</em> processing applications directly in <em>Python</em> without using any C, C++ <em>or</em> Fortran code (as well as doing such <em>analysis</em> interactively within <em>Python</em> <em>or</em> PyRAF).", "explanation": null, "document": "A high-level interpreted language for numerical analysis that borrows ideas from languages like C, MATLAB and APL. It was developed because of a perceived need for a free, efficient, and versatile language capable of handling large problems.\nSignificant/unique Algae features include:\nspeed that is generally (and often significantly) faster than MATLAB (at least until recent major improvements in that product), RLaB and Octave ;\nthe capability of storing arrays in sparse form wherein only the nonzero elements and their locations are stored;\npersistent labels for each dimension of a matrix or vector (i.e. a vector element can have a label such as `mojo rate' to distinguish it from the other 5000 elements);\nscalars, vectors and arrays as distinct data types;\nand a statistical profiling capability that can show, by file and line number, where the code spends most of its time.\nAlgae is distributed as source code and also in binary form for Linux ELF platforms. Other packages that aren't required but are quite helpful and will be used by the package if available are Gnuplot , BLAS (although the generic version is supplied with the package), LAPACK (with the generic version of this also supplied with the package), and the GNU Readline library. Algae will also recognize the proprietary Boeing BCSLIB package. The documentation is supplied in both HTML and GNU Info formats.\nBasis\nBasis is the name of the program which results from loading the Basis System with no attached physics. It is a useful program for interactive calculations and graphics. Authors create other programs by specifying one or more packages of variables and modules to be loaded. A package is specified using a Fortran source and a variable description file in which the user specifies the common blocks to be used in the Fortran source and the functions or subroutines that are to be callable from the interactive language parser.\nBasis programs are steerable applications, that is, applications whose behavior can be greatly modified by their users. Basis also contains optional facilities to help authors do their jobs more easily. A library of Basis packages is available that can be easily added to a program. The progammable nature of the application simplifies testing and debugging.\nThe Basis Language includes variable and function declarations, graphics, several looping and conditional control structures, array syntax, operators for matrix multiplication, dot product, transpose, array or character concatenation, and a stream I/O facility. Data types include real, double, integer, complex, logical, character, chameleon, and structure. There are more than 100 built-in functions, including all the Fortran intrinsics.\nBasis' interaction with compiled routines is particularly powerful. When calling a compiled routine from the interactive language, Basis verifies the number of arguments and coerces the types of the actual arguments to match those expected by the function. A compiled function can also call a user-defined function passing arguments through common.\nThe Basis system includes many facilities designed to ease porting of Fortran codes among the supported platforms, including a Fortran preprocessor, MPPL, and a system for creating Makefiles for multi-platform, multi-directory development.\nBasis runs on UNIX and Linux operating systems. It requires Perl 5.0 or later. Basis can be built with NCAR graphics, PGS graphics or with no graphics. If you build basis with PACT , then you can use the interface to PACT's PDB portable self-describing data files. There's an overview article about this package by the chief author, Paul Dubois, in Computers in Physics, Vol. 8, 1994, pp. 70-73.\nCactus\nCactus is an open source problem solving environment designed for scientists and engineers. Its modular structure easily enables parallel computation across different architectures and collaborative code development between different groups. Cactus originated in the academic research community, where it was developed and used over many years by a large international collaboration of physicists and computational scientists.\nThe name Cactus comes from the design of a central core (or \"flesh\") which connects to application modules (or \"thorns\") through an extensible interface. Thorns can implement custom developed scientific or engineering applications, such as computational fluid dynamics. Other thorns from a standard computational toolkit provide a range of computational capabilities, such as parallel I/O, data distribution, or checkpointing.\nCactus runs on many architectures. Applications, developed on standard workstations or laptops, can be seamlessly run on clusters or supercomputers. Cactus provides easy access to many cutting edge software technologies being developed in the academic research community, including the Globus Metacomputing Toolkit, HDF5 parallel file I/O, the PETSc scientific library, adaptive mesh refinement, web interfaces, and advanced visualization tools.\nCCSE Application Suite\nThe CCSE Applications Suite consists of:\ntwo application codes for adaptively solving time-dependent partial differential equations (a solver for a Hyperbolic system of Conservation Laws (HyperCLaw) and an incompressible flow solver (IAMR)),\nC++ class libraries (BoxLib,AmrLib, and others),\na 2- and 3-dimensional visualization system (AmrVis),\na package for post-processing data generated by the above AMR applications (AmrDerive), and\na single grid, i.e. non-adaptive, application code for solving incompressible flows (VarDen in serial, pVarDen in parallel).\nThe most basic components for building AMR applications are contained in BoxLib and AmrLib; the most complete applications are the two solvers. The capability to solve elliptic equations on an adaptive hierarchy of grids with either cell-centered or node-centered data is contained in two of the multigrid libraries (MGLib and HGProj). Because the applications depend on some or all of the more fundamental software libraries, users must download the entire package,\nAMR uses block-structured refinement, so that the solution to an AMR calculation is composed of data in multiple non-intersecting rectangular grids at multiple levels with different resolution. The visualization and post-processing tools are specially constructed to efficiently handle this type of data set.\nAll components of the CCSE Applications Suite except VarDen are designed to execute in parallel using MPI. However, the user has the option of building executables without MPI to run on serial platforms.\nCWP/SU\nIn 1987, Jack K. Cohen and Shuki Ronen of the Center for Wave Phenomena (CWP) at the Colorado School of Mines (CSM) conceived a bold plan. This plan was to create a seismic processing environment for Unix-based systems, written in the C language, that would extend the Unix operating system to seismic processing and research tasks. Furthermore, they intended that the package be freely available as full source code to anyone who would want it.\nThe package is not necessarily restricted to seismic processing tasks, however. A broad suite of wave-related processing can be done with SU, making it a somewhat more general package than the word ``seismic'' implies. SU is intended as an extension of the Unix operating system, and therefore shares many characteristics of the Unix, including Unix flexibility and expandibility. The fundamental Unix philosophy is that all operating system commands are programs run under that operating system. The idea is that individual tasks be identified, and that small programs be written to do those tasks, and those tasks alone.\nThe core of the Seismic Unix program set performs a broad collection of tasks, which may be viewed as being common to a large collection of research and processing disciplines. Many, however, are purely seismic in nature. The task categories include:\ninput/output issues;\nsetting, viewing and editing trace header fields;\nviewing SU data;\nwindowing, sorting and editing data;\ngeneral operations;\nseismic operations on SU data.\n[http://www.cwp.mines.edu/cwpcodes/]\nEuler\nA numerical math program that can handle real and complex numbers, vectors and matrices, can produce 2D/3D graphics, and has a built-in programming language. The features include interactive evaluation of numerical expressions, matrix functions, statistical functions and random numbers, online help, and more. It's written in C for generic UNIX/X Windows platforms and the source code is available. An Euler tutorial is available online. It compiled and installed easily on my Linux box.\nFudgit\nA multi-purpose fitting program where columns of numbers can be manipulated in the form of vector arithmetic. It is also an expression language interpreter capable of understanding most of C grammar. It is also a front end for any plotting program supporting commands from stdin, e.g. Gnuplot. Some features include Fourier transforms, spline interpolation, smoothing, a built-in calculator, access to the C math library, and a collection of fitting routines.\n[http://packages.debian.org/stable/math/fudgit]\nGnans\nA program and language for the numerical study of deterministic and stochastic dynamical systems, which may evolve in continuous or discrete time. It loads the definition of a dynamical system in an equation oriented language, translates this into C++, compiles and links this, and solves the equations numerically with the speed of a compiled (rather than interpreted) program. It has a GUI that allows the program to be controlled and the parameters changed easily. It has been ported to Sun and SGI platforms for which binaries are available along with the source code. I'm not sure how easy this is to port elsewhere.\n[ftp://ftp.uu.net/pub/math/gnans/]\nGnatlab\nThe Gnatlab program is designed to be a tool for fast matrix manipulation and calculation using the Matrix Template Library. It is designed to be easily expandable, with a simple programming interface. It uses the Matrix Template Library to do the calculations and it interfaces with the MTL in such a way that updates and changes to the MTL can be quickly incorperated into the program. Gnatlab aims towards the full functionality of similar commercial products. This software was last updated in 1999.\n"}, {"score": 1414.9869, "uuid": "db1e191d-1d19-5a79-9707-1bc2fdb7a609", "index": "cw12", "trec_id": "clueweb12-0400wb-22-16407", "target_hostname": "stommel.tamu.edu", "target_uri": "http://stommel.tamu.edu/~baum/graphics-analysis.html", "page_rank": 1.3617181e-09, "spam_rank": 91, "title": "Graphics and <em>Data</em> <em>Analysis</em> - <em>Analysis</em>", "snippet": "Using numarray, it is possible to write many efficient numerical <em>data</em> processing applications directly in <em>Python</em> without using any C, C++ <em>or</em> Fortran code (as well as doing such <em>analysis</em> interactively within <em>Python</em> <em>or</em> PyRAF).", "explanation": null, "document": "A high-level interpreted language for numerical analysis that borrows ideas from languages like C, MATLAB and APL. It was developed because of a perceived need for a free, efficient, and versatile language capable of handling large problems.\nSignificant/unique Algae features include:\nspeed that is generally (and often significantly) faster than MATLAB (at least until recent major improvements in that product), RLaB and Octave ;\nthe capability of storing arrays in sparse form wherein only the nonzero elements and their locations are stored;\npersistent labels for each dimension of a matrix or vector (i.e. a vector element can have a label such as `mojo rate' to distinguish it from the other 5000 elements);\nscalars, vectors and arrays as distinct data types;\nand a statistical profiling capability that can show, by file and line number, where the code spends most of its time.\nAlgae is distributed as source code and also in binary form for Linux ELF platforms. Other packages that aren't required but are quite helpful and will be used by the package if available are Gnuplot , BLAS (although the generic version is supplied with the package), LAPACK (with the generic version of this also supplied with the package), and the GNU Readline library. Algae will also recognize the proprietary Boeing BCSLIB package. The documentation is supplied in both HTML and GNU Info formats.\nBasis\nBasis is the name of the program which results from loading the Basis System with no attached physics. It is a useful program for interactive calculations and graphics. Authors create other programs by specifying one or more packages of variables and modules to be loaded. A package is specified using a Fortran source and a variable description file in which the user specifies the common blocks to be used in the Fortran source and the functions or subroutines that are to be callable from the interactive language parser.\nBasis programs are steerable applications, that is, applications whose behavior can be greatly modified by their users. Basis also contains optional facilities to help authors do their jobs more easily. A library of Basis packages is available that can be easily added to a program. The progammable nature of the application simplifies testing and debugging.\nThe Basis Language includes variable and function declarations, graphics, several looping and conditional control structures, array syntax, operators for matrix multiplication, dot product, transpose, array or character concatenation, and a stream I/O facility. Data types include real, double, integer, complex, logical, character, chameleon, and structure. There are more than 100 built-in functions, including all the Fortran intrinsics.\nBasis' interaction with compiled routines is particularly powerful. When calling a compiled routine from the interactive language, Basis verifies the number of arguments and coerces the types of the actual arguments to match those expected by the function. A compiled function can also call a user-defined function passing arguments through common.\nThe Basis system includes many facilities designed to ease porting of Fortran codes among the supported platforms, including a Fortran preprocessor, MPPL, and a system for creating Makefiles for multi-platform, multi-directory development.\nBasis runs on UNIX and Linux operating systems. It requires Perl 5.0 or later. Basis can be built with NCAR graphics, PGS graphics or with no graphics. If you build basis with PACT , then you can use the interface to PACT's PDB portable self-describing data files. There's an overview article about this package by the chief author, Paul Dubois, in Computers in Physics, Vol. 8, 1994, pp. 70-73.\nCactus\nCactus is an open source problem solving environment designed for scientists and engineers. Its modular structure easily enables parallel computation across different architectures and collaborative code development between different groups. Cactus originated in the academic research community, where it was developed and used over many years by a large international collaboration of physicists and computational scientists.\nThe name Cactus comes from the design of a central core (or \"flesh\") which connects to application modules (or \"thorns\") through an extensible interface. Thorns can implement custom developed scientific or engineering applications, such as computational fluid dynamics. Other thorns from a standard computational toolkit provide a range of computational capabilities, such as parallel I/O, data distribution, or checkpointing.\nCactus runs on many architectures. Applications, developed on standard workstations or laptops, can be seamlessly run on clusters or supercomputers. Cactus provides easy access to many cutting edge software technologies being developed in the academic research community, including the Globus Metacomputing Toolkit, HDF5 parallel file I/O, the PETSc scientific library, adaptive mesh refinement, web interfaces, and advanced visualization tools.\nCCSE Application Suite\nThe CCSE Applications Suite consists of:\ntwo application codes for adaptively solving time-dependent partial differential equations (a solver for a Hyperbolic system of Conservation Laws (HyperCLaw) and an incompressible flow solver (IAMR)),\nC++ class libraries (BoxLib,AmrLib, and others),\na 2- and 3-dimensional visualization system (AmrVis),\na package for post-processing data generated by the above AMR applications (AmrDerive), and\na single grid, i.e. non-adaptive, application code for solving incompressible flows (VarDen in serial, pVarDen in parallel).\nThe most basic components for building AMR applications are contained in BoxLib and AmrLib; the most complete applications are the two solvers. The capability to solve elliptic equations on an adaptive hierarchy of grids with either cell-centered or node-centered data is contained in two of the multigrid libraries (MGLib and HGProj). Because the applications depend on some or all of the more fundamental software libraries, users must download the entire package,\nAMR uses block-structured refinement, so that the solution to an AMR calculation is composed of data in multiple non-intersecting rectangular grids at multiple levels with different resolution. The visualization and post-processing tools are specially constructed to efficiently handle this type of data set.\nAll components of the CCSE Applications Suite except VarDen are designed to execute in parallel using MPI. However, the user has the option of building executables without MPI to run on serial platforms.\nCWP/SU\nIn 1987, Jack K. Cohen and Shuki Ronen of the Center for Wave Phenomena (CWP) at the Colorado School of Mines (CSM) conceived a bold plan. This plan was to create a seismic processing environment for Unix-based systems, written in the C language, that would extend the Unix operating system to seismic processing and research tasks. Furthermore, they intended that the package be freely available as full source code to anyone who would want it.\nThe package is not necessarily restricted to seismic processing tasks, however. A broad suite of wave-related processing can be done with SU, making it a somewhat more general package than the word ``seismic'' implies. SU is intended as an extension of the Unix operating system, and therefore shares many characteristics of the Unix, including Unix flexibility and expandibility. The fundamental Unix philosophy is that all operating system commands are programs run under that operating system. The idea is that individual tasks be identified, and that small programs be written to do those tasks, and those tasks alone.\nThe core of the Seismic Unix program set performs a broad collection of tasks, which may be viewed as being common to a large collection of research and processing disciplines. Many, however, are purely seismic in nature. The task categories include:\ninput/output issues;\nsetting, viewing and editing trace header fields;\nviewing SU data;\nwindowing, sorting and editing data;\ngeneral operations;\nseismic operations on SU data.\n[http://www.cwp.mines.edu/cwpcodes/]\nEuler\nA numerical math program that can handle real and complex numbers, vectors and matrices, can produce 2D/3D graphics, and has a built-in programming language. The features include interactive evaluation of numerical expressions, matrix functions, statistical functions and random numbers, online help, and more. It's written in C for generic UNIX/X Windows platforms and the source code is available. An Euler tutorial is available online. It compiled and installed easily on my Linux box.\nFudgit\nA multi-purpose fitting program where columns of numbers can be manipulated in the form of vector arithmetic. It is also an expression language interpreter capable of understanding most of C grammar. It is also a front end for any plotting program supporting commands from stdin, e.g. Gnuplot. Some features include Fourier transforms, spline interpolation, smoothing, a built-in calculator, access to the C math library, and a collection of fitting routines.\n[http://packages.debian.org/stable/math/fudgit]\nGnans\nA program and language for the numerical study of deterministic and stochastic dynamical systems, which may evolve in continuous or discrete time. It loads the definition of a dynamical system in an equation oriented language, translates this into C++, compiles and links this, and solves the equations numerically with the speed of a compiled (rather than interpreted) program. It has a GUI that allows the program to be controlled and the parameters changed easily. It has been ported to Sun and SGI platforms for which binaries are available along with the source code. I'm not sure how easy this is to port elsewhere.\n[ftp://ftp.uu.net/pub/math/gnans/]\nGnatlab\nThe Gnatlab program is designed to be a tool for fast matrix manipulation and calculation using the Matrix Template Library. It is designed to be easily expandable, with a simple programming interface. It uses the Matrix Template Library to do the calculations and it interfaces with the MTL in such a way that updates and changes to the MTL can be quickly incorperated into the program. Gnatlab aims towards the full functionality of similar commercial products. This software was last updated in 1999.\n"}, {"score": 1352.1869, "uuid": "e851e180-d676-555b-8012-5b095853fc8e", "index": "cw12", "trec_id": "clueweb12-0000wb-41-29385", "target_hostname": "farmdev.com", "target_uri": "http://farmdev.com/thoughts/42/data-mining-in-python-and-beyond-/", "page_rank": 1.2011553e-09, "spam_rank": 82, "title": "Farmdev: <em>Data</em> mining in <em>Python</em> and beyond?", "snippet": "But, now-a-days I&#x27;m trying to <em>learn</em> &quot;<em>R</em>&quot;, which of course is excellent <em>for</em> any statistical related work. It has a steep learning curve however.", "explanation": null, "document": "Data mining in Python and beyond?\nposted in Data Mining , Python by kumar on Wednesday Mar 12th, 2008 at 4:49p.m.\nDear Lazy Web,\nI am a software engineer who hasn't done any college level math (gasp!). Recently, I've been having a lot of fun transforming data into more meaningful data. This, I believe, is more commonly known as data mining and I'd like to learn more about it.\nSpecifically, I've been looking at Internet search data where keywords are buried in some kind of template like {foo}::{bar}::{keywords}, placeholders replaced with actual content; there are many different, disparate template formats and no template ID to go by. So, I spent some quality time with my favorite programming language, Python , and identified as many patterns as I could in a sampling of 5 million candidate strings. After much tweakage, my pattern recognition became 94.54% accurate. This rate was more than good enough to the users of the data so I basked in sweet triumph and called it a day :)\nIn another project I used frequency analysis and deduction to turn eBay auction titles into more meaningful identifiers of items up for sale. This worked fairly well because my dataset was large and all the auctions were for a specific kind of item.\nMy question to you is where can I find out more about this kind of fun stuff? Can you recommend a good book on data mining? Any good blogs to read? Should I take some math classes before I get too deep into this? If so, which ones?\nRe: Data mining in Python and beyond?\nposted by xxx on Wednesday Mar 12th, 2008 at 8:23p.m.\nYou may want to try out \"Programming Collective Intelligence\" by Toby Segaran, all the code snippets are in Python (http://www.amazon.com/Programming-Collective-Intelligence-Building-Applications/dp/0596529325)\nRe: Data mining in Python and beyond?\nposted by Maikel on Wednesday Mar 12th, 2008 at 9:07p.m.\nTry Orange\nRe: Data mining in Python and beyond?\nposted by Kumar McMillan on Wednesday Mar 12th, 2008 at 9:14p.m.\nYeah, Orange looks great. I was having trouble getting it installed on OS X though (I wanted the Python lib version, not the GUI).\nRe: Data mining in Python and beyond?\nposted by SDC on Wednesday Mar 12th, 2008 at 9:37p.m.\nI second the 'Programming Collective Intelligence' recommendation. I have the book and it's a good starting point, and will give you some good ideas.\nRe: Data mining in Python and beyond?\nposted by Kumar McMillan on Wednesday Mar 12th, 2008 at 10:41p.m.\nok, I need to get this book ... thanks!\nRe: Data mining in Python and beyond?\nposted by phleabo on Wednesday Mar 12th, 2008 at 10:41p.m.\nHave a look at Weka:\nhttp://www.cs.waikato.ac.nz/ml/weka/\nThere's also a book published by the guys that wrote the software.\nRe: Data mining in Python and beyond?\nposted by Luke Opperman on Wednesday Mar 12th, 2008 at 10:49p.m.\nAnd a third for Programming Collective Intelligence. The collective intelligence part of it refers to very much the problems you're talking about, the ways we can derive useful information from large collections of data. And the programming part means every chapter uses python to build concrete examples of a technique, using everyday data available on the web.\nCategorization, prediction, finding exemplar or independent features, detecting similarity - all sides of a similar coin, Programming Collective Intelligence does a good job of covering a decent number of algorithms and approaches, and illustrating by it's choices of datasets how they're more or less useful with varying types/quantities of input and output data.\nOn the math side, it's something I struggle with as well. Another good book is \"Geometry and Meaning\" by Dominic Widdows, a survey of primarily graphs and vectors in the context of programming (but not so much code in the text) to derive useful information about words from large corpuses. Programming Collective Intelligence concepts stretched out and diving into a bit more math.\nI'm clearly not a good person to knowledgeably recommend math studies, but personally I wish I'd learned statistics and linear algebra rigorously.\nNo time like the present, that's the plan of the moment.\nRe: Data mining in Python and beyond?\nposted by Pradeep Gowda on Thursday Mar 13th, 2008 at 6:05a.m.\nI have used both Orange and WEKA.\nOrange can be tad difficult to get going. esp on Linux/mac.\nWEKA is written in Java. works well on all oses. Also, it has extensive set of algorithms/data manipulation/attribute selection etc.,\nWEKA has multiple User interfaces (CLI, Explorer and Workflow based).\nOnce you get familiar with the UI, you might even want to explore further using Jython. I've used Jython+Weka with with some success.\nBut, now-a-days I'm trying to learn \"R\", which of course is excellent for any statistical related work. It has a steep learning curve however.\nRe: Data mining in Python and beyond?\nposted by C.V.Krishnakumar on Friday Mar 14th, 2008 at 5:37a.m.\nFor Python, go for Orange library and \" Programming Collective Intelligence\"\nWeka is very good too... and Ive found the java implementations to be very useful...\nRe: Data mining in Python and beyond?\nposted by Ricardo Cabral on Sunday Mar 16th, 2008 at 6:04p.m.\nYaLE (http://rapid-i.com/) is also great for doing experiments, trying out algorithms and visualizing results. Most of the learners in WEKA are part of it.\nRe: Data mining in Python and beyond?\nposted by Luke Stanley on Monday Jul 28th, 2008 at 8:17p.m.\nYou could help out with ThoughtTrail. It's a cross-platform open, Semantic framework.\nWe can easily reuse web data. E.g: getting del.icio.us tags for a url.\ndef toptags(url):\npath=\"//div[@class='list']/div[@class='sidebar-inner']/ul[1]/li/ul/li/a\"\nreturn getWebXpath(\"http://del.icio.us/url/check?url=\"+url, path, None, {'Cookie':'_url_tagview=list'})\n#simple Google search\n"}, {"score": 1326.9148, "uuid": "3b60ffb0-a50b-5efe-ae6b-2a87e911ccd5", "index": "cw12", "trec_id": "clueweb12-1216wb-87-01667", "target_hostname": "www.decisionstats.com", "target_uri": "http://www.decisionstats.com/tag/data-analysis/", "page_rank": 1.3105359e-09, "spam_rank": 73, "title": "<em>Data</em> <em>Analysis</em> | DecisionStats", "snippet": "<em>I</em> believe that a course in <em>R</em> <em>should</em> be obligatory <em>for</em> all students that are likely to come close to any <em>data</em> <em>analysis</em> in their careers.", "explanation": null, "document": "Here is an interview with JJ Allaire, founder of RStudio. RStudio is the IDE that has overtaken other IDE within the R Community in terms of ease of usage. On the eve of their latest product launch, JJ talks to DecisionStats on RStudio and more. Ajay- \u00a0So what is new in the latest version of RStudio and how exactly is it useful for people? JJ- The initial release of RStudio as well as the two follow-up releases we did last year were focused\u00a0on the core elements of using R: editing and running code, getting help, and managing files, history,\u00a0workspaces, plots, and packages. In the meantime users have also been asking for some bigger features\u00a0that would improve the overall work-flow of doing analysis with R. In this release (v0.95) we focused on\u00a0three of these features: Projects. R developers tend to have several (and often dozens) of working contexts associated\u00a0with different clients, analyses, data sets, etc. RStudio projects make it easy to keep these\u00a0contexts well separated (with distinct R sessions, working directories, environments, command\u00a0histories, and active source documents), switch quickly between project contexts, and even work\u00a0with multiple projects at once (using multiple running versions of RStudio). Version Control. The benefits of using version control for collaboration are well known, but we\u00a0also believe that solo data analysis can achieve significant productivity gains by using version\u00a0control (this discussion on Stack Overflow talks about why). In this release we introduced\u00a0integrated support for the two most popular open-source version control systems: Git and\u00a0Subversion. This includes changelist management, file diffing, and browsing of project history, all\u00a0right from within RStudio. Code Navigation. When you look at how programmers work a surprisingly large amount of time\u00a0is spent simply navigating from one context to another. Modern programming environments for\u00a0general purpose languages like C++ and Java solve this problem using various forms of code\u00a0navigation, and in this release we\u2019ve brought these capabilities to R. The two main features here\u00a0are the ability to type the name of any file or function in your project and go immediately to it; and\u00a0the ability to navigate to the definition of any function under your cursor (including the definition of\u00a0functions within packages) using a keystroke (F2) or mouse gesture (Ctrl+Click). Ajay- What\u2019s the product road map for RStudio? When can we expect the IDE to turn into a full\u00a0fledged GUI? JJ-\u00a0Linus Torvalds has said that \u201cLinux is evolution, not intelligent design.\u201d RStudio tries to operate on a\u00a0similar principle\u2014the world of statistical computing is too deep, diverse, and ever-changing for any one\u00a0person or vendor to map out in advance what is most important. So, our internal process is to ship a new\u00a0release every few months, listen to what people are doing with the product (and hope to do with it), and\u00a0then start from scratch again making the improvements that are considered most important. Right now some of the things which seem to be top of mind for users are improved support for authoring\u00a0and reproducible research, various editor enhancements including code folding, and debugging tools. What you\u2019ll see is us do in a given release is to work on a combination of frequently requested features,\u00a0smaller improvements to usability and work-flow, bug fixes, and finally architectural changes required to\u00a0support current or future feature requirements. While we do try to base what we work on as closely as possible on direct user-feedback, we also adhere\u00a0to some core principles concerning the overall philosophy and direction of the product. So for example\u00a0the answer to the question about the IDE turning into a full-fledged GUI is: never. We believe that textual\u00a0representations of computations provide fundamental advantages in transparency, reproducibility,\u00a0collaboration, and re-usability. We believe that writing code is simply the right way to do complex\u00a0technical work, so we\u2019ll always look for ways to make coding better, faster, and easier rather than try to\u00a0eliminate coding altogether. Ajay -Describe your journey in science from a high school student to your present work in R. I\u00a0noticed you have been very successful in making software products that have been mostly\u00a0proprietary products or sold to companies. Why did you get into open source products with\u00a0RStudio? What are your plans for monetizing RStudio further down the line? JJ-\u00a0In high school and college my principal areas of study were Political Science and Economics. I also had\u00a0a very strong parallel interest in both computing and quantitative analysis. My first job out of college was\u00a0as a financial analyst at a government agency. The tools I used in that job were SAS and Excel. I had a\u00a0dim notion that there must be a better way to marry computation and data analysis than those tools, but\u00a0of course no concept of what this would look like. From there I went more in the direction of general purpose computing, starting a couple of companies\u00a0where I worked principally on programming languages and authoring tools for the Web. These companies\u00a0produced proprietary software, which at the time (between 1995 and 2005) was a workable model\u00a0because it allowed us to build the revenue required to fund development and to promote and distribute\u00a0the software to a wider audience. By 2005 it was however becoming clear that proprietary software would ultimately be overtaken by open\u00a0source software in nearly all domains. The cost of development had shrunken dramatically thanks to both\u00a0the availability of high-quality open source languages and tools as well as the scale of global collaboration\u00a0possible on open source projects. The cost of promoting and distributing software had also collapsed\u00a0thanks to efficiency of both distribution and information diffusion on the Web. When I heard about R and learned more about it, I become very excited and inspired by what the\u00a0project had accomplished. A group of extremely talented and dedicated users had created the software\u00a0they needed for their work and then shared the fruits of that work with everyone. R was a platform that\u00a0everyone could rally around because it worked so well, was extensible in all the right ways, and most\u00a0importantly was free (as in speech) so users could depend upon it as a long-term foundation for their\u00a0work. So I started RStudio with the aim of making useful contributions to the R community. We started with\u00a0building an IDE because it seemed like a first-rate development environment for R that was both powerful\u00a0and easy to use was an unmet need. Being aware that many other companies had built successful\u00a0businesses around open-source software, we were also convinced that we could make RStudio available\u00a0under a free and open-source license (the AGPLv3) while still creating a viable business. At this point\u00a0RStudio is exclusively focused on creating the best IDE for R that we can. As the core product gets where\u00a0it needs to be over the next couple of years we\u2019ll then also begin to sell other products and services\u00a0related to R and RStudio. About- http://rstudio.org/docs/about JJ Allaire JJ Allaire is a software engineer and entrepreneur who has created a wide variety of products including\u00a0ColdFusion,Windows Live Writer,\u00a0Lose It!, and\u00a0RStudio. From\u00a0http://en.wikipedia.org/wiki/Joseph_J._Allaire In 1995 Joseph J. (JJ) Allaire co-founded Allaire Corporation with his brother Jeremy Allaire, creating the web development tool ColdFusion.[1] In March 2001, Allaire was sold to Macromedia where ColdFusion was integrated into the Macromedia MX product line. Macromedia was subsequently acquired by Adobe Systems, which continues to develop and market ColdFusion. After the sale of his company, Allaire became frustrated at the difficulty of keeping track of research he was doing using Google. To address this problem, he co-founded Onfolio in 2004 with Adam Berrey, former Allaire co-founder and VP of Marketing at Macromedia. On March 8, 2006, Onfolio was acquired by Microsoft where many of the features of the original product are being incorporated into the Windows Live Toolbar.\u00a0On August 13, 2006, Microsoft released the public beta of a new desktop blogging client called Windows Live Writer that was created by Allaire's team at Microsoft. Starting in 2009, Allaire has been developing a web-based interface to the widely used R technical computing environment. A beta version of RStudio was publicly released on February 28, 2011. JJ Allaire received his B.A. from Macalester College (St. Paul, MN) in 1991. RStudio- RStudio is an integrated development environment (IDE) for R which works with the standard version of R available from CRAN. Like R, RStudio is available under a free software license. RStudio is designed to be as straightforward and intuitive as possible to provide a friendly environment for new and experienced R users alike. RStudio is also a company, and they plan to sell services (support, training, consulting, hosting) related to the open-source software they distribute.\nHere is an interview with JJ Allaire, founder of RStudio. RStudio is the IDE that has overtaken other IDE within the R Community in terms of ease of usage. On the eve of their latest product launch, JJ talks to DecisionStats on RStudio and more.\nAjay- \u00a0So what is new in the latest version of RStudio and how exactly is it useful for people?\nJJ- The initial release of RStudio as well as the two follow-up releases we did last year were focused\u00a0on the core elements of using R: editing and running code, getting help, and managing files, history,\u00a0workspaces, plots, and packages. In the meantime users have also been asking for some bigger features\u00a0that would improve the overall work-flow of doing analysis with R. In this release (v0.95) we focused on\u00a0three of these features:\nProjects. R developers tend to have several (and often dozens) of working contexts associated\u00a0with different clients, analyses, data sets, etc. RStudio projects make it easy to keep these\u00a0contexts well separated (with distinct R sessions, working directories, environments, command\u00a0histories, and active source documents), switch quickly between project contexts, and even work\u00a0with multiple projects at once (using multiple running versions of RStudio).\nVersion Control. The benefits of using version control for collaboration are well known, but we\u00a0also believe that solo data analysis can achieve significant productivity gains by using version\u00a0control (this discussion on Stack Overflow talks about why). In this release we introduced\u00a0integrated support for the two most popular open-source version control systems: Git and\u00a0Subversion. This includes changelist management, file diffing, and browsing of project history, all\u00a0right from within RStudio.\nCode Navigation. When you look at how programmers work a surprisingly large amount of time\u00a0is spent simply navigating from one context to another. Modern programming environments for\u00a0general purpose languages like C++ and Java solve this problem using various forms of code\u00a0navigation, and in this release we\u2019ve brought these capabilities to R. The two main features here\u00a0are the ability to type the name of any file or function in your project and go immediately to it; and\u00a0the ability to navigate to the definition of any function under your cursor (including the definition of\u00a0functions within packages) using a keystroke (F2) or mouse gesture (Ctrl+Click).\nAjay- What\u2019s the product road map for RStudio? When can we expect the IDE to turn into a full fledged GUI?\nJJ- Linus Torvalds has said that \u201cLinux is evolution, not intelligent design.\u201d RStudio tries to operate on a\u00a0similar principle\u2014the world of statistical computing is too deep, diverse, and ever-changing for any one\u00a0person or vendor to map out in advance what is most important. So, our internal process is to ship a new\u00a0release every few months, listen to what people are doing with the product (and hope to do with it), and\u00a0then start from scratch again making the improvements that are considered most important.\nRight now some of the things which seem to be top of mind for users are improved support for authoring\u00a0and reproducible research, various editor enhancements including code folding, and debugging tools.\nWhat you\u2019ll see is us do in a given release is to work on a combination of frequently requested features,\u00a0smaller improvements to usability and work-flow, bug fixes, and finally architectural changes required to\u00a0support current or future feature requirements.\nWhile we do try to base what we work on as closely as possible on direct user-feedback, we also adhere\u00a0to some core principles concerning the overall philosophy and direction of the product. So for example\u00a0the answer to the question about the IDE turning into a full-fledged GUI is: never. We believe that textual\u00a0representations of computations provide fundamental advantages in transparency, reproducibility,\u00a0collaboration, and re-usability. We believe that writing code is simply the right way to do complex\u00a0technical work, so we\u2019ll always look for ways to make coding better, faster, and easier rather than try to\u00a0eliminate coding altogether.\nAjay -Describe your journey in science from a high school student to your present work in R. I noticed you have been very successful in making software products that have been mostly proprietary products or sold to companies.\nWhy did you get into open source products with RStudio? What are your plans for monetizing RStudio further down the line?\nJJ- In high school and college my principal areas of study were Political Science and Economics. I also had\u00a0a very strong parallel interest in both computing and quantitative analysis. My first job out of college was\u00a0as a financial analyst at a government agency. The tools I used in that job were SAS and Excel. I had a\u00a0dim notion that there must be a better way to marry computation and data analysis than those tools, but\u00a0of course no concept of what this would look like.\nFrom there I went more in the direction of general purpose computing, starting a couple of companies\u00a0where I worked principally on programming languages and authoring tools for the Web. These companies\u00a0produced proprietary software, which at the time (between 1995 and 2005) was a workable model\u00a0because it allowed us to build the revenue required to fund development and to promote and distribute\u00a0the software to a wider audience.\nBy 2005 it was however becoming clear that proprietary software would ultimately be overtaken by open\u00a0source software in nearly all domains. The cost of development had shrunken dramatically thanks to both\u00a0the availability of high-quality open source languages and tools as well as the scale of global collaboration\u00a0possible on open source projects. The cost of promoting and distributing software had also collapsed\u00a0thanks to efficiency of both distribution and information diffusion on the Web.\nWhen I heard about R and learned more about it, I become very excited and inspired by what the\u00a0project had accomplished. A group of extremely talented and dedicated users had created the software\u00a0they needed for their work and then shared the fruits of that work with everyone. R was a platform that\u00a0everyone could rally around because it worked so well, was extensible in all the right ways, and most\u00a0importantly was free (as in speech) so users could depend upon it as a long-term foundation for their\u00a0work.\nSo I started RStudio with the aim of making useful contributions to the R community. We started with\u00a0building an IDE because it seemed like a first-rate development environment for R that was both powerful\u00a0and easy to use was an unmet need. Being aware that many other companies had built successful\u00a0businesses around open-source software, we were also convinced that we could make RStudio available\u00a0under a free and open-source license (the AGPLv3) while still creating a viable business. At this point\u00a0RStudio is exclusively focused on creating the best IDE for R that we can. As the core product gets where\u00a0it needs to be over the next couple of years we\u2019ll then also begin to sell other products and services\u00a0related to R and RStudio.\nAbout-\nhttp://rstudio.org/docs/about\nJJ Allaire\nJJ Allaire is a software engineer and entrepreneur who has created a wide variety of products including ColdFusion , Windows Live Writer , Lose It! , and RStudio .\nFrom http://en.wikipedia.org/wiki/Joseph_J._Allaire\nIn 1995 Joseph J. (JJ) Allaire co-founded Allaire Corporation with his brother Jeremy Allaire, creating the web development tool ColdFusion.[1] In March 2001, Allaire was sold to Macromedia where ColdFusion was integrated into the Macromedia MX product line. Macromedia was subsequently acquired by Adobe Systems, which continues to develop and market ColdFusion.\nAfter the sale of his company, Allaire became frustrated at the difficulty of keeping track of research he was doing using Google. To address this problem, he co-founded Onfolio in 2004 with Adam Berrey, former Allaire co-founder and VP of Marketing at Macromedia.\nOn March 8, 2006, Onfolio was acquired by Microsoft where many of the features of the original product are being incorporated into the Windows Live Toolbar.\u00a0On August 13, 2006, Microsoft released the public beta of a new desktop blogging client called Windows Live Writer that was created by Allaire\u2019s team at Microsoft.\nStarting in 2009, Allaire has been developing a web-based interface to the widely used R technical computing environment. A beta version of RStudio was publicly released on February 28, 2011.\nJJ Allaire received his B.A. from Macalester College (St. Paul, MN) in 1991.\nRStudio-\nRStudio is an integrated development environment (IDE) for R which works with the standard version of R available from CRAN. Like R, RStudio is available under a free software license. RStudio is designed to be as straightforward and intuitive as possible to provide a friendly environment for new and experienced R users alike. RStudio is also a company, and they plan to sell services (support, training, consulting, hosting) related to the open-source software they distribute.\n"}, {"score": 1225.161, "uuid": "f4c74c96-b64f-5f67-bce1-99673526a06e", "index": "cw12", "trec_id": "clueweb12-1200tw-22-19345", "target_hostname": "www.computerworld.com", "target_uri": "http://www.computerworld.com/s/article/9225318/8_cool_tools_for_data_analysis_visualization_and_presentation?source=CTWNLE_nlt_dailyam_2012-03-27", "page_rank": 1.1700305e-09, "spam_rank": 96, "title": "8 cool tools <em>for</em> <em>data</em> <em>analysis</em>, visualization and presentation - Computerworld", "snippet": "At this year&#x27;s conference, <em>I</em> learned about other free (<em>or</em> at least inexpensive) tools <em>for</em> <em>data</em> <em>analysis</em> and presentation. Want to see all the tools from last year and 2012? <em>For</em> quick reference, check out our chart listing all 30 free <em>data</em> visualization and <em>analysis</em> tools.", "explanation": null, "document": "Review\n8 cool tools for data analysis, visualization and presentation\nLast year, we looked at 22 data analysis tools. This year, we add 8 more to the mix.\nBy Sharon Machlis\nMarch 27, 2012 06:00 AM ET\nComputerworld - Reporters wrangle all sorts of data, from analyzing property tax valuations to mapping fatal accidents -- and, here at Computerworld, for stories about IT salaries and H-1B visas . In fact, tools used by data-crunching journalists are generally useful for a wide range of other, non-journalistic tasks -- and that includes software that's been specifically designed for newsroom use. And, given the generally thrifty culture of your average newsroom, these tools often have the added appeal of little or no cost.\nI came back from last year's National Institute for Computer-Assisted Reporting (NICAR) conference with 22 free tools for data visualization and analysis -- most of which are still popular and worth a look. At this year's conference, I learned about other free (or at least inexpensive) tools for data analysis and presentation.\nWant to see all the tools from last year and 2012?\nFor quick reference, check out our chart listing all 30 free data visualization and analysis tools.\nLike that previous group of 22 tools, these range from easy enough for a beginner (i.e., anyone who can do rudimentary spreadsheet data entry) to expert (requiring hands-on coding). Here are eight of the best:\nCSVKit\nWhat it does: This utility suite from GitHub has a host of Unix-like command-line tools for importing, analyzing and reformatting comma-separated data files.\nWhat's cool: Sure, you could pull your file into Excel to examine it, but CSVKit makes it quick and easy to preview, slice and summarize.\nFor example, you can see all your column headers in a list -- which is handy for super-wide, many-column files -- and then just pull data from a few of those columns. In addition to inputting CSV files, it can import several fixed-width file formats -- for example, there are libraries available for the specific fixed-width formats used by the Census Bureau and Federal Elections Commission.\nTwo simple commands will generate a data structure that can, in turn, be used by several SQL database formats ( Mr. Data Converter handles only MySQL). The SQL code will create a table, inferring the proper data type for each field as well as the insert commands for adding data to the table.\nCSVKit offers Unix-like command-line tools for importing, analyzing and reformatting comma-separated data files.\nThe Unix-like interface will be familiar to anyone who has worked on a *nix system, and makes it easy to save multiple frequently used commands in a batch file.\nDrawbacks: Working on a command line means learning new text commands (not to mention the likely risk of typing errors), which might not be worthwhile unless you work with CSV files fairly often. Also, be advised that this tool suite is written in Python, so Windows users will need that installed on their system as well.\nSkill level: Expert\nRuns on: Any Windows, Mac or Linux system with Python installed.\nLearn more: The documentation includes an easy-to-follow tutorial . There's also a brief introductory slide presentation that was given at the NICAR conference last month.\nRelated tools: Google Refine is a desktop application that can do some rudimentary file analysis as well as its core task of data cleaning; and The R Project for Statistical Computing can do more powerful statistical analysis on CSV and other files.\nWhat it does: This popular jQuery plug-in (which was designed and created by Allan Jardine) creates sortable, searchable HTML tables from a variety of data sources -- say, an existing, static HTML table, a JavaScript array, JSON or server-side SQL.\nApple device sales\n"}, {"score": 1224.7905, "uuid": "2be6e9a5-166e-57cf-9c92-90c1bbb086a9", "index": "cw12", "trec_id": "clueweb12-1412wb-45-10812", "target_hostname": "thomas-cokelaer.info", "target_uri": "http://thomas-cokelaer.info/blog/page/8/", "page_rank": 1.1752802e-09, "spam_rank": 76, "title": "Thomas Cokelaer&#x27;s blog | Notes on <em>Data</em> <em>Analysis</em>, Computer Science, <em>Python</em>", "snippet": "<em>I</em> consider it as a draft, but it <em>should</em> already be of interest <em>for</em> some of you. Posted in <em>Python</em> | Tagged <em>python</em>, rest, sphinx | Leave a comment <em>Python</em> is a script language that has evolved into a \u201cdo whatever you want\u201d.", "explanation": null, "document": "Posted on March 12, 2011 by admin\nSometimes, you want to convert a document into a PDF and there is no such options in you preferred software. For instance, in OpenOffice you have the save option as PDF, so there is no problem here. However, in some other software (I won\u2019t give any name; I let you guess) it is not always possible. More generally, let us say that the output is buggy. I had the case where links were not rendered probably. In such situations, you want to use an external tool. I tried to convert in postscript and then to use tools such as ps2pdf. Again, there were rendering issues. So, I looked around on the web and found conv2pdf . It provides a very simple interface to convert documents into PDF. I have used it a few times and it was just brilliant, in particular for the links.\nOf course dedicated software such as OpenOffice becomes better and better. Links were working well the last time I used them. But just in case, it is good to know that this service exists and works.\n"}], [{"score": 1224.274, "uuid": "26cdc4c0-afcb-5b1c-b5dc-8b640a0c347f", "index": "cw12", "trec_id": "clueweb12-1200tw-21-01991", "target_hostname": "www.computerworld.com", "target_uri": "http://www.computerworld.com/s/article/9225318/8_cool_tools_for_data_analysis_visualization_and_presentation", "page_rank": 1.2202203e-09, "spam_rank": 96, "title": "8 cool tools <em>for</em> <em>data</em> <em>analysis</em>, visualization and presentation - Computerworld", "snippet": "At this year&#x27;s conference, <em>I</em> learned about other free (<em>or</em> at least inexpensive) tools <em>for</em> <em>data</em> <em>analysis</em> and presentation. Want to see all the tools from last year and 2012? <em>For</em> quick reference, check out our chart listing all 30 free <em>data</em> visualization and <em>analysis</em> tools.", "explanation": null, "document": "Review\n8 cool tools for data analysis, visualization and presentation\nLast year, we looked at 22 data analysis tools. This year, we add 8 more to the mix.\nBy Sharon Machlis\nMarch 27, 2012 06:00 AM ET\nComputerworld - Reporters wrangle all sorts of data, from analyzing property tax valuations to mapping fatal accidents -- and, here at Computerworld, for stories about IT salaries and H-1B visas . In fact, tools used by data-crunching journalists are generally useful for a wide range of other, non-journalistic tasks -- and that includes software that's been specifically designed for newsroom use. And, given the generally thrifty culture of your average newsroom, these tools often have the added appeal of little or no cost.\nI came back from last year's National Institute for Computer-Assisted Reporting (NICAR) conference with 22 free tools for data visualization and analysis -- most of which are still popular and worth a look. At this year's conference, I learned about other free (or at least inexpensive) tools for data analysis and presentation.\nWant to see all the tools from last year and 2012?\nFor quick reference, check out our chart listing all 30 free data visualization and analysis tools.\nLike that previous group of 22 tools, these range from easy enough for a beginner (i.e., anyone who can do rudimentary spreadsheet data entry) to expert (requiring hands-on coding). Here are eight of the best:\nCSVKit\nWhat it does: This utility suite from GitHub has a host of Unix-like command-line tools for importing, analyzing and reformatting comma-separated data files.\nWhat's cool: Sure, you could pull your file into Excel to examine it, but CSVKit makes it quick and easy to preview, slice and summarize.\nFor example, you can see all your column headers in a list -- which is handy for super-wide, many-column files -- and then just pull data from a few of those columns. In addition to inputting CSV files, it can import several fixed-width file formats -- for example, there are libraries available for the specific fixed-width formats used by the Census Bureau and Federal Elections Commission.\nTwo simple commands will generate a data structure that can, in turn, be used by several SQL database formats ( Mr. Data Converter handles only MySQL). The SQL code will create a table, inferring the proper data type for each field as well as the insert commands for adding data to the table.\nCSVKit offers Unix-like command-line tools for importing, analyzing and reformatting comma-separated data files.\nThe Unix-like interface will be familiar to anyone who has worked on a *nix system, and makes it easy to save multiple frequently used commands in a batch file.\nDrawbacks: Working on a command line means learning new text commands (not to mention the likely risk of typing errors), which might not be worthwhile unless you work with CSV files fairly often. Also, be advised that this tool suite is written in Python, so Windows users will need that installed on their system as well.\nSkill level: Expert\nRuns on: Any Windows, Mac or Linux system with Python installed.\nLearn more: The documentation includes an easy-to-follow tutorial . There's also a brief introductory slide presentation that was given at the NICAR conference last month.\nRelated tools: Google Refine is a desktop application that can do some rudimentary file analysis as well as its core task of data cleaning; and The R Project for Statistical Computing can do more powerful statistical analysis on CSV and other files.\nWhat it does: This popular jQuery plug-in (which was designed and created by Allan Jardine) creates sortable, searchable HTML tables from a variety of data sources -- say, an existing, static HTML table, a JavaScript array, JSON or server-side SQL.\nApple device sales\n"}, {"score": 1215.9475, "uuid": "c64cd3bf-5323-5df4-86f9-c4e8829612dd", "index": "cw12", "trec_id": "clueweb12-1601wb-44-10502", "target_hostname": "www.cv.nrao.edu", "target_uri": "http://www.cv.nrao.edu/~rfisher/DIYanalysis/diy_analysis.html", "page_rank": 1.193098e-09, "spam_rank": 80, "title": "Do-It-Yourself <em>Data</em> <em>Analysis</em>", "snippet": "Since many observers do not see the unprocessed <em>data</em> <em>or</em> realize that it is accessible without great effort, this lecture will concentrate on this lower <em>data</em> level. Because <em>I</em> know the GBT best <em>I</em> will draw most of my examples from this telescope.", "explanation": null, "document": "Abstract\nThe common single-dish spectral line and continuum data reduction packages let observers get on with the business of doing science with their chosen instruments. However, there are plenty of reasons why some of us need or simply prefer to use our own software tools to analyze data drawn from intermediate stages in the data stream or from the original data files. This lecture the outlines data paths from the telescope, discusses low-level data formats and how to read them, and offers a few example programs in three different programming languages as starter examples. We also mention a number of necessary data operations that need to be performed on the raw data, such as Van Vleck corrections, power transfer function linearization, and synchronization of data with antenna position. A number of Web resources are given.\nIntroduction\nThe scientific capabilities of the Arecibo and Green Bank instruments must be accessible to all astronomers without requiring detailed knowledge of the electronics, software, and observing techniques. Hence, the Observatories supply standard setups and data reduction software and procedures for some of the most commonly used observing modes. However, there are many cases where observers must or prefer to write their own data analysis software. This lecture is intended to present enough information to allow you to access the fundamental telescope data and process it with routines of your own design. Example programs in IDL, glish/aips++, and Python programming languages are provided as templates.\nThe scientific software world has changed in rather fundamental ways over the last ten or fifteen years. A major operating system, Linux, is in the public domain and very widely supported, particularly in academia. Several powerful, general purpose commercial packages, notably IDL and Matlab, are available for data analysis, display, and creation of publication quality illustrations. These are often available as university services. Perhaps most important to astronomers is the expanding array of high level languages, tool kits, and support available on the Internet free of charge. One can now chose from astronomy-specific packages (CLASS, AIPS, AIPS++, IRAF, etc.), scripting languages (Python, glish, Perl, Tcl, Java, etc.), and major tool kits (PGPLOT, Tk, cfitsio, scientific libraries, etc.) without too much concern about reinventing wheels in the process of creating your own data analysis environment. The old debates about the best programming languages and methods (FORTRAN, C, C++, object orient programming, etc.) have largely been replaced by choices of the best combination of scripting language, display package, high-level mathematical tool kit, and high-level astronomical package(s) that best suit your scientific needs. You should expect to continually change the mix of your analysis environment as new and better selections become available.\nNot every astronomer nor even a majority of astronomers should be doing a lot of programming outside of the astronomical data reduction packages, but the science you that do and the observing techniques you that use should not be confined by the scope of these packages. Observational astronomy depends on innovation and improvements to existing techniques. If you do choose to assemble your own data analysis environment from a range of high and low level pieces, keep in mind that you are not trying to rewrite a complete analysis package. Writing software routines for specific scientific objectives is a much more limited objective. If any of your inventions are of use to others, they can be cleaned up, documented, and \"published\" later on the Web.\nOne can write new analysis procedures at many levels from scripts within astronomical packages to low-level routines beginning with raw telescope data. Since many observers do not see the unprocessed data or realize that it is accessible without great effort, this lecture will concentrate on this lower data level. Because I know the GBT best I will draw most of my examples from this telescope.\nData Paths from the Telescope\nFigure 1 shows an outline of the flow of data from the GBT IF electronics outputs to the data analysis software. There is a mix of data processing electronics provided by NRAO and university groups, and the data produced by the NRAO devices may be analyzed entirely by NRAO software or by more specialized software supplied by observers.\nFigure 1. Data flow and analysis software at the GBT. Rounded boxes are components supplied by the user community.\nData formats written by the signal processing \"back-ends\" depend on how widely accessible the data are intended to be. It is generally easier and sometimes significantly faster to write data in binary format close to the native format produced by the electronics. However, these data require detailed external documentation and specialized software routines to read them. If the data are used by only one or two groups, a binary format is usually simple and efficient. We'll say a little more about binary formats later.\nData that are intended to be read by more than one or two programmers are generally best written in a standard, self-documenting format. In the astronomical world this is Flexible Image Transport System (FITS) or, more specifically, FITS Binary Table format. The original FITS was pure text data so you could read the whole file with a text reader or any program that reads ASCII (American Standard Code for Information Interchange) text. However, large numeric data sets required too much extra storage space and a lot of processing power to convert between ASCII and native machine binary formats so a hybrid ASCII/binary format standard was developed. The self-documentation is in ASCII headers, and the data arrays are recorded as binary integers or floating point numbers. This is probably the most common FITS format that you will see at various observatories.\nIdeally, every bit of information about your astronomical observation (antenna position, bandwidth settings, cal state, etc.) is stored with the signal processor output so that you have all of the information necessary to interpret your data without need for a manually-written observing log. In an automated telescope system there are too many parameters to keep track of manually. At the GBT every subsystem (receiver, antenna, spectrometer, etc.) writes its own FITS file for every observing scan. These files are stored in the GBT data directory (check with GBT staff for this directory path) under the subdirectory of your project ID. For example, the antenna information for the project AGBT02A_069 of a scan started at 23:58:48 UTC on April 26, 2003 is in the file\n[data directory]/AGBT02A_069_01/Antenna/2003_04_26_23:58:48.fits\nPlease remember that observing data are proprietary to the observer under whose program they were recorded for 18 months so access of data other than your own without permission is a breach of professional ethics. The data under project AGBT02A_069 is for a public domain survey so you are welcome to read any of its data files.\nCollation of the information from all of the GBT subsystems is one of the functions of the \"Filler\" shown in Figure 1 . When you use your own software to read these data files you will need to draw from several files or each scan. All files for one scan have the same date and time string in their file name. Work is underway to provide a collation utility that will create one FITS file per scan, but everything you learn here about reading FITS files will still apply.\nFITS File Contents\nThere are a number of FITS file readers, both stand-alone and as functions in different programming languages. We'll discuss these below, but, first, just try reading one of the GBT files in a text window with something like the UNIX 'more' command. You should get something that looks like\n$ more\n/AGBT02A_069_01/Antenna/2003_04_26_23:58:48.fits\nSIMPLE  =                    T / file does conform to FITS stan\ndard             BITPIX  =                    8 / number of bit\ns per data pixel                  NAXIS   =                    \n0 / number of data axes                            EXTEND  =   \n                 T / FITS dataset may contain extensions       \n   -------- etc. --------\nThe ASCII text does not contain end-of-line characters so the format looks a bit scrambled unless you set your text window to be exactly 80 characters wide. A bit further down in the file you'll find lines that begin to make sense, such as\nDATE-OBS= '2003-04-26T23:58:48' / Manager parameter startTime  ...\nTIMESYS = 'UTC     '           / time scale specification for D...\nTELESCOP= 'NRAO_GBT'           / Green Bank Telescope (Robert C...\nOBJECT  = 'LVM160. '           / Manager parameter source      ...\nPROJID  = 'AGBT02A_069_01'     / Manager parameter projectId   ...\nIt's not too hard to write a little C program to print the ASCII header lines while skipping the binary tables. An example can be found in the file list_fits.c The FITS rules used to write this program are: 1. all header lines ('cards') are 80 characters long; 2. headers and binary table lengths are integer multiples of 2880 bytes, which are padded at the ends with blank lines or undefined binary data to make up the length; 3. the last non-blank ASCII header line contains only the END keyword; and 4. the dimensions of the useful binary table is specified by the values of the keywords, NAXIS and NAXISn. FITS binary tables are organized in Header Data Units (HDU's) with an ASCII header and a binary table in each HDU. The binary table may be omitted in an HDU, as is always the case in the first HDU.\nWhether you are interested in C programming or not, you should try the utility 'fv' for looking at FITS files. Its operation is fairly easy to figure out just by clicking buttons. Type fv at the UNIX prompt followed by the path name of your FITS file. By comparing the header keywords with the table organization in 'fv' you can get a pretty good idea how the table format is specified by the TTYPEn, TFORMn, and TUNITn keywords. If you're interested in the gory details of the FITS standard, have a look at Hanish et al (2001) . and/or the Web sites [1] and [2] .\nOf course, there is no need to write your own C code routines to access FITS file. A standard access library, CFITSIO, is freely available at http://heasarc.gsfc.nasa.gov/docs/software/fitsio/fitsio.html This library has been attached to many of the higher level scripting languages and data analysis packages from which it is easier to call the keyword and data access routines.\nDoing Something with the Data\nProbably the quickest way to learn how to make use of telescope data is to begin with example scripts for accessing and displaying the contents of different types of data files. There is no \"best\" high level language for this so we'll offer parallel examples in three languages available in Green Bank: glish, Python, and IDL. These have the tools necessary for most tasks that you might want to apply to your data, but they are not the only software environments that could be used to work directly with telescope data files.\nBelow is a list of text files containing instructions and example scripts for working with telescope data FITS files in the languages glish, Python, and IDL.\nstartup: Instructions for starting the language interpreter in Green Bank. [Python] [glish] [IDL]\nlist_scans: Lists the scan number and source name from all FITS files under the specified project name to either stdout or a specified file. [Python] [glish] [IDL]\nscan_info: Prints some keywords and their values from several GBT device FITS fits for a specified scan. [Python] [glish] [IDL]\nget_fits: Retrieves or prints the full contents of a fits file. [Python] [glish] [IDL]\nplot_tsys: Plots system temperature spectra from a spectral processor scan that contains cal on and cal off spectra. [Python] [glish] [IDL]\nplot_actsys: Plots system temperature spectra from an autocorrelation spectrometer scan that contains cal on and cal off spectra. [Python] [glish] [IDL]\nBack-End and Telescope Data Notes\nThe basic data from a radio telescope is simply signal intensity as a function of time, frequency, antenna position and possibly one or more hardware states, such as the position of a beam switch. The method for synchronizing the intensity information with the independent variables varies somewhat from one telescope to the next. On the GBT the antenna position, local oscillator frequency, and signal intensities are sampled asynchronously with time tags so an averaging or interpolation operation is required to determine the antenna position and observing frequency at the mean time of each intensity sample.\nHardware states are much more directly synchronized with intensity accumulation in real time with common switching signals that send all data from a given combination of switch states to the same accumulator and data from different combinations to different accumulators. For example, if a spectrometer setup is to frequency switch at a two-second period and turn the calibration signal on and off at a synchronous one-second period, you will get four spectra from each IF channel corresponding to the state combinations (cal~off, frequency~1), (cal~on, frequency~1), (cal~off, frequency~2), and (cal~on, frequency~2). The state combinations are found in the STATE variables of the STATE header data unit of the back-end and LO FITS files. These correspond to the phase or state index in the DATA arrays.\nThe intensity measurements can be simple total power integrations from the full IF passband from the digital continuum receiver (DCR), accumulated spectra from the spectral processor, accumulated autocorrelation functions (ACF) from the spectrometer, or a high-speed stream of IF signal voltage samples from a VLBI, pulsar, or other direct-sampling device. The DCR and spectral processor outputs may be used more or less without further processing to derive system temperatures from the cal-on/cal-off measurements and spectral line or continuum intensities from on-source/off-source values. A few comments about processing direct IF signal voltage samples will be given in Section~\\ref{dirsamp}.\nThe output of the autocorrelation spectrometer requires three signal processing operations to turn the recorded ACF's into spectra: quantization corrections (often called van Vleck corrections), linearization, and Fourier Transform. At Arecibo these three operations are performed in real time so that the autocorrelation spectrometer output is corrected spectra that you can use directly. The GBT design choice was to record the raw autocorrelation data to retain maximum flexibility.\nAt first glance the quantization and linearization corrections appear to be shrouded in the esotericism of integrals of complicated error functions , but the basic ideas are pretty simple. To save on hardware cost and maximize signal processing speed an autocorrelator generally samples the IF voltage with only 2, 3, or 9 levels of quantization. Most of the spectral and intensity information of noise-dominated signals is retained, but distortions are introduced that must be corrected to recover the true autocorrelation and spectrum values.\nThese corrections are derived by starting with the assumption that the sampler input voltage has a normal probability distribution of amplitudes. From this we can compute the response of a given sampler to a range of known signal autocorrelation values over a range of sampler input levels. We can then fit an analytic function to a chosen accuracy to this two-dimensional array of true-to-measured correlation value ratios as a function of sampler input level and measured correlation values. This function is then the quantization correction function. The same is done to get the ratio of true sampler input power to measured output power as a function of measured output power to get the linearization function. These computations are straightforward but a bit tedious to program so the coefficients can be computed once and packaged as a module to be accessed by a higher level data analysis language, as has been done for the GBT spectrometer. One point to make here is that the quantization and linearization corrections are not necessarily a compute-and-forget black box. The assumptions of normal voltage distribution and symmetric, evenly spaced sampler levels are different from reality at some level. This might be an area for further improvement by someone working closely with the data.\nTransforming the ACF to a power spectrum is pretty straightforward with any of a number of FFT tools. The main trick is to create a real, symmetric data array of twice the length of the measured ACF by reflecting the ACF around its zero-delay data value. An example of the application of the corrections and Fourier transform are shown in the plot_actsys code module listed in Section on Example Code .\nBinary Data Formats\nIf you are reading FITS data files, you don't have to worry much about the details of how the binary data is stored on disk. This is generally taken care of by the FITS reader code. However, if your data is a stream of IF voltage samples or in a format defined by a low-level language data structure, the details are important.\nBinary data comes in three flavors: character, integer, and floating point. A character is always 8 bits (1 byte) long, where the translation from numeric value to alphabetic letter is standardized in the ASCII code table .\nIntegers can be 8, 16, 32, and sometimes 64 bits long (1, 2, 4, or 8 bytes), and they can be signed or unsigned. The distinction between signed and unsigned determines the interpreted data range. A 16-bit, unsigned integer can store values from 0 to 65535 while a signed, 16-bit number runs from -32768 to +32767. The binary bits in the unsigned value of 65535 are the same as in the signed value of -1.\nFloating point binary numbers are almost always 32 bits (single precision) or 64 bits (double precision) long (4 or 8 bytes). Very very rarely you may see 16- or 128-bit floating point numbers, but these are not generally supported by most computing languages. The allocation of bits between exponent and mantissa in the data word is specified by the IEEE Standard 754 .\nTo add variety to life, there is no standard on the order in which computers store the bytes of a multi-byte integer or floating point number on disk. Intel processors store the least significant byte first (called \"little endian\"), and Sun computers and the Motorola processor store the most significant byte first (\"big endian\"). (See Gulliver's Travels by Jonathan Swift.) If you read data written by a Sun computer with an Intel machine or vice versa, you will see data garbage unless your reading program is aware of the problem. You can reverse the byte swap by treating all bytes as characters and swapping the order in each 2-, 4-, or 8-byte word.\nProcessing Direct IF Voltage Samples\nThe most basic form of data collection is to directly sample band-limited IF output voltages with an analog to digital (A/D) converter. If the data sampling rate is twice the IF bandwidth, all information is retained. You are then free to process and reprocess your data any way you like on a general-purpose computer - Fourier transform spectroscopy, autocorrelation functions, pulse period and dispersion searches, interference excision, etc. Since there is no data rate reduction in this type of data collection it uses a prodigious amount of disk space and requires considerable post-processing resources, but the rapid growth of processing power and disk sizes has moved this option into astronomically interesting bandwidths and integration times. Some pulsar searches and planetary radar are now done this way. One terabyte of disk space, costing about $5000, can store about 14 hours of directly-sampled data from a 10 MHz bandwidth using 8-bit samples.\nIf you would like to try your hand at extracting hydrogen line profiles from some directly sampled GBT data you are welcome to have a go at the files on the local disk of workstation 'rfimit' at /export/raid/scratch. The data are 16-bit signed integers written by an Intel machine sampled at 5 megasamples per second. Two data channels are interleaved so that alternating data words are from the same channel. The A/D converter has only 12 bits in its output word so the least significant 4 bits of each 16-bit word should be discarded. Files named with an off.dat suffix are the 'off' scans to be paired with the 'on' scan file with the same name prefix.\nWeb Resources\nKeep in mind that the examples given in this lecture are only a few of many ways of accomplishing given tasks, none of which is necessarily the best way. Web searches with applicable keywords, like \"Python crosscorrelation\" or \"IDL Doppler\" can turn up a lot of useful tools. Here is a collection of Web sites worth a look.\nglish: http://aips2.nrao.edu/docs/reference/Glish/Glish.html http://aips2.nrao.edu/docs/user/Refman/Refman.html http://aips2.nrao.edu/docs/notes/195/195.html Python: http://www.python.org/ http://pfdubois.com/numpy/ http://www.python.org/topics/scicomp/ http://astrosun.tn.cornell.edu/staff/loredo/statpy/ http://scom.hud.ac.uk/scomdjd/public/python/pgplot/ http://www.gb.nrao.edu/~esessoms/igkya/ IDL: http://www.rsinc.com/idl/ http://idlastro.gsfc.nasa.gov/homepage.html http://cow.physics.wisc.edu/~craigm/idl/ephem.html http://idlastro.gsfc.nasa.gov/fitsio.html\nReference\nHanish , R. J., Farris, A., Greisen, E. W., Pence, W. D., Schlesinger, B. M., Teuben, P. J., Thompson, R. W., Warnock, A. III, 2001, Astron. & Astrophys., vol. 376, pp359-380.\n"}, {"score": 1210.3436, "uuid": "27dee7ff-cf51-5473-894e-4393d1b74ac5", "index": "cw12", "trec_id": "clueweb12-1509wb-05-28610", "target_hostname": "scienceoss.com", "target_uri": "http://scienceoss.com/categories/python/", "page_rank": 2.2878412e-09, "spam_rank": 76, "title": "<em>Python</em> \u00ab scienceoss.com", "snippet": "If you have RPy installed and the vegan package installed, you <em>should</em> be able to just run this <em>Python</em> script. Often-run analyses that you need <em>R</em> <em>for</em> can be wrapped in a class <em>or</em> module to encapsulate your <em>data</em> <em>analysis</em> needs, so you don\u2019t need to clutter your code with it.", "explanation": null, "document": "Published on September 20, 2009 in excel , other software and Python . 8 Comments\nIn a previous post (which turned out to be pretty popular) I showed you how to read Excel files with Python. Now for the reverse: writing Excel files.\nFirst, you\u2019ll need to install the xlwt package by John Machin.\nThe basics\nIn order to write data to an Excel spreadsheet, first you have to initialize a Workbook object and then add a Worksheet object to that Workbook. It goes something like this:\nimport xlwt\nwbk = xlwt.Workbook()\nsheet = wbk.add_sheet('sheet 1')\nNow that the sheet is created, it\u2019s very easy to write data to it.\n# indexing is zero based, row then column\nsheet.write(0,1,'test text')\nWhen you\u2019re done, save the workbook (you don\u2019t have to close it like you do with a file object)\nwbk.save('test.xls')\nOverwriting cells\nWorksheet objects, by default, give you a warning when you try to overwrite:\nsheet.write(0,0,'test')\nsheet.write(0,0,'oops') \n\n# returns error:\n# Exception: Attempt to overwrite cell: sheetname=u'sheet 1' rowx=0 colx=0\nTo change this behavior, use the cell_overwrite_ok=True kwarg when creating the worksheet, like so:\nsheet2 = wbk.add_sheet('sheet 2', cell_overwrite_ok=True)\nsheet2.write(0,0,'some text')\nsheet2.write(0,0,'this should overwrite')\nNow you can overwrite sheet 2 (but not sheet 1).\nMore goodies\n# Initialize a style\nstyle = xlwt.XFStyle()\n\n# Create a font to use with the style\nfont = xlwt.Font()\nfont.name = 'Times New Roman'\nfont.bold = True\n\n# Set the style's font to this new one you set up\nstyle.font = font\n\n# Use the style when writing\nsheet.write(0, 0, 'some bold Times text', style)\nxlwt allows you to format your spreadsheets on a cell-by-cell basis or by entire rows; it also allows you to add hyperlinks or even formulas. Rather than recap it all here, I encourage you to grab a copy of the source code, in which you can find the examples directory. Some highlights from the examples directory in the source code:\ndates.py, which shows how to use the different date formats\nhyperlinks.py, which shows how to create hyperlinks (hint: you need to use a formula)\nmerged.py, which shows how to merge cells\nrow_styles.py, which shows how to apply styles to entire rows.\nNon-trivial example\nHere\u2019s an example of some data where the dates not formatted well for easy import into Excel:\n20 Sep, 263, 1148,   0,   1,   0,   0,   1,   12.1,   13.9, 1+1, 19.9\n 20 Sep, 263, 1118,   0,   1,   0, 360,   0,   14.1,   15.3, 1+1, 19.9\n 20 Sep, 263, 1048,   0,   1,   0,   0,   0,   14.2,   15.1, 1+1, 19.9\n 20 Sep, 263, 1018,   0,   1,   0, 360,   0,   14.2,   15.9, 1+1, 19.9\n 20 Sep, 263, 0948,   0,   1,   0,   0,   0,   14.4,   15.3, 1+1, 19.9\nThe first column has the day and month separated by a space. The second column is year-day, which we\u2019ll ignore. The third column has the time. The data we\u2019re interested in is in the 9th column (temperature). The goal is to have a simple Excel file where the first column is date, and the second column is temperature.\nHere\u2019s a [heavily commented] script to do just that. It assumes that you have the data saved as weather.data.example.\n'''\nScript to convert awkwardly-formatted weather data\ninto an Excel spreadsheet using Python and xlwt.\n'''\n\nfrom datetime import datetime\nimport xlwt\n\n# Create workbook and worksheet\nwbk = xlwt.Workbook()\nsheet = wbk.add_sheet('temperatures')\n\n# Set up a date format style to use in the\n# spreadsheet\nexcel_date_fmt = 'M/D/YY h:mm'\nstyle = xlwt.XFStyle()\nstyle.num_format_str = excel_date_fmt\n\n# Weather data has no year, so assume it's the current year.\nyear = datetime.now().year\n\n# Convert year to a string because we'll be\n# building a date string below\nyear = str(year)\n\n# The format of the date string we'll be building\npython_str_date_fmt = '%d %b-%H%M-%Y'\n\nrow = 0  # row counter\nf = open('weather.data.example')\nfor line in f:\n    # separate fields by commas\n    L = line.rstrip().split(',')\n\n    # skip this line if all fields not present\n    if len(L) < 12:\n        continue\n\n    # Fields have leading spaces, so strip 'em\n    date = L[0].strip()\n    time = L[2].strip()\n\n    # Datatypes matter. If we kept this as a string\n    # in Python, it would be a string in the Excel sheet.\n    temperature = float(L[8])\n\n    # Construct a date string based on the string\n    # date format  we specified above\n    date_string = date + '-' + time + '-' + year\n\n    # Use the newly constructed string to create a\n    # datetime object\n    date_object = datetime.strptime(date_string,\n                                    python_str_date_fmt)\n\n    # Write the data, using the style defined above.\n    sheet.write(row,0,date_object, style)\n    sheet.write(row,1,temperature)\n\n    row += 1\n\nwbk.save('reformatted.data.xls')\nStill curious? Other questions? Check out the python-excel google group ! Also check out xlutils for more functionality, which I plan to play around with next.\nPublished on March 13, 2009 in latex and Python . 3 Comments Tags: archiving , latex , Python .\nThis is probably one of those scripts that will evolve over time, but I\u2019m posting it now in case someone can get some use out of it. My problem was this:\nI had many, many figures in my working directory, but I didn\u2019t use all of them in the Latex document. I was trying to figure out a way to send the source files \u2014 *.tex, *.cls, *.bst, *.bib, etc, plus only the images files that were actually in the document \u2014 to someone else so they could edit on their own and compile on their own. I didn\u2019t want to set up a version control (SVN, etc), I just wanted a tar file.\nAfter some poking around I couldn\u2019t find anything already made that would do this (Kile has an Archive menu item, but this doesn\u2019t include figures). It was easy enough to get a Python script going.\nThis script parses an input file, looks at the various documents and figures that are included, and archives them in a tar.gz file which can then be sent to someone. Note that as it stands, it only looks two levels deep for \\include tags. If I use this more I\u2019ll have to make it recursive (it\u2019s not obvious to me how to do that, I haven\u2019t used recursion much before).\nConsider this script a rough draft. It worked perfectly for me, but your mileage may vary.\n\"\"\"\nThis script gathers the necessary images and files (from\nan arbitrarily large number of unneeded figures) and\nputs it all in a tarball for distribution.\n\nUsage: latexpackager.py main.tex dissertation.tar.gz\n\"\"\"\n\nimport sys\nimport re\nimport os\nimport tarfile\n\ndef find_references(f):\n    '''Returns a list of Latex files that f refers to,\n    by parsing \\include, \\bibliography, \\bibliographystyle,\n    \\input, etc.\n\n    If nothing was found, returns an empty list.'''\n\n    s = open(f).read()\n\n    # Find the .tex files.\n    texs = []\n    for i in re.finditer(r\"\"\"[^%]\\\\include\\{(.*)\\}\"\"\", s):\n        texs.append(i.groups()[0]+'.tex')\n\n    # Find the .bib files.\n    bibs = []\n    for i in re.finditer(r\"\"\"[^%]\\\\bibliography\\{(.*)\\}\"\"\", s):\n        bibs.append(i.groups()[0]+'.bib')\n\n    # Find the styles.\n    styles = []\n    for i in re.finditer(r\"\"\"[^%]\\\\bibliographystyle\\{(.*)\\}\"\"\", s):\n        styles.append(i.groups()[0]+'.bst')\n\n    # Find the document class description file\n    docclass = []\n    for i in re.finditer(r\"\"\"[^%]\\\\documentclass\\{(.*)\\}\"\"\", s):\n        docclass.append(i.groups()[0]+'.cls')\n\n    # Look for any inputs.\n    inputs = []\n    for i in re.finditer(r\"\"\"[^%]\\\\input\\{(.*)\\}\"\"\", s):\n        texs.append(i.groups()[0]+'.tex')\n\n    # Here is everything that was referenced in f:\n    return texs + bibs + styles + docclass + inputs\n\ndef find_figures(f):\n    '''Returns a list of figures found in the file.  Only\n    looks in .tex files.  If not a .tex file or no figures found,\n    returns an empty list.'''\n\n    # Short circuit if not a .tex file.\n    if f[-4:] != '.tex':\n        return []\n\n    includegraphics = r\"\"\"[^%].*\\\\includegraphics\\[.*\\]\\{([^\\}]*)\\}\"\"\"\n    figures = []\n    s = open(f).read()\n    matches = re.finditer(includegraphics, s)\n\n    for match in matches:\n        basename = match.groups()[0]\n        if basename[-4] == '.':\n            # that is, it has an extension already.\n            # This is for things like .png images.\n            figures.append(basename)\n        else:\n            figures.append(basename + '.pdf')\n            figures.append(basename + '.eps')\n\n    return figures\n\nmain = sys.argv[1]\ntarfn = sys.argv[2]\n\nprojectdir, main = os.path.split(main)\nif projectdir == '':\n    projectdir = os.getcwd()\n\nkeepers = find_references(main)\n\n# Don't forget to add the main .tex file.\nkeepers.append(main)\n\n# For each of those that main.tex referenced, look for more.\n# These are files referenced two levels deep.\n\nfor f in keepers:\n    if f[-4:] != '.tex':\n        continue\n    keepers.extend(find_references(f))\n\n# Now look for graphics.\n\nfigures = []\nfor f in keepers:\n    figures.extend(find_figures(f))\n\n#paths = [os.path.join(projectdir, i) for i in keepers + figures]\npaths = keepers + figures\n\ntarball = tarfile.open(tarfn, 'w:gz')\nfor path in paths:\n    print path\n    tarball.add(path)\ntarball.close()\nPublished on July 25, 2008 in Python and R . 3 Comments Tags: linear regression , Python , R , statistics .\nR is a free, open source statistics package written by statisticians, for statisticians. Python on the other hand lacks a comprehensive statistics package. RPy allows you to combine the power of Python with the power of R for an unbeatable combination in data analysis.\nNote that in order to use R from Python, you need to know a little of both . . . so the learning curve can be steep. You also need to have a feel for what would be easy in R and what would be easy in Python.\nThere are some detailed examples below if you want to skip right to \u2018em.\nI use Python for most tasks, but when I need high-powered stats, I embed R code in my Python scripts to perform the analysis.\nDisclaimer: I figured all of this stuff out by trial and error. The RPy documentation, while complete, was difficult for me to make sense of when I was learning. If there\u2019s a better way to do things, please let me know! For the details that I don\u2019t cover here, check the online documentation\nWhy use R?\nYou\u2019ll need R if you want to do any sort of sophisticated (or even not-so-sophisiticated) statistical analysis. There are no solid statistics libraries that I\u2019ve come across for Python . . . but maybe that\u2019s because R is the best possible statistics library there could be.\nBe warned however that accessing R from Python can get tricky at times. I\u2019ve tried to outline some of what I\u2019ve learned here to make it easier for others.\nWhy use RPy instead of writing files out to R, then using R scripts to deal with it? I did this for a little while and found that it was too much work to maintain two separate code bases . . . one for Python, then one for R. If I changed anything in the output of a Python script, I\u2019d have to fire up R and open my R scripts to modify and debug them. I\u2019ve found that using RPy lets me put all my code in one spot, resulting in fewer bugs and less maintenance.\nR and Python are separate . . .\nI found that the easiest way to think about this is to think about doing things \u201cinside R\u201d or \u201cinside Python\u201d. Things that are to be done inside R are typically wrapped in a string (a Python string). For example, this creates a variable inside R called x with a value of 5.\nfrom rpy import *\nr('x=5')\nAssuming this was typed into a fresh Python session, Python has no idea about the existence of the variable x! It works in reverse, too: R has no idea about what\u2019s in the Python namespace. So you can do this in Python:\nx = 'I'm a Python string'\nand the variable x inside R is still the same:\nr('print(x)')  # still 5\n. . . but they can talk to each other\nRPy does some automatic conversions:\nx_from_R = r('x')  # 5\nWhat happened here is that RPy looked at what x was inside R, saw that it was an integer, and returned that integer to Python, which assigned it to the Python variable x_from_R. So that\u2019s how you get data from R to Python: by sending a string (the variable name you want to retrieve in R) to the r object.\nAt first you might think this is how you send data from Python to R:\nr('x_from_python') = x\n#SyntaxError: can't assign to function call\nNope. Turns out you have to use the r.assign() function to do that:\nr.assign('x_from_python', x)\nr('print(x_from_python)')  # 'I'm a Python string'\nSo that\u2019s how you get data from Python to R: by using the r.assign() function, first giving the name of the variable you want to be assigned in R followed by the Python object to be sent to R.\nOther data types\nOK, so you can get integers back from R. And as you can imagine, strings work the same way. But what about more complex data types? This list of conversions tells you which R objects will be converted into which Python objects. It\u2019s pretty intuitive, a string becomes a string, a list becomes a list, etc.\nBut then there are things like data frames in R, which have row names and column names.\nIt\u2019s not on that list linked above, but an R data frame is converted to a Python dictionary. For example, the Motor Trend car data set, which comes standard in R, is a data frame.\nfrom rpy import *\nr('print(head(mtcars))') # print just the first 6 lines.  Note the variable names.\n\n# Returns:\n#                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\n# Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n# Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n# Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n# Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n# Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n# Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nNow send the whole thing to Python and check the keys of the dictionary that is created:\nmt = r('mtcars')\nmt.keys()\nNote that the keys are the same as the variable names in the dataframe.\nJust like you get a Python dictionary from a dataframe, you can send a dictionary to R:\nr.assign('df', dict(a=1, b=2, c=3))\nr('print(df)')\nr('names(df)')\nMay have to convert it into a dataframe once inside R though:\nr('df = data.frame(df)')\nR functions\nSo far, with the exception of r.assign(), we\u2019ve just been sending strings to the r object. But the r object also has methods. Unfortunately, you can\u2019t see them all using IPython\u2019s introspection. Personally I find that I don\u2019t use this functionality that much, (I use r.assign() to get the data into R and then operate on it in there) but here it is for completeness.\nThere is a trick here. Remember, before we were sending a string to the r object and it was executing the code inside R:\nr('x=5')\nBut when you use a method of the r object, you pass it raw Python objects. For example, you can plot a Python list in R using the plot() method of the r object:\nx = [1,2,3]\nr.plot(x)\nThere are some slight name changes though. R tends to use a \u201c.\u201d as a spacer in function names, like \u201c_\u201d tends to be used in Python. The \u201c.\u201d however is special in Python, so in method names of the r object, \u201c.\u201d is converted to \u201c_\u201d. For example, R\u2019s t.test() function becomes r.t_test().\nThese methods of the r object are what Python sees, so that\u2019s why their names have to be changed. On the other hand, you call R function with its true name when you send the r object a string, like we were doing before. So both of these refer to the same underlying t-test function in R:\nr.t_test\nr('t.test')\nThis next one is tricky. First, since print is a Python function, it needs to have a slightly different name when you want to use the version in R. So an underscore is added to the end. Second, what\u2019s in the parentheses is a Python string. So all that will get printed is the string, \u2018x\u2019 . . . not 5, or \u201cI\u2019m a Python string\u201d or anything else.\nr.print_('x') # 'x'\nIn practice though, if I want to print something I\u2019ll either use Python\u2019s print or if I want to print something from R, I\u2019ll do this:\nr('print(x)')  # prints 5\nPlotting examples\nHere\u2019s are a couple of examples of creating a plot. In each case a plot is created of the list 1,2,3. These are trivial examples, but they illustrate different ways of getting data to and from R.\nOption 1: Do everything in R\nYou can execute arbitrary R commands by sending them as a string to the r object. Here, everything is done in R: a list is created and plotted. In this example, the variable x is never seen by Python.\nfrom rpy import *\nr(\"\"\"\ny = c(1,2,3)\nplot(y)\n\"\"\")\nNote that you can send many R commands in a multi-line string.\nOption 2: Use a method of the r object\nHere, we start with a Python list, and then send it as the argument to the r.plot() method.\nfrom  rpy import *\ny = [1,2,3]\nr.plot(y)\nOption 3: Get a list from R and plot it with matplotlib in Python\nThis trivial because you don\u2019t gain anything from making a list in R instead of Python, but it shows that you can send data both ways.\nfrom r import *\nimport pylab as p\ny = r('c(1,2,3)')\np.plot(y)\np.show()\nOption 4: Use r.assign() to get data to R, then call it inside R\nI tend to use this method a lot with large data sets. The idea is to pass the data into R once, then you can use it from inside R. The trick is to use the r.assign() method.\nfrom rpy import *\ny = [1,2,3]\nr.assign('Y', y)\nr('plot(Y)')\nGetting help on R functions\nUse the r.help() function. For example, to view the help on anova:\nr.help(anova)\nThis displays the help on screen; it doesn\u2019t return a string.\nNon-trivial examples\nPlotting and printing things are not what you\u2019d want to use R and RPy for. Instead, you\u2019d want to use them for things that you can\u2019t do in available packages for Python.\nHere are some examples where R can really fill in the gaps in Python\u2019s statistical functionality. Anything you can do in R, you can do from Python. Given the wide variety of packages available for R, this is some stupendous power at your fingertips. Now to learn how to wield it!\nLinear models in R\nSay I have a Python script already up and running, and it returns some data . . . and I want to know if the slope of two variables is significant. I haven\u2019t found any statistics libraries for Python, but in R this kind of functionality comes standard, in the function lm().\nViewing the help for lm(), you can see that it takes a model specification, like \u201cy~x\u201d which means \u201cy on x\u201d. Now, the components of this model specification, y and x, can either refer to variables in the R workspace (which is separate from Python, remember) or they can be variables in a dataframe which is supplied in an optional argument to lm().\nSo first we need to figure out how to send the data to R; performing the linear regression should be trivial, then we need to get the data back out.\nFirst, let\u2019s set up some test data in Python:\nimport numpy as npy\nx = npy.arange(10)\ny = npy.arange(10) + npy.random.standard_normal(x.shape)</pre>\n\nNow send it to R:\n<pre>r.assign('x',x)\nr.assign('y',y)\n(exercise for the reader: instead of assigning x and y individually, how would you get them into R as a dataframe?)\nIn R, run the linear model and save it as a variable in R. Here, I\u2019m simultaneously saving it as a Python dictionary (sneaky!)\nLM = r('linear_model = lm(y~x)')\nOK, here\u2019s where it take a little exploring. The dictionary you get back may take some navigating. Looking at it for a little bit, you might notice the \u2018coefficients\u2019 key of the dictionary LM, which in turn has two more keys: \u2018(Intercept)\u2019 and \u2018x\u2019.\n{'assign': [0, 1],\n 'call': <Robj object at 0xb7d3e790>,\n 'coefficients': {'(Intercept)': 0.28490682478866736,\n                  'x': 0.86209804871669171},\n 'df.residual': 8,\n 'effects': array([-13.16882479,   7.83039439,   1.22245056,   0.18398967,\n         0.51108108,   0.8141431 ,  -0.45120018,  -1.1985602 ,\n         1.54636612,   0.51341949]),\n 'fitted.values': array([ 0.28490682,  1.14700487,  2.00910292,  2.87120097,  3.73329902,\n        4.59539707,  5.45749512,  6.31959317,  7.18169121,  8.04378926]),\n 'model': {'x': array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.]),\n           'y': array([-0.64212347,  1.39389811,  3.06676323,  2.84957073,  3.99793052,\n        5.12226093,  4.67818603,  4.7520944 ,  8.3182891 ,  8.10661086])},\n 'qr': {'pivot': [1, 2],\n        'qr': array([[ -3.16227766, -14.23024947],\n       [  0.31622777,   9.08295106],\n       [  0.31622777,   0.15621147],\n       [  0.31622777,   0.0461151 ],\n       [  0.31622777,  -0.06398128],\n       [  0.31622777,  -0.17407766],\n       [  0.31622777,  -0.28417403],\n       [  0.31622777,  -0.39427041],\n       [  0.31622777,  -0.50436679],\n       [  0.31622777,  -0.61446316]]),\n        'qraux': [1.316227766016838, 1.2663078500948464],\n        'rank': 2,\n        'tol': 9.9999999999999995e-08},\n 'rank': 2,\n 'residuals': array([-0.92703029,  0.24689324,  1.05766031, -0.02163025,  0.2646315 ,\n        0.52686386, -0.77930909, -1.56749877,  1.13659789,  0.0628216 ]),\n 'terms': <Robj object at 0xb7d3e780>,\n 'xlevels': {}}\nSo if all we were after were the slope and intercept, then\nslope = LM['coefficients']['x']\nintercept = LM['coefficients']['(Intercept)']\nBut what about a P-value for the slope? It\u2019s nowhere to be seen in that dictionary. Turns out, you need the summary() function in R, and it takes as its input a linear model (among other possible inputs, but here we\u2019re just using a linear model). So save it in R (just in case) and simultaneously save it in Python:\nsummary = r('LM_summary = summary(linear_model)')\nHmm.\n{'adj.r.squared': 0.88847497651170382,\n 'aliased': {'(Intercept)': False, 'x': False},\n 'call': <Robj object at 0xb7d3e770>,\n 'coefficients': array([[  2.84906825e-01,   5.39776217e-01,   5.27823968e-01,\n          6.11943659e-01],\n       [  8.62098049e-01,   1.01109349e-01,   8.52639301e+00,\n          2.75251311e-05]]),\n 'cov.unscaled': array([[ 0.34545455, -0.05454545],\n       [-0.05454545,  0.01212121]]),\n 'df': [2, 8, 2],\n 'fstatistic': {'dendf': 8.0, 'numdf': 1.0, 'value': 72.699377758431851},\n 'r.squared': 0.90086664578818121,\n 'residuals': array([-0.92703029,  0.24689324,  1.05766031, -0.02163025,  0.2646315 ,\n        0.52686386, -0.77930909, -1.56749877,  1.13659789,  0.0628216 ]),\n 'sigma': 0.9183712712215929,\n 'terms': <Robj object at 0xb7d3e7c0>}\nThere\u2019s the r-squared and adjusted r-squared,\nR_squared = summary['adj.r.squared']\nbut no P value. What gives? Turns out Python can\u2019t convert everything perfectly, and a little more exploration is in order. Try printing the summary from R:\nr('print(LM_summary)')\nWell, that makes more sense, and you can see the P value for the slope is 2.75E-5. But how to extract it from Python?\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max\n-1.5675 -0.5899  0.1549  0.4613  1.1366 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.2849     0.5398   0.528    0.612\nx             0.8621     0.1011   8.526 2.75e-05 ***\n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n\nResidual standard error: 0.9184 on 8 degrees of freedom\nMultiple R-squared: 0.9009,\tAdjusted R-squared: 0.8885\nF-statistic:  72.7 on 1 and 8 DF,  p-value: 2.753e-05\nThe trick is to match output from the summary printout in R with the dictionary returned to Python. Here, it looks like the key \u2018coefficients\u2019 in the summary dictionary in Python gives the numbers in the 2nd row, 3rd column:\nP = summary['coefficients'][1,2]\nWhew, and there you have it. See, it takes some digging around to get what you need, but now since I\u2019ve done the work for you, you can now do linear regressions from Python. All together it looks like this (can be wrapped in a function or class for your own reuse):\nr.assign('x', x)\nr.assign('y', y)\nLM = r('linear_model = lm(y~x)')\nsummary = r('summary_LM = summary(linear_model)')\nslope = LM['coefficients']['x']\nintercept = LM['coefficients']['(Intercept)']\nP = summary['coefficients'][1,2]\nRedundancy analysis\nOK, say you have this data set to perform redundancy analysis (RDA) on. First, you need the package vegan installed, which is fantastic for multivariate stats. It\u2019s probably best to fire up R proper (from a command line, or the GUI if you have it in Windows or OSX) and run\ninstall.packages(\"vegan\", dep=T)\nHere\u2019s a heavily commented script, rpy-demo.py , that will:\nload and format the data included in the script\nsend the data to R\nperform an RDA in R\nplot the ordination\nsave the ordination as a PNG\nprint the variance explained by constrained and unconstrained axes as well as each RDA axis.\nIf you have RPy installed and the vegan package installed, you should be able to just run this Python script.\nOften-run analyses that you need R for can be wrapped in a class or module to encapsulate your data analysis needs, so you don\u2019t need to clutter your code with it. Once things are set up that way, it would be as easy as\nfrom myRstuff import lm, rda\nresults = lm(x,y)\nordination = rda(data)\nFor much, much more see the online documentation for RPy, but hopefully I gave you enough to at least get started.\nPublished on July 20, 2008 in matplotlib , plotting and Python . 2 Comments\nHere\u2019s how to create a polar bar plot in matplotlib.\nThe trick is just to specify that you want polar coordinates when you create the axis. Then create a bar plot as normal.\nfrom matplotlib.pyplot import figure, show\nfrom math import pi\n\nfig = figure()\nax = fig.add_subplot(111, polar=True)\nx = [30,60,90,120,150,180]\nx = [i*pi/180 for i in x]  # convert to radians\n\nax.bar(x,[1,2,3,4,5,6], width=0.4)\nshow()\nNote that in the above example the \u201cright\u201d or \u201cclockwise-most\u201d edge is lined up with each specified x value. You can change this by subtracting width / 2 to each of the x values to center the bars on the x-values, like this:\nfrom matplotlib.pyplot import figure, show\nfrom math import pi\n\nwidth = 0.4  # width of the bars (in radians)\n\nfig = figure()\nax = fig.add_subplot(111, polar=True)\nx = [30,60,90,120,150,180]\n\n# Convert to radians and subtract half the width\n# of a bar to center it.\nx = [i*pi/180 - width/2 for i in x]\nax.bar(x,[1,2,3,4,5,6], width=width)\nshow()\nGet funky . . .\nThe following is slightly modifed from the matplotlib examples:\nimport numpy as npy\nimport matplotlib.cm as cm\nfrom matplotlib.pyplot import figure, show, rc\n\n# force square figure and square axes (looks better for polar, IMHO)\nfig = figure(figsize=(8,8))\nax = fig.add_axes([0.1, 0.1, 0.8, 0.8], polar=True)\n\nN = 20\ntheta = npy.arange(0.0, 2*npy.pi, 2*npy.pi/N)  # random angles\nradii = 10*npy.random.rand(N)  # random bar heights\nwidth = npy.pi/4*npy.random.rand(N) # random widths\n\n# Create the bar plot\nbars = ax.bar(theta, radii, width=width, bottom=0.0)\n\n# Step through bars (a list of Rectangle objects) and\n# change color based on its height and set its alpha transparency\n# to 0.5\n\nfor r,bar in zip(radii, bars):\n    bar.set_facecolor( cm.jet(r/10.))\n    bar.set_alpha(0.5)\n\nshow()\nAnd the result:\nPublished on May 3, 2008 in matplotlib , plotting and Python . 2 Comments Tags: axes , intertactive plotting , subplot .\nIt\u2019s very easy to make subplots that share an x-axis, so that when you pan and zoom on one axis, the others automatically pan and zoom as well. The key to this functionality is the sharex keyword argument, which is used when creating an axis. Here\u2019s some example code and a video of the resulting interaction. Continue reading \u2018Interactive subplots: make all x-axes move together\u2019\n"}, {"score": 1202.7946, "uuid": "c3877d25-8bf0-5ab3-b8f0-d3a43ccd1ef1", "index": "cw12", "trec_id": "clueweb12-0610wb-73-17843", "target_hostname": "www.unidata.ucar.edu", "target_uri": "http://www.unidata.ucar.edu/software/netcdf/docs/software.html", "page_rank": 1.1741018e-09, "spam_rank": 99, "title": "Software <em>for</em> Manipulating <em>or</em> Displaying NetCDF <em>Data</em>", "snippet": "David Pierce has contributed the ncdf4 package <em>for</em> reading netCDF <em>data</em> into <em>R</em> and <em>for</em> creating new netCDF dimensions, variables, and files, <em>or</em> manipulating existing netCDF files from <em>R</em>.", "explanation": null, "document": "Visit Unidata\nSoftware for Manipulating or Displaying NetCDF Data\nThis document provides references to software packages that may be used for manipulating or displaying netCDF data. We include information about both freely-available and licensed (commercial) software that can be used with netCDF data. We rely on developers to help keep this list up-to-date. If you know of corrections or additions, please send them to us . Where practical, we would like to include WWW links to information about these packages in the HTML version of this document.\nOther useful guides to utilities that can handle netCDF data include ARM's list of ARM-tested netCDF data tools , which includes some downloadable binaries and the NOAA Geophysical Fluid Dynamics Laboratory guide to netCDF utilities .\nANDX and ANAX\nThe ARM Program has developed ANDX (ARM NetCDF Data eXtract) , a command-line utility designed for routine examination and extraction of data from netcdf files. Data can be displayed graphically (line-plot, scatter-plot, overlay, color-intensity, etc.) or extracted as ASCII data. Whether displayed graphically or extracted as ASCII, results can be saved to disk or viewed on screen.\nANAX (ARM NetCDF ASCII eXtract) is a scaled-down version of ANDX -- it is designed to only extract ASCII data. All features of ANDX pertaining to non-graphic data extraction are included in ANAX.\nANTS\nThe ARM Program has developed ANTS (ARM NetCDF Tool Suite) , a collection of netCDF tools and utilities providing various means of creating and modifying netcdf files. ANTS is based on nctools written by Chuck Denham. The utilities within nctools were modified to compile with version 3.5 of the netCDF library, the command syntax was modified for consistency with other tools, and changes were made to accommodate ARM standard netCDF.\nThe original functions from nctools were intended mainly for the creation, definition, and copying of fundamental netCDF elements. ARM added others which focus on manipulation of data within existing netCDF files. Additional functions have special support for multi-dimensional data such as \"slicing\" cross sections from multi-dimensional variable data or joining lesser-dimensional fields to form multi-dimensional structures. Functions have been added to support execution of arithmetic and logical operations, bundling or splitting netCDF files, comparing the structure or content of files, and so on.\nEssentially every type of netCDF library function call is exercised in ANTS. In this way then, this open-source collection of tools also represents a library of coding examples for fundamental netCDF tasks. See the website for more information.\nARGOS\nARGOS (interActive thRee-dimensional Graphics ObServatory) is a new IDL-based interactive 3D visualization tool, developed by David N. Bresch and Mark A. Liniger at the Institute for Atmospheric Science at the Swiss Federal Institute of Technology, ETH, Z\u00fcrich.\nA highly optimized graphical user interface allows quick and elegant creation of even complex 3D graphics (volume rendering, isosurfaces,...), including Z-buffered overlays (with hidden lines), light and data shading, Xray images, 3D trajectories, animations and virtual flights around your data, all documented in a full on-line html-help . The netCDF data format is preferred, but any other format can be read by providing an IDL (or FORTRAN or C or C++) interface. Some toolboxes (for atmospheric model output, trajectory display, radar data) have already been written, others might easily be added (in IDL, FORTRAN or C code). All interactive activities are tracked in a script, allowing quick reconstruction of anything done as well as running ARGOS in batch script mode.\nInformation about copyright and licensing conditions are available. For further information and installation, please E-mail to: bresch@atmos.umnw.ethz.ch\nCDAT\nThe Climate Data Analysis Tool (CDAT) , developed by the Program for Climate Model Diagnosis and Intercomparison (PCMDI) at Lawrence Livermore National Laboratory, provides the capabilities needed to analyze model data, perform complex mathematical calculations, and graphically display the results. It provides the necessary tools to diagnose, validate, and intercompare large observational and global climate model data sets.\nIt includes the ability to ingest large climate datasets in netCDF, HDF, DRS, and GrADS/GRIB format; the Visualization and Computation System (VCS) module, visually displays and animates ingested or created data; and the Library of AMIP Data Transmission Standards (LATS) module outputs data in the machine-independent netCDF or GrADS/GRIB file formats.\nIn addition, the Command Line Interface (CLI) module allows CDAT to receive argument and function input via the command line, and the Graphical User Interface (GUI) allows CDAT to receive argument and function input via a point-and-click environment.\nThe software, which runs as a standalone process or within PCMDI's Visualization and Computation System (VCS), provides climate scientists with an easy and fast method to read different file formats, and to analyze and graphically display climate data in an integrated fashion. CDAT includes a set of pre-defined functions to allow the user to manipulate the data and send the output to a file which can be viewed as an image, or as a collection of images in an animation. The software has a gradual learning curve, allowing the novice user to quickly obtain useful results.\nCDFconvert\nThe MRG CDFconvert package provided by the Mesoscale Research Group, McGill University/SUNY Albany, is designed to address data conversion issues for gridded datasets stored under the COARDS convention. CDFconvert converts regular Cylindrical Equidistant (Lat/Long) and Gaussian (Spherical) netCDF grids into either the Canadian RPN Standard File or GEMPAK file formats. MRG CDFconvert has the flexibility to handle netCDF files generated by a number of sources, including NCEP and ECMWF. User-definable conversion tables make the extension of the package to different datasets possible.\ncdfsync\nJoe Sirott of NOAA's Pacific Marine Environmental Laboratory has developed cdfsync, a program that allows users to rapidly synchronize a set of netCDF files over a network. Fast synchronization times are achieved by only transmitting the differences between files. It is built on the Open Source rsync program, but contains a number of optimizations including:\nSpecial handling of netCDF files for faster synchronization calculations\nMuch faster updates of large numbers of small netCDF files\nIn-place updates of large netCDF files\nThe latest version should run on Linux variants and Solaris.\nMore information is available at the cdfsync website .\nCDO (Climate Data Operators)\nUwe Schulzweida at the Max Planck Institute for Meteorology has developed CDO , a collection of Operators to manipulate and analyze Climate Data files. Supported file formats include netCDF and GRIB. There are more than 350 operators available. The following table provides a brief overview of the main categories.\nFile information (info, sinfo, diff, ...)\nFile operations (copy, cat, merge, split*, ...)\nSelection (selcode, selvar, sellevel, seltimestep, ...)\nMissing values (setctomiss, setmisstoc, setrtomiss)\nArithmetic (add, sub, mul, div, ...)\nMathematical functions (sqrt, exp, log, sin, cos, ...)\nComparision (eq, ne, le, lt, ge, gt, ...)\nConditions (ifthen, ifnotthen, ifthenc, ifnotthenc)\nField statistics (fldsum, fldavg, fldstd, fldmin, fldmax, ...)\nVertical statistics (vertsum, vertavg, vertstd, vertmin, ...)\nTime range statistics (timavg, yearavg, monavg, dayavg, ...)\nField interpolation (remapbil, remapcon, remapdis, ...)\nVertical interpolation (ml2pl, ml2hl)\nTime interpolation (inttime, intyear)\nAs an example of use of CDO, converting from GRIB to netCDF can be as simple as\ncdo -f nc copy file.grb file.nc\nor with relative time axis (for usage with GrADS)\ncdo -r -f nc copy file.grb file.nc\nor using ECMWF reanalysis on a reduced grid\ncdo -R -f nc copy file.grb file.nc\nMore information is available on the CDO homepage .\nThe Center for Clouds Chemistry and Climate ( C4 ) Integrated Data Systems ( CIDS ) group has developed several useful netCDF utilities:\ncdf2idl: Writes an IDL script to read a NetCDF file.\ncdf2c: Writes C code to read a NetCDF file.\ncdf2fortran: Writes FORTRAN source code to read a NetCDF file.\ncdf2asc: Dumps NetCDF data to an ASCII file.\nThe CSIRO MATLAB/netCDF interface is now available from the CSIRO Marine Laboratories .\nThe CSIRO MATLAB/netCDF interface is run from within MATLAB and has a simple syntax. It has options for automatically handling missing values, scale factors, and permutation of hyperslabs. It is, however, limited to retrieving data from, and information about, existing netCDF files.\nThe basis of the interface is a machine-dependent mex-file called mexcdf53. Rather than call the mex-file directly users are advised to employ both Chuck Denham's netCDF toolbox and the CSIRO MATLAB/netCDF interface described here. For read-only access to existing netCDF data, the CSIRO interface has a simpler syntax than the netCDF Toolbox, but the latter may also be used to create and manipulate netCDF variables and datasets.\nEPIC\nNOAA's Pacific Marine Environmental Laboratory ( PMEL ) has developed the EPIC software package for oceanographic data. EPIC provides graphical display and data field manipulation for multi-dimensional netCDF files (up to 4 dimensions). PMEL has been using this software on Unix and VMS several years. At present, they have:\na data file I/O library ( epslib , which is layered on top of the netCDF library).\nepslib allows transparent access to multiple data file formats\na MATLAB MexEPS interface for using any supported EPIC file with MATLAB\nsuite of EPIC programs for graphics and analysis of hydrographic profile data and time series data.\nThis software was developed on Sun/Unix and is also supported for DEC/Ultrix and VAX/VMS as a system for data management, display and analysis system for observational oceanographic time series and hydrographic data. The EPIC software includes over 50 programs for oceanographic display and analysis, as well as utilities for putting in-situ or observational data on-line (with on-the-fly graphics and data download) on the WWW.\nThe developers are interested in coordinating with others who may be developing oceanographic software for use with netCDF files. The EPIC software is available via anonymous FTP from ftp.noaapmel.gov in the epic/ and /eps directories. To obtain the EPIC software, please see Web pages at http://www.pmel.noaa.gov/epic/download/index.html . For information about EPIC, please see the Web pages at http://www.pmel.noaa.gov/epic/index.html . Contact epic@pmel.noaa.gov, or Nancy Soreide, nns@noaapmel.gov, for more information.\nEzGet\nA FORTRAN library called EzGet has been developed at PCMDI to facilitate retrieval of modeled and observed climate data stored in popular formats including DRS , netCDF , GrADS , and, if a control file is supplied, GRIB . You can specify how the data should be structured and whether it should undergo a grid transformation before you receive it, even when you know little about the original structure of the stored data (e.g., its original dimension order, grid, and domain).\nThe EzGet library comprises a set of subroutines that can be linked to any FORTRAN program. EzGet reads files through the cdunif interface, but use of EzGet does not require familiarity with cdunif. The main advantages of using EzGet instead of the lower level cdunif library include:\nSubstantial error trapping capabilities and detailed error messages\nVersatile capability of conveniently selecting data from specified regions (e.g., oceans, North America, all land areas north of 45 degrees latitude, etc.)\nAbility to map data to a new grid at the time it is retrieved by EzGet\nAutomatic creation of ``weights'' for use in subsequent averaging or masking of data\nIncreased control in specifying the domain of the data to be retrieved.\nFor more information about EzGet, including instructions for downloading the documentation or software, see the EzGet home page at http://www-pcmdi.llnl.gov/ktaylor/ezget/ezget.html . For questions or comments on EzGet, contact Karl Taylor (taylor13@llnl.gov).\nFAN\nFAN (File Array Notation) is Harvey Davies' package for extracting and manipulating array data from netCDF files. The package includes the three utilities nc2text, text2nc, and ncrob for printing selected data from netCDF arrays, copying ASCII data into netCDF arrays, and performing various operations (sum, mean, max, min, product, ...) on netCDF arrays. A library (fanlib) is also included that supports the use of FAN from C programs. The package is available via anonymous FTP from ftp://ftp.unidata.ucar.edu/pub/netcdf/contrib/fan.tar.Z . Questions and comments may be sent to Harvey Davies, harvey.davies@csiro.au.\nFERRET\nFERRET is an interactive computer visualization and analysis environment designed to meet the needs of oceanographers and meteorologists analyzing large and complex gridded data sets. It is available by anonymous ftp from abyss.pmel.noaa.gov for a number of computer systems: SUN (Solaris and SUNOS), DECstation (Ultrix and OSF/1), SGI, VAX/VMS and Macintosh (limited support), and IBM RS-6000 (soon to be released).\nFERRET offers a Mathematica-like approach to analysis; new variables may be defined interactively as mathematical expressions involving data set variables. Calculations may be applied over arbitrarily shaped regions. Fully documented graphics are produced with a single command. Graphics styles included line plots, scatter plots, contour plots, color-filled contour plots, vector plots, wire frame plots, etc. Detailed controls over plot characteristics, page layout and overlays are provided. NetCDF is supported both as an input and an output format.\nMany excellent software packages have been developed recently for scientific visualization. The features that make FERRET distinctive among these packages are Mathematica-like flexibility, geophysical formatting (latitude/longitude/date), \"intelligent\" connection to its data base, special memory management for very large calculations, and symmetrical processing in 4 dimensions. Contact Steve Hankin, hankin@noaapmel.gov, for more information.\nFimex\nHeiko Klein (Norwegian Meteorological Institute) has developed the fimex (File Interpolation, Manipulation, and EXtraction) C++ library for gridded geospatial data. It converts between several data formats (currently netCDF, NcML, GRIB1 or GRIB2, and felt). Fimex also enables you to change the projection and interpolation of scalar and vector grids, to subset the gridded data, and to extract only parts of the files. Fimex supports a growing list of other features , including support for most NcML features and for netCDF-4 compression.\nFor simple usage, Fimex also comes with the command line program fimex.\nDocumentation and downloads are available from the fimex web site .\nGDAL\nFrank Warmerdam's GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license. As a library, it presents a single abstract data model to the calling application for all supported formats. The related OGR library (which lives within the GDAL source tree) provides a similar capability for simple features vector data.\nGDAL is in active use in several projects, and includes roughly 40 format drivers, including a translator for netCDF (read/write). Other translators include GeoTIFF (read/write), Erdas Imagine (read/write), ESRI .BIL (read), .aux labeled raw (read/write), DTED (read), SDTS DEM (read), CEOS (read), JPEG (read/write), PNG (read/write), Geosoft GXF (read) and Arc/Info Binary Grid (read). A full list is available in Supported Formats .\nGDAL has recently included support for the netCDF-4 enhanced data model and netCDF-4 format, as well as improved support for recent additions to the CF conventions.\nAs an example of the use of GDAL, converting an ArcInfo ASCII grid to netCDF (GMT conventions) as easy as:\ngdal_translate arc_ascii.grd -of GMT gmt_grid.nc\nGfdnavi (Geophysical fluid data navigator)\nGfdnavi is a web-based tool to archive, share, distribute, analyze, and visualize geophysical fluid data and knowledge. The software is under development by members of the GFD Dennou Club, including T. Horinouchi (RISH, Kyoto U.), S. Nishizawa (RIMS, Kyoto U.), and colleagues. Gfdnavi uses a metadata database for managing and analyzing data and visualizations. It also permits publishing data for web access and will soon support access to data on other Gfdnavi servers. Web service APIs are now under development. A presentation Introducing Gfdnavi describes the architecture and shows examples of use.\nGfdnavi is dependent on two technologies:\nRuby on Rails , a framework for web applications, and\nthe Dennou Ruby Project , a collection of tools for geophysical data. These tools include GPhys software to handle GRIB, GrADS, and netCDF data uniformly.\nAs an example of this technology, Takuji Kubota has established a Gfdnavi server for the Global Satellite Mapping of Precipitation ( GSMaP ) project.\nGMT\nGMT (Generic Mapping Tools) is an open source collection of about 60 tools for manipulating geographic and Cartesian data sets (including filtering, trend fitting, gridding, projecting, etc.) and producing Encapsulated PostScript File (EPS) illustrations ranging from simple x-y plots via contour maps to artificially illuminated surfaces and 3-D perspective views. GMT supports 30 map projections and transformations and comes with support data such as coastlines, rivers, and political boundaries. GMT is developed and maintained by Paul Wessel and Walter H. F. Smith with help from a global set of volunteers, and is supported by the National Science Foundation. It is released under the GNU General Public License.\nThe package can access COARDS-compliant netCDF grids as well as ASCII, native binary, or user-defined formats. The GMT package is available via anonymous ftp from several servers; see gmt.soest.hawaii.edu for installation information.\nGrace\nGrace is a tool to make two-dimensional plots of scientific data, including 1D netCDF variables. It runs under the X Window System and OSF Motif (recent versions of LessTif are, by and large, fine, too). Grace runs on practically any version of Unix. As well, it has been successfully ported to VMS, OS/2 and Win9*/NT (some functionality may be missing, though). Grace is a descendant of ACE/gr.\nA few features of Grace are:\nUser defined scaling, tick marks, labels, symbols, line styles, colors.\nBatch mode for unattended plotting.\nRead and write parameters used during a session.\nRegressions, splines, running averages, DFT/FFT, cross/auto-correlation, ...\nSupport for dynamic module loading.\nHardcopy support for PostScript, PDF, GIF, and PNM formats.\nDevice-independent Type1 font rastering.\nGrADS\nGrADS (Grid Analysis and Display System) is an interactive desktop tool from COLA/IGES that is currently in use worldwide for the analysis and display of earth science data. GrADS is implemented on all commonly available UNIX workstations, Apple Macintosh, and DOS or Linux based PCs, and is freely available via anonymous ftp. GrADS provides an integrated environment for access, manipulation, and display of earth science data in several forms, including GRIB and netCDF. For more information, see the GrADS User's Guide .\nGri\nGri is an extensible plotting language for producing scientific graphs, such as x-y plots, contour plots, and image plots. Dan Kelley of Dalhousie University is the author of Gri, which can read data from netCDF files as well as ASCII and native binary data. For more information on Gri, see the URL http://gri.sourceforge.net/ .\nGXSM\nThe GXSM is the Gnome X Scanning Microscopy project, it is a bit more than just a piece of software (the GXSM itself), there is full hardware support for DSP cards including open source DSP software and a growing set of SPM related electronics. For more information, see http://gxsm.sourceforge.net/ .\nHDF interface\nThe National Center for Supercomputing Applications (NCSA) has added the netCDF interface to their Hierarchical Data Format (HDF) software. HDF is an extensible data format for self-describing files. A substantial set of applications and utilities based on HDF is available; these support raster-image manipulation and display and browsing through multidimensional scientific data. An implementation is now available that provides the netCDF interface to HDF. With this software, it is possible to use the netCDF calling interface to place data into an HDF file. The netCDF calling interface has not changed and netCDF files stored in XDR format are readable, so existing programs and data will still be usable (although programs will need to be relinked to the new library). There is currently no support for the mixing of HDF and netCDF structures. For example, a raster image can exist in the same file as a netCDF object, but you have to use the Raster Image interface to read the image and the netCDF interface to read the netCDF object. The other HDF interfaces are currently being modified to allow multi-file access, closer integration with the netCDF interface will probably be delayed until the end of that project.\nEventually, it will be possible to integrate netCDF objects with the rest of the HDF tool suite. Such an integration will then allow tools written for netCDF and tools written for HDF to both interact intelligently with the new data files.\nThe Goddard Earth Sciences Data and Information Services Center ( GES DISC ) has developed an on-the-fly HDF-EOS to netCDF/CF converter for the following products, making them easier to use in the Unidata IDV and McIDAS-V :\nAIRS Level 2 (scene) profiles of moisture, air temperature and trace gases\nAIRS Level 3 (global grid) profiles of moisture, air temperature and trace gases\nOMI UV-B at the surface\nTOMS ozone and aerosols\nInstructions are available for searching and converting these data. More information on AIRS products is available at http://disc.gsfc.nasa.gov/AIRS/index.html .\nHIPHOP\nHIPHOP , developed by Dominik Brunner, is a widget based IDL application that largely facilitates the visualization and analysis of 2D, 3D, and 4D atmospheric science data, in particular atmospheric tracer distributions and meteorological fields.\nGraphical output of (atmospheric model) data can be quickly generated in a large number of different ways, including horizontal maps at selected model or pressure levels, vertical north-south, east-west, or slant cross-sections (including zonal averages), time slices, animations, etc. It also allows mathematical operations on the existing fields to generate new fields for further analysis, and it can be run as a batch application.\nThe program handles data in netCDF, HDF and GRIB format. Interfaces to other data formats (e.g. ASCII and binary data) can be added easily.\nBeginning with Version 4.0, it also supports the ability to overlay meteorological fields on a number of different satellite images, and to draw air parcel trajectories.\nHyperslab OPerator Suite (HOPS)\nHyperslab OPerator Suite ( HOPS ), developed by R. Saravanan at NCAR, is a bilingual, multi-platform software package for processing data in netCDF files conforming to the NCAR-CCM format or the NCAR Ocean Model format. HOPS is implemented in IDL , the widely-used commercial interpreted language, and also in Yorick , a public-domain interpreted language that is freely available from the Lawrence Livermore National Laboratory. The IDL version of HOPS should run on any platform supported by IDL. The Yorick version too runs on most common UNIX platforms, such as Sun, SGI, Cray, and LINUX computers.\nHOPS is not a monolithic program, but a suite of operators that act on data units called \"hyperslabs\". The design of HOPS is object-oriented, rather than procedure-oriented; the operators treat the numeric data and the associated meta-data (like coordinate information) as a single object.\nNote that HOPS is not a general purpose netCDF utility and works only for the NCAR CSM netCDF formats. For more information, check the HOPS home page .\niCDF (imports chromatographic netCDF data into MATLAB)\nKlavs M. S\u00f8rensen, Thomas Skov and Rasmus Bro (Faculty of Life Sciences, University of Copenhagen) have developed iCDF , a free and documented toolbox for importing chromatographic data in the netCDF-based format that most manufacturers of chromatographic software support.\nThe iCDF software is currently for XC-MS data (X: GC, LC, HPLC), but soon it will be able to import data using other detectors as well. It can be used to open netCDF files from many different instruments (e.g. Agilent, Bruker) and many chromatographic software packages (e.g. ChemStation).\nFor more information, see the paper\nSkov T and Bro R. (2008) Solving fundamental problems in chromatographic analysis Analytical and Bioanalytical Chemistry, 390 (1): 281-285.\nUnidata's Integrated Data Viewer (IDV) is a Java application (for Java 1.4 or later) that can be used to display a variety of netCDF files, particularly well formatted, geolocated datasets. Features include:\nAccess to local and remote netCDF files and a variety of other data formats\nSlicing and probing of multidimensional data\nSupport for netCDF conventions (CF, COARDS, NUWG, AWIPS)\nInstallAnywhere installers for easy download and installation\nSave display state to a bundle for easy recreation of views\nSupport for non-gridded data through the Common Data Model (CDM)\nThe IDV uses the VisAD Java library for interactive and collaborative visualization and analysis and the netCDF Java library for reading and manipulating netCDF files.\nIngrid\nIngrid , by M. Benno Blumenthal <benno@ldeo.columbia.edu>, is designed to manipulate large datasets and model input/output. It can read data from its data catalog, a netCDF file, or a directly attached model, and output the data, either by feeding it to a model, creating a netCDF file, or creating plots and other representations of the data.\nIngrid has a number of filters which allow simple data manipulations, such as adding two datasets together, smoothing, averaging, and regridding to a new coordinate. In addition to netCDF, it also reads HDF, CDF, VOGL, and SGI GL.\nIngrid is currently running as a WWW daemon that can be accessed through http://rainbow.ldgo.columbia.edu/datacatalog.html to see some of its capabilities on a climate data catalog maintained by the Climate Group of the Lamont-Doherty Earth Observatory of Columbia University. To quote the introduction:\nThe Data Catalog is both a catalog and a library of datasets, i.e. it both helps you figure out which data you want, and helps you work with the data. The interface allows you to make plots, tables, and files from any dataset, its subsets, or processed versions thereof.\nThis data server is designed to make data accessible to people using WWW clients (viewers) and to serve as a data resource for WWW documents. Since most documents cannot use raw data, the server is able to deliver the data in a variety of ways: as data files (netCDF and HDF), as tables (html), and in a variety of plots (line, contour, color, vector) and plot formats (PostScript and gif). Processing of the data, particularly averaging, can be requested as well.\nThe Data Viewer in particular demonstrates the power of the Ingrid daemon.\nIngrid currently runs on Linux, for which binaries are available. CVS access to the current source can be arranged.\nIntel Array Visualizer\nThe Intel\u00ae Array Visualizer and Intel\u00ae Array Viewer are available as free downloads for Windows platforms. They offer an application and a set of software tools and components, which include C, Fortran, and .Net libraries, for developing scientific visualization applications and for creating interactive graphs of array data in various formats, including HDF and netCDF.\nIVE\nIVE (Interactive Visualization Environment) is a software package designed to interactively display and analyze gridded data. IVE assumes the data to be displayed are contained in one- two-, three- or four-dimensional arrays. By default, the numbers within these arrays are assumed to represent grid point values of some field variable (such as pressure) on a rectangular evenly spaced grid. IVE is, nevertheless, capable of displaying data on arbitrary curvilinear grids.\nIf the data points are not evenly spaced on a rectangular grid, IVE must be informed of the grid structure, either by specifying \"attributes\" in the data input or by specifying the coordinate transform in a user supplied subroutine. Stretched rectangular grids (which occur when the stretching along a given coordinate is a function only of the value of that coordinate) can be accommodated by specifying one-dimensional arrays containing the grid-point locations along the stretched coordinate as part of the IVE input data. Staggered meshes can also be accommodated by setting \"attributes\" in the input data. The structure of more complicated curvilinear grids must be communicated to IVE via user supplied \"transforms,\" which define the mapping between physical space and the array indices.\nSince four-dimensional data cannot be directly displayed on a flat computer screen, it is necessary to reduced the dimensionality of the data before it is displayed. One of IVE's primary capabilities involves dimension reduction or \"data slicing.\" IVE allows the user to display lower-dimensional subsets of the data by fixing a coordinate or by averaging over the coordinate.\nIVE currently has the capability to display\nscalar fields as\n"}, {"score": 1201.9197, "uuid": "4bb17c85-e85a-563f-814f-9fdd46e198c7", "index": "cw12", "trec_id": "clueweb12-0301wb-24-03664", "target_hostname": "www.unidata.ucar.edu", "target_uri": "http://www.unidata.ucar.edu/software/netcdf/utilities.html", "page_rank": 1.1700305e-09, "spam_rank": 99, "title": "Software <em>for</em> Manipulating <em>or</em> Displaying NetCDF <em>Data</em>", "snippet": "David Pierce has contributed the ncdf4 package <em>for</em> reading netCDF <em>data</em> into <em>R</em> and <em>for</em> creating new netCDF dimensions, variables, and files, <em>or</em> manipulating existing netCDF files from <em>R</em>.", "explanation": null, "document": "Visit Unidata\nSoftware for Manipulating or Displaying NetCDF Data\nThis document provides references to software packages that may be used for manipulating or displaying netCDF data. We include information about both freely-available and licensed (commercial) software that can be used with netCDF data. We rely on developers to help keep this list up-to-date. If you know of corrections or additions, please send them to us . Where practical, we would like to include WWW links to information about these packages in the HTML version of this document.\nOther useful guides to utilities that can handle netCDF data include ARM's list of ARM-tested netCDF data tools , which includes some downloadable binaries and the NOAA Geophysical Fluid Dynamics Laboratory guide to netCDF utilities .\nANDX and ANAX\nThe ARM Program has developed ANDX (ARM NetCDF Data eXtract) , a command-line utility designed for routine examination and extraction of data from netcdf files. Data can be displayed graphically (line-plot, scatter-plot, overlay, color-intensity, etc.) or extracted as ASCII data. Whether displayed graphically or extracted as ASCII, results can be saved to disk or viewed on screen.\nANAX (ARM NetCDF ASCII eXtract) is a scaled-down version of ANDX -- it is designed to only extract ASCII data. All features of ANDX pertaining to non-graphic data extraction are included in ANAX.\nANTS\nThe ARM Program has developed ANTS (ARM NetCDF Tool Suite) , a collection of netCDF tools and utilities providing various means of creating and modifying netcdf files. ANTS is based on nctools written by Chuck Denham. The utilities within nctools were modified to compile with version 3.5 of the netCDF library, the command syntax was modified for consistency with other tools, and changes were made to accommodate ARM standard netCDF.\nThe original functions from nctools were intended mainly for the creation, definition, and copying of fundamental netCDF elements. ARM added others which focus on manipulation of data within existing netCDF files. Additional functions have special support for multi-dimensional data such as \"slicing\" cross sections from multi-dimensional variable data or joining lesser-dimensional fields to form multi-dimensional structures. Functions have been added to support execution of arithmetic and logical operations, bundling or splitting netCDF files, comparing the structure or content of files, and so on.\nEssentially every type of netCDF library function call is exercised in ANTS. In this way then, this open-source collection of tools also represents a library of coding examples for fundamental netCDF tasks. See the website for more information.\nARGOS\nARGOS (interActive thRee-dimensional Graphics ObServatory) is a new IDL-based interactive 3D visualization tool, developed by David N. Bresch and Mark A. Liniger at the Institute for Atmospheric Science at the Swiss Federal Institute of Technology, ETH, Z\u00fcrich.\nA highly optimized graphical user interface allows quick and elegant creation of even complex 3D graphics (volume rendering, isosurfaces,...), including Z-buffered overlays (with hidden lines), light and data shading, Xray images, 3D trajectories, animations and virtual flights around your data, all documented in a full on-line html-help . The netCDF data format is preferred, but any other format can be read by providing an IDL (or FORTRAN or C or C++) interface. Some toolboxes (for atmospheric model output, trajectory display, radar data) have already been written, others might easily be added (in IDL, FORTRAN or C code). All interactive activities are tracked in a script, allowing quick reconstruction of anything done as well as running ARGOS in batch script mode.\nInformation about copyright and licensing conditions are available. For further information and installation, please E-mail to: bresch@atmos.umnw.ethz.ch\nCDAT\nThe Climate Data Analysis Tool (CDAT) , developed by the Program for Climate Model Diagnosis and Intercomparison (PCMDI) at Lawrence Livermore National Laboratory, provides the capabilities needed to analyze model data, perform complex mathematical calculations, and graphically display the results. It provides the necessary tools to diagnose, validate, and intercompare large observational and global climate model data sets.\nIt includes the ability to ingest large climate datasets in netCDF, HDF, DRS, and GrADS/GRIB format; the Visualization and Computation System (VCS) module, visually displays and animates ingested or created data; and the Library of AMIP Data Transmission Standards (LATS) module outputs data in the machine-independent netCDF or GrADS/GRIB file formats.\nIn addition, the Command Line Interface (CLI) module allows CDAT to receive argument and function input via the command line, and the Graphical User Interface (GUI) allows CDAT to receive argument and function input via a point-and-click environment.\nThe software, which runs as a standalone process or within PCMDI's Visualization and Computation System (VCS), provides climate scientists with an easy and fast method to read different file formats, and to analyze and graphically display climate data in an integrated fashion. CDAT includes a set of pre-defined functions to allow the user to manipulate the data and send the output to a file which can be viewed as an image, or as a collection of images in an animation. The software has a gradual learning curve, allowing the novice user to quickly obtain useful results.\nCDFconvert\nThe MRG CDFconvert package provided by the Mesoscale Research Group, McGill University/SUNY Albany, is designed to address data conversion issues for gridded datasets stored under the COARDS convention. CDFconvert converts regular Cylindrical Equidistant (Lat/Long) and Gaussian (Spherical) netCDF grids into either the Canadian RPN Standard File or GEMPAK file formats. MRG CDFconvert has the flexibility to handle netCDF files generated by a number of sources, including NCEP and ECMWF. User-definable conversion tables make the extension of the package to different datasets possible.\ncdfsync\nJoe Sirott of NOAA's Pacific Marine Environmental Laboratory has developed cdfsync, a program that allows users to rapidly synchronize a set of netCDF files over a network. Fast synchronization times are achieved by only transmitting the differences between files. It is built on the Open Source rsync program, but contains a number of optimizations including:\nSpecial handling of netCDF files for faster synchronization calculations\nMuch faster updates of large numbers of small netCDF files\nIn-place updates of large netCDF files\nThe latest version should run on Linux variants and Solaris.\nMore information is available at the cdfsync website .\nCDO (Climate Data Operators)\nUwe Schulzweida at the Max Planck Institute for Meteorology has developed CDO , a collection of Operators to manipulate and analyze Climate Data files. Supported file formats include netCDF and GRIB. There are more than 350 operators available. The following table provides a brief overview of the main categories.\nFile information (info, sinfo, diff, ...)\nFile operations (copy, cat, merge, split*, ...)\nSelection (selcode, selvar, sellevel, seltimestep, ...)\nMissing values (setctomiss, setmisstoc, setrtomiss)\nArithmetic (add, sub, mul, div, ...)\nMathematical functions (sqrt, exp, log, sin, cos, ...)\nComparision (eq, ne, le, lt, ge, gt, ...)\nConditions (ifthen, ifnotthen, ifthenc, ifnotthenc)\nField statistics (fldsum, fldavg, fldstd, fldmin, fldmax, ...)\nVertical statistics (vertsum, vertavg, vertstd, vertmin, ...)\nTime range statistics (timavg, yearavg, monavg, dayavg, ...)\nField interpolation (remapbil, remapcon, remapdis, ...)\nVertical interpolation (ml2pl, ml2hl)\nTime interpolation (inttime, intyear)\nAs an example of use of CDO, converting from GRIB to netCDF can be as simple as\ncdo -f nc copy file.grb file.nc\nor with relative time axis (for usage with GrADS)\ncdo -r -f nc copy file.grb file.nc\nor using ECMWF reanalysis on a reduced grid\ncdo -R -f nc copy file.grb file.nc\nMore information is available on the CDO homepage .\nThe Center for Clouds Chemistry and Climate ( C4 ) Integrated Data Systems ( CIDS ) group has developed several useful netCDF utilities:\ncdf2idl: Writes an IDL script to read a NetCDF file.\ncdf2c: Writes C code to read a NetCDF file.\ncdf2fortran: Writes FORTRAN source code to read a NetCDF file.\ncdf2asc: Dumps NetCDF data to an ASCII file.\nThe CSIRO MATLAB/netCDF interface is now available from the CSIRO Marine Laboratories .\nThe CSIRO MATLAB/netCDF interface is run from within MATLAB and has a simple syntax. It has options for automatically handling missing values, scale factors, and permutation of hyperslabs. It is, however, limited to retrieving data from, and information about, existing netCDF files.\nThe basis of the interface is a machine-dependent mex-file called mexcdf53. Rather than call the mex-file directly users are advised to employ both Chuck Denham's netCDF toolbox and the CSIRO MATLAB/netCDF interface described here. For read-only access to existing netCDF data, the CSIRO interface has a simpler syntax than the netCDF Toolbox, but the latter may also be used to create and manipulate netCDF variables and datasets.\nEPIC\nNOAA's Pacific Marine Environmental Laboratory ( PMEL ) has developed the EPIC software package for oceanographic data. EPIC provides graphical display and data field manipulation for multi-dimensional netCDF files (up to 4 dimensions). PMEL has been using this software on Unix and VMS several years. At present, they have:\na data file I/O library ( epslib , which is layered on top of the netCDF library).\nepslib allows transparent access to multiple data file formats\na MATLAB MexEPS interface for using any supported EPIC file with MATLAB\nsuite of EPIC programs for graphics and analysis of hydrographic profile data and time series data.\nThis software was developed on Sun/Unix and is also supported for DEC/Ultrix and VAX/VMS as a system for data management, display and analysis system for observational oceanographic time series and hydrographic data. The EPIC software includes over 50 programs for oceanographic display and analysis, as well as utilities for putting in-situ or observational data on-line (with on-the-fly graphics and data download) on the WWW.\nThe developers are interested in coordinating with others who may be developing oceanographic software for use with netCDF files. The EPIC software is available via anonymous FTP from ftp.noaapmel.gov in the epic/ and /eps directories. To obtain the EPIC software, please see Web pages at http://www.pmel.noaa.gov/epic/download/index.html . For information about EPIC, please see the Web pages at http://www.pmel.noaa.gov/epic/index.html . Contact epic@pmel.noaa.gov, or Nancy Soreide, nns@noaapmel.gov, for more information.\nEzGet\nA FORTRAN library called EzGet has been developed at PCMDI to facilitate retrieval of modeled and observed climate data stored in popular formats including DRS , netCDF , GrADS , and, if a control file is supplied, GRIB . You can specify how the data should be structured and whether it should undergo a grid transformation before you receive it, even when you know little about the original structure of the stored data (e.g., its original dimension order, grid, and domain).\nThe EzGet library comprises a set of subroutines that can be linked to any FORTRAN program. EzGet reads files through the cdunif interface, but use of EzGet does not require familiarity with cdunif. The main advantages of using EzGet instead of the lower level cdunif library include:\nSubstantial error trapping capabilities and detailed error messages\nVersatile capability of conveniently selecting data from specified regions (e.g., oceans, North America, all land areas north of 45 degrees latitude, etc.)\nAbility to map data to a new grid at the time it is retrieved by EzGet\nAutomatic creation of ``weights'' for use in subsequent averaging or masking of data\nIncreased control in specifying the domain of the data to be retrieved.\nFor more information about EzGet, including instructions for downloading the documentation or software, see the EzGet home page at http://www-pcmdi.llnl.gov/ktaylor/ezget/ezget.html . For questions or comments on EzGet, contact Karl Taylor (taylor13@llnl.gov).\nFAN\nFAN (File Array Notation) is Harvey Davies' package for extracting and manipulating array data from netCDF files. The package includes the three utilities nc2text, text2nc, and ncrob for printing selected data from netCDF arrays, copying ASCII data into netCDF arrays, and performing various operations (sum, mean, max, min, product, ...) on netCDF arrays. A library (fanlib) is also included that supports the use of FAN from C programs. The package is available via anonymous FTP from ftp://ftp.unidata.ucar.edu/pub/netcdf/contrib/fan.tar.Z . Questions and comments may be sent to Harvey Davies, harvey.davies@csiro.au.\nFERRET\nFERRET is an interactive computer visualization and analysis environment designed to meet the needs of oceanographers and meteorologists analyzing large and complex gridded data sets. It is available by anonymous ftp from abyss.pmel.noaa.gov for a number of computer systems: SUN (Solaris and SUNOS), DECstation (Ultrix and OSF/1), SGI, VAX/VMS and Macintosh (limited support), and IBM RS-6000 (soon to be released).\nFERRET offers a Mathematica-like approach to analysis; new variables may be defined interactively as mathematical expressions involving data set variables. Calculations may be applied over arbitrarily shaped regions. Fully documented graphics are produced with a single command. Graphics styles included line plots, scatter plots, contour plots, color-filled contour plots, vector plots, wire frame plots, etc. Detailed controls over plot characteristics, page layout and overlays are provided. NetCDF is supported both as an input and an output format.\nMany excellent software packages have been developed recently for scientific visualization. The features that make FERRET distinctive among these packages are Mathematica-like flexibility, geophysical formatting (latitude/longitude/date), \"intelligent\" connection to its data base, special memory management for very large calculations, and symmetrical processing in 4 dimensions. Contact Steve Hankin, hankin@noaapmel.gov, for more information.\nFimex\nHeiko Klein (Norwegian Meteorological Institute) has developed the fimex (File Interpolation, Manipulation, and EXtraction) C++ library for gridded geospatial data. It converts between several data formats (currently netCDF, NcML, GRIB1 or GRIB2, and felt). Fimex also enables you to change the projection and interpolation of scalar and vector grids, to subset the gridded data, and to extract only parts of the files. Fimex supports a growing list of other features , including support for most NcML features and for netCDF-4 compression.\nFor simple usage, Fimex also comes with the command line program fimex.\nDocumentation and downloads are available from the fimex web site .\nGDAL\nFrank Warmerdam's GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license. As a library, it presents a single abstract data model to the calling application for all supported formats. The related OGR library (which lives within the GDAL source tree) provides a similar capability for simple features vector data.\nGDAL is in active use in several projects, and includes roughly 40 format drivers, including a translator for netCDF (read/write). Other translators include GeoTIFF (read/write), Erdas Imagine (read/write), ESRI .BIL (read), .aux labeled raw (read/write), DTED (read), SDTS DEM (read), CEOS (read), JPEG (read/write), PNG (read/write), Geosoft GXF (read) and Arc/Info Binary Grid (read). A full list is available in Supported Formats .\nAs an example of the use of GDAL, converting an ArcInfo ASCII grid to netCDF (GMT conventions) as easy as:\ngdal_translate arc_ascii.grd -of GMT gmt_grid.nc\nGfdnavi (Geophysical fluid data navigator)\nGfdnavi is a web-based tool to archive, share, distribute, analyze, and visualize geophysical fluid data and knowledge. The software is under development by members of the GFD Dennou Club, including T. Horinouchi (RISH, Kyoto U.), S. Nishizawa (RIMS, Kyoto U.), and colleagues. Gfdnavi uses a metadata database for managing and analyzing data and visualizations. It also permits publishing data for web access and will soon support access to data on other Gfdnavi servers. Web service APIs are now under development. A presentation Introducing Gfdnavi describes the architecture and shows examples of use.\nGfdnavi is dependent on two technologies:\nRuby on Rails , a framework for web applications, and\nthe Dennou Ruby Project , a collection of tools for geophysical data. These tools include GPhys software to handle GRIB, GrADS, and netCDF data uniformly.\nAs an example of this technology, Takuji Kubota has established a Gfdnavi server for the Global Satellite Mapping of Precipitation ( GSMaP ) project.\nGMT\nGMT (Generic Mapping Tools) is an open source collection of about 60 tools for manipulating geographic and Cartesian data sets (including filtering, trend fitting, gridding, projecting, etc.) and producing Encapsulated PostScript File (EPS) illustrations ranging from simple x-y plots via contour maps to artificially illuminated surfaces and 3-D perspective views. GMT supports 30 map projections and transformations and comes with support data such as coastlines, rivers, and political boundaries. GMT is developed and maintained by Paul Wessel and Walter H. F. Smith with help from a global set of volunteers, and is supported by the National Science Foundation. It is released under the GNU General Public License.\nThe package can access COARDS-compliant netCDF grids as well as ASCII, native binary, or user-defined formats. The GMT package is available via anonymous ftp from several servers; see gmt.soest.hawaii.edu for installation information.\nGrace\nGrace is a tool to make two-dimensional plots of scientific data, including 1D netCDF variables. It runs under the X Window System and OSF Motif (recent versions of LessTif are, by and large, fine, too). Grace runs on practically any version of Unix. As well, it has been successfully ported to VMS, OS/2 and Win9*/NT (some functionality may be missing, though). Grace is a descendant of ACE/gr.\nA few features of Grace are:\nUser defined scaling, tick marks, labels, symbols, line styles, colors.\nBatch mode for unattended plotting.\nRead and write parameters used during a session.\nRegressions, splines, running averages, DFT/FFT, cross/auto-correlation, ...\nSupport for dynamic module loading.\nHardcopy support for PostScript, PDF, GIF, and PNM formats.\nDevice-independent Type1 font rastering.\nGrADS\nGrADS (Grid Analysis and Display System) is an interactive desktop tool from COLA/IGES that is currently in use worldwide for the analysis and display of earth science data. GrADS is implemented on all commonly available UNIX workstations, Apple Macintosh, and DOS or Linux based PCs, and is freely available via anonymous ftp. GrADS provides an integrated environment for access, manipulation, and display of earth science data in several forms, including GRIB and netCDF. For more information, see the GrADS User's Guide .\nGri\nGri is an extensible plotting language for producing scientific graphs, such as x-y plots, contour plots, and image plots. Dan Kelley of Dalhousie University is the author of Gri, which can read data from netCDF files as well as ASCII and native binary data. For more information on Gri, see the URL http://gri.sourceforge.net/ .\nGXSM\nThe GXSM is the Gnome X Scanning Microscopy project, it is a bit more than just a piece of software (the GXSM itself), there is full hardware support for DSP cards including open source DSP software and a growing set of SPM related electronics. For more information, see http://gxsm.sourceforge.net/ .\nHDF interface\nThe National Center for Supercomputing Applications (NCSA) has added the netCDF interface to their Hierarchical Data Format (HDF) software. HDF is an extensible data format for self-describing files. A substantial set of applications and utilities based on HDF is available; these support raster-image manipulation and display and browsing through multidimensional scientific data. An implementation is now available that provides the netCDF interface to HDF. With this software, it is possible to use the netCDF calling interface to place data into an HDF file. The netCDF calling interface has not changed and netCDF files stored in XDR format are readable, so existing programs and data will still be usable (although programs will need to be relinked to the new library). There is currently no support for the mixing of HDF and netCDF structures. For example, a raster image can exist in the same file as a netCDF object, but you have to use the Raster Image interface to read the image and the netCDF interface to read the netCDF object. The other HDF interfaces are currently being modified to allow multi-file access, closer integration with the netCDF interface will probably be delayed until the end of that project.\nEventually, it will be possible to integrate netCDF objects with the rest of the HDF tool suite. Such an integration will then allow tools written for netCDF and tools written for HDF to both interact intelligently with the new data files.\nThe Goddard Earth Sciences Data and Information Services Center ( GES DISC ) has developed an on-the-fly HDF-EOS to netCDF/CF converter for the following products, making them easier to use in the Unidata IDV and McIDAS-V :\nAIRS Level 2 (scene) profiles of moisture, air temperature and trace gases\nAIRS Level 3 (global grid) profiles of moisture, air temperature and trace gases\nOMI UV-B at the surface\nTOMS ozone and aerosols\nInstructions are available for searching and converting these data. More information on AIRS products is available at http://disc.gsfc.nasa.gov/AIRS/index.html .\nHIPHOP\nHIPHOP , developed by Dominik Brunner, is a widget based IDL application that largely facilitates the visualization and analysis of 2D, 3D, and 4D atmospheric science data, in particular atmospheric tracer distributions and meteorological fields.\nGraphical output of (atmospheric model) data can be quickly generated in a large number of different ways, including horizontal maps at selected model or pressure levels, vertical north-south, east-west, or slant cross-sections (including zonal averages), time slices, animations, etc. It also allows mathematical operations on the existing fields to generate new fields for further analysis, and it can be run as a batch application.\nThe program handles data in netCDF, HDF and GRIB format. Interfaces to other data formats (e.g. ASCII and binary data) can be added easily.\nBeginning with Version 4.0, it also supports the ability to overlay meteorological fields on a number of different satellite images, and to draw air parcel trajectories.\nHyperslab OPerator Suite (HOPS)\nHyperslab OPerator Suite ( HOPS ), developed by R. Saravanan at NCAR, is a bilingual, multi-platform software package for processing data in netCDF files conforming to the NCAR-CCM format or the NCAR Ocean Model format. HOPS is implemented in IDL , the widely-used commercial interpreted language, and also in Yorick , a public-domain interpreted language that is freely available from the Lawrence Livermore National Laboratory. The IDL version of HOPS should run on any platform supported by IDL. The Yorick version too runs on most common UNIX platforms, such as Sun, SGI, Cray, and LINUX computers.\nHOPS is not a monolithic program, but a suite of operators that act on data units called \"hyperslabs\". The design of HOPS is object-oriented, rather than procedure-oriented; the operators treat the numeric data and the associated meta-data (like coordinate information) as a single object.\nNote that HOPS is not a general purpose netCDF utility and works only for the NCAR CSM netCDF formats. For more information, check the HOPS home page .\niCDF (imports chromatographic netCDF data into MATLAB)\nKlavs M. S\u00f8rensen, Thomas Skov and Rasmus Bro (Faculty of Life Sciences, University of Copenhagen) have developed iCDF , a free and documented toolbox for importing chromatographic data in the netCDF-based format that most manufacturers of chromatographic software support.\nThe iCDF software is currently for XC-MS data (X: GC, LC, HPLC), but soon it will be able to import data using other detectors as well. It can be used to open netCDF files from many different instruments (e.g. Agilent, Bruker) and many chromatographic software packages (e.g. ChemStation).\nFor more information, see the paper\nSkov T and Bro R. (2008) Solving fundamental problems in chromatographic analysis Analytical and Bioanalytical Chemistry, 390 (1): 281-285.\nUnidata's Integrated Data Viewer (IDV) is a Java application (for Java 1.4 or later) that can be used to display a variety of netCDF files, particularly well formatted, geolocated datasets. Features include:\nAccess to local and remote netCDF files and a variety of other data formats\nSlicing and probing of multidimensional data\nSupport for netCDF conventions (CF, COARDS, NUWG, AWIPS)\nInstallAnywhere installers for easy download and installation\nSave display state to a bundle for easy recreation of views\nSupport for non-gridded data through the Common Data Model (CDM)\nThe IDV uses the VisAD Java library for interactive and collaborative visualization and analysis and the netCDF Java library for reading and manipulating netCDF files.\nIngrid\nIngrid , by M. Benno Blumenthal <benno@ldeo.columbia.edu>, is designed to manipulate large datasets and model input/output. It can read data from its data catalog, a netCDF file, or a directly attached model, and output the data, either by feeding it to a model, creating a netCDF file, or creating plots and other representations of the data.\nIngrid has a number of filters which allow simple data manipulations, such as adding two datasets together, smoothing, averaging, and regridding to a new coordinate. In addition to netCDF, it also reads HDF, CDF, VOGL, and SGI GL.\nIngrid is currently running as a WWW daemon that can be accessed through http://rainbow.ldgo.columbia.edu/datacatalog.html to see some of its capabilities on a climate data catalog maintained by the Climate Group of the Lamont-Doherty Earth Observatory of Columbia University. To quote the introduction:\nThe Data Catalog is both a catalog and a library of datasets, i.e. it both helps you figure out which data you want, and helps you work with the data. The interface allows you to make plots, tables, and files from any dataset, its subsets, or processed versions thereof.\nThis data server is designed to make data accessible to people using WWW clients (viewers) and to serve as a data resource for WWW documents. Since most documents cannot use raw data, the server is able to deliver the data in a variety of ways: as data files (netCDF and HDF), as tables (html), and in a variety of plots (line, contour, color, vector) and plot formats (PostScript and gif). Processing of the data, particularly averaging, can be requested as well.\nThe Data Viewer in particular demonstrates the power of the Ingrid daemon.\nIngrid currently runs on Linux, for which binaries are available. CVS access to the current source can be arranged.\nIntel Array Visualizer\nThe Intel\u00ae Array Visualizer and Intel\u00ae Array Viewer are available as free downloads for Windows platforms. They offer an application and a set of software tools and components, which include C, Fortran, and .Net libraries, for developing scientific visualization applications and for creating interactive graphs of array data in various formats, including HDF and netCDF.\nIVE\nIVE (Interactive Visualization Environment) is a software package designed to interactively display and analyze gridded data. IVE assumes the data to be displayed are contained in one- two-, three- or four-dimensional arrays. By default, the numbers within these arrays are assumed to represent grid point values of some field variable (such as pressure) on a rectangular evenly spaced grid. IVE is, nevertheless, capable of displaying data on arbitrary curvilinear grids.\nIf the data points are not evenly spaced on a rectangular grid, IVE must be informed of the grid structure, either by specifying \"attributes\" in the data input or by specifying the coordinate transform in a user supplied subroutine. Stretched rectangular grids (which occur when the stretching along a given coordinate is a function only of the value of that coordinate) can be accommodated by specifying one-dimensional arrays containing the grid-point locations along the stretched coordinate as part of the IVE input data. Staggered meshes can also be accommodated by setting \"attributes\" in the input data. The structure of more complicated curvilinear grids must be communicated to IVE via user supplied \"transforms,\" which define the mapping between physical space and the array indices.\nSince four-dimensional data cannot be directly displayed on a flat computer screen, it is necessary to reduced the dimensionality of the data before it is displayed. One of IVE's primary capabilities involves dimension reduction or \"data slicing.\" IVE allows the user to display lower-dimensional subsets of the data by fixing a coordinate or by averaging over the coordinate.\nIVE currently has the capability to display\nscalar fields as\n"}, {"score": 1201.5854, "uuid": "fd911552-23c2-5c31-b06c-eea297aec491", "index": "cw12", "trec_id": "clueweb12-0300wb-06-22548", "target_hostname": "www.unidata.ucar.edu", "target_uri": "http://www.unidata.ucar.edu/software/netcdf/software.html", "page_rank": 1.5372247e-09, "spam_rank": 99, "title": "Software <em>for</em> Manipulating <em>or</em> Displaying NetCDF <em>Data</em>", "snippet": "David Pierce has contributed the ncdf4 package <em>for</em> reading netCDF <em>data</em> into <em>R</em> and <em>for</em> creating new netCDF dimensions, variables, and files, <em>or</em> manipulating existing netCDF files from <em>R</em>.", "explanation": null, "document": "Visit Unidata\nSoftware for Manipulating or Displaying NetCDF Data\nThis document provides references to software packages that may be used for manipulating or displaying netCDF data. We include information about both freely-available and licensed (commercial) software that can be used with netCDF data. We rely on developers to help keep this list up-to-date. If you know of corrections or additions, please send them to us . Where practical, we would like to include WWW links to information about these packages in the HTML version of this document.\nOther useful guides to utilities that can handle netCDF data include ARM's list of ARM-tested netCDF data tools , which includes some downloadable binaries and the NOAA Geophysical Fluid Dynamics Laboratory guide to netCDF utilities .\nANDX and ANAX\nThe ARM Program has developed ANDX (ARM NetCDF Data eXtract) , a command-line utility designed for routine examination and extraction of data from netcdf files. Data can be displayed graphically (line-plot, scatter-plot, overlay, color-intensity, etc.) or extracted as ASCII data. Whether displayed graphically or extracted as ASCII, results can be saved to disk or viewed on screen.\nANAX (ARM NetCDF ASCII eXtract) is a scaled-down version of ANDX -- it is designed to only extract ASCII data. All features of ANDX pertaining to non-graphic data extraction are included in ANAX.\nANTS\nThe ARM Program has developed ANTS (ARM NetCDF Tool Suite) , a collection of netCDF tools and utilities providing various means of creating and modifying netcdf files. ANTS is based on nctools written by Chuck Denham. The utilities within nctools were modified to compile with version 3.5 of the netCDF library, the command syntax was modified for consistency with other tools, and changes were made to accommodate ARM standard netCDF.\nThe original functions from nctools were intended mainly for the creation, definition, and copying of fundamental netCDF elements. ARM added others which focus on manipulation of data within existing netCDF files. Additional functions have special support for multi-dimensional data such as \"slicing\" cross sections from multi-dimensional variable data or joining lesser-dimensional fields to form multi-dimensional structures. Functions have been added to support execution of arithmetic and logical operations, bundling or splitting netCDF files, comparing the structure or content of files, and so on.\nEssentially every type of netCDF library function call is exercised in ANTS. In this way then, this open-source collection of tools also represents a library of coding examples for fundamental netCDF tasks. See the website for more information.\nARGOS\nARGOS (interActive thRee-dimensional Graphics ObServatory) is a new IDL-based interactive 3D visualization tool, developed by David N. Bresch and Mark A. Liniger at the Institute for Atmospheric Science at the Swiss Federal Institute of Technology, ETH, Z\u00fcrich.\nA highly optimized graphical user interface allows quick and elegant creation of even complex 3D graphics (volume rendering, isosurfaces,...), including Z-buffered overlays (with hidden lines), light and data shading, Xray images, 3D trajectories, animations and virtual flights around your data, all documented in a full on-line html-help . The netCDF data format is preferred, but any other format can be read by providing an IDL (or FORTRAN or C or C++) interface. Some toolboxes (for atmospheric model output, trajectory display, radar data) have already been written, others might easily be added (in IDL, FORTRAN or C code). All interactive activities are tracked in a script, allowing quick reconstruction of anything done as well as running ARGOS in batch script mode.\nInformation about copyright and licensing conditions are available. For further information and installation, please E-mail to: bresch@atmos.umnw.ethz.ch\nCDAT\nThe Climate Data Analysis Tool (CDAT) , developed by the Program for Climate Model Diagnosis and Intercomparison (PCMDI) at Lawrence Livermore National Laboratory, provides the capabilities needed to analyze model data, perform complex mathematical calculations, and graphically display the results. It provides the necessary tools to diagnose, validate, and intercompare large observational and global climate model data sets.\nIt includes the ability to ingest large climate datasets in netCDF, HDF, DRS, and GrADS/GRIB format; the Visualization and Computation System (VCS) module, visually displays and animates ingested or created data; and the Library of AMIP Data Transmission Standards (LATS) module outputs data in the machine-independent netCDF or GrADS/GRIB file formats.\nIn addition, the Command Line Interface (CLI) module allows CDAT to receive argument and function input via the command line, and the Graphical User Interface (GUI) allows CDAT to receive argument and function input via a point-and-click environment.\nThe software, which runs as a standalone process or within PCMDI's Visualization and Computation System (VCS), provides climate scientists with an easy and fast method to read different file formats, and to analyze and graphically display climate data in an integrated fashion. CDAT includes a set of pre-defined functions to allow the user to manipulate the data and send the output to a file which can be viewed as an image, or as a collection of images in an animation. The software has a gradual learning curve, allowing the novice user to quickly obtain useful results.\nCDFconvert\nThe MRG CDFconvert package provided by the Mesoscale Research Group, McGill University/SUNY Albany, is designed to address data conversion issues for gridded datasets stored under the COARDS convention. CDFconvert converts regular Cylindrical Equidistant (Lat/Long) and Gaussian (Spherical) netCDF grids into either the Canadian RPN Standard File or GEMPAK file formats. MRG CDFconvert has the flexibility to handle netCDF files generated by a number of sources, including NCEP and ECMWF. User-definable conversion tables make the extension of the package to different datasets possible.\ncdfsync\nJoe Sirott of NOAA's Pacific Marine Environmental Laboratory has developed cdfsync, a program that allows users to rapidly synchronize a set of netCDF files over a network. Fast synchronization times are achieved by only transmitting the differences between files. It is built on the Open Source rsync program, but contains a number of optimizations including:\nSpecial handling of netCDF files for faster synchronization calculations\nMuch faster updates of large numbers of small netCDF files\nIn-place updates of large netCDF files\nThe latest version should run on Linux variants and Solaris.\nMore information is available at the cdfsync website .\nCDO (Climate Data Operators)\nUwe Schulzweida at the Max Planck Institute for Meteorology has developed CDO , a collection of Operators to manipulate and analyze Climate Data files. Supported file formats include netCDF and GRIB. There are more than 350 operators available. The following table provides a brief overview of the main categories.\nFile information (info, sinfo, diff, ...)\nFile operations (copy, cat, merge, split*, ...)\nSelection (selcode, selvar, sellevel, seltimestep, ...)\nMissing values (setctomiss, setmisstoc, setrtomiss)\nArithmetic (add, sub, mul, div, ...)\nMathematical functions (sqrt, exp, log, sin, cos, ...)\nComparision (eq, ne, le, lt, ge, gt, ...)\nConditions (ifthen, ifnotthen, ifthenc, ifnotthenc)\nField statistics (fldsum, fldavg, fldstd, fldmin, fldmax, ...)\nVertical statistics (vertsum, vertavg, vertstd, vertmin, ...)\nTime range statistics (timavg, yearavg, monavg, dayavg, ...)\nField interpolation (remapbil, remapcon, remapdis, ...)\nVertical interpolation (ml2pl, ml2hl)\nTime interpolation (inttime, intyear)\nAs an example of use of CDO, converting from GRIB to netCDF can be as simple as\ncdo -f nc copy file.grb file.nc\nor with relative time axis (for usage with GrADS)\ncdo -r -f nc copy file.grb file.nc\nor using ECMWF reanalysis on a reduced grid\ncdo -R -f nc copy file.grb file.nc\nMore information is available on the CDO homepage .\nThe Center for Clouds Chemistry and Climate ( C4 ) Integrated Data Systems ( CIDS ) group has developed several useful netCDF utilities:\ncdf2idl: Writes an IDL script to read a NetCDF file.\ncdf2c: Writes C code to read a NetCDF file.\ncdf2fortran: Writes FORTRAN source code to read a NetCDF file.\ncdf2asc: Dumps NetCDF data to an ASCII file.\nThe CSIRO MATLAB/netCDF interface is now available from the CSIRO Marine Laboratories .\nThe CSIRO MATLAB/netCDF interface is run from within MATLAB and has a simple syntax. It has options for automatically handling missing values, scale factors, and permutation of hyperslabs. It is, however, limited to retrieving data from, and information about, existing netCDF files.\nThe basis of the interface is a machine-dependent mex-file called mexcdf53. Rather than call the mex-file directly users are advised to employ both Chuck Denham's netCDF toolbox and the CSIRO MATLAB/netCDF interface described here. For read-only access to existing netCDF data, the CSIRO interface has a simpler syntax than the netCDF Toolbox, but the latter may also be used to create and manipulate netCDF variables and datasets.\nEPIC\nNOAA's Pacific Marine Environmental Laboratory ( PMEL ) has developed the EPIC software package for oceanographic data. EPIC provides graphical display and data field manipulation for multi-dimensional netCDF files (up to 4 dimensions). PMEL has been using this software on Unix and VMS several years. At present, they have:\na data file I/O library ( epslib , which is layered on top of the netCDF library).\nepslib allows transparent access to multiple data file formats\na MATLAB MexEPS interface for using any supported EPIC file with MATLAB\nsuite of EPIC programs for graphics and analysis of hydrographic profile data and time series data.\nThis software was developed on Sun/Unix and is also supported for DEC/Ultrix and VAX/VMS as a system for data management, display and analysis system for observational oceanographic time series and hydrographic data. The EPIC software includes over 50 programs for oceanographic display and analysis, as well as utilities for putting in-situ or observational data on-line (with on-the-fly graphics and data download) on the WWW.\nThe developers are interested in coordinating with others who may be developing oceanographic software for use with netCDF files. The EPIC software is available via anonymous FTP from ftp.noaapmel.gov in the epic/ and /eps directories. To obtain the EPIC software, please see Web pages at http://www.pmel.noaa.gov/epic/download/index.html . For information about EPIC, please see the Web pages at http://www.pmel.noaa.gov/epic/index.html . Contact epic@pmel.noaa.gov, or Nancy Soreide, nns@noaapmel.gov, for more information.\nEzGet\nA FORTRAN library called EzGet has been developed at PCMDI to facilitate retrieval of modeled and observed climate data stored in popular formats including DRS , netCDF , GrADS , and, if a control file is supplied, GRIB . You can specify how the data should be structured and whether it should undergo a grid transformation before you receive it, even when you know little about the original structure of the stored data (e.g., its original dimension order, grid, and domain).\nThe EzGet library comprises a set of subroutines that can be linked to any FORTRAN program. EzGet reads files through the cdunif interface, but use of EzGet does not require familiarity with cdunif. The main advantages of using EzGet instead of the lower level cdunif library include:\nSubstantial error trapping capabilities and detailed error messages\nVersatile capability of conveniently selecting data from specified regions (e.g., oceans, North America, all land areas north of 45 degrees latitude, etc.)\nAbility to map data to a new grid at the time it is retrieved by EzGet\nAutomatic creation of ``weights'' for use in subsequent averaging or masking of data\nIncreased control in specifying the domain of the data to be retrieved.\nFor more information about EzGet, including instructions for downloading the documentation or software, see the EzGet home page at http://www-pcmdi.llnl.gov/ktaylor/ezget/ezget.html . For questions or comments on EzGet, contact Karl Taylor (taylor13@llnl.gov).\nFAN\nFAN (File Array Notation) is Harvey Davies' package for extracting and manipulating array data from netCDF files. The package includes the three utilities nc2text, text2nc, and ncrob for printing selected data from netCDF arrays, copying ASCII data into netCDF arrays, and performing various operations (sum, mean, max, min, product, ...) on netCDF arrays. A library (fanlib) is also included that supports the use of FAN from C programs. The package is available via anonymous FTP from ftp://ftp.unidata.ucar.edu/pub/netcdf/contrib/fan.tar.Z . Questions and comments may be sent to Harvey Davies, harvey.davies@csiro.au.\nFERRET\nFERRET is an interactive computer visualization and analysis environment designed to meet the needs of oceanographers and meteorologists analyzing large and complex gridded data sets. It is available by anonymous ftp from abyss.pmel.noaa.gov for a number of computer systems: SUN (Solaris and SUNOS), DECstation (Ultrix and OSF/1), SGI, VAX/VMS and Macintosh (limited support), and IBM RS-6000 (soon to be released).\nFERRET offers a Mathematica-like approach to analysis; new variables may be defined interactively as mathematical expressions involving data set variables. Calculations may be applied over arbitrarily shaped regions. Fully documented graphics are produced with a single command. Graphics styles included line plots, scatter plots, contour plots, color-filled contour plots, vector plots, wire frame plots, etc. Detailed controls over plot characteristics, page layout and overlays are provided. NetCDF is supported both as an input and an output format.\nMany excellent software packages have been developed recently for scientific visualization. The features that make FERRET distinctive among these packages are Mathematica-like flexibility, geophysical formatting (latitude/longitude/date), \"intelligent\" connection to its data base, special memory management for very large calculations, and symmetrical processing in 4 dimensions. Contact Steve Hankin, hankin@noaapmel.gov, for more information.\nFimex\nHeiko Klein (Norwegian Meteorological Institute) has developed the fimex (File Interpolation, Manipulation, and EXtraction) C++ library for gridded geospatial data. It converts between several data formats (currently netCDF, NcML, GRIB1 or GRIB2, and felt). Fimex also enables you to change the projection and interpolation of scalar and vector grids, to subset the gridded data, and to extract only parts of the files. Fimex supports a growing list of other features , including support for most NcML features and for netCDF-4 compression.\nFor simple usage, Fimex also comes with the command line program fimex.\nDocumentation and downloads are available from the fimex web site .\nGDAL\nFrank Warmerdam's GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license. As a library, it presents a single abstract data model to the calling application for all supported formats. The related OGR library (which lives within the GDAL source tree) provides a similar capability for simple features vector data.\nGDAL is in active use in several projects, and includes roughly 40 format drivers, including a translator for netCDF (read/write). Other translators include GeoTIFF (read/write), Erdas Imagine (read/write), ESRI .BIL (read), .aux labeled raw (read/write), DTED (read), SDTS DEM (read), CEOS (read), JPEG (read/write), PNG (read/write), Geosoft GXF (read) and Arc/Info Binary Grid (read). A full list is available in Supported Formats .\nAs an example of the use of GDAL, converting an ArcInfo ASCII grid to netCDF (GMT conventions) as easy as:\ngdal_translate arc_ascii.grd -of GMT gmt_grid.nc\nGfdnavi (Geophysical fluid data navigator)\nGfdnavi is a web-based tool to archive, share, distribute, analyze, and visualize geophysical fluid data and knowledge. The software is under development by members of the GFD Dennou Club, including T. Horinouchi (RISH, Kyoto U.), S. Nishizawa (RIMS, Kyoto U.), and colleagues. Gfdnavi uses a metadata database for managing and analyzing data and visualizations. It also permits publishing data for web access and will soon support access to data on other Gfdnavi servers. Web service APIs are now under development. A presentation Introducing Gfdnavi describes the architecture and shows examples of use.\nGfdnavi is dependent on two technologies:\nRuby on Rails , a framework for web applications, and\nthe Dennou Ruby Project , a collection of tools for geophysical data. These tools include GPhys software to handle GRIB, GrADS, and netCDF data uniformly.\nAs an example of this technology, Takuji Kubota has established a Gfdnavi server for the Global Satellite Mapping of Precipitation ( GSMaP ) project.\nGMT\nGMT (Generic Mapping Tools) is an open source collection of about 60 tools for manipulating geographic and Cartesian data sets (including filtering, trend fitting, gridding, projecting, etc.) and producing Encapsulated PostScript File (EPS) illustrations ranging from simple x-y plots via contour maps to artificially illuminated surfaces and 3-D perspective views. GMT supports 30 map projections and transformations and comes with support data such as coastlines, rivers, and political boundaries. GMT is developed and maintained by Paul Wessel and Walter H. F. Smith with help from a global set of volunteers, and is supported by the National Science Foundation. It is released under the GNU General Public License.\nThe package can access COARDS-compliant netCDF grids as well as ASCII, native binary, or user-defined formats. The GMT package is available via anonymous ftp from several servers; see gmt.soest.hawaii.edu for installation information.\nGrace\nGrace is a tool to make two-dimensional plots of scientific data, including 1D netCDF variables. It runs under the X Window System and OSF Motif (recent versions of LessTif are, by and large, fine, too). Grace runs on practically any version of Unix. As well, it has been successfully ported to VMS, OS/2 and Win9*/NT (some functionality may be missing, though). Grace is a descendant of ACE/gr.\nA few features of Grace are:\nUser defined scaling, tick marks, labels, symbols, line styles, colors.\nBatch mode for unattended plotting.\nRead and write parameters used during a session.\nRegressions, splines, running averages, DFT/FFT, cross/auto-correlation, ...\nSupport for dynamic module loading.\nHardcopy support for PostScript, PDF, GIF, and PNM formats.\nDevice-independent Type1 font rastering.\nGrADS\nGrADS (Grid Analysis and Display System) is an interactive desktop tool from COLA/IGES that is currently in use worldwide for the analysis and display of earth science data. GrADS is implemented on all commonly available UNIX workstations, Apple Macintosh, and DOS or Linux based PCs, and is freely available via anonymous ftp. GrADS provides an integrated environment for access, manipulation, and display of earth science data in several forms, including GRIB and netCDF. For more information, see the GrADS User's Guide .\nGri\nGri is an extensible plotting language for producing scientific graphs, such as x-y plots, contour plots, and image plots. Dan Kelley of Dalhousie University is the author of Gri, which can read data from netCDF files as well as ASCII and native binary data. For more information on Gri, see the URL http://gri.sourceforge.net/ .\nGXSM\nThe GXSM is the Gnome X Scanning Microscopy project, it is a bit more than just a piece of software (the GXSM itself), there is full hardware support for DSP cards including open source DSP software and a growing set of SPM related electronics. For more information, see http://gxsm.sourceforge.net/ .\nHDF interface\nThe National Center for Supercomputing Applications (NCSA) has added the netCDF interface to their Hierarchical Data Format (HDF) software. HDF is an extensible data format for self-describing files. A substantial set of applications and utilities based on HDF is available; these support raster-image manipulation and display and browsing through multidimensional scientific data. An implementation is now available that provides the netCDF interface to HDF. With this software, it is possible to use the netCDF calling interface to place data into an HDF file. The netCDF calling interface has not changed and netCDF files stored in XDR format are readable, so existing programs and data will still be usable (although programs will need to be relinked to the new library). There is currently no support for the mixing of HDF and netCDF structures. For example, a raster image can exist in the same file as a netCDF object, but you have to use the Raster Image interface to read the image and the netCDF interface to read the netCDF object. The other HDF interfaces are currently being modified to allow multi-file access, closer integration with the netCDF interface will probably be delayed until the end of that project.\nEventually, it will be possible to integrate netCDF objects with the rest of the HDF tool suite. Such an integration will then allow tools written for netCDF and tools written for HDF to both interact intelligently with the new data files.\nThe Goddard Earth Sciences Data and Information Services Center ( GES DISC ) has developed an on-the-fly HDF-EOS to netCDF/CF converter for the following products, making them easier to use in the Unidata IDV and McIDAS-V :\nAIRS Level 2 (scene) profiles of moisture, air temperature and trace gases\nAIRS Level 3 (global grid) profiles of moisture, air temperature and trace gases\nOMI UV-B at the surface\nTOMS ozone and aerosols\nInstructions are available for searching and converting these data. More information on AIRS products is available at http://disc.gsfc.nasa.gov/AIRS/index.html .\nHIPHOP\nHIPHOP , developed by Dominik Brunner, is a widget based IDL application that largely facilitates the visualization and analysis of 2D, 3D, and 4D atmospheric science data, in particular atmospheric tracer distributions and meteorological fields.\nGraphical output of (atmospheric model) data can be quickly generated in a large number of different ways, including horizontal maps at selected model or pressure levels, vertical north-south, east-west, or slant cross-sections (including zonal averages), time slices, animations, etc. It also allows mathematical operations on the existing fields to generate new fields for further analysis, and it can be run as a batch application.\nThe program handles data in netCDF, HDF and GRIB format. Interfaces to other data formats (e.g. ASCII and binary data) can be added easily.\nBeginning with Version 4.0, it also supports the ability to overlay meteorological fields on a number of different satellite images, and to draw air parcel trajectories.\nHyperslab OPerator Suite (HOPS)\nHyperslab OPerator Suite ( HOPS ), developed by R. Saravanan at NCAR, is a bilingual, multi-platform software package for processing data in netCDF files conforming to the NCAR-CCM format or the NCAR Ocean Model format. HOPS is implemented in IDL , the widely-used commercial interpreted language, and also in Yorick , a public-domain interpreted language that is freely available from the Lawrence Livermore National Laboratory. The IDL version of HOPS should run on any platform supported by IDL. The Yorick version too runs on most common UNIX platforms, such as Sun, SGI, Cray, and LINUX computers.\nHOPS is not a monolithic program, but a suite of operators that act on data units called \"hyperslabs\". The design of HOPS is object-oriented, rather than procedure-oriented; the operators treat the numeric data and the associated meta-data (like coordinate information) as a single object.\nNote that HOPS is not a general purpose netCDF utility and works only for the NCAR CSM netCDF formats. For more information, check the HOPS home page .\niCDF (imports chromatographic netCDF data into MATLAB)\nKlavs M. S\u00f8rensen, Thomas Skov and Rasmus Bro (Faculty of Life Sciences, University of Copenhagen) have developed iCDF , a free and documented toolbox for importing chromatographic data in the netCDF-based format that most manufacturers of chromatographic software support.\nThe iCDF software is currently for XC-MS data (X: GC, LC, HPLC), but soon it will be able to import data using other detectors as well. It can be used to open netCDF files from many different instruments (e.g. Agilent, Bruker) and many chromatographic software packages (e.g. ChemStation).\nFor more information, see the paper\nSkov T and Bro R. (2008) Solving fundamental problems in chromatographic analysis Analytical and Bioanalytical Chemistry, 390 (1): 281-285.\nUnidata's Integrated Data Viewer (IDV) is a Java application (for Java 1.4 or later) that can be used to display a variety of netCDF files, particularly well formatted, geolocated datasets. Features include:\nAccess to local and remote netCDF files and a variety of other data formats\nSlicing and probing of multidimensional data\nSupport for netCDF conventions (CF, COARDS, NUWG, AWIPS)\nInstallAnywhere installers for easy download and installation\nSave display state to a bundle for easy recreation of views\nSupport for non-gridded data through the Common Data Model (CDM)\nThe IDV uses the VisAD Java library for interactive and collaborative visualization and analysis and the netCDF Java library for reading and manipulating netCDF files.\nIngrid\nIngrid , by M. Benno Blumenthal <benno@ldeo.columbia.edu>, is designed to manipulate large datasets and model input/output. It can read data from its data catalog, a netCDF file, or a directly attached model, and output the data, either by feeding it to a model, creating a netCDF file, or creating plots and other representations of the data.\nIngrid has a number of filters which allow simple data manipulations, such as adding two datasets together, smoothing, averaging, and regridding to a new coordinate. In addition to netCDF, it also reads HDF, CDF, VOGL, and SGI GL.\nIngrid is currently running as a WWW daemon that can be accessed through http://rainbow.ldgo.columbia.edu/datacatalog.html to see some of its capabilities on a climate data catalog maintained by the Climate Group of the Lamont-Doherty Earth Observatory of Columbia University. To quote the introduction:\nThe Data Catalog is both a catalog and a library of datasets, i.e. it both helps you figure out which data you want, and helps you work with the data. The interface allows you to make plots, tables, and files from any dataset, its subsets, or processed versions thereof.\nThis data server is designed to make data accessible to people using WWW clients (viewers) and to serve as a data resource for WWW documents. Since most documents cannot use raw data, the server is able to deliver the data in a variety of ways: as data files (netCDF and HDF), as tables (html), and in a variety of plots (line, contour, color, vector) and plot formats (PostScript and gif). Processing of the data, particularly averaging, can be requested as well.\nThe Data Viewer in particular demonstrates the power of the Ingrid daemon.\nIngrid currently runs on Linux, for which binaries are available. CVS access to the current source can be arranged.\nIntel Array Visualizer\nThe Intel\u00ae Array Visualizer and Intel\u00ae Array Viewer are available as free downloads for Windows platforms. They offer an application and a set of software tools and components, which include C, Fortran, and .Net libraries, for developing scientific visualization applications and for creating interactive graphs of array data in various formats, including HDF and netCDF.\nIVE\nIVE (Interactive Visualization Environment) is a software package designed to interactively display and analyze gridded data. IVE assumes the data to be displayed are contained in one- two-, three- or four-dimensional arrays. By default, the numbers within these arrays are assumed to represent grid point values of some field variable (such as pressure) on a rectangular evenly spaced grid. IVE is, nevertheless, capable of displaying data on arbitrary curvilinear grids.\nIf the data points are not evenly spaced on a rectangular grid, IVE must be informed of the grid structure, either by specifying \"attributes\" in the data input or by specifying the coordinate transform in a user supplied subroutine. Stretched rectangular grids (which occur when the stretching along a given coordinate is a function only of the value of that coordinate) can be accommodated by specifying one-dimensional arrays containing the grid-point locations along the stretched coordinate as part of the IVE input data. Staggered meshes can also be accommodated by setting \"attributes\" in the input data. The structure of more complicated curvilinear grids must be communicated to IVE via user supplied \"transforms,\" which define the mapping between physical space and the array indices.\nSince four-dimensional data cannot be directly displayed on a flat computer screen, it is necessary to reduced the dimensionality of the data before it is displayed. One of IVE's primary capabilities involves dimension reduction or \"data slicing.\" IVE allows the user to display lower-dimensional subsets of the data by fixing a coordinate or by averaging over the coordinate.\nIVE currently has the capability to display\nscalar fields as\n"}, {"score": 1175.8287, "uuid": "8ec52df8-2631-5026-bc7f-2f3d956264b7", "index": "cw12", "trec_id": "clueweb12-0303wb-27-25048", "target_hostname": "www.voidspace.org.uk", "target_uri": "http://www.voidspace.org.uk/ironpython/python-for-programmers.shtml", "page_rank": 1.226992e-09, "spam_rank": 75, "title": "<em>Python</em> <em>for</em> .NET Programmers", "snippet": "<em>I</em> also know a <em>Python</em> programmer (<em>Python</em> and SQL actually) who does much of his work at the interactive interpreter - he likes to suck <em>data</em> in, transform it, push it back out and then walk away.", "explanation": null, "document": "if 1 > 2: print 'not possible' else: print \"that's better\"\nIteration (looping):\nfor a in range(100): if a % 2: # % is the modulo operator continue print a else: # only entered if the loop is # exited normally (without a break) pass\nThe while loop:\na = [1, 2, 3, 4] while a: b = a.pop() if b > 3: break else: # only entered if the loop doesn't break pass\nException handling:\ntry: raise Exception('boom') except: print 'an exception was raised' try: raise Exception except Exception, e: print 'Exception', e try: raise KeyError('ouch!') except (IOError, KeyError), e: # a bare raise re-raises the last exception raise else: # entered if no exception is raised pass try: pass finally: print 'a finally block'\nFunctions\nWe've already seen the basic syntax for defining functions. Arbitrary number of arguments can be collected with *args syntax (equivalent to the .NET params and collected as a tuple) and an arbitrary number of keyword arguments can be collected as a dictionary with **kwargs syntax.:\ndef function(*args, **kwargs): assert isinstance(args, tuple), 'args is always a tuple' assert isinstance(kwargs, dict), 'kwargs is always a dictionary'\nassert is a statement - it can be used for runtime design by contract. isinstance is one of Python's built-in functions. If there is no explicit return statement then a function returns None.\n*args and **kwargs can be used to call functions with multiple arguments / keyword arguments from a tuple or dictionary:\na = (1, 2, 3) b = {1: 'one', 2: 'two'} result = function(*args, **kwargs)\nPython also has anonymous functions; lambdas. Lambda functions can take arguments but the body can only be an expression. When the lambda function is called the expression is evaluated and the result returned:\n>>> anonymous = lambda arg: arg * 2 >>> anonymous(3) 6\nThe above lambda function is exactly equivalent to:\ndef anonymous(arg): return arg * 2\nClasses\nWe've also seen the basic syntax for classes. Methods are created using the same syntax as normal function definitions (which is what they are) - but instance methods explicitly take self as the first parameter. self is the equivalent of this in C#, it is the current instance in use, and is passed in automatically as the first parameter:\nclass ClassName(object): def print_self(self): print self\n>>> first = ClassName() >>> first.print_self() <__main__.ClassName object at 0x780d0> >>> second = ClassName() >>> second.print_self() <__main__.ClassName object at 0x780f0>\nPython doesn't have explicit access modifiers (no public, protected, etc). You'll be surprised by how little you miss them...\nUnlike C# Python doesn't have method overloading. If you need this you can collect arguments with the * and ** syntax and do dynamic dispatch on the type or number of arguments. There are external libraries that use decorators (explained shortly) to implement generic functions, a more general system of overloading.\nClass bodies can contain assignment statements. These create class attributes that are shared between all instances. Updating a class attribute will make the change visible to all instances:\nclass ClassName(object): attribute = 3\n>>> first = ClassName() >>> first.attribute 3 >>> second = ClassName() >>> second.attribute 3 >>> ClassName.attribute = 6 >>> first.attribute 6 >>> second.attribute 6\nYou can even put arbitrary code in the body of the class. This can be useful for providing different implementations of methods on different platforms, but isn't a very common technique in practise:\nimport sys class ClassName(object): if sys.platform == 'cli': def method(self): # implementation for IronPython else: def method(self): # implementation for other platforms\nInheritance works straightforwardly in Python, until you start using multiple inheritance that is:\nclass BaseClass(object): def method(self): print 'method on base' def other_method(self): print 'other_method on base' class SomeClass(BaseClass): def other_method(self): print 'other_method on some class' BaseClass.other_method(self)\n>>> something = SomeClass() >>> something.method() method on base >>> something.other_method() other_method on some class other_method on base\nThe explicit self parameter makes it very easy for inherited methods to call up to the methods they override on a base class.\nMultiple inheritance is perfectly valid in Python, but should not be overused. It is most often used to provide mixin functionality.\nPython magic methods\nWe've seen that the constructor for Python classes is the oddly named __init__ method. Methods that start and end with double-underscores (often shortened to 'dunder-method name' for convenience) are special methods, called the 'magic methods'. These implement Python protocols, roughly the equivalent of interfaces in C#, and are called for you by the interpreter rather than being explicitly called by the programmer (usually anyway).\nThere are lots of different protocol, you can find a good reference to all the Python magic methods in IronPython in Action or online on the book website . If you come across a protocol method that you don't recognise this is the place to turn.\nInterfaces are used in C# to specify behavior of objects. For example, if a class implements the IDisposable interface, then you can provide a Dispose method to release resources used by your objects. .NET has a whole host of interfaces, and you can create new ones. If a class implements an interface, it provides a static target for the compiler to call whenever an operation provided by that interface is used in your code.\nIn Python, you don\u2019t need to provide static targets for the compiler, and you can use the principle of duck typing. Many operations are provided through a kind-of-soft interface mechanism called protocols. This isn\u2019t to say that formal interface specification is decried in Python\u00e2\u0080\u0094how could you use an API if you didn\u2019t know what interface it exposed?\u00e2\u0080\u0094but, again, Python chooses not to enforce this in the language design.\nThe mapping and sequence protocols use the __getitem__ and __setitem__ methods:\nclass DataStore(object): def __init__(self): self._store = {} def __getitem__(self, name): return self._store[name] def __setitem__(self, name, value): return self._store[name] = value\n>>> store = DataStore() >>> store['foo'] = 'bar' >>> store['foo'] 'bar'\nThe consequence of this is that Python programmers are much more interested in the behavior of objects than the type. It is common to see the specification of a function or a method that it takes a mapping type or a sequence type - meaning any object that implements these methods. This is the essence of duck-typing, if you know what methods / properties of objects are used you can provide an alternative implementation. So long as the object quacks like a duck and walks like a duck Python will treat it like a duck...\nThere are lots of other standard protocol methods for containers, here are a few of them:\nclass DataStore(object): def __init__(self): self._store = {} def __getitem__(self, name): return self._store[name] def __setitem__(self, name, value): return self._store[name] = value def __len__(self): # number of elements return len(self._store) def __nonzero__(self): # boolean value return bool(self._store) def __iter__(self): # iteration return iter(self._store) def __contains__(self, name): # membership test return name in self._store\nOther protocols include the rich comparison methods (__eq__, __lt__ and friends) the numeric methods (__add__, __sub__ and friends) plus a whole host more. Python supports operator overloading, and implementing protocol methods is how you do it.\nThere are also some special magic methods we can use to customize attribute access, particularly useful for creating fluent interfaces. The three methods are __getattr__ for fetching attributes, __setattr__ for setting them and __delattr__ for deleting them. Here's an example using __getattr__ to build up messages:\nclass Fluid(object): def __init__(self): self._message = [] def __getattr__(self, name): self._message.append(name) return self def __str__(self): # using the join method on string return ' '.join(self._message).strip()\n>>> f = Fluid() >>> f.hello.everyone.welcome.to.Python <__main__.Fluid object at 0x782b0> >>> str(f) 'Hello everyone welcome to Python'\nYou've probably already used an API built in a similar way in Javascript, where you an traverse the DOM as attributes on document. Creating APIs like this is very easy in dynamic languages. __getattr__ and friends have some complexity, so it is worth reading up on them if you want to use them. They're covered in IronPython in Action.\nAlthough these methods allow us to implement custom behavior for attribute access, they aren't a replacement for properties which we'll look at next.\nProperties & decorators\nWe haven't yet looked at properties in Python. Instead of having first class syntax for properties Python uses the 'descriptor protocol', along with normal Python syntax, to provide class methods, static methods and properties. The descriptor protocol is considered to be fairly deep Python 'magic'. It's actually fairly easy to understand but beyond the scope of this article. Let's look at how we use the built-in classmethod, staticmethod and property descriptors.\nThe easiest way to use these descriptors are as decorators. Decorators are a way of transforming functions and methods and are nominally similar to .NET attributes or Java annotations. They work due to the way that functions are first class objects in Python and are examples of higher order functions (functions that receive functions as arguments or return functions).\nThe syntax to create a static method in Python is:\nclass Static(object): @staticmethod def static_one(): return 1 @staticmethod def static_two(): return 2\n>>> Static.static_one() 1 >>> Static.static_two() 2\nA class method is a method that receives the class as the first argument instead of the instance. They are often used to create alternative constructors. There isn't much need for static methods in Python.\nstaticmethod behaves like a function (it is actually a type), it wraps the methods. The @ syntax is pure syntactic sugar. The following two snippets of code are identical:\n@decorator def function(): pass def function(): pass function = decorator(function)\nThe decorator is called with the function it wraps as an argument. The function name is bound to whatever the decorator returns. Here's a decorator that checks arguments for null values (None):\ndef checkarguments(function): def decorated(*args): if None in args: raise TypeError(\"Invalid Argument\") return function(*args) return decorated class MyClass(object): @checkarguments def method(self, arg1, arg2): return arg1 + arg2\n>>> instance = MyClass() >>> instance.method(1, 2) 3 >>> instance.method(2, None) Traceback (most recent call last): ... TypeError: Invalid Argument\nThe checkarguments decorator takes a function as the argument. It creates a new inner function, which it returns. When this function is called it checks all the arguments and then calls the original function, which it still has a reference to through the closure. It uses the *args syntax to collect all the arguments the method is called with and then call the original method with the same arguments.\nPython 2.6 introduces class decorators in addition to function / method decorators. They also wrap functions and can be used for many of the same purposes as metaclasses (for checking or transforming classes).\nWe can use property as a decorator to create get only properties:\nclass SomeClass(object): @property def three(self): print 'Three fetched' return 3\n>>> something = SomeClass() >>> something.three Three fetched 3\nThe old way of creating get and set properties (in Python you can also use properties to customize deletion but it is there for symmetry and not used very often) is less attractive. This is one area where the C# syntax is nicer than Python:\nclass SomeClass(object): def get_three(self): print 'Three fetched' return 3 def set_three(self, value): if value != 3: raise ValueError('Three has to be equal to 3!') three = property(get_three, set_three)\n>>> something = SomeClass() >>> something.three Three fetched 3 >>> something.three = 4 Traceback (most recent call last): ... ValueError: Three has to be equal to 3!\nPython 2.6 introduces a new technique that is slightly better looking:\nclass SomeClass(object): @property def three(self): return 3 @three.setter def three(self, value): if value != 3: raise ValueError('Three has to be equal to 3!')\nStill not as nice as C#, but better...\nModules and Packages\nThe last thing you want when programming is to have all your code contained in a single monolithic file. This makes it almost impossible to find anything. Ideally, you want to break your program down into small files containing only closely related classes or functionality. In Python, these are called modules.\nNote\nA module is a Python source file (a text file) whose name ends with .py. Objects (names) defined in a module can be imported and used elsewhere. They\u2019re very different from .NET modules, which are partitions of assemblies.\nThe import statement has several different forms.\nimport module from module import name1, name2 from module import name as anotherName from module import *\nImporting a module executes the code it contains and creates a module object. The names you\u2019ve specified are then available from where you imported them.\nIf you use the first form, you receive a reference to the module object. Needless to say, these are first-class objects that you can pass around and access attributes on (including setting and deleting attributes). If a module defines a class SomeClass, then you can access it using module.SomeClass.\nIf you need access to only a few objects from the module, you can use the second form. It imports only the names you\u2019ve specified from the module.\nIf a name you wish to import would clash with a name in your current namespace, you can use the third form. This imports the object you specify, but binds it to an alternative name.\nThe fourth form is the closest to the C# using directive. It imports all the names (except ones that start with an underscore) from the module into your namespace. In Python, this is generally frowned on. You may import names that clash with other names you\u2019re using without realizing it; when reading your code, it\u2019s not possible to see where names are defined.\nPython allows you to group related modules together as a package. The structure of a Python package, with subpackages, is shown in the image below.\nNote\nA package is a directory containing Python files and a file called __init__.py. A package can contain sub- packages (directories), which also have an __init__.py. Directories and subdirectories must have names that are valid Python identifiers.\nA package is a directory on the Python search path. Importing anything from the package will execute __init__.py and insert the resulting module into sys.modules under the package name. You can use __init__.py to customize what importing the package does, but it\u2019s also common to leave it as an empty file and expose the package functionality via the modules in the package.\nYou import a module from a package using dot syntax.\nimport package.module from package import module\nPackages themselves may contain packages; these are subpackages. To access subpackages, you just need to use a few more dots.\nimport package.subpackage.module from package.subpackage import module\nPython also contains several built-in modules. You still need to import these to have access to them, but no code is executed when you do the import. We mention these because one of them is very important to understanding imports. This is the sys module.\nWhen you import a module, the first thing that Python does is look inside sys.modules to see if the module has already been imported. sys.modules is a dictionary, keyed by module name, containing the module objects. If the module is already in sys.modules, then it will be fetched from there rather than re-executed. Importing a module (or name) from different places will always give you a reference to the same object.\nIf the module hasn\u2019t been imported yet, Python searches its path to look for a file named module.py. If it finds a Python file corresponding to the import, Python executes the file and creates the module object. If the module isn\u2019t found, then an ImportError is raised.\nAs well as searching for a corresponding Python file, IronPython looks for a package directory, a built-in module, or .NET classes. You can even add import hooks to further customize the way imports work.\nThe list of paths that Python searches is stored in sys.path. This is a list of strings that always includes the directory of the main script that\u2019s running. You can add (or remove) paths from this list if you want.\nSome Python files can be used both as libraries, to be imported from, and as scripts that provide functionality when they\u2019re executed directly. For example, consider a library that provides routines for converting files from one format to another. Programs may wish to import these functions and classes for use within an application, but the library itself might be capable of acting as a command-line utility for converting files.\nIn this case, the code needs to know whether it\u2019s running as the main script or has been imported from somewhere else. You can do this by checking the value of the variable __name__. This is normally set to the current module name unless the script is running as the main script, in which case its name will be __main__.\ndef main(): \"docstring\" # code to execute functionality # when run as a script if __name__ == '__main__': main()\nThis segment of code will only call the function main if run as the main script and not if imported.\nPython has lots of other language features that make it a pleasure to work with. These features include:\ntuple unpacking\na, b = (1, 2) a, b = get_tuple() for a, b, (c, d) in some_iterator: pass\nlist comprehensions and generator expressions. These allow you to combine a loop and a filter in a single expression (similar to LINQ over objects).\n>>> # list comprehensions are eager >>> a = [value ** 2 for value in some_iterator if value > minimum] >>> # generator expressions are lazy >>> a = (value ** 2 for value in some_iterator if value > minimum) >>> a <generator object at 0x77be8>\niterators and generators (iterators are implemented with the __iter__ and next protocol methods whilst Python's yield is similar to C#'s Yield Return but with added capabilities ).\nthe with statement (similar to the C# using statement but able to detect and optionally handle exceptional exits)\nternary expressions (unlike other languages the expression in the middle is evaluated first. If this evaluates to True then the left hand expression is evaluated and returned otherwise the right hand expression is evaluated and returned.)\na = 1 if x > 3 else None\nScripting\nPython is sometimes categorized as a 'scripting language', implying it is only suitable for scripting tasks. Whilst that certainly isn't true Python does make an excellent scripting language.\nIf you want to write a script to automate a regular task, you aren\u2019t forced to write an object-oriented application; you aren\u2019t even forced to write functions if the task at hand doesn\u2019t call for them. This next listing is a script for a typical admin task of clearing out the temp folder of files that haven\u2019t been modified for more than seven days.\nimport os, stat from datetime import datetime, timedelta tempdir = os.environ[\"TEMP\"] max_age = datetime.now() - timedelta(7) for filename in os.listdir(tempdir): path = os.path.join(tempdir, filename) if os.path.isdir(path): continue date_stamp = os.stat(path).st_mtime mtime = datetime.fromtimestamp(date_stamp) if mtime < max_age: mode = os.stat(path).st_mode os.chmod(path, mode | stat.S_IWRITE) os.remove(path)\nPython has a rich tradition of being used for shell scripting, particularly on the Linux platform.\nProcedural\nThe code above works fine, but it runs whenever the script is executed and so isn't reusable. We can make it more useful by refactoring it into functions. If we have an if __name__ == '__main__' block then the Python file retains the same behavior when executed as a script but also behaves as a module that can be imported.\nimport os, stat from datetime import datetime, timedelta tempdir = os.environ[\"TEMP\"] max_age = datetime.now() - timedelta(7) def delete_old_files_in_directory(directory): for filename in os.listdir(directory): path = os.path.join(directory, filename) if os.path.isdir(path): continue delete_old_file(path) def delete_old_file(path, max_age=max_age): date_stamp = os.stat(path).st_mtime mtime = datetime.fromtimestamp(date_stamp) if mtime < max_age: mode = os.stat(path).st_mode os.chmod(path, mode | stat.S_IWRITE) os.remove(path) if __name__ == '__main__': delete_old_files_in_directory(tempdir)\nFunctional\nFunctions are first class objects. First class functions, in combination with closures, make higher-order functions (functions that take or return functions) common in Python.\nClosures are a fundamental concept to functional programming. A closure is basically a scope. Functions have access to all the variables in their enclosing scope. When you create a function it is said to 'close over' all the variables in its scope that it uses. Here's a simple closure:\n>>> def f(): ... a = 1 ... def inner(): ... print a ... return inner ... >>> function = f() >>> function() 1\nThe inner function has access to ('closes over') the variable a defined in its enclosing scope. When f is called it returns the inner function. When the inner function is called it prints the value of the variable a from the scope it was defined in.\nParameters that are passed into a function become local variables within the scope of the function. We can use this to create function factories based on the parameters we pass in. Every time a function is called a new scope is created, so our factories can be called multiple times with different values.\ndef makeAdder(x): def adder(y): return x + y return adder\n>>> add3 = makeAdder(3) >>> add3(5) 8 >>> add2 = makeAdder(2) >>> add2(2) 4\nIn makeAdder we bind the inner function to the argument (x) passed in. makeAdder returns a new function that takes a single argument (y). When this new function is called it returns the result of adding the new argument to the original value of x when it was created.\nThe generalisation of this is called partial application. In the next example the function partial takes a function that takes two arguments (as its first argument - func) and the first argument (x). partial returns a new function with its first argument bound to it.\nThe returned function (called inner) takes one argument, and when called it calls the original function with its first argument and the new one. This sounds more complicated than it is. We can rewrite makeAdder to use it:\ndef add(x, y): return x + y def partial(func, x): def inner(y): return func(x, y) return inner\n>>> add2 = partial(add, 2) >>> add2(3) 5 >>> add1 = partial(add, 1) >>> add1(1) 2\nThese are examples of the common pattern, the 'factory function'; functions that return a function specialised on the input parameters passed to them.\nAn extension of this is the 'class factory', which is a function that returns a class specialized on its input. We saw an example of this from the Python standard library in the Introduction to IronPython article - in the form of namedtuple.\nThe most basic form of metaprogramming is code-generation and execution at runtime, which in Python means exec and eval. eval is for evaluating expressions and returning a result:\n>>> a = eval(\"1 + 2\") >>> a 3\nFor executing statements we can use the exec statement. We can use a dictionary as the namespace the code is executed in. If the code creates variables, functions or classes then they will be accessible in the dictionary after execing:\n>>> namespace = {} >>> code = \"\"\" ... def function(): ... print 'w00t!' ... a = 3\"\"\" >>> >>> exec code in namespace >>> namespace['a'] 3 >>> function = namespace['function'] >>> function() w00t! >>>\nCode generation is a bit of a blunt instrument when it comes to metaprogramming. A more common way of metaprogramming in Python is with metaclasses.\nMetaclasses are a particularly interesting feature of Python. Classes are first class objects in Python, and like all objects they have a type. Classes are instances of their metaclass, which defines some of the ways they behave. Metaclasses are seen as advanced Python but although the usecases for implementing them yourself are rare the mechanisms involved are easy to understand. For a good introduction to metaclasses read: Metaclasses in five minutes .\nWhat Next?\nPython is a full programming language and although it is very easy to learn the basics it can take time to become an idiomatic Python programmer. Fortunately there are many free online resources to help. Here are a few of the best ones:\n"}, {"score": 1138.4274, "uuid": "eb584fc8-33ae-5f0a-892b-e7ede3281500", "index": "cw12", "trec_id": "clueweb12-1213wb-15-28891", "target_hostname": "dalkescientific.com", "target_uri": "http://dalkescientific.com/training/", "page_rank": 1.7428308e-09, "spam_rank": 69, "title": "Dalke Scientific: <em>Python</em> Training <em>for</em> Cheminformatics", "snippet": "<em>R</em> is a great software environment <em>for</em> statistical computing and generating plots. If you are building models <em>or</em> doing <em>data</em> mining then you <em>should</em> know about this project. <em>R</em> includes its own programming language and a number of high-quality <em>analysis</em> packages.", "explanation": null, "document": "trainingdiscard@dalkescientific.com\nPython Training for Cheminformatics\nI teach training courses in Python programming for computational chemistry, with an emphasis on cheminformatics. No public courses are currently scheduled. I will be in Ireland for OpenEye's CUP conference in September and in England in October. If you are interested in on-site training during those trips, let me know!\nTo apply or ask questions, send email to trainingdiscard@dalkescientific.com . Up-to-date information can always be found at http://dalkescientific.com/training/ .\nWhy programming courses for chemists?\nComputational chemists are not programmers, but programming is an essential skill for developing new algorithms, generating data, and analyzing results. Most working scientists have little training in programming and end up spending a lot of time figuring out how to parse a file format or work with a software library, rather than figuring out the science.\nMy training course is designed for just these people.\nI teach both corporate courses and public ones. The presentations are a mixture of lecture and hands-on exercises in a similar style to my NBN courses . The specific topics will vary based on the needs of the audience. Contact me if there's something you specifically want me to discuss, and see the bottom of this page for a partial list of topics I can cover.\nMy name is Andrew Dalke. I am a professional software developer with years of experience creating tools for cheminformatics, molecular modeling, bioinformatics, and related fields. Some the more public projects I've been part of are VMD, NAMD, BioPython, PyDaylight, and the Open Bioinformatics Foundation. Thoughout my career I have worked closely with chemists to help them be more effective, by developing software, providing one-on-one advice, training, and writing essays about software side of this field .\nMost of my work over the last 10 years has been in Python, which is the most popular high-level language in computational chemistry. Many tools, especially in molecular visualization and chemical informatics, have Python interfaces. Python is also one of the most popular computer languages in the world, with mature software libraries for everything from image manipulation and SQL databases to GUI and web development. I am a member of the Python Software Foundation, which is the non-profit that holds the copyright to Python.\nScheduled courses\nThe following courses are meant for computational chemists with some programming background. It is not an introduction to programming course. You must know how to write programs * and should have some experience with Python.\nCourse fees include coffe breaks, lunch, and all presentation materials. Each day starts at 9.00 and ends around 17.30.\nIf you have any questions, send me an email at trainingdiscard@dalkescientific.com . If you are from an academic or non-profit group then you may qualify for a discount.\n"}, {"score": 1124.5546, "uuid": "9621734a-d181-574f-af47-ed127b385af8", "index": "cw12", "trec_id": "clueweb12-0200wb-03-08165", "target_hostname": "www.developer.com", "target_uri": "http://www.developer.com/lang/other/article.php/628641/Learn-to-Program-using-Python-Strings-Part-II.htm", "page_rank": 1.1722054e-09, "spam_rank": 85, "title": "<em>Learn</em> to Program using <em>Python</em>: Strings, Part II - Developer.com", "snippet": "<em>For</em> example, if you were very careful, you could use a felt tip pen to assign an ordinal index to each of the twelve eggs contained in a carton containing a dozen eggs. (<em>Should</em> you start with zero <em>or</em> one?)", "explanation": null, "document": "You have learned how to write some simple programs and execute them interactively.\nYou have learned how to capture simple programs in script files and to execute those script files.\nYou have learned how to construct programs, including the indentation concepts involved in Python.\nYou have also learned some of the fundamental concepts involving strings.\nWhat you will learn\nThis lesson will expand your knowledge of strings, and in addition will introduce you to some concepts that will be useful with other data types as well: indexing and slicing.\nWhat is indexing?\nAccording to a definition that I found on the web, \"... an ordinal number is an adjective which describes the numerical position of an object, e.g., first, second, third, etc.\"\nA practical example\nMany years ago when I did a tour of duty as an enlisted man in the U.S. Air Force, they had a habit of lining us up and requiring us to \"count off.\"\nWhat this meant was that the first person in the line called out the number one, the person behind him called out the number two, the person behind him called out the number three, etc.\u00a0 (Since learning about computer programming, I now wonder if the first person should have called out zero.)\nAssigning an ordinal index\nI'm sure they didn't realize that what they were doing was assigning an ordinal index value to each person in the line (and neither did I at the time).\nUsing an ordinal index\nEven though they didn't know the technical details of ordinal indices, they didn't have any difficulty saying, \"Number six, wash dishes, number fourteen, peel potatoes, number twenty-two, carry out the garbage, etc.\"\nThis is indexing\nThat is what indexing is all about.\nIn the context of this lesson, indexing is the process of assigning an ordinal index value to each data item contained in some sort of a container.\nIn other words, we assign an ordinal number to each item, which describes the numerical position of the item in the container.\nFor example, if you were very careful, you could use a felt tip pen to assign an ordinal index to each of the twelve eggs contained in a carton containing a dozen eggs.\u00a0 (Should you start with zero or one?)\nThen you could extract the egg whose index value is 9 from the container and eat it for breakfast.\nHaving assigned the index, we can use that index to access the data item corresponding to that index, as in \"Number six, wash dishes.\"\n(Note that this process is also referred to as a subscription in the Python Reference Manual.)\nIndex values automatically assigned\nIn this lesson, we will be using the index values that are automatically assigned to the characters in a string for the purpose of accessing those characters, both individually, and in groups.\nWhat is slicing?\nHere is what Magnus Lie Hetland has to say on the topic of slicing (and indexing as well.)\u00a0 Although this quotation was taken from a discussion of lists, it applies equally well to strings.\n\"One of the nice things about lists is that you can access their elements separately or in groups, through indexing and slicing.\nIndexing is done (as in many other languages) by appending the index in brackets to the list. (Note that the first element has index 0). (This is the answer to the question about the first egg -- Baldwin)\n...\nSlicing is almost like indexing, except that you indicate both the start and stop index of the result, with a colon (\":\") separating them:\n..\nNotice that the end is non-inclusive. If one of the indices is dropped, it is assumed that you want everything in that direction. i.e. list[:3] means \"every element from the beginning of list up to element 3, non-inclusive.\"\n...\nlist[3:] would, on the other hand, mean \"every element from list, starting at element 3 (inclusive) up to, and including, the last one.\"\nFor really interesting results, you can use negative numbers too: list[-3] is the third element from the end of the list...\"\nSome material deleted for brevity\nI added the boldface and the red comment for emphasis.\u00a0 I also deleted some of the material from this quotation for brevity, but I will cover that material later in conjunction with my discussion of indexing and slicing strings.\nA Sample Program\nI will illustrate indexing and slicing of strings using a sample program contained in a script file named String01.py.\nThe program listing\nA complete listing of the program, and the output produced by the program, are provided at the end of the lesson.\nWill discuss in fragments\nI will discuss the program in fragments, illustrating particular aspects of indexing and slicing in each fragment.\u00a0 This is a scheme that I will use frequently in this set of tutorial lessons.\nInteresting Code Fragments\nA single character can be extracted from a string by referring to the string and indicating the index of the character in square brackets, as shown in the code fragment in Figure 1.\naStr = \"This is a string\"\nprint aStr[0] #print T\nprint aStr[3] #print s\nFigure 1\n(Note that this is a fragment from a script file, not from an interactive Python program.)\nFirst create a string to work with\nThe fragment in Figure 1 creates a string (highlighted in boldface) and assigns it to a variable named aStr.\u00a0 From this point on, the contents of the string can be accessed by referring to the variable.\nIndex values always begin with zero\nUnlike eggs and Air Force enlisted men, the first character in a string is always located at index 0, as in aStr[0].\nThus, the second statement in the fragment extracts and prints the T, which is the first character in the word This.\nCharacter at index 3, display yourself\nSimilarly, the last statement in the fragment extracts and prints the s from index position 3, as in aStr[3].\nThe character at this index position is the s that ends the word This.\nThe last statement is equivalent to the following request, \"Will the character at index position 3 please display yourself on the screen.\"\nIs this the fourth character?\n(You would probably refer to this as the fourth character, and you would be correct if you did.\u00a0 The character at index 3 is the fourth character in the string.\u00a0 The character at index 0 is the first character in the string.\u00a0 First does not equate to index 1.\u00a0 You need to think about this, because this can be a confusing topic for new programmers.)\nAn important, and potentially confusing point\nAt the risk of becoming boring, there is an important point here that you might as well get used to right now.\nThe s is located at index value 3.\u00a0 However, according to the way you are probably accustomed to counting, this is actually the fourth character in the string.\u00a0 You might be inclined to refer to this character as character number 4.\nThis is because index values always begin with zero, while you are probably accustomed to counting things beginning with one, not zero.\nNot like eggs\nIf you access the egg at index value 4 in the container and eat it for breakfast, it cannot be accessed again (because it will be gone).\nHowever, if you access the character at index value 4 in the string and use it for some purpose, what you really use is a copy of the character.\u00a0 It is still there and can be accessed again.\n(Some data containers do allow for the removal of data elements in much the same sense that we can remove an egg from its container.\u00a0 However, a string is not such a container.)\nA simple slice\nFor convenience, here is another copy of the fragment that created the string.\naStr = \"This is a string\"\nThe fragment in Figure 2 cuts a couple of slices out of that string and displays them on the screen.\nprint aStr[0:4] #print This\nprint aStr[10:16] #print string\nFigure 2\nSlice Notation\nThe slice notation uses two index values separated by a colon, as shown in boldface in Figure 2.\nThe end is non-inclusive\nAs was indicated in the earlier quotation, \"... the end is non-inclusive.\"\u00a0 This means that the character whose index value is the number following the colon is not included in the slice.\nExtract the first word in the string\nThus, the first statement containing the reference aStr[0:4] extracts and prints the character sequence beginning with index value 0 and ending with index value 3 (not 4).\u00a0 This causes the word This to be extracted and printed.\nExtract the last word in the string\nSimilarly, the second statement in the above fragment (aStr[10:16]) extracts and prints the characters having index values from 10 through 15, inclusive (not 16).\u00a0 This causes the word string to be extracted and printed.\nOmitting the first index\nIf you omit the first index value, as shown in Figure 3, it defaults to the value zero.\nprint aStr[:4] #print This\nFigure 3\nTherefore, the statement in Figure 3 extracts and prints the first word in the string, which is This.\nOmitting the second index\nIf you omit the second index, as shown in Figure 4, it defaults to a value that includes the last character in the string.\nprint aStr[10:] #print string\nFigure 4\nThus, the statement in Figure 4 extracts and prints the last word in the string, which is string.\nPrint the entire string\nFigure 5 shows two different ways to extract and print the entire string.\u00a0 I won't comment on this, but will leave the analysis as an exercise for the student.\nprint aStr[:5] + aStr[5:]\nprint aStr[:100]\n(Hint:\u00a0 Remember that the plus sign when used with strings is the string concatenation operator.)\nPrint an empty string\nThere are several ways that you can specify the index values that will produce an empty string.\u00a0 One of those ways is shown following the plus sign in Figure 6.\nprint \"Empty: \" + aStr[16:100]\nFigure 6\nIn Figure 6, both index values are outside the bounds of the index values of the characters in the string, which range from 0 through 15 inclusive.\nNegative indices\nAlthough it can be a little confusing, negative index values can be used to count from the right, as shown in Figure 7.\nprint aStr[-5:-2] #print tri\nFigure 7\nThis fragment extracts and prints the characters tri from the word string, which is the last word in the string.\nEliminating confusion\nOnce you allow negative indices for slicing, thing can become very confusing.\u00a0 The following explanation of how indices work with slicing is attributed to Guido van Rossum .\nIn this example, Mr. van Rossum is referring to a five-character string with a value of \"HelpA\".\nThe best way to remember how slices work is to think of the indices as pointing between characters, with the left edge of the first character numbered 0. Then the right edge of the last character of a string of n characters has index n, for example:\n+---+---+---+---+---+\n| H | e | l | p | A |\n+---+---+---+---+---+\n0\u00a0\u00a0 1\u00a0\u00a0 2\u00a0\u00a0 3\u00a0\u00a0 4\u00a0\u00a0 5\n-5\u00a0 -4\u00a0 -3\u00a0 -2\u00a0 -1\nThe first row of numbers gives the position of the indices 0...5 in the string; the second row gives the corresponding negative indices.\nThe slice from i to j consists of all characters between the edges labeled i and j, respectively.\nFor nonnegative indices, the length of a slice is the difference of the indices, if both are within bounds, e.g., the length of word[1:3] is 2.\nHopefully, this explanation will help you to understand and to remember how index values are used for the extraction of substrings from strings using slicing.\nGetting the length of a string\nAnd finally, a built-in function named len() can be used to determine the number of characters actually contained in a string as shown in Figure 8.\nprint len(aStr)\nFigure 8\nFor the example string used in this lesson, Figure 8 gets and prints the length of the string as 16.\nIf you count the characters in the string (beginning with 1), you will conclude that there are 16 characters in the string.\nNote the difference between the number of characters and the maximum index value\nFor a string containing 16 characters, the valid index values range from 0 through 15 inclusive.\nThe complete output\nThis Python script file produces the output shown in Figure 9 on my computer (boldface added for emphasis).\nD:\\Baldwin\\AA-School\\PyProg>python strings01.py\nT\n"}, {"score": 1111.973, "uuid": "5e841adf-5d64-5388-8de6-3369cdec2cc8", "index": "cw12", "trec_id": "clueweb12-0112wb-63-17962", "target_hostname": "www.developer.com", "target_uri": "http://www.developer.com/open/article.php/628641", "page_rank": 1.1728359e-09, "spam_rank": 85, "title": "<em>Learn</em> to Program using <em>Python</em>: Strings, Part II - Developer.com", "snippet": "<em>For</em> example, if you were very careful, you could use a felt tip pen to assign an ordinal index to each of the twelve eggs contained in a carton containing a dozen eggs. (<em>Should</em> you start with zero <em>or</em> one?)", "explanation": null, "document": "You have learned how to write some simple programs and execute them interactively.\nYou have learned how to capture simple programs in script files and to execute those script files.\nYou have learned how to construct programs, including the indentation concepts involved in Python.\nYou have also learned some of the fundamental concepts involving strings.\nWhat you will learn\nThis lesson will expand your knowledge of strings, and in addition will introduce you to some concepts that will be useful with other data types as well: indexing and slicing.\nWhat is indexing?\nAccording to a definition that I found on the web, \"... an ordinal number is an adjective which describes the numerical position of an object, e.g., first, second, third, etc.\"\nA practical example\nMany years ago when I did a tour of duty as an enlisted man in the U.S. Air Force, they had a habit of lining us up and requiring us to \"count off.\"\nWhat this meant was that the first person in the line called out the number one, the person behind him called out the number two, the person behind him called out the number three, etc.\u00a0 (Since learning about computer programming, I now wonder if the first person should have called out zero.)\nAssigning an ordinal index\nI'm sure they didn't realize that what they were doing was assigning an ordinal index value to each person in the line (and neither did I at the time).\nUsing an ordinal index\nEven though they didn't know the technical details of ordinal indices, they didn't have any difficulty saying, \"Number six, wash dishes, number fourteen, peel potatoes, number twenty-two, carry out the garbage, etc.\"\nThis is indexing\nThat is what indexing is all about.\nIn the context of this lesson, indexing is the process of assigning an ordinal index value to each data item contained in some sort of a container.\nIn other words, we assign an ordinal number to each item, which describes the numerical position of the item in the container.\nFor example, if you were very careful, you could use a felt tip pen to assign an ordinal index to each of the twelve eggs contained in a carton containing a dozen eggs.\u00a0 (Should you start with zero or one?)\nThen you could extract the egg whose index value is 9 from the container and eat it for breakfast.\nHaving assigned the index, we can use that index to access the data item corresponding to that index, as in \"Number six, wash dishes.\"\n(Note that this process is also referred to as a subscription in the Python Reference Manual.)\nIndex values automatically assigned\nIn this lesson, we will be using the index values that are automatically assigned to the characters in a string for the purpose of accessing those characters, both individually, and in groups.\nWhat is slicing?\nHere is what Magnus Lie Hetland has to say on the topic of slicing (and indexing as well.)\u00a0 Although this quotation was taken from a discussion of lists, it applies equally well to strings.\n\"One of the nice things about lists is that you can access their elements separately or in groups, through indexing and slicing.\nIndexing is done (as in many other languages) by appending the index in brackets to the list. (Note that the first element has index 0). (This is the answer to the question about the first egg -- Baldwin)\n...\nSlicing is almost like indexing, except that you indicate both the start and stop index of the result, with a colon (\":\") separating them:\n..\nNotice that the end is non-inclusive. If one of the indices is dropped, it is assumed that you want everything in that direction. i.e. list[:3] means \"every element from the beginning of list up to element 3, non-inclusive.\"\n...\nlist[3:] would, on the other hand, mean \"every element from list, starting at element 3 (inclusive) up to, and including, the last one.\"\nFor really interesting results, you can use negative numbers too: list[-3] is the third element from the end of the list...\"\nSome material deleted for brevity\nI added the boldface and the red comment for emphasis.\u00a0 I also deleted some of the material from this quotation for brevity, but I will cover that material later in conjunction with my discussion of indexing and slicing strings.\nA Sample Program\nI will illustrate indexing and slicing of strings using a sample program contained in a script file named String01.py.\nThe program listing\nA complete listing of the program, and the output produced by the program, are provided at the end of the lesson.\nWill discuss in fragments\nI will discuss the program in fragments, illustrating particular aspects of indexing and slicing in each fragment.\u00a0 This is a scheme that I will use frequently in this set of tutorial lessons.\nInteresting Code Fragments\nA single character can be extracted from a string by referring to the string and indicating the index of the character in square brackets, as shown in the code fragment in Figure 1.\naStr = \"This is a string\"\nprint aStr[0] #print T\nprint aStr[3] #print s\nFigure 1\n(Note that this is a fragment from a script file, not from an interactive Python program.)\nFirst create a string to work with\nThe fragment in Figure 1 creates a string (highlighted in boldface) and assigns it to a variable named aStr.\u00a0 From this point on, the contents of the string can be accessed by referring to the variable.\nIndex values always begin with zero\nUnlike eggs and Air Force enlisted men, the first character in a string is always located at index 0, as in aStr[0].\nThus, the second statement in the fragment extracts and prints the T, which is the first character in the word This.\nCharacter at index 3, display yourself\nSimilarly, the last statement in the fragment extracts and prints the s from index position 3, as in aStr[3].\nThe character at this index position is the s that ends the word This.\nThe last statement is equivalent to the following request, \"Will the character at index position 3 please display yourself on the screen.\"\nIs this the fourth character?\n(You would probably refer to this as the fourth character, and you would be correct if you did.\u00a0 The character at index 3 is the fourth character in the string.\u00a0 The character at index 0 is the first character in the string.\u00a0 First does not equate to index 1.\u00a0 You need to think about this, because this can be a confusing topic for new programmers.)\nAn important, and potentially confusing point\nAt the risk of becoming boring, there is an important point here that you might as well get used to right now.\nThe s is located at index value 3.\u00a0 However, according to the way you are probably accustomed to counting, this is actually the fourth character in the string.\u00a0 You might be inclined to refer to this character as character number 4.\nThis is because index values always begin with zero, while you are probably accustomed to counting things beginning with one, not zero.\nNot like eggs\nIf you access the egg at index value 4 in the container and eat it for breakfast, it cannot be accessed again (because it will be gone).\nHowever, if you access the character at index value 4 in the string and use it for some purpose, what you really use is a copy of the character.\u00a0 It is still there and can be accessed again.\n(Some data containers do allow for the removal of data elements in much the same sense that we can remove an egg from its container.\u00a0 However, a string is not such a container.)\nA simple slice\nFor convenience, here is another copy of the fragment that created the string.\naStr = \"This is a string\"\nThe fragment in Figure 2 cuts a couple of slices out of that string and displays them on the screen.\nprint aStr[0:4] #print This\nprint aStr[10:16] #print string\nFigure 2\nSlice Notation\nThe slice notation uses two index values separated by a colon, as shown in boldface in Figure 2.\nThe end is non-inclusive\nAs was indicated in the earlier quotation, \"... the end is non-inclusive.\"\u00a0 This means that the character whose index value is the number following the colon is not included in the slice.\nExtract the first word in the string\nThus, the first statement containing the reference aStr[0:4] extracts and prints the character sequence beginning with index value 0 and ending with index value 3 (not 4).\u00a0 This causes the word This to be extracted and printed.\nExtract the last word in the string\nSimilarly, the second statement in the above fragment (aStr[10:16]) extracts and prints the characters having index values from 10 through 15, inclusive (not 16).\u00a0 This causes the word string to be extracted and printed.\nOmitting the first index\nIf you omit the first index value, as shown in Figure 3, it defaults to the value zero.\nprint aStr[:4] #print This\nFigure 3\nTherefore, the statement in Figure 3 extracts and prints the first word in the string, which is This.\nOmitting the second index\nIf you omit the second index, as shown in Figure 4, it defaults to a value that includes the last character in the string.\nprint aStr[10:] #print string\nFigure 4\nThus, the statement in Figure 4 extracts and prints the last word in the string, which is string.\nPrint the entire string\nFigure 5 shows two different ways to extract and print the entire string.\u00a0 I won't comment on this, but will leave the analysis as an exercise for the student.\nprint aStr[:5] + aStr[5:]\nprint aStr[:100]\n(Hint:\u00a0 Remember that the plus sign when used with strings is the string concatenation operator.)\nPrint an empty string\nThere are several ways that you can specify the index values that will produce an empty string.\u00a0 One of those ways is shown following the plus sign in Figure 6.\nprint \"Empty: \" + aStr[16:100]\nFigure 6\nIn Figure 6, both index values are outside the bounds of the index values of the characters in the string, which range from 0 through 15 inclusive.\nNegative indices\nAlthough it can be a little confusing, negative index values can be used to count from the right, as shown in Figure 7.\nprint aStr[-5:-2] #print tri\nFigure 7\nThis fragment extracts and prints the characters tri from the word string, which is the last word in the string.\nEliminating confusion\nOnce you allow negative indices for slicing, thing can become very confusing.\u00a0 The following explanation of how indices work with slicing is attributed to Guido van Rossum .\nIn this example, Mr. van Rossum is referring to a five-character string with a value of \"HelpA\".\nThe best way to remember how slices work is to think of the indices as pointing between characters, with the left edge of the first character numbered 0. Then the right edge of the last character of a string of n characters has index n, for example:\n+---+---+---+---+---+\n| H | e | l | p | A |\n+---+---+---+---+---+\n0\u00a0\u00a0 1\u00a0\u00a0 2\u00a0\u00a0 3\u00a0\u00a0 4\u00a0\u00a0 5\n-5\u00a0 -4\u00a0 -3\u00a0 -2\u00a0 -1\nThe first row of numbers gives the position of the indices 0...5 in the string; the second row gives the corresponding negative indices.\nThe slice from i to j consists of all characters between the edges labeled i and j, respectively.\nFor nonnegative indices, the length of a slice is the difference of the indices, if both are within bounds, e.g., the length of word[1:3] is 2.\nHopefully, this explanation will help you to understand and to remember how index values are used for the extraction of substrings from strings using slicing.\nGetting the length of a string\nAnd finally, a built-in function named len() can be used to determine the number of characters actually contained in a string as shown in Figure 8.\nprint len(aStr)\nFigure 8\nFor the example string used in this lesson, Figure 8 gets and prints the length of the string as 16.\nIf you count the characters in the string (beginning with 1), you will conclude that there are 16 characters in the string.\nNote the difference between the number of characters and the maximum index value\nFor a string containing 16 characters, the valid index values range from 0 through 15 inclusive.\nThe complete output\nThis Python script file produces the output shown in Figure 9 on my computer (boldface added for emphasis).\nD:\\Baldwin\\AA-School\\PyProg>python strings01.py\nT\n"}], [{"score": 1093.8126, "uuid": "3cc4d26d-c451-57eb-88c0-1c2abf65c26a", "index": "cw12", "trec_id": "clueweb12-0405wb-12-21312", "target_hostname": "www.developer.com", "target_uri": "http://www.developer.com/article.php/628641", "page_rank": 1.1771407e-09, "spam_rank": 84, "title": "<em>Learn</em> to Program using <em>Python</em>: Strings, Part II - Developer.com", "snippet": "<em>For</em> example, if you were very careful, you could use a felt tip pen to assign an ordinal index to each of the twelve eggs contained in a carton containing a dozen eggs. (<em>Should</em> you start with zero <em>or</em> one?)", "explanation": null, "document": "You have learned how to write some simple programs and execute them interactively.\nYou have learned how to capture simple programs in script files and to execute those script files.\nYou have learned how to construct programs, including the indentation concepts involved in Python.\nYou have also learned some of the fundamental concepts involving strings.\nWhat you will learn\nThis lesson will expand your knowledge of strings, and in addition will introduce you to some concepts that will be useful with other data types as well: indexing and slicing.\nWhat is indexing?\nAccording to a definition that I found on the web, \"... an ordinal number is an adjective which describes the numerical position of an object, e.g., first, second, third, etc.\"\nA practical example\nMany years ago when I did a tour of duty as an enlisted man in the U.S. Air Force, they had a habit of lining us up and requiring us to \"count off.\"\nWhat this meant was that the first person in the line called out the number one, the person behind him called out the number two, the person behind him called out the number three, etc.\u00a0 (Since learning about computer programming, I now wonder if the first person should have called out zero.)\nAssigning an ordinal index\nI'm sure they didn't realize that what they were doing was assigning an ordinal index value to each person in the line (and neither did I at the time).\nUsing an ordinal index\nEven though they didn't know the technical details of ordinal indices, they didn't have any difficulty saying, \"Number six, wash dishes, number fourteen, peel potatoes, number twenty-two, carry out the garbage, etc.\"\nThis is indexing\nThat is what indexing is all about.\nIn the context of this lesson, indexing is the process of assigning an ordinal index value to each data item contained in some sort of a container.\nIn other words, we assign an ordinal number to each item, which describes the numerical position of the item in the container.\nFor example, if you were very careful, you could use a felt tip pen to assign an ordinal index to each of the twelve eggs contained in a carton containing a dozen eggs.\u00a0 (Should you start with zero or one?)\nThen you could extract the egg whose index value is 9 from the container and eat it for breakfast.\nHaving assigned the index, we can use that index to access the data item corresponding to that index, as in \"Number six, wash dishes.\"\n(Note that this process is also referred to as a subscription in the Python Reference Manual.)\nIndex values automatically assigned\nIn this lesson, we will be using the index values that are automatically assigned to the characters in a string for the purpose of accessing those characters, both individually, and in groups.\nWhat is slicing?\nHere is what Magnus Lie Hetland has to say on the topic of slicing (and indexing as well.)\u00a0 Although this quotation was taken from a discussion of lists, it applies equally well to strings.\n\"One of the nice things about lists is that you can access their elements separately or in groups, through indexing and slicing.\nIndexing is done (as in many other languages) by appending the index in brackets to the list. (Note that the first element has index 0). (This is the answer to the question about the first egg -- Baldwin)\n...\nSlicing is almost like indexing, except that you indicate both the start and stop index of the result, with a colon (\":\") separating them:\n..\nNotice that the end is non-inclusive. If one of the indices is dropped, it is assumed that you want everything in that direction. i.e. list[:3] means \"every element from the beginning of list up to element 3, non-inclusive.\"\n...\nlist[3:] would, on the other hand, mean \"every element from list, starting at element 3 (inclusive) up to, and including, the last one.\"\nFor really interesting results, you can use negative numbers too: list[-3] is the third element from the end of the list...\"\nSome material deleted for brevity\nI added the boldface and the red comment for emphasis.\u00a0 I also deleted some of the material from this quotation for brevity, but I will cover that material later in conjunction with my discussion of indexing and slicing strings.\nA Sample Program\nI will illustrate indexing and slicing of strings using a sample program contained in a script file named String01.py.\nThe program listing\nA complete listing of the program, and the output produced by the program, are provided at the end of the lesson.\nWill discuss in fragments\nI will discuss the program in fragments, illustrating particular aspects of indexing and slicing in each fragment.\u00a0 This is a scheme that I will use frequently in this set of tutorial lessons.\nInteresting Code Fragments\nA single character can be extracted from a string by referring to the string and indicating the index of the character in square brackets, as shown in the code fragment in Figure 1.\naStr = \"This is a string\"\nprint aStr[0] #print T\nprint aStr[3] #print s\nFigure 1\n(Note that this is a fragment from a script file, not from an interactive Python program.)\nFirst create a string to work with\nThe fragment in Figure 1 creates a string (highlighted in boldface) and assigns it to a variable named aStr.\u00a0 From this point on, the contents of the string can be accessed by referring to the variable.\nIndex values always begin with zero\nUnlike eggs and Air Force enlisted men, the first character in a string is always located at index 0, as in aStr[0].\nThus, the second statement in the fragment extracts and prints the T, which is the first character in the word This.\nCharacter at index 3, display yourself\nSimilarly, the last statement in the fragment extracts and prints the s from index position 3, as in aStr[3].\nThe character at this index position is the s that ends the word This.\nThe last statement is equivalent to the following request, \"Will the character at index position 3 please display yourself on the screen.\"\nIs this the fourth character?\n(You would probably refer to this as the fourth character, and you would be correct if you did.\u00a0 The character at index 3 is the fourth character in the string.\u00a0 The character at index 0 is the first character in the string.\u00a0 First does not equate to index 1.\u00a0 You need to think about this, because this can be a confusing topic for new programmers.)\nAn important, and potentially confusing point\nAt the risk of becoming boring, there is an important point here that you might as well get used to right now.\nThe s is located at index value 3.\u00a0 However, according to the way you are probably accustomed to counting, this is actually the fourth character in the string.\u00a0 You might be inclined to refer to this character as character number 4.\nThis is because index values always begin with zero, while you are probably accustomed to counting things beginning with one, not zero.\nNot like eggs\nIf you access the egg at index value 4 in the container and eat it for breakfast, it cannot be accessed again (because it will be gone).\nHowever, if you access the character at index value 4 in the string and use it for some purpose, what you really use is a copy of the character.\u00a0 It is still there and can be accessed again.\n(Some data containers do allow for the removal of data elements in much the same sense that we can remove an egg from its container.\u00a0 However, a string is not such a container.)\nA simple slice\nFor convenience, here is another copy of the fragment that created the string.\naStr = \"This is a string\"\nThe fragment in Figure 2 cuts a couple of slices out of that string and displays them on the screen.\nprint aStr[0:4] #print This\nprint aStr[10:16] #print string\nFigure 2\nSlice Notation\nThe slice notation uses two index values separated by a colon, as shown in boldface in Figure 2.\nThe end is non-inclusive\nAs was indicated in the earlier quotation, \"... the end is non-inclusive.\"\u00a0 This means that the character whose index value is the number following the colon is not included in the slice.\nExtract the first word in the string\nThus, the first statement containing the reference aStr[0:4] extracts and prints the character sequence beginning with index value 0 and ending with index value 3 (not 4).\u00a0 This causes the word This to be extracted and printed.\nExtract the last word in the string\nSimilarly, the second statement in the above fragment (aStr[10:16]) extracts and prints the characters having index values from 10 through 15, inclusive (not 16).\u00a0 This causes the word string to be extracted and printed.\nOmitting the first index\nIf you omit the first index value, as shown in Figure 3, it defaults to the value zero.\nprint aStr[:4] #print This\nFigure 3\nTherefore, the statement in Figure 3 extracts and prints the first word in the string, which is This.\nOmitting the second index\nIf you omit the second index, as shown in Figure 4, it defaults to a value that includes the last character in the string.\nprint aStr[10:] #print string\nFigure 4\nThus, the statement in Figure 4 extracts and prints the last word in the string, which is string.\nPrint the entire string\nFigure 5 shows two different ways to extract and print the entire string.\u00a0 I won't comment on this, but will leave the analysis as an exercise for the student.\nprint aStr[:5] + aStr[5:]\nprint aStr[:100]\n(Hint:\u00a0 Remember that the plus sign when used with strings is the string concatenation operator.)\nPrint an empty string\nThere are several ways that you can specify the index values that will produce an empty string.\u00a0 One of those ways is shown following the plus sign in Figure 6.\nprint \"Empty: \" + aStr[16:100]\nFigure 6\nIn Figure 6, both index values are outside the bounds of the index values of the characters in the string, which range from 0 through 15 inclusive.\nNegative indices\nAlthough it can be a little confusing, negative index values can be used to count from the right, as shown in Figure 7.\nprint aStr[-5:-2] #print tri\nFigure 7\nThis fragment extracts and prints the characters tri from the word string, which is the last word in the string.\nEliminating confusion\nOnce you allow negative indices for slicing, thing can become very confusing.\u00a0 The following explanation of how indices work with slicing is attributed to Guido van Rossum .\nIn this example, Mr. van Rossum is referring to a five-character string with a value of \"HelpA\".\nThe best way to remember how slices work is to think of the indices as pointing between characters, with the left edge of the first character numbered 0. Then the right edge of the last character of a string of n characters has index n, for example:\n+---+---+---+---+---+\n| H | e | l | p | A |\n+---+---+---+---+---+\n0\u00a0\u00a0 1\u00a0\u00a0 2\u00a0\u00a0 3\u00a0\u00a0 4\u00a0\u00a0 5\n-5\u00a0 -4\u00a0 -3\u00a0 -2\u00a0 -1\nThe first row of numbers gives the position of the indices 0...5 in the string; the second row gives the corresponding negative indices.\nThe slice from i to j consists of all characters between the edges labeled i and j, respectively.\nFor nonnegative indices, the length of a slice is the difference of the indices, if both are within bounds, e.g., the length of word[1:3] is 2.\nHopefully, this explanation will help you to understand and to remember how index values are used for the extraction of substrings from strings using slicing.\nGetting the length of a string\nAnd finally, a built-in function named len() can be used to determine the number of characters actually contained in a string as shown in Figure 8.\nprint len(aStr)\nFigure 8\nFor the example string used in this lesson, Figure 8 gets and prints the length of the string as 16.\nIf you count the characters in the string (beginning with 1), you will conclude that there are 16 characters in the string.\nNote the difference between the number of characters and the maximum index value\nFor a string containing 16 characters, the valid index values range from 0 through 15 inclusive.\nThe complete output\nThis Python script file produces the output shown in Figure 9 on my computer (boldface added for emphasis).\nD:\\Baldwin\\AA-School\\PyProg>python strings01.py\nT\n"}, {"score": 1093.3335, "uuid": "c02c84af-8539-5827-a9e9-7eccdb59aacf", "index": "cw12", "trec_id": "clueweb12-1806wb-25-26496", "target_hostname": "www.floss4science.com", "target_uri": "http://www.floss4science.com/interview-jeffrey-long-longitudinal-data-analysis-for-the-behavioral-sciences-using-r/", "page_rank": 2.082805e-09, "spam_rank": 89, "title": "Long, author of Longitudinal <em>Data</em> <em>Analysis</em> <em>for</em> the Behavioral", "snippet": "<em>Analysis</em> of Questionnaire <em>Data</em> with <em>R</em>, an interview with author Bruno Falissard A Primer on Scientific Programming with <em>Python</em>, an interview with author Hans Petter Langtangen Comments are closed.", "explanation": null, "document": "\u2013 March 28, 2012 Posted in: Books , Featured , Interviews , Statistics\nThis is an interview with Jeffrey D. Long, Professor in the Department of Psychiatry, Carver College of Medicine at the University of Iowa (USA) and author of the book \u201c Longitudinal Data Analysis for the Behavioral Sciences Using R\n\u201c. Dr. Long answers questions about his book and how he uses R in his work in behavioral sciences.\nF4S: Hello Jeffrey. Please, give us a brief introduction about yourself.\nI am a Professor in the Department of Psychiatry, Carver College of Medicine, University of Iowa (USA). My expertise is applied statistics in the behavioral and medical sciences. I am the head statistician for Neurobiological Predictors of Huntington\u2019s Disease (PREDICT-HD), funded by the National Institutes of Health and the CHDI Foundation, Inc. PREDICT-HD is a longitudinal observational study of individuals at-risk for Huntington\u2019s Disease (HD), which is an inherited neurodegenerative disease. PREDICT-HD has several scientific sections that concentrate on different aspects of HD, including brain imaging, cognitive functioning, motor impairment, and psychiatric problems. My biostatistics team analyzes data from the scientific sections to answer substantive research questions.\nF4S: How did you get involved with open source software?\nI became involved with open source software through two routes: teaching and my daily professional activities. I taught applied statistics courses for many years to students with diverse backgrounds working in various setting. Commercial software was a concern because of its cost and availability, and I turned to the open source R system for statistical computing and graphics ( http://www.r-project.org/ ). My professional activities involve statistical analysis and writing reports. I needed an integrated process that will handle both aspects. This is provided through R, and the other open source packages, Emacs ( http://www.gnu.org/software/emacs/ ), Emacs Speaks Statistics (ESS; http://ess.r-project.org/ ), and Sweave ( http://www.stat.uni-muenchen.de/~leisch/Sweave/ ).\nThe integrated components allow a program file to have both R and LaTex syntax interweaved. The statistical analysis is performed in a pre-compilation step, and the results are embedded in the Tex file, which is compiled to produce the final PDF report. The method is highly desirable in my work, as databases are updated continually requiring the updating of reports. To the best of my knowledge, this cannot be performed in commercial software packages, as there is little or no cooperation.\nF4S: Tell us the story behind your book \u201c Longitudinal Data Analysis for the Behavioral Sciences Using R\n\u201c.\nThe book is a culmination of teaching an applied longitudinal data analysis course for many years. There are relatively few people who use R in the behavioral sciences and I wanted to emphasize that it is a good option for researchers and students. R is free software that offers add-on packages authored by world leading authorities in longitudinal data analysis and graphing.\nF4S: Who will benefit from reading it?\nThe beneficiaries are researchers and students who want a practical introduction to the analysis of longitudinal behavioral data. Longitudinal data is defined as repeated measurements taken on the same subjects over time. Such data is correlated within an individual and this should be considered in statistical modeling. The book focuses on the application of linear mixed effects regression (LMER). The book is specifically written for those who want a hands-on approach to using R for LMER modeling. The software and the data sets can be freely downloaded and all the examples can be reproduced on the reader\u2019s own computer.\nF4S: How will you describe your experience writing the book?\nThe joy of the book was digging deeper into the open source software. The entire book was written using the integrated components that I previously mentioned. I learned many tips and tricks along the way that have increased efficiency in my daily professional work.\nF4S: Have you published other FLOSS related books?\nYes. Two former students (A. Zieffler and J. Harring) and I have published another book focusing on the application of R for the statistical comparison of groups: \u201cComparing Groups: Randomization and Bootstrap Methods Using R\u201d\n.\nF4S: Do you have plans for other books?\nNot in the near-future.\nF4S: Why is free/libre open source scientific software important for your field?\nFor students, open source is important because it makes world-class statistical software available regardless of one\u2019s personal or institutional resources (except for an Internet connection). For professionals in my field, open source means that software can be made available simultaneously with new methods that are introduced in professional publications. The result is that new and useful tools can be used immediately for researchers who need them.\nF4S: Which projects, books, blogs or sites related to open source software for science can you recommend?\nThere are many websites I regularly consult for help with R, LaTex, Emacs, and ESS. I will list the website by topic.\nR:\n"}, {"score": 1087.3496, "uuid": "4c701621-8d32-5d08-9405-c37133356807", "index": "cw12", "trec_id": "clueweb12-0112wb-26-12055", "target_hostname": "www.geospatialtraining.com", "target_uri": "http://www.geospatialtraining.com/blog/?cat=30", "page_rank": 1.4894882e-09, "spam_rank": 89, "title": "GeoChalkboard \u00bb <em>Python</em>", "snippet": "<em>Python</em> scripts that open files from various sources <em>should</em> use this mode in conjunction with \u2018<em>r</em>\u2019,\u2019w\u2019, <em>or</em> \u2018a\u2019 to gracefully handle the different possible newline characters. After a file is open, <em>data</em> can be read from a file in a number of ways and through various methods. ", "explanation": null, "document": "Free E-Learning Module from the ArcGIS 10 Version of our GIS Programming 101 Course\nWant to learn more about the ArcPy Mapping module and other programming ArcGIS Desktop with Python topics?\u00a0 The next session of our Internet based, instructor led course \u201c GIS Programming 101: Mastering Python for Geoprocessing in ArcGIS \u201d begins October 25th, 2010.\u00a0 You can still register even though class has already begun.\nUsing ArcPy Mapping to Add and Remove Layers in a Map Document\nMonday, September 13th, 2010\nIn previous posts on the new ArcPy.Mapping module, introduced at ArcGIS 10, I introduced some of the capabilities of the module and showed you how to programmatically access and manipulate layout elements .\u00a0 Today I\u2019d like to show you how to programmatically add and remove layers and group layers in your map document file with ArcPy.Mapping.\nArcpy.Mapping provides the ability to add layers or group layers into an existing map document file.\u00a0 You can take advantage of the ArcMap \u201cauto-arrange\u201d functionality which automatically places a layer in the data frame for visibility.\u00a0 This is essentially the same functionality provided by the Add Data button in ArcMap which positions a layer in the data frame based on geometry type and layer weight rules.\u00a0 Do note that while it is possible to add layers to a map document, this does not apply to adding layers to a layer file.\nWhen adding a layer to a map document the layer must reference an existing layer found in a layer file on disk, the same map document and data frame, the same map document with a different data frame, or a completely separate map document.\u00a0 To add a layer to a map document you must first create an instance of the Layer class and then call the AddLayer() function passing in the new layer along with the data frame where it should be placed and rules for how it is positioned. \u00a0\u00a0I\u2019ve shown this in the code example below.\nLet\u2019s discuss what this code block accomplishes.\u00a0 In the first two lines we simply reference arcpy and get a reference to the currently active map document.\u00a0 Next, we create a new variable called \u2018df\u2019 which holds a reference to the Crime data frame which was obtained through the ListDataFrames() function which filtered the data frames to return the specific Crime data frame.\u00a0 A new Layer instance, called \u2018addLayer\u2019 is then created from a layer file stored on disk.\u00a0 This layer file is called School_Districts.lyr.\u00a0 Finally, we call the AddLayer() function, passing in the data frame where the layer will reside along with a reference to the layer, and a parameter indicating that we would like to use the auto arrange feature.\nIn addition to providing the capability of adding a layer to a map document, arcpy.mapping also provides an AddLayerToGroup() function which can be used to add a layer to a group layer.\u00a0 The layer can be added to the top or bottom of the group layer or you can use auto-arrange for placement.\u00a0 You may also add layers to an empty group layer.\u00a0 However, just as with regular layer, group layers can\u2019t be added to a layer file.\nYou can also remove layers from a map document.\u00a0 RemoveLayer() removes a layer or group layer from a specific data frame.\u00a0 In the event that two layers have the same name only the first is removed unless your script is set up to iterate.\nYou can also insert layers into specific positions in a data frame or group layer.\u00a0 The InsertLayer() method allows for more precise positioning of a new layer into a data frame or group layer.\u00a0 It uses a reference layer to specify a location and the layer is added either before or after the reference layer as specified in your code.\u00a0 Since InsertLayer() requires the use of a reference layer you can\u2019t use this method on an empty data frame.\u00a0 The figure below show District_Crime_Join as the reference layer.\u00a0 Using this reference layer you could place the School_Districts layer either before or after the layer.\nMoveLayer() provides the ability to reposition the layer within a data frame or group layer.\u00a0 Movement of a layer must be within the same data frame.\u00a0 You can\u2019t move a layer from one data frame to another.\u00a0 Just as with InsertLayer(), MoveLayer() uses a reference layer to reposition the layer.\nNext time we\u2019ll cover map printing and exporting with ArcPy.Mapping.\nWant to learn more about the ArcPy Mapping module and other programming ArcGIS Desktop with Python topics?\u00a0 The next session of our Internet based, instructor led course \u201c GIS Programming 101: Mastering Python for Geoprocessing in ArcGIS \u201d begins October 25th, 2010.\nWorking with Layout Elements in the ArcPy Mapping Module\nMonday, August 9th, 2010\nIn my last post I introduced the ArcPy Mapping module, released with ArcGIS 10, which provides a number of capabilities including the management of map documents and layers, printing, exporting, ArcGIS Server publishing, map automation, and the creation of PDF map books.\u00a0 Today we\u2019re going to take a closer look at the Layout classes provided by the module.\nThe ListLayoutElements() function returns a list of Layout elements in the form of various Layout classes.\u00a0 Each element can be one of: GraphicElement, LegendElement, PictureElement, TextElement, MapSurroundElement, or PictureElement as seen in the graphic below.\nListLayoutElements() is a versatile function which in its most basic form is used to return a list of all the layout elements on the page layout of a map document.\u00a0 However, there are two optional parameters that you can supply to filter this list.\u00a0 The first type of filter is an element type filter in which you specify that you only want to return one of the layout element types.\u00a0 You can also apply a wildcard to filter the returned list.\u00a0 These two types of filters can be used in combination.\u00a0 For example, in the code below we are specifying that we only want to return LEGEND_ELEMENT objects with the name \u201cLegend Crime\u201d.\u00a0 This would likely result in a highly filtered list, and as we can see only a single item was returned.\nEach graphic on a layout has a \u2018name\u2019 property that you can view in ArcMap by right clicking the element and selecting Properties.\u00a0 Most elements are assigned a default name if you don\u2019t specifically supply one.\u00a0 You should make it a best practice to assign a unique \u2018name\u2019 to each element on your page layout.\u00a0 This will make it easier for you to access particular elements from your Python scripts.\nEach element on the page layout has various properties that can be set through your Python code.\u00a0 For example, in the code block below we are accessing a specific Legend element (\u201cLegend Crime\u201d) and assigning a new title to the legend.\nIn this case we are only changing the Legend title.\u00a0 However, LegendElement provides other properties that enable it to be repositioned on the page layout as well as getting legend items and the parent data frame.\nBelow is a brief description of each of the layout elements accessible through Python.\nDataFrame\nThe DataFrame class provides access to data frame properties in the map document file.\u00a0 This object can work with both map units and page layout units depending upon the property being used.\u00a0 Page layout properties such as positioning and sizing can be applied to the properties on this slide including elementPositionX, elementPositionY, elementWidth, and elementHeight.\nGraphicElement\nThe GraphicElement object is a generic object for various graphics that can be added to the page layout including tables, graphs, neatlines, markers, lines, and area shapes.\u00a0 You\u2019ll want to make sure that you set the \u2018name\u2019 property for each graphic element (and any other element for that matter) if you intend to access it through a Python script.\nLegendElement\nThe LegendElement provides operations for positioning of the legend on the page layout, modification of the legend title, and also provides access to the legend items and the parent data frame.\u00a0 A LegendElement can be associated with only a single data frame.\nMapsurroundElement\nThe MapsurroundElement can refer to north arrows, scale bars, and scale text and like LegendElement is associated with a single data frame.\u00a0 Properties on this object enable repositioning on the page.\nPictureElement\nPictureElement represents a raster or image on the page layout.\u00a0 The most useful property on this object allows for getting and setting the data source which can be extremely helpful when you need to change out a picture such as logo in multiple map documents.\u00a0 For example, you could write a script that iterates through all your map document files and replaces the current logo with a new logo.\u00a0 You can also reposition the object.\nTextElement\nTextElement represents text on a page layout including inserted text, callouts, rectangle text and titles, but does not include legend titles or text that is part of a table or chart.\u00a0 Properties enable modifying the text string which can be extremely useful in situations where you need to make the same text string change in multiple places in the page layout or over multiple map documents, and of course repositioning of the object is also available.\nThe next session of our Internet based, instructor led course, GIS Programming 101: Mastering Python for Geoprocessing in ArcGIS begins October 25th.\u00a0 This course is only $499 when you register by August 31st.\u00a0 The regular price of this course is $667.\nIntroducing the ArcPy.Mapping Module in ArcGIS 10\nMonday, August 2nd, 2010\nThe ArcPy.Mapping module is new to ArcGIS 10 and brings some really exciting features for mapping automation including the ability to manage map documents and layer files as well as the data within these files.\u00a0 Support is also provided for automating map export and printing as well as the creation of PDF map books and the publication of map documents to ArcGIS Server map services.\u00a0 I\u2019m going to be writing a series of posts of the next few weeks covering some of the functionality provided by this module.\nArcPy.Mapping scripts can be run from a variety of environments just like any other geoprocessing script that you\u2019ve developed with Python for use with the ArcGIS geoprocessor.\u00a0 The new Python Window in ArcMap is a great interface for writing and testing small code blocks.\u00a0 Once tested, these code blocks are often moved into stand-alone Python scripts which can be executed from an IDE such as PythonWin, IDLE, or Wing, but they can also be executed from custom script tools in ArcToolbox, the command line or as scheduled tasks.\u00a0 Many people choose to attach their scripts to script tools in ArcMap or ArcCatalog to provide a visual interface for the scripts within the familiar ArcGIS Desktop environment.\u00a0 Finally, scripts can also be published as geoprocessing tasks in ArcGIS Server.\nBefore you can actually perform any operations on a map document file you need to get a reference to it in your Python script.\u00a0 This is done by calling the MapDocument() method on the arcpy.mapping module.\u00a0 You can reference either the currently running document or a document at a specific location.\u00a0 To reference the currently active document you simply supply the keyword \u201cCURRENT\u201d as a parameter to the MapDocument() function.\u00a0 This gets the currently active document in ArcMap.\u00a0 To reference a map document on disk you simply supply the path to the map document as well as the map document name as a parameter to MapDocument(). \u00a0The two code examples below illustrate each of these operations.\nGet the active map document:\nGet a map document on disk:\nIn today\u2019s post I\u2019m going to cover a number of the new list functions provided by Arcpy.Mapping.\u00a0 There are a number of list functions provided by Arcpy.Mapping.\u00a0 Each of these list functions returns a Python list which is a highly functional data structure for storing information.\u00a0 Normally, the list functions are used as part of a multi-step process where creating a list is only the first step.\u00a0 Subsequent processing in the script will iterate over one or more of the items in this list.\u00a0 For example, you might obtain a list of broken data links in your map document file and iterate over these links, fixing each as you progress.\nThe ListLayers() function returns a Python list of all the layers within an mxd file, a data frame within an mxd file, or a layer file.\u00a0 As with all other list functions you can iterate this list through the use of a \u2018for\u2019 loop which cycles through each element in the list.\nListDataFrames() returns a Python list of data frames in a map document file.\u00a0 An integer index value can be used to access an individual data frame or you can test for a specific data frame name before applying further processing to the data within the data frame.\u00a0 You can also use a wildcard to filter the data frames that are returned.\nData sources in your map document files are often broken due to data being deleted or moved to a new location.\u00a0 The result can be broken data sources in many map document files.\u00a0 ListBrokenDataSources() returns a list of layer objects that have a broken data connection.\u00a0 In ArcMap, a broken data connection is signified by the red exclamation point just before the layer name.\nTypically, the ListBrokenDataSources() function is used as the first step in a script that iterates through the list and fixes the data source. \u00a0\u00a0In a future post I\u2019ll show an example script that lists and fixes broken data sources.\nListTableViews() returns a list of Table objects in a map document.\u00a0 You can use a wildcard to return a filtered list of tables.\nListLayoutElements() returns a list of Layout elements in the form of various Layout classes.\u00a0 Each element can be one of: GraphicElement, LegendElement, PictureElement, TextElement, MapSurroundElement, or PictureElement.\u00a0 I\u2019ve outlined some of these layout elements below.\nListLayoutElements() is a versatile function which in its most basic form is used to return a list of all the layout elements on the page layout of a map document.\u00a0 However, there are two optional parameters that you can supply to filter this list.\u00a0 The first type of filter is an element type filter in which you specify that you only want to return one of the layout element types seen on this slide.\u00a0 You can also apply a wildcard to filter the returned list.\u00a0 These two types of filters can be used in combination.\u00a0 For example, in the code example below we are specifying that we only want to return LEGEND_ELEMENT objects with the name \u201cLegend Crime\u201d.\u00a0 This would likely result in a highly filtered list.\nIn the next post in this series you\u2019ll learn how to fix broken data sources based on a list generated by the ListBrokenDatasources() function.\nThe next session of our Internet based, instructor led course, GIS Programming 101: Mastering Python for Geoprocessing in ArcGIS begins October 25th.\u00a0 This course is only $499 when you register by August 31st.\u00a0 The regular price of this course is $667.\n"}, {"score": 1081.2067, "uuid": "d4f5e341-9eb6-5e49-a768-d0fb90d9d294", "index": "cw12", "trec_id": "clueweb12-0307wb-18-08182", "target_hostname": "brainimaging.waisman.wisc.edu", "target_uri": "http://brainimaging.waisman.wisc.edu/~tjohnstone/AFNI_I.html", "page_rank": 1.2914317e-09, "spam_rank": 87, "title": "<em>Analysis</em> of fMRI <em>data</em> with AFNI", "snippet": "The purpose of this document is to provide an overview of a typical processing pathway <em>for</em> the <em>analysis</em> of fMRI <em>data</em> with AFNI software. Throughout <em>I</em> emphasize the commands that are used to process <em>data</em>.", "explanation": null, "document": "<back to menu>\n11. Group analysis of individual statistical maps using ANOVA\nMany experimental designs will incorporate either more than two experimental conditions or more than two experimental groups, or both. In this case, analysis of variance (ANOVA) is appropriate. Consider an example where we performed an experiment in which subjects saw pictures of either happy faces, fearful faces, or neutral faces. Consider that we want to measure which areas of the brain show differential activation to the different types of faces. In addition, let us assume that there were two experimental groups \u00e2\u0080\u0093 say a patient group and a control group. In this experiment we could use the 3dANOVA3 command to perform a 3 factor ANOVA.\nFor this design, the within-subject condition (face expression) is a fixed factor, group is a fixed factor, and subjects should be considered a random factor nested within group (since we want to be able to generalise beyond this particular set of subjects to the general population from which they are \u201crandomly\u201d sampled). So we need to perform a 3-way mixed effects ANOVA. The command for this is 3dANOVA3.\nLet us assume that the beta coefficients from a 3 condition (happy, fearful, neutral) GLM performed on individual subject data are contained in sub-bricks 4, 5 and 6 respectively of the GLM output bucket files. Then the command below could be used:\n3dANOVA3 \u00e2\u0080\u0093type 5 \u00e2\u0080\u0093alevels 2 \u00e2\u0080\u0093blevels 3 \u00e2\u0080\u0093clevels 8 \\\nHere we specify some general aspects of the design. The \u00e2\u0080\u0093type option specifies which type of design. Options are:\n1\u00a0\u00a0 A,B,C fixed;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 AxBxC\n3\u00a0\u00a0 A fixed; B,C random;\u00a0 AxBxC\n4\u00a0\u00a0 A,B fixed; C random;\u00a0 AxBxC\n5\u00a0\u00a0 A,B fixed; C random;\u00a0 AxB,BxC,C(A)\nWe also specify that the first factor (group) has 2 levels, the second factor (face) has 3 levels, and the final factor (subject) has 8 levels.\nNext, we specify the datasets to analyse, including their respective factor levels:\n-dset 1 1 1 control1/glm_out_5mm+tlrc[4] \\\n-dset 1 1 2 control2/glm_out_5mm+tlrc[4] \\\n-dset 1 1 3 control3/glm_out_5mm+tlrc[4] \\\n-dset 1 1 4 control4/glm_out_5mm+tlrc[4] \\\n-dset 1 1 5 control5/glm_out_5mm+tlrc[4] \\\n-dset 1 1 6 control6/glm_out_5mm+tlrc[4] \\\n-dset 1 1 7 control7/glm_out_5mm+tlrc[4] \\\n-dset 1 1 8 control8/glm_out_5mm+tlrc[4] \\\n-dset 1 2 1 control1/glm_out_5mm+tlrc[5] \\\n-dset 1 2 2 control2/glm_out_5mm+tlrc[5] \\\n-dset 1 2 3 control3/glm_out_5mm+tlrc[5] \\\n-dset 1 2 4 control4/glm_out_5mm+tlrc[5] \\\n-dset 1 2 5 control5/glm_out_5mm+tlrc[5] \\\n-dset 1 2 6 control6/glm_out_5mm+tlrc[5] \\\n-dset 1 2 7 control7/glm_out_5mm+tlrc[5] \\\n-dset 1 2 8 control8/glm_out_5mm+tlrc[5] \\\n-dset 1 3 1 control1/glm_out_5mm+tlrc[6] \\\n-dset 1 3 2 control2/glm_out_5mm+tlrc[6] \\\n-dset 1 3 3 control3/glm_out_5mm+tlrc[6] \\\n-dset 1 3 4 control4/glm_out_5mm+tlrc[6] \\\n-dset 1 3 5 control5/glm_out_5mm+tlrc[6] \\\n-dset 1 3 6 control6/glm_out_5mm+tlrc[6] \\\n-dset 1 3 7 control7/glm_out_5mm+tlrc[6] \\\n-dset 1 3 8 control8/glm_out_5mm+tlrc[6] \\\n-dset 2 1 1 patient1/glm_out_5mm+tlrc[4] \\\n-dset 2 1 2 patient2/glm_out_5mm+tlrc[4] \\\n-dset 2 1 3 patient3/glm_out_5mm+tlrc[4] \\\n-dset 2 1 4 patient4/glm_out_5mm+tlrc[4] \\\n-dset 2 1 5 patient5/glm_out_5mm+tlrc[4] \\\n-dset 2 1 6 patient6/glm_out_5mm+tlrc[4] \\\n-dset 2 1 7 patient7/glm_out_5mm+tlrc[4] \\\n-dset 2 1 8 patient8/glm_out_5mm+tlrc[4] \\\n-dset 2 2 1 patient1/glm_out_5mm+tlrc[5] \\\n-dset 2 2 2 patient2/glm_out_5mm+tlrc[5] \\\n-dset 2 2 3 patient3/glm_out_5mm+tlrc[5] \\\n-dset 2 2 4 patient4/glm_out_5mm+tlrc[5] \\\n-dset 2 2 5 patient5/glm_out_5mm+tlrc[5] \\\n-dset 2 2 6 patient6/glm_out_5mm+tlrc[5] \\\n-dset 2 2 7 patient7/glm_out_5mm+tlrc[5] \\\n-dset 2 2 8 patient8/glm_out_5mm+tlrc[5] \\\n-dset 2 3 1 patient1/glm_out_5mm+tlrc[6] \\\n-dset 2 3 2 patient2/glm_out_5mm+tlrc[6] \\\n-dset 2 3 3 patient3/glm_out_5mm+tlrc[6] \\\n-dset 2 3 4 patient4/glm_out_5mm+tlrc[6] \\\n-dset 2 3 5 patient5/glm_out_5mm+tlrc[6] \\\n-dset 2 3 6 patient6/glm_out_5mm+tlrc[6] \\\n-dset 2 3 7 patient7/glm_out_5mm+tlrc[6] \\\n-dset 2 3 8 patient8/glm_out_5mm+tlrc[6] \\\nWe now specify the output that we want:\n-fa group \u00e2\u0080\u0093fb face \u00e2\u0080\u0093fab groupByFace \\\nThis line specifies that we want the F-test for the group main effect, the F-test for the face main effect, and the F-test for the group by face interaction.\nNext we can look at specific contrasts:\n-acontr 1 0 controls \u00e2\u0080\u0093acontr 0 1 patients \\\n-bcontr 1 0 0 happy \u00e2\u0080\u0093bcontr 0 1 0 fearful \u00e2\u0080\u0093bcontr 0 0 1 neutral \\\nThese two lines specify that we want the factor level means and statistical tests of whether those means are significantly different from 0.\n-acontr 1 \u00e2\u0080\u00931 contr_pat_diff \\\n-bcontr 1 \u00e2\u0080\u00931 0 happ_fear_diff \u00e2\u0080\u0093bcontr 1 0 \u00e2\u0080\u00931 happ_neut_diff \u00e2\u0080\u0093bcontr 0 1 \u00e2\u0080\u00931 fear_neut_diff \u00e2\u0080\u0093bcontr 0.5 0.5 \u00e2\u0080\u00931 emo_neut_diff \\\nThese lines test specific contrasts for each factor, averaged across the levels of the other factors.\n-aBcontr 1 \u00e2\u0080\u00931 : 1 cont_pat_diff_happ \u00e2\u0080\u0093aBcontr 1 \u00e2\u0080\u00931 : 2 cont_pat_diff_fear \u00e2\u0080\u0093aBcontr 1 \u00e2\u0080\u00931 : 3 cont_pat_diff_neut \\\n-Abcontr 1 : 1 \u00e2\u0080\u00931 0 happ_fear_diff_cont \u00e2\u0080\u0093Abcontr 2 : 1 \u00e2\u0080\u00931 0 happ_fear_diff_pat \\\nThese two lines test contrasts for one factor calculated within a specific level of the other factor.\nThere are abviously many more contrasts that could be specified than the ones we have here. Bear in mind that you should really only be looking at these contrasts if i) you have an apriori hypothesis about a specific contrast, ii) the main effect F-test for a given factor is significant and you want to know which factor level differences are driving the main effect, or ii) the interaction of two factors is significant and you need to know what differences are driving the interaction. Don\u2019t fall victim to a fishing expedition in which you test every single possible contrast, and possibly wind up with a catch of junk. If you must do exploratory analyses, then you should guard against Type I error by adopting a suitably more stringent threshold.\n-bucket anova\nFinally, we specify that all the results should be saved in a statistical bucket dataset called anova+tlrc.\nAt this stage you have completed a basic statistical analysis of fMRI data.\n<back to menu>\n12. Group analysis of individual statistical maps using regression\nIt is increasingly common for researchers to want to correlate one or more behavioural or physiological measures with a particular brain activation contrast across a group of subjects. For example, we might be interested in how negative-neutral BOLD contrast correlates with individual differences in some measure of anxiety (e.g. measures on an anxiety self report scale, or mean negative-neutral difference in the amplitude eyeblink startle). In this case, regression is an appropriate analysis technique.\nLet us assume that the negative-neutral contrast from a GLM performed on individual subject data is stored in sub-brick 4 of the GLM output bucket files, which are stored in a sub-directory as follows:\nsubjectN/-xydata 11 subject001/glm_out_5mm+tlrc\nwhere N is the subject number. Let's additional assume that we have an anxiety rating for each subject as follows:\nsubject    anxiety\n1             11\n2              8\n3              1\n4              3\n5              6\n6              4\n7             15\n8             11\nThen the command below could be used to run a voxelwise regression, using the anxiety ratings as predictors of the BOLD contrast:\n3dRegAna \u00e2\u0080\u0093rows 8 -cols 1\nThis first line specifies that we would like to perform a regression with 8 datasets (in this case subjects) and 1 predictor variable (in this case anxiety score)\n-xydata 11 subject001/glm_out_5mm+tlrc[4]\n-xydata 8 subject002/glm_out_5mm+tlrc[4]\n-xydata 1 subject003/glm_out_5mm+tlrc[4]\n-xydata 3 subject004/glm_out_5mm+tlrc[4]\n-xydata 6 subject005glm_out_5mm+tlrc[4]\n-xydata 4 subject006/glm_out_5mm+tlrc[4]\n-xydata 15 subject007/glm_out_5mm+tlrc[4]\n-xydata 11 subject008/glm_out_5mm+tlrc[4]\nThese lines specify the respective predictors (anxiety scores in this case) and corresponding \u201cdependent\u201d measures (BOLD contrast maps in this case). I enclose \u201cdependent\u201d in quotes here because although the BOLD contrast is treated as a dependent measure in the analysis, the analysis is correlational, so unless the predictor variables represent a directly manipulated quantity (usually not the case), we cannot interpret the results in causal terms.\nNow we need to specify a few more details about the model:\n-model 1:0\nThis sub-command often gets people confused. The purpose of this option is to specify the reduced and full models. Basically you can think of the full model as including all predictors of interest as well as predictors of no interest, whereas the reduced model only includes the predictors of no interest. In the current example we have 1 predictor of interest (anxiety score) and 1 predictor of no interest (the regression constant term, which we did not specify since it's included by default). Predictor variables are numbered sequentially from 0 (constant), 1 (first non-constant predictor), 2 (second non-constant predictor) etc. So in this example, we specify -model 1:0 which means the reduced model is composed of the 0th predictor (the constant), and the full model is composed of everything in the reduced model, plus the 1st non-constant predictor (the anxiety score). Further down the page there is an example of a multiple regression with non-constant predictors of interest and no interest, which you can look at to see how the model option is specified for such cases.\n-rmsmin 0\nThis command can be used to accept the constant-only model for voxels with a root mean square (RMS) value of less than the specified value, thus speeding up execution. If set to zero, the program will attempt to fit a full regression model to every voxel, which in many cases is the best option, since it's difficult to know beforehand what a reasonable RMS cutoff is, especially when analysing percent signal change contrast maps.\n-bucket 0 anxiety_regression_out\nThis specifies that we want the default output saved in the AFNI bucket dataset anxiety_regression_out+tlrc.HEAD/BRIK. This dataset will contain sub-bricks with the predictor parameter estimates (coefficients), associated t-tests, and an F-test of the full (versus reduced) model.\nNOTE: Do not make the mistake of thinking that the first coefficient and t-test pair refer to the 1st non-constant predictor. They do not. They refer to the constant term in the model (often, but not always, of no interest).\nLet's look at an example in which we have another predictor of interest, say a rating of depression, as well as a predictor of no interest, say the age of the subjects. We would set up the model as follows:\n3dRegAna \u00e2\u0080\u0093rows 8 -cols 3\nThis first line specifies that we would like to perform a regression with 8 datasets (in this case subjects) and 3 predictor variables (anxiety, depression, age)\n-xydata 11 5 32 subject001/glm_out_5mm+tlrc[4]\n-xydata 8 8 21 subject002/glm_out_5mm+tlrc[4]\n-xydata 1 4 43 subject003/glm_out_5mm+tlrc[4]\n.... and so on for the rest of the subjects\n-model 1 2:0 3\nHere we specify that the reduced model consists of the constant term and the 3rd non-constant predictor (age), while the full model additionally includes the axiety and depression scores (the 1st and 2nd non-constant predictors).\nThe rest of the command would be the same as before. Now in the output, the model F-test refers to how significantly anxiety and depression predict BOLD response, taking into account (i.e. covarying for) age. By including age in the model, we hopefully explain some variance that would otherwise be left unexplained and end up in the error term. Thus we can improve our sensitivity by including appropriate covariates. Note also though, that if age covaries with either of our predictors of interest, then we might reduce our sensitivity, a consequence of predictor collinearity.\nAt this stage you have completed a basic statistical analysis of fMRI data.\n<back to menu>\n13. Correction for multiple comparisons: Monte Carlo simulation\nOne major consideration when conducting voxelwise statistical tests is the sheer number of tests that are being performed. In such cases, Type I error inflation is a problem. A common approach to correct for this is to use a clustering method that tries to determine how big a contiguous cluster of voxels, each one significant at an uncorrected threshold of Pu, has to be to in order to be significant at a threshold Pc \u00e2\u0080\u0093 that is corrected for multiple comparisons. To determine how big such a cluster needs to be, there are 3 common methods:\n1. Guassian Random Field Theory: Uses the mathematical properties of random fields to derive a corrected cluster size.\n2. Permutation tests: Uses the data itself to create a null distribution of cluster sizes, from which the cluster size corresopnding to a desired corrected P can be read off.\n3. Monte Carlo simulation: Creates multiple simulated null datasets, and from them creates a distribution of cluster sizes, from which the cluster size corresponding to a desired corrected P can be read off.\nWe could go into a lot of detail about the relative merits of each method here, but that would be somewhat irrelevant for AFNI analysis, since the Monte Carlo method is the only one currently implemented in AFNI (other techniques might become available in the near future).\nIn order to carry out the Monte Carlo simulation, we need some basic information about the data itself, as well as the uncorrected Pu that we plan on using for creating clusters, and a few other parameters that control the clustering operation.\nFirst we need an estimate of how spatially smooth the noise in the dataset is. To estimate this, we use the program 3dFWHM:\n3dFWHM \u00e2\u0080\u0093dset err_ts+orig[0] \u00e2\u0080\u0093mask \u00e2\u0080\u00983dcalc( \u00e2\u0080\u0093a glm_out+orig[0] \u00e2\u0080\u0093expr step(a-500))\u2019\nHere we have used the residual time series from an individual subject GLM to estimate the spatial smoothness. We have masked the time series with the thresholded baseline estimate so as to exclude voxels from outside the brain. This command will return output like the following:\nGaussian filter widths: sigmax = 1.72 FWHMx = 4.05 sigmay = 1.56 FWHMy = 3.67 sigmaz = 1.70 FWHMz = 4.00\nSo for this dataset, our estimate would be that the full-width-at-half-maximum (FWHM) estimate of spatial smoothness is approximately 4mm. We could then do this for all our subjects and average the results. However, if we used spatial blurring on our statistical percent signal change estimates of more that 4mm, we should use the FWHM of the spatial blur that we applied. In the example above, we used a spatial blur of 5mm, so our estimate of spatial smoothness that we use in Monte Carlo simulations should be 5mm.\nThe next step is to decide upon the uncorrected Pu that we plan on using for creating clusters. This choice is fairly arbitrary. If we choose a fairly liberal voxelwise Pu, say Pu = 0.01, we will be able to detect large clusters of voxels that are activated at a fairly liberal threshold, but we will miss small clusters that are made up of highly significantly activated voxels. If, on the other hand, we use a conservative Pu, say Pu = 0.0001, we will be able to detect small clusters of highly activated voxels, but not larger clusters of less activated voxels. The final choice on a Pu depends on the size of clusters that we are looking for, as well as considerations of what our overall statistical power is likely to be.\nLet\u2019s say for the current example that we choose a Pu = 0.01.\nThe other parameter that we need to determine is what we want our cluster connection radius to be. This specifies how close two voxels need to be to one another in order to be considered contiguous. Usually we would use the voxel size of the datasets being analysed, in this case 2mm.\nSo now we can run the simulation:\nAlphaSim \u00e2\u0080\u0093mask glm_out_perc+tlrc[0] \u00e2\u0080\u0093fwhm 5 \u00e2\u0080\u0093rmm 2 \u00e2\u0080\u0093pthr 0.01 \u00e2\u0080\u0093iter 1000\nThe \u00e2\u0080\u0093iter 1000 specifies that we want to run 1000 simulations. This is usually enough. The program will take quite a few minutes to run, and then produce output like the following:\nCl Size Frequency Cum Prop p/Voxel Max Freq Alpha 1 229840 0.328718 0.01012002 0 1.000000 2 108491 0.483882 0.00947785 0 1.000000 3 66742 0.579336 0.00887161 0 1.000000 4 51187 0.652544 0.00831218 0 1.000000 5 39186 0.708588 0.00774011 0 1.000000\nand so on. What you now need to do is read down the right-most column, which gives the corrected Pc for each minimum cluster size (left hand column).\n84 4 0.999920 0.00001630 4 0.058000 85 5 0.999927 0.00001536 5 0.054000 86 1 0.999929 0.00001417 1 0.049000 87 5 0.999936 0.00001393 4 0.048000\nSo from this we can see that according to Monte Carlo simulations, thresholding the statistical images with an uncorrected Pu = 0.01, and clustering with a cluster connection radius of 2mm, the resulting clusters need to be at least 86 voxels in size in order to achieve a corrected Pc < 0.05. We can now use this information to threshold and cluster our group analysis statistical maps, which is explained in the next section.\n<back to menu>\n14. Correction for multiple comparisons: Clustering\nWe can now use the information from Monte Carlo simulations to threshold and cluster our group analysis statistical maps, so as to get thresholded statistical images corrected for multiple comparisons. This we must do separately for each contrast or F-test that we are interested in. For the preceding example, imagine that we are interested in the F-test for the interaction of group by face. First we need to find out where this information is stored in the anova+tlrc bucket file:\n3dinfo anova+tlrc\nThis produces output such as:\n-- At sub-brick #0 'group:Inten' datum type is short: 0 to 32767 [internal] [* 0.000644032] 0 to 21.103 [scaled] -- At sub-brick #1 'group:F-stat' datum type is short: 0 to 2562 [internal] [* 0.01] 0 to 25.62 [scaled] statcode = fift; statpar = 1 14 -- At sub-brick #2 'face:Inten' datum type is short: 0 to 32767 [internal] [* 0.000571107] 0 to 18.7135 [scaled] -- At sub-brick #3 'face:F-stat' datum type is short: 0 to 3689 [internal] statcode = fift; statpar = 2 28 -- At sub-brick #4 'groupByFace:Inten' datum type is short: 0 to 32767 [internal] [* 0.000571007] 0 to 19.7125 [scaled] -- At sub-brick #5 'groupByFace:F-stat' datum type is short: 0 to 3489 [internal] statcode = fift; statpar = 2 28\nThis tells us that the sub-brick we want to threshold with is sub-brick 5. We can also keep the intensity sub-brick for the interaction, although this would make more sense for a t-test of a specific contrast, since the intensity sub-brick in that case contains the value of the contrast itself (e.g. percent signal change contrast between two conditions).\nBefore we run the cluster command, we need to determine the F-statistic that corresponds to a Pu of 0.01, for the degrees of freedom that we have for the interaction F-test. This can be read off from tables, calculated using Excel or an online calculator, or read off from the threshold slider in AFNI if viewing the interaction statistical map. In this case, the correct F-statistic is 5.45.\nNow we can use the 3dmerge command to perform the clustering:\n3dmerge \u00e2\u0080\u0093session . \u00e2\u0080\u0093prefix group_by_face_clust \u00e2\u0080\u00931thresh 5.45 -1clust 2 \u00e2\u0080\u009386 anova+tlrc[4,5]\n3dmerge \u00e2\u0080\u0093session . \u00e2\u0080\u0093prefix group_by_face_clust_order \u00e2\u0080\u00931thresh 5.45 -1clust_order 2 \u00e2\u0080\u009386 anova+tlrc[4,5]\nThe first command will identify clusters of voxels that exceed an F-value of 5.45, using a cluster connectivity radius of 2mm, and minimum cluster size of 86 voxels, using sub-brick 5 as the thresholding brick, saving the thresholded, clustered data in a new bucket dataset called group_by_face+tlrc. The second command will do the same thing, except all voxel intensities within a cluster will be replaced by the cluster size index (largest cluster=1, next largest=2, ...). This will be useful for extracting data from specific clusters later on. The reason why we specify the minimum cluster size as a negative number is that denotes cluster size in voxels. If we were to put a positive 86 there, that would denote 86mm3.\nSome of the other options in the command 3dmerge might be useful. In particular, you can play around with the \u00e2\u0080\u00931erode and \u00e2\u0080\u00931dilate options, which are useful in separating clusters that are joined by a narrow \u201cneck\u201d.\nWe would typically want to carry out the same basic operation on all the other F-tests and contrasts of interest. When this is done, it is possible to combine all the resulting clustered datasets into one big buckets file using the command 3dbucket.\nThis basically completes the voxelwise group analysis of data.\n<back to menu>\n15. Extracting cluster means from individual subjects\nOne remaining step that you might want to take is to extract each subject\u2019s individual percent signal change parameter estimates from the cluster regions identified in the preceding step, for entry into a statistics package or for graphing results. To do this, you can use the 3dmaskave command. Say, for example, that when looking at the face by group clusters identified in the preceding step, we were interested in a cluster in prefrontal cortex, which happens to be the 5th largest cluster in the group by face interaction dataset. Thus voxels in this cluster would have a value of 5 in the group_by_face_clust_order+tlrc dataset. We can thus use this to extract mean voxel percent signal change for individual subjects as follows:\n3dmaskave \u00e2\u0080\u0093mask group_by_face_clust_order+tlrc \u00e2\u0080\u0093mrange 5 5 control001/glm_out_5mm+tlrc[4]\nThis command would print on the screen the mean percent signal change for this cluster for control subject 1 for the happy face condition. We could repeat this command for the other face conditions, as well as for all the other subjects. The mean percent signal change values could then be used to create graphs with error bars etc.\nAlternatively, if want to output the mean percent signal change for each cluster, we could use the following command:\n3dROIstats -\u00e2\u0080\u0093mask group_by_face_clust_order+tlrc control001/glm_out_5mm+tlrc[4]\nThis would cycle through each cluster in group_by_face_clust_order+orig and output the mean for each one for control subject 1, face condition happy.\nBecause these commands simply print their output to the screen, you\u2019ll probably want to use them with the Linux redirection operators > and >> to save the output to a text file. Obviously these commands are probably best used in a script that cycles through subjects and conditions. For example, the following simple Python script will cycle through 6 subjects and extract the cluster means from 4 designated clusters and two conditions.\n#!/usr/bin/python # filename: extract_clusters.py # simple script to cycle through 6 subjects and extract the cluster means from 4 designated clusters and two conditions # means are first appended to a temporary text file, with subject number, cluster number, and condition brick # appended to a separate temporary text file. The two files are then combined using 1dcat to give a properly indexed # file which could be imported into a stats package or spreadsheet program import os,sys # set up study-specific directories and file names top_dir = '/study/my_study/analysis/' cluster_mask_file = top_dir+'group_by_face_clust_order+tlrc' subject_stats_filename = 'glm_out_5mm+tlrc' output_file = top_dir+'clust_means.txt' # specify the subjects, clusters and bricks that you want to extract subjects = ['001','002','004','008','009','010'] # these are the subjects (should be same as subject directory names) clusters = [2,5,6,8] # these are the cluster numbers that we want to extract condition_bricks = [4,6] # these are the bricks in the individual subject data files that contain the conditions of interest # check to see for existence of temporary files if os.isfile('tmp.txt') or os.isfile('tmp2.txt'): print 'You first need to delete or rename the file(s) tmp.txt and tmp2.txt' sys.exit() # check to see for existence of output file if os.isfile(output_file): print 'You first need to delete or rename the output file '+output_file sys.exit() # now loop through subjects, clusters and bricks to get data for subject in subjects: for cluster in clusters: for brick in condition_bricks: subject_dir = top_dir+subject+'/' data_file = subject_dir+subject_stats_filename+'['+str(brick)+']' command = '3dmaskave \u00e2\u0080\u0093mask '+cluster_mask_file+' -mrange '+str(cluster)+' '+str(cluster)+' '+data_file+' >> tmp.txt' print command os.system(command) command = 'echo '+subject+' '+str(cluster)+' '+str(brick)+' >> tmp2.txt' print command os.system(command) # create the output file with appropriate column headings command = 'echo subject cluster brick contrast > '+output_file print command os.system(command) # now combine the two temporary file and then delete them command = '1dcat tmp2.txt tmp.txt[0] >> '+output_file print command os.system(command) os.remove('tmp.txt') os.remove('tmp2.txt')\nAssuming the script is saved as extract_clusters.py, this script can easily be run by typing:\npython extract_clusters.py\n"}, {"score": 1079.6348, "uuid": "686e7f6c-327b-5bd3-8aef-6d81d7df3c45", "index": "cw12", "trec_id": "clueweb12-0308wb-46-27893", "target_hostname": "brainimaging.waisman.wisc.edu", "target_uri": "http://brainimaging.waisman.wisc.edu/~tjohnstone/AFNI_fundamental.html", "page_rank": 1.2329497e-09, "spam_rank": 91, "title": "<em>Analysis</em> of fMRI <em>data</em> with AFNI", "snippet": "The purpose of this document is to provide an overview of a typical processing pathway <em>for</em> the <em>analysis</em> of fMRI <em>data</em> with AFNI software. Throughout <em>I</em> emphasize the commands that are used to process <em>data</em>.", "explanation": null, "document": "# specifies the base directory in which sub-directories corresponding to each scan run will be created\nWorking directory = /study/training/afni/block/s001\n# specifies the name of the log file to be created\nSIF log filename = /study/training/afni/block/s001/trainingBlockS001.log\n#specifies the P-files for each scan run. The items \u201crun_1\u201d and \u201crun_2\u201d here are the names to be given to the sub-directories for each scan run\nrun_1 Pfile = /study/training/afni/block/s001/raw/P05120.7\nrun_2 Pfile = /study/training/afni/block/s001/raw/P06656.7\n# specifies the ref.dat file that was created during the scan session and that is used for calibration during reconstruction\nReference filename = /study/training/afni/block/s001/raw/ref.dat.s001\n# Spatial reconstruction filter = Fermi, Hamming, or None\nReconstruction filter = Fermi\n# specifies the type of file to create = usually you will want to specify BRIK (for AFNI) or ANA (for ANALYZE) here. Other options are SDT, FNI, IFH\nReconstructed files to create = BRIK\n# specifies the scaling factor to be applied during reconstruction \u00e2\u0080\u0093 specify 0 to use whole-brain autoscaling\nScaling factor = 0\n# specifies how many images at the start of each run to discard. These images correspond to the time that it takes for the scanner to stabilize\nPre-images to discard = 5\n# other reconstruction parameters that generally should be left as shown here\nZX = NA\nZY = NA\nApply BP asymm correction = Y\nApart from the file names specific to the study and subject being analysed, a few other options will vary according to your study\u2019s needs:\nThe reconstruction filter chosen depends whether you want to filter your data now, and not later (use \u201cHamming\u201d), or apply no filtering now and wait until just before Talairach transforming your data before filtering (use \u201cNone\u201d), or whether you want to apply a small amount of filtering now to remove ghosting and other artifacts (as done by GE, use \u201cFermi\u201d) but still filter the data further before Talairaching.\nThe choice of output file format might depend to some extent on which software is going to be used for analysis. In general, we suggest the AFNI format, since it has some features not available in other formats, but can easily be converted to other formats at a later stage if desired. The AFNI format consists of a binary BRIK file that contains the data, and a text HEAD file that contains useful information about the data. One of the AFNI-specific features is that information about the position of the imaged brain volume in relation to the scanner coordinates can be stored in the header. This makes it possible for AFNI to align or overlay fMRI data with completely different acquisition parameters. e.g. a high resolution anatomical image acquired axially can be aligned with a low resolution functional image acquired saggitally. In addition to information about the format and position of data in the BRIK file, the HEAD file also contains a command history of the AFNI commands used to produce the file. More details about AFNI dataset formats is available here.\nThe reconstruction will produce a subdirectory for each scan run. Inside the subdirectory will be a reconstruction log file, called recon.log, which is a plain text file containing a record of the slice by slice reconstruction. You should open this file in a text editor and look at it for any error messages. Also in the directory will be the reconstructed data (i.e. a *.HEAD and a *.BRIK file if you chose AFNI format).\n<back to menu>\n2. Conversion of anatomical images to AFNI format using the command to3d:\nNOTE: These instructions are for scans collected since the scanner upgrade in April 2005. For scans collected before that time, you should use the instructions here .\nTo convert anatomical files to AFNI format, we use the AFNI command to3d. In your data directory, you'll have a bunch of subdirectories such as:\nS1_3-Plane\nS2_EFGRE3D\netc. Each of these contains a different type of anatomical scan, but the overall conversion process should be the same. To take the example of the S2_EFGRE3D directory, which contains the high resolution, T1-weighted anatomical scan, this directory contains a numerically ordered set of files:\nS2_EFGRE3D.0001\n...\nS2_EFGRE3D.0124\nTo convert these to AFNI format, cd into the directory where you want the AFNI format file stored, and then execute the following command:\nto3d \u00e2\u0080\u0093session . -prefix T1High /study/<path to files>/S2_EFGRE3D.*\nwhere <path to files> completes the full Linux path to the directory containing the files that you want converted.\nMake sure you visually inspect the created file in AFNI. Also, overlay your reconstructed functional files on the anatomical file and visually inspect to make sure they are properly aligned. This can be done by choosing the anatomical file as the underlay, choosing the functional file as the overlay, and setting the overlay threshold to a value such that only voxels in and immediately around the brain are shown.\n<back to menu>\n3. Slice timing correction and motion correction of functional images\nFor a given brain volume, each slice image is acquired at a slightly different time. The exact timing of the slices depends on the acquisition setup. Often slices are either acquired in a straightforward sequence, or in an interleaved sequence in which the even numbered slices are acquired first, followed by the odd numbered slices. Particularly in the latter case, the time offset between adjacent slices will lead to differences in signal that can be substantial, particularly in event related studies. Although such differences will not affect individual voxel analyses, any process that involves averaging or interpolating the functional time series across slices will be affected by slice timing differences. Such processes include 3 dimensional spatial smoothing, cluster averaging, motion correction, and spatial normalization (e.g. to Talairach space). When any of these processes are to be applied to the functional data, it might be desirable to correct for slice timing differences. In AFNI this can be done using the command 3dTshift, but a more simple way is to use the \u00e2\u0080\u0093tshift option during motion correction (see example below). The correction is applied prior to motion correction, since motion correction involves interpolating between adjacent slices \u00e2\u0080\u0093 such interpolation will be inaccurate if adjacent slices represent different time points.\nMotion correction seeks to correct for small subject head motion during the scan. The command 3dvolreg is used as shown below.\n3dvolreg -prefix run_1_vr \u00e2\u0080\u0093tshift -Fourier -verbose -base run_1+orig[0] -dfile run_1_motion.txt run_1+orig\nYou can use different images as the base image - in this case and in most cases you will want to use an image taken with the same scan parameters that was acquired closest in time to your anatomical images. The example here uses the first image of the first scan run, which would be appropriate if the anatomical scans were acquired before the functional scans. As another example, if an experiment acquired 5 functional scans, each of length 150 TRs, before the anatomical scans, the following command might be appropriate:\n3dvolreg -prefix run_1_vr \u00e2\u0080\u0093tshift -Fourier -verbose -base run_5+orig[149] -dfile run_1_motion.txt run_1+orig\nThis command specifies the final time point (in AFNI, time indices start at 0, so a 150 TR scan run has time points 0 to 149) of the last functional run as the reference, or base image.\nThe 3dvolreg command creates a new AFNI dataset, consisting of a HEAD file and a BRIK file with the prefix given in the command line.\nTo see the extent to which there were any large translations or rotations, you can look at the motion text file (use 1dplot for a graphical output).\n1dplot -volreg run_1_motion.txt[1..6]\n(The [1..6] designates the 6 columns of the motion file containing translation and rotation estimates).\n3dvolreg can handle small motions fairly well, but larger motion (> 1mm) might not be properly corrected. To see whether this is the case, check the motion-corrected dataset using AFNI. How do the time series look (i.e. are there obvious discontinuities in the data that could be motion-related)? Is there still apparent motion when scrolling through the time index? Make notes!\nNOTE: For earlier versions of AFNI there was a bug in the 3dTshift routine, such that samples are \u201cshifted\u201d in the wrong temporal direction. It is important that you use a recent version of AFNI for motion correction and slice-time correction \u00e2\u0080\u0093 if in doubt, use the latest version.\n<back to menu>\n4. Temporal filtering of functional images\nThe data can be filtered prior to linear model fitting, using 3dFourier, which applies a rather brutal frequency domain filter to the time-series. For event-related designs, this is not really necessary now that the AFNI GLM program, 3dDeconvolve, includes options for more precise baseline drift estimation. For block designs it might still be desirable to eliminate high frequency noise terms:\n3dFourier -prefix run_1_vr_lp -lowpass 0.05 -retrend run_1_vr+orig\nHere the low pass cutoff value was based upon the spectrum of the input stimulus model. This will tend to smooth the data before linear fitting, thus providing a better model fit. You should always be conservative when choosing frequency cutoffs, and visually inspect the time series in AFNI after temporal filtering.\n<back to menu>\n5. Spatial blurring of functional images\nAt some point you will probably want to spatially smooth or blur your data. This is done for two main reasons. For one, as with any smoothing operation, spatial smoothing will tend to average out high spatial frequency noise in your image. Thus areas of activation with a greater spatial extent will remain, but those with a small spatial extent will be eliminated. Second, before aggregating data across subjects, spatial blurring needs to be performed because subjects\u2019 brain anatomies vary, even after normalization to a standard brain space (e.g. Talairach space), and thus their areas of functional activation are likely not to exactly overlap.\nThere are basically three points in the data analysis process that spatial smoothing can be performed. Data can be smoothed during reconstruction (i.e. using the hamming or fermi filter option). This has the advantage of eliminating unwanted noise and artifacts from the images before they can be compounded by further processing. At least some spatial smoothing at this stage is thus probably warranted. However, spatial smoothing during reconstruction is only performed in-plane. That is, it is applied separately for each brain slice, and thus does not account for unwanted high spatial frequency variation between slices, nor does it help align functional activation across subjects in the slice direction.\nA second time to spatially smooth is just before first level statistical analysis of individual subject data. This is the last stage at which the (essentially) raw data time series will be used. After individual subject statistical analysis, further processing is applied to statistical meta-data, to which it is arguably less valid to apply spatial smoothing. Spatial smoothing at this point is advised if i) you want to interpret statistical maps from single subjects, since it will give you improved spatial signal to noise ratio (SNR) and thus greater sensitivity, or ii) you want to include the variance estimates from individual subject statistical analysis in higher level (e.g. group-level) statistical modeling, as is done in FMRISTAT, FSL and SPM, but not currently in AFNI. This second option might be preferable if you have unbalanced data \u00e2\u0080\u0093 i.e. different numbers of observations for different conditions, scan sessions, or subjects.\nThe third possibility is to apply spatial smoothing to the statistical maps produced by individual subject statistical analysis, questions of validity notwithstanding (this is discussed briefly below). Here the advantage is primarily one of processing speed, since instead of smoothing every time point (and you probably have hundreds of time points), you would only need to smooth a smaller number of statistical images.\nTo opt for the second approach, we can spatially smooth the functional dataset using the command 3dmerge:\n3dmerge -1blur_fwhm 5 -doall -session . -prefix run_1_vr_lp_5mm run_1_vr_lp+orig\nThis will apply a Gaussian blur with a full width at half maximum (FWHM) of 5mm to the dataset.\n<back to menu>\n6. Single-subject analysis of functional images with General Linear Modeling (GLM)\nTo go beyond the simple example given here, see these explanations of including motion estimates as covariates and estimating the hemodynamic response function (\u201cdeconvolution\u201d) .\nIn AFNI, the command that implements single-subject GLM is called 3dDeconvolve. This is not, however, a completely appropriate name, because 3dDeconvolve doesn\u2019t necessarily perform a deconvolution analysis. It can do deconvolution, but it more generally does GLM \u00e2\u0080\u0093 please use the term GLM rather than deconvolution (it is correct for all types of analysis that can be performed using 3dDeconvolve).\nNote that there are a number of new features that make the 3dDeconvolve command of AFNI more simple, yet more powerful. These are explained in detail at:\nhttp://afni.nimh.nih.gov/afni/doc/misc/3dDeconvolveSummer2004\nBefore performing the GLM analysis, we need to make one or more stimulus timing files, which are text files containing the time onsets (in seconds) for each type of experimental stimulus for which we want to separately model the BOLD response. These timing files have the file extension .1D by convention. There should be a separate timing file for each stimulus condition to be modeled. tname is the name of the file that contains the stimulus times (in units of seconds, as in the TR of the -input file). There are two formats for timing files:\n1. A single column of numbers, in which case each time is relative to the start of the first imaging run (\"global times\"). This is the older technique and generally not preferred for multiple scan run analyses.\n2. If there are 'R' runs catenated together (either directly on the command line, or as represented in the -concat option), the second format is to give the times within each run separately. In this format, the timing file would have R rows, one per run; the times for each run taking up one row. For example, with two runs, R=2:\n12.3 19.8 23.7 29.2 39.8 52.7 66.6\n16.8 21.8 32.7 38.9 41.9 55.5 71.2\nThese times will be converted to global times by the 3dDeconvolve program, by adding the time offset for each imaging run, thus eliminating the possibility of human error in making the calculation. When using the multi-row input style, you may have the situation where the particular class of stimulus does not occur at all in a given imaging run. To encode this, the corresponding row of the timing file should consist of a single '*' character\nNOTE: The times are relative to the start of the data time series as input to 3dDeconvolve. If the first few points of each imaging run have been cut off (e.g. during reconstruction), then the actual stimulus times must be adjusted correspondingly (e.g., if 5 time points were dropped with TR=2, then the actual stimulus times should be reduced by 10.0 in the stimulus timing files).\nNow for fitting the linear model using 3dDeconvolve. The best way to explain this is with a simple example. Say we have an event-related experiment in which subjects are shown a sequence of positive and negative images, presented in a pseudorandom order. They undergo two scan runs, each one 180 seconds long. During reconstruction we drop 5 TRs off each scan run, meaning that we have 170 seconds of fMRI data for each scan run left. Let\u2019s imagine that the time onsets (corrected for the 5 dropped TRs) for the two types of stimuli for the two runs are given in 2 files:\npos_onsets.1D:\n10.0\u00a0\u00a0\u00a0\u00a0 19.0\u00a0\u00a0\u00a0\u00a0 30.9\u00a0\u00a0\u00a0\u00a0 40.8\u00a0\u00a0\u00a0\u00a0 53.1\u00a0\u00a0\u00a0\u00a0 65.0\u00a0\u00a0\u00a0\u00a0 74.7\u00a0\u00a0\u00a0\u00a0 84.6\u00a0\u00a0\u00a0\u00a0 94.5\u00a0\u00a0\u00a0\u00a0 103.3\u00a0\u00a0 115.7\u00a0\u00a0 124.7\u00a0\u00a0 134.8\u00a0\u00a0 144.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 153.7\u00a0\u00a0 165.7\n16.0\u00a0\u00a0\u00a0\u00a0 25.4\u00a0\u00a0\u00a0\u00a0 34.8\u00a0\u00a0\u00a0\u00a0 43.0\u00a0\u00a0\u00a0\u00a0 53.5\u00a0\u00a0\u00a0\u00a0 60.6\u00a0\u00a0\u00a0\u00a0 73.1\u00a0\u00a0\u00a0\u00a0 85.5\u00a0\u00a0\u00a0\u00a0 97.3\u00a0\u00a0\u00a0\u00a0 107.0\u00a0\u00a0 118.3\u00a0\u00a0 128.1\u00a0\u00a0 140.4\u00a0\u00a0 151.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 160.9\u00a0\u00a0 170.0\nneg_onsets.1D:\n15.0\u00a0\u00a0\u00a0\u00a0 23.7\u00a0\u00a0\u00a0\u00a0 34.6\u00a0\u00a0\u00a0\u00a0 46.8\u00a0\u00a0\u00a0\u00a0 58.6\u00a0\u00a0\u00a0\u00a0 68.6\u00a0\u00a0\u00a0\u00a0 80.4\u00a0\u00a0\u00a0\u00a0 92.9\u00a0\u00a0\u00a0\u00a0 104.1\u00a0\u00a0 115.4\u00a0\u00a0 128.2\u00a0\u00a0 139.8\u00a0\u00a0 148.8\u00a0\u00a0 161.0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 173.6\u00a0\u00a0 183.9\n10.0\u00a0\u00a0\u00a0\u00a0 19.4\u00a0\u00a0\u00a0\u00a0 31.5\u00a0\u00a0\u00a0\u00a0 41.6\u00a0\u00a0\u00a0\u00a0 49.0\u00a0\u00a0\u00a0\u00a0 57.2\u00a0\u00a0\u00a0\u00a0 66.6\u00a0\u00a0\u00a0\u00a0 74.4\u00a0\u00a0\u00a0\u00a0 85.4\u00a0\u00a0\u00a0\u00a0 93.0\u00a0\u00a0\u00a0\u00a0 103.6\u00a0\u00a0 115.3\u00a0\u00a0 127.8\u00a0\u00a0 135.1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 144.4\u00a0\u00a0 151.4\nLet\u2019s also assume that the two scan runs are stored in the files run_1+orig and run_2+orig (remember that the extension .HEAD/.BRIK are not required when referring to AFNI format files in AFNI commands). The start of the command is simple enough:\n3dDeconvolve \u00e2\u0080\u0093input run_1_vr+orig run_2_vr+orig -nfirst 0 \u00e2\u0080\u0093polort 2 \\\n(the backslash at the end of the line here is a bash convention used to denote to the operating system that the command continues on the following line)\nHere we specify the files to be analysed. AFNI automatically takes into account that there are 2 scan runs (because there are 2 files), and will calculate a separate baseline for each scan run. Analysis should start from the first time point (-nfirst 0) and we want to model the baseline with a polynomial of degree 2. Roughly speaking, a 2nd degree polynomial will model the baseline with a constant value, a linear drift, and a quadratic-shaped curve, which should be sufficient to capture most signal drift in each of these fairly short scan runs. For longer runs, it might be desirable to use a polort of 3 or 4, thus including cubic and quartic baseline trends.\nNOTE: It is no longer necessary to concatenate the runs before analysis, nor is it necessary to make their baselines equal. Each scan run will have its baseline individually modeled with a polynomial of the specified degree.\nNow to specify the experimental conditions and timing:\n-num_stimts 2 \u00e2\u0080\u0093basis_normall 1 \\\n-stim_times 1 pos_onsets.1D \u00e2\u0080\u0098GAM\u2019 \\\n-stim_label 1 positive \\\n-stim_times 2 neg_onsets.1D \u00e2\u0080\u0098GAM\u2019 \\\n-stim_label 2 negative \\\nThe first line here specifies that we have two stimulus conditions, and that the model time series for each should be normalised to have a peak amplitude of 1. The latter allows us to convert estimated b-weights to estimates of % signal change later in the analysis chain.\nThen we specify the timing file for each condition, and specify what model we want to use for the hemodynamic response. Here we have chosen the simplest option, a simple Gamma variate function (\u00e2\u0080\u0098GAM\u2019). Other options are:\n'GAM'\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 = 0 parameter gamma variate\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n'GAM(p,q)'\u00a0\u00a0\u00a0\u00a0 = 2 parameter gamma variate\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n'SPMG'\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 = 2 parameter SPM gamma variate + derivative\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n'POLY(b,c,n)'\u00a0 = n parameter polynomial expansion\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n'SIN(b,c,n)'\u00a0\u00a0 = n parameter sine series expansion\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n'TENT(b,c,n)'\u00a0 = n parameter tent function expansion\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n'BLOCK(d,p)'\u00a0\u00a0 = 1 parameter block stimulus of duration 'd'\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n'EXPR(b,c) exp1 ... expn' = n parameter; arbitrary expressions\nEach of these models has advantages and disadvantages, that we won\u2019t go into here, but basically they represent different degrees of tradeoff between a well-specified, but constrained model with few estimated parameters versus a poorly specified, but flexible model with many estimated parameters. The former (exemplified by the simple \u00e2\u0080\u0098GAM\u2019 model) affords more statistical power, but cannot properly model different shapes or latencies of hemodynamic response. More details of each model type are given at:\nNow we can specify contrasts between different experimental conditions. In this instance, there are only a few possibilities:\n-num_glt 2 \\\n-glt_label 1 neg_pos_diff \\\n-gltsym 'SYM: +negative -positive' \\\nThis will calculate the linear contrast negative-positive, giving us the difference in estimated response to the two conditions. It is not necessary to specify the reverse contrast, since all contrasts in AFNI are purely mathematical i.e. they do not mask out negative values, unlike other software.\n-glt_label 2 neg_pos_ave \\\n-gltsym 'SYM: +0.5*negative +0.5*positive' \\\nThis will calculate the mean estimated response to the two conditions.\nFinally, we need to specify the type of output we want:\n-fitts fit_ts \u00e2\u0080\u0093errts error_ts \\\n-xjpeg glm_matrix.jpg \u00e2\u0080\u0093tout \u00e2\u0080\u0093fout \u00e2\u0080\u0093bucket glm_out\n(note the absence of a backslash on the last line, since it\u2019s the last line of the command)\nThe first of these lines specifies that we want the fitted time series and error time series to be output for each voxel, in the form of two 3D+time AFNI datasets. These can be useful for diagnostic purposes and for calculating signal to noise ratios.\nThe second line specifies that we want a graphical representation of the GLM design matrix output to a JPEG file called glm_matrix.jpg, and that we want all the estimated b-coefficients, fit statistics, and contrast estimates output to an AFNI bucket file called glm_out+orig.\nPutting this all together, we have:\n3dDeconvolve \u00e2\u0080\u0093input run_1_vr+orig run_2_vr+orig -nfirst 0 \u00e2\u0080\u0093polort 2 \\\n-num_stimts 2 \u00e2\u0080\u0093basis_normall 1 \\\n-stim_times 1 pos_onsets \u00e2\u0080\u0098GAM\u2019 \\\n-stim_label 1 positive \\\n-stim_times 2 neg_onsets \u00e2\u0080\u0098GAM\u2019 \\\n-stim_label 2 negative \\\n-num_glt 2 \\\n-glt_label 1 neg_pos_diff \\\n-gltsym 'SYM: +negative -positive'\n-glt_label 2 neg_pos_ave \\\n-gltsym 'SYM: +0.5*negative +0.5*positive' \\\n-fitts fit_ts \u00e2\u0080\u0093errts error_ts \\\n-xjpeg glm_matrix.jpg \u00e2\u0080\u0093tout \u00e2\u0080\u0093fout -bucket glm_out\nWhen we run this, AFNI will churn away for some minutes, reporting progress as it goes. When it is finished, you should use AFNI to view the statistical maps overlayed on the original EPI data or on the anatomical images. You should assess the goodness of fit of this model just as you would for a linear regression or GLM analysis with any other type of data. The beta weights and associated t-tests give you an indication of the functional activation in response to each experimental condition for this subject.\nThe process of applying GLM to individual subject data is far more flexible, and potentially far more complex than can be presented here. For a more complete treatment, see:\n<back to menu>\n7. Scaling parameter estimates to percent signal change, and calculating SNR\nBefore combining the results from each subject into a group-level analysis, it is useful to convert each subject\u2019s parameter estimates to a common scale. This way, differences between subjects in the overall scaling of fMRI data (which are meaningless because fMRI is an arbitrarily scaled, purely relative measure) are removed. There is no one correct way to scale each subject\u2019s data, but the method most commonly used (and which seems to give the best combination of sensitivity and reliability) is percent signal change. This is simply calculated as the estimated signal change for a given experimental condition (or contrast between conditions) expressed as a percentage of the baseline signal.\nNOTE: the following discussion and method is only appropriate when using the \u00e2\u0080\u0098GAM\u2019 model, or similar single-parameter models. For other, multi-parameter models, percent signal change should be calculated on the basis of the estimated response functions, which are output from 3dDeconvolve using the \u00e2\u0080\u0093iresp option.\nNow it becomes apparent why we specified the \u00e2\u0080\u0093normall option in the 3dDeconvolve command, above. By doing this, our condition regressors were normalised to a peak amplitude of 1. This means that the parameter estimates for a given condition will be estimates of the amplitude of the BOLD response for that condition. Thus we simply use 3dcalc to perform the arithmetic on the appropriate sub-bricks of the output glm bucket dataset: First we need to determine which sub-bricks of the statistical bucket dataset contains the beta coefficients and contrasts that we want to scale. We use 3dinfo for this:\n3dinfo glm_out+orig\nthis command will give us information about the contents of the statistical bucket file, including lines like the following:\n-- At sub-brick #0 'Run #1 P_0 Coef' datum type is short:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 32767 [internal] [*\u00a0 0.712808]\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 23356.6 [scaled] -- At sub-brick #6 'Run #2 P_0 Coef' datum type is short:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 32767 [internal] [*\u00a0 0.722834]\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 24671.6 [scaled] -- At sub-brick #12 'positive[0] Coef' datum type is short:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -26364 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 21149 [internal] [*\u00a0 0.000433336]\u00a0\u00a0\u00a0\u00a0\u00a0 -11.4245 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 9.16462 [scaled] -- At sub-brick #16 'negative[0] Coef' datum type is short:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -26364 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 21149 [internal] [*\u00a0 0.000433336]\u00a0\u00a0\u00a0\u00a0\u00a0 -11.4245 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 9.16462 [scaled] -- At sub-brick #20 'neg_pos_diff LC[0] Coef' datum type is short:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -18586 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 29350 [internal] [*\u00a0\u00a0 0.00044168]\u00a0\u00a0\u00a0\u00a0\u00a0 -8.20907 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 12.9633 [scaled] -- At sub-brick #24 'Neg_pos_ave LC[0] Coef' datum type is short:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -26364 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 21149 [internal] [*\u00a0 0.000433336]\u00a0\u00a0\u00a0\u00a0\u00a0 -11.4245 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 9.16462 [scaled]\nThis tells us that sub-bricks 0 and 6 contain the baseline estimates (P_0 denotes the parameter for the 0-order baseline regressor, which is a constant equal to the estimated baseline) for each run, and sub-bricks 12, 16, 20, and 24 contain the beta coefficients and estimated linear contrasts. Thus:\n3dcalc \u00e2\u0080\u0093prefix glm_estimates_perc \u00e2\u0080\u0093fscale \\\n\u00e2\u0080\u0093a glm_out+orig[0] \u00e2\u0080\u0093b glm_out+orig[6] \\\n\u00e2\u0080\u0093c glm_out+orig[12,16,20,24] \\\n\u00e2\u0080\u0093expr \u201cstep(mean(a,b)-500)*100*c/mean(a,b)\u201d\nThe \u00e2\u0080\u0093fscale option forces 3dalc to internally use the full range of possible integer values to store the results \u00e2\u0080\u0093 this avoids rounding and truncation errors, but is otherwise invisible to the user.\nThe step(mean(a+b)-500) term here returns 1 if the mean of the two baselines in greater than 500, and 0 otherwise. This is used to mask out all voxels with a mean baseline signal below a certain level (500 is usually appropriate for our lab\u2019s internal files when reconstructed with autoscaling as described above, but you should check by overlaying the estimated baseline sub-bricks in the glm_out file on the subject\u2019s anatomical image and setting the threshold to 500). It also ensures that we don\u2019t get ridiculously high percent signal change values for voxels outside the brain with very low baseline signal, but possibly high parameter estimates due to noise.\nThis technique will produce a new bucket dataset with only the percent signal change parameter and contrast estimates \u00e2\u0080\u0093 we have dropped all the t-statistics, F-statistics, R2 estimates etc., which is fine because none of these are used in the higher level (group) analysis. However, two things that are worth including at this stage are the mean baseline estimate, and an estimate of the signal to noise ratio (SNR). The former we can easily get by averaging the baseline estimates from the different runs using 3dcalc:\n3dcalc \u00e2\u0080\u0093prefix glm_baseline \u00e2\u0080\u0093fscale \\\n\u00e2\u0080\u0093a glm_out+orig[0] \u00e2\u0080\u0093b glm_out+orig[6] \\\n\u00e2\u0080\u0093expr \u201cstep(mean(a,b)-500)*mean(a,b)\u201d\nTo estimate the SNR, we need to use both this mean estimate, as well as the residual time series that was saved with the \u00e2\u0080\u0093errts option in the 3dDeconvolve command. The residual time series is our best estimate of the noise in the data. To calculate the SNR, we need to divide the mean baseline estimate (the signal) by the standard devaition of the residual time series (the noise). To calculate the latter:\n3dTstat \u00e2\u0080\u0093prefix err_ts_stdev \u00e2\u0080\u0093stdev \u00e2\u0080\u0093datum short err_ts+orig\nHence to calculate the SNR:\n3dcalc \u00e2\u0080\u0093prefix snr \u00e2\u0080\u0093fscale \\\n-a glm_baseline+orig \u00e2\u0080\u0093b err_ts_stdev+orig \\\n-expr \u201ca/b\u201d\nNow that we have both the mean baseline and the SNR, we want to put these into the same bucket dataset that contains the percent signal change parameter and contrast estimates. To do this, we can use the 3dbucket command:\n3dbucket \u00e2\u0080\u0093session . \u00e2\u0080\u0093prefix glm_out_perc glm_baseline+orig snr+orig glm_estimates_perc+orig\nWe now have the basic dataset that we will want to use for second level, group analysis.\n<back to menu>\n8. Transforming of subjects' images to a standard space\nAt this stage, if satisfied with the individual subject results, we would transform the statistical images from each subject to a standard space (so that they are aligned) and perform a second level (i.e. group) analysis. This involves two steps. First, we need to calculate the transformation from the individual subject brain space to the standard space, which we do based on the high resolution anatomicals, since they have the best anatomical detail. Then we apply the same spatial transform to our functional statistical data, thus bringing them into the standard space. To perform both these steps, we can choose from two options:\n8.1. Manual Talairach transformation technique (traditional)\nIn this technique, which is the traditional one in AFNI, we manually mark each subject's high resolution anatomical image and then have AFNI use those markers to transform the data into Talairach Space. First we need to landmark and transform a high resolution anatomical image of each subject's brain, using interactive landmarking in AFNI, which will produce a new file with +tlrc in place of +orig in the filename. This step is discussed here:\nWe then normalise the statistical map using the adwarp program.\nadwarp -apar T1High+tlrc -dpar glm_out_perc+orig -prefix glm_out_perc -dxyz 2 \u00e2\u0080\u0093verbose\nIn this example we use the (already marked and Talairach transformed) high resolution T1-weighted anatomical image as the basis for normalizing the statistical data into Talairach space. After normalization we interpolate and resample the data to a 2mm cubic voxel size. The default interpolation method, linear interpolation, was used in this example. This interpolation method is appropriate for beta coefficients and contrast estimates, but possibly not for parameters such as t-values and F-values. Since typically beta coefficients or contrast estimates will be the only parameters carried into the next analysis step, this is a reasonable approach.\n8.2. Automatic transformation technique (new)\nIn this newer technique, AFNI can automatically calculate the transform of each subject's anatomy to a template anatomy of our choosing. A number of templates are included in the AFNI distribution, including:\nTT_N27+tlrc: This is the \u201cColin\u201d brain \u00e2\u0080\u0093 one subject scanned 27 times and averaged\nTT_icbm452+tlrc: This is the International Consortium for Brain Mapping template, created by averaging 452 normal brains\nTT_avg152T1+tlrc: The Montreal Neurological Institute template, creating by averaging 152 normal brains\nIt might also be desirable to use a different template, for example a custom made template if examining a childhood population.\nIMPORTANT NOTE: Please read the information here (slides 24 on) for a description of the templates and differences between automatic and manual transformation techniques. It is very important that you understand the differences in coordinates that arise from using different templates, and that you use only a single template for your study. It is also worth noting that the manual technique might give better alignment across subjects for structures close to the anterior and posterior commisures, such as the amygdala, striatum, etc.\nFor automatic transformation, the first step is to calculate and apply the transform from individual subject space to template space based on the subject's high resolution anatomical:\n@auto_tlrc -base TT_icbm452+tlrc -suffix _icbm452 -input T1High+orig\nThis step should strip the skull off the anatomical image, and then calculate the best spatial transform to warp the individual's anatomical data into the same space as the template image (in this case the icbm452 template). A few things can go wrong at this stage, so you should open up AFNI and display your transformed anatomical overlayed on the icbm452 template to make sure the alignment looks okay. Also look at the transformed anatomical as an underlay to make sure that the skull stripping has done a decent job (e.g. hasn't cut off large chunks of brain). If the stripping wasn't good, it will be necessary to manually skull strip your anatomical, either using AFNI programs or perhaps the BET program from the FSL package.\nAssuming all went well with transforming the anatomical, you can now apply the same transformation to your functional data, re-sampling the data to 2 mm cubic voxels at the same time:\n@auto_tlrc -apar  T1High_icbm452+tlrc -input  glm_out_perc+orig -suffix _icbm452 -dxyz 2\n<back to menu>\n9. Spatial blurring of statistical map <optional>\nAs mentioned above , spatial smoothing can be applied to the statistical maps rather than to raw data. The main reason for spatially smoothing at this stage rather than earlier on, is that it is typically far less computationally expensive, since there are not multiple time points in the statistical map. In addition, because the voxels have been interpolated and up-sampled (e.g. to 2mm cubic voxels) the result of smoothing will be also be smoother and the resulting statistical map will have a more Gaussian spatial distribution (which can have advantages when correcting for multiple comparisons at a later stage).\nHowever, it must be noted that in smoothing beta coefficients and contrast estimates we are smoothing meta-data; i.e. we are smoothing estimates of effects rather than actual fMRI data. This should be a reasonable approach, since such effect estimates are clearly and directly interpretable as estimates of fMRI signal values.\nWe blur the individual normalized statistical maps using 3dmerge:\n3dmerge -1blur_fwhm 5 -doall -session . -prefix glm_out_5mm glm_out_perc+tlrc\nThis will apply a Gaussian blur with a full width at half maximum (FWHM) of 5mm to the statistical map.\n<back to menu>\n10. Group analysis of individual statistical maps using T-tests\nThe final stage of analysis is to perform a group level analysis. In many cases you might have between-subjects factors to include in your analysis. In a first example here, we will deal with a simple completely within-subjects design, in which the question is which areas of the brain become activated in response to the negative stimuli. First we need to determine which sub-brick of the statistical bucket dataset contains the percent signal change estimates for the negative condition. We can use 3dinfo for this, or we can simply recall that our dataset contains the mean baseline in the first sub-brick (sub-brick 0), the SNR in the second sub_brick (sub-brick 1), and then our percent signal change estimates of the 2 parameter estimates and 2 contrasts of interest in sub-bricks 2 to 5. Thus sub-brick 3 should correspond to the percent signal change estimated response to the negative stimuli.:\nTo perform the analysis we would use a 1-sample t-test as follows:\n3dttest \u00e2\u0080\u0093session . \u00e2\u0080\u0093prefix ttest \u00e2\u0080\u0093base1 0 \u00e2\u0080\u0093set2 \\\nsubj1/glm_out_5mm+tlrc[3] \\\nsubj2/glm_out_5mm+tlrc[3] \\\nsubj3/glm_out_5mm+tlrc[3] \\\nsubj4/glm_out_5mm+tlrc[3] \\\nsubj5/glm_out_5mm+tlrc[3] \\\nsubj6/glm_out_5mm+tlrc[3] \\\nsubj7/glm_out_5mm+tlrc[3] \\\nsubj8/glm_out_5mm+tlrc[3] \\\nsubj9/glm_out_5mm+tlrc[3] \\\nsubj10/glm_out_5mm+tlrc[3] \\\nsubj11/glm_out_5mm+tlrc[3] \\\nsubj12/glm_out_5mm+tlrc[3] \\\nsubj13/glm_out_5mm+tlrc[3] \\\nsubj14/glm_out_5mm+tlrc[3] \\\nsubj15/glm_out_5mm+tlrc[3]\nThis tests whether the percent signal change stimated reponse for the negative condition (sub-brick 1) is significantly different from 0 (the \u00e2\u0080\u0093base1 0 term).\nA slightly different example concerns when there are two groups for which we want to compare functional activation in response to the task. In this case we would perform a two-group t-test. Let\u2019s say we now wanted to compare the contrast between positive and negative images, which would be sub-brick 4:\n3dttest \u00e2\u0080\u0093session . \u00e2\u0080\u0093prefix ttest \\\n\u00e2\u0080\u0093set1 \\\nsubj1/glm_out_5mm+tlrc[2] \\\nsubj2/glm_out_5mm+tlrc[2] \\\nsubj3/glm_out_5mm+tlrc[2] \\\nsubj4/glm_out_5mm+tlrc[2] \\\nsubj5/glm_out_5mm+tlrc[2] \\\nsubj6/glm_out_5mm+tlrc[2] \\\nsubj7/glm_out_5mm+tlrc[2] \\\nsubj8/glm_out_5mm+tlrc[2] \\\n\u00e2\u0080\u0093set2 \\\nsubj9/glm_out_5mm+tlrc[2] \\\nsubj10/glm_out_5mm+tlrc[2] \\\nsubj11/glm_out_5mm+tlrc[2] \\\nsubj12/glm_out_5mm+tlrc[2] \\\nsubj13/glm_out_5mm+tlrc[2] \\\nsubj14/glm_out_5mm+tlrc[2] \\\nsubj15/glm_out_5mm+tlrc[2] \\\nsubj16/glm_out_5mm+tlrc[2]\nIn the case that we had a single group that was measured twice; say, before and after a treatment, we would perform a paired t-test by adding the \u00e2\u0080\u0093paired option:\n3dttest \u00e2\u0080\u0093session . \u00e2\u0080\u0093paired \u00e2\u0080\u0093prefix ttest \\\n\u00e2\u0080\u0093set1 \\\nsubj1/glm_out_5mm+tlrc[4] \\\nsubj2/glm_out_5mm+tlrc[4] \\\nsubj3/glm_out_5mm+tlrc[4] \\\nsubj4/glm_out_5mm+tlrc[4] \\\nsubj5/glm_out_5mm+tlrc[4] \\\nsubj6/glm_out_5mm+tlrc[4] \\\nsubj7/glm_out_5mm+tlrc[4] \\\nsubj8/glm_out_5mm+tlrc[4] \\\n\u00e2\u0080\u0093set2 \\\nsubj9/glm_out_5mm+tlrc[4] \\\nsubj10/glm_out_5mm+tlrc[4] \\\nsubj11/glm_out_5mm+tlrc[4] \\\nsubj12/glm_out_5mm+tlrc[4] \\\nsubj13/glm_out_5mm+tlrc[4] \\\nsubj14/glm_out_5mm+tlrc[4] \\\nsubj15/glm_out_5mm+tlrc[4] \\\nsubj16/glm_out_5mm+tlrc[4]\nAt this stage you have completed a basic voxelwise statistical analysis of fMRI data.\n<back to menu>\n11. Group analysis of individual statistical maps using ANOVA\nMany experimental designs will incorporate either more than two experimental conditions or more than two experimental groups, or both. In this case, analysis of variance (ANOVA) is appropriate. Consider an example where we performed an experiment in which subjects saw pictures of either happy faces, fearful faces, or neutral faces. Consider that we want to measure which areas of the brain show differential activation to the different types of faces. In addition, let us assume that there were two experimental groups \u00e2\u0080\u0093 say a patient group and a control group. In this experiment we could use the 3dANOVA3 command to perform a 3 factor ANOVA.\nFor this design, the within-subject condition (face expression) is a fixed factor, group is a fixed factor, and subjects should be considered a random factor nested within group (since we want to be able to generalise beyond this particular set of subjects to the general population from which they are \u201crandomly\u201d sampled). So we need to perform a 3-way mixed effects ANOVA. The command for this is 3dANOVA3.\nLet us assume that the beta coefficients from a 3 condition (happy, fearful, neutral) GLM performed on individual subject data are contained in sub-bricks 4, 5 and 6 respectively of the GLM output bucket files. Then the command below could be used:\n3dANOVA3 \u00e2\u0080\u0093type 5 \u00e2\u0080\u0093alevels 2 \u00e2\u0080\u0093blevels 3 \u00e2\u0080\u0093clevels 8 \\\nHere we specify some general aspects of the design. The \u00e2\u0080\u0093type option specifies which type of design. Options are:\n1\u00a0\u00a0 A,B,C fixed;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 AxBxC\n3\u00a0\u00a0 A fixed; B,C random;\u00a0 AxBxC\n4\u00a0\u00a0 A,B fixed; C random;\u00a0 AxBxC\n5\u00a0\u00a0 A,B fixed; C random;\u00a0 AxB,BxC,C(A)\nWe also specify that the first factor (group) has 2 levels, the second factor (face) has 3 levels, and the final factor (subject) has 8 levels.\nNext, we specify the datasets to analyse, including their respective factor levels:\n-dset 1 1 1 control1/glm_out_5mm+tlrc[4] \\\n-dset 1 1 2 control2/glm_out_5mm+tlrc[4] \\\n-dset 1 1 3 control3/glm_out_5mm+tlrc[4] \\\n-dset 1 1 4 control4/glm_out_5mm+tlrc[4] \\\n-dset 1 1 5 control5/glm_out_5mm+tlrc[4] \\\n-dset 1 1 6 control6/glm_out_5mm+tlrc[4] \\\n-dset 1 1 7 control7/glm_out_5mm+tlrc[4] \\\n-dset 1 1 8 control8/glm_out_5mm+tlrc[4] \\\n-dset 1 2 1 control1/glm_out_5mm+tlrc[5] \\\n-dset 1 2 2 control2/glm_out_5mm+tlrc[5] \\\n-dset 1 2 3 control3/glm_out_5mm+tlrc[5] \\\n-dset 1 2 4 control4/glm_out_5mm+tlrc[5] \\\n-dset 1 2 5 control5/glm_out_5mm+tlrc[5] \\\n-dset 1 2 6 control6/glm_out_5mm+tlrc[5] \\\n-dset 1 2 7 control7/glm_out_5mm+tlrc[5] \\\n-dset 1 2 8 control8/glm_out_5mm+tlrc[5] \\\n-dset 1 3 1 control1/glm_out_5mm+tlrc[6] \\\n-dset 1 3 2 control2/glm_out_5mm+tlrc[6] \\\n-dset 1 3 3 control3/glm_out_5mm+tlrc[6] \\\n-dset 1 3 4 control4/glm_out_5mm+tlrc[6] \\\n-dset 1 3 5 control5/glm_out_5mm+tlrc[6] \\\n-dset 1 3 6 control6/glm_out_5mm+tlrc[6] \\\n-dset 1 3 7 control7/glm_out_5mm+tlrc[6] \\\n-dset 1 3 8 control8/glm_out_5mm+tlrc[6] \\\n-dset 2 1 1 patient1/glm_out_5mm+tlrc[4] \\\n-dset 2 1 2 patient2/glm_out_5mm+tlrc[4] \\\n-dset 2 1 3 patient3/glm_out_5mm+tlrc[4] \\\n-dset 2 1 4 patient4/glm_out_5mm+tlrc[4] \\\n-dset 2 1 5 patient5/glm_out_5mm+tlrc[4] \\\n-dset 2 1 6 patient6/glm_out_5mm+tlrc[4] \\\n-dset 2 1 7 patient7/glm_out_5mm+tlrc[4] \\\n-dset 2 1 8 patient8/glm_out_5mm+tlrc[4] \\\n-dset 2 2 1 patient1/glm_out_5mm+tlrc[5] \\\n-dset 2 2 2 patient2/glm_out_5mm+tlrc[5] \\\n-dset 2 2 3 patient3/glm_out_5mm+tlrc[5] \\\n-dset 2 2 4 patient4/glm_out_5mm+tlrc[5] \\\n-dset 2 2 5 patient5/glm_out_5mm+tlrc[5] \\\n-dset 2 2 6 patient6/glm_out_5mm+tlrc[5] \\\n-dset 2 2 7 patient7/glm_out_5mm+tlrc[5] \\\n-dset 2 2 8 patient8/glm_out_5mm+tlrc[5] \\\n-dset 2 3 1 patient1/glm_out_5mm+tlrc[6] \\\n-dset 2 3 2 patient2/glm_out_5mm+tlrc[6] \\\n-dset 2 3 3 patient3/glm_out_5mm+tlrc[6] \\\n-dset 2 3 4 patient4/glm_out_5mm+tlrc[6] \\\n-dset 2 3 5 patient5/glm_out_5mm+tlrc[6] \\\n-dset 2 3 6 patient6/glm_out_5mm+tlrc[6] \\\n-dset 2 3 7 patient7/glm_out_5mm+tlrc[6] \\\n-dset 2 3 8 patient8/glm_out_5mm+tlrc[6] \\\nWe now specify the output that we want:\n-fa group \u00e2\u0080\u0093fb face \u00e2\u0080\u0093fab groupByFace \\\nThis line specifies that we want the F-test for the group main effect, the F-test for the face main effect, and the F-test for the group by face interaction.\nNext we can look at specific contrasts:\n-acontr 1 0 controls \u00e2\u0080\u0093acontr 0 1 patients \\\n-bcontr 1 0 0 happy \u00e2\u0080\u0093bcontr 0 1 0 fearful \u00e2\u0080\u0093bcontr 0 0 1 neutral \\\nThese two lines specify that we want the factor level means and statistical tests of whether those means are significantly different from 0.\n-acontr 1 \u00e2\u0080\u00931 contr_pat_diff \\\n-bcontr 1 \u00e2\u0080\u00931 0 happ_fear_diff \u00e2\u0080\u0093bcontr 1 0 \u00e2\u0080\u00931 happ_neut_diff \u00e2\u0080\u0093bcontr 0 1 \u00e2\u0080\u00931 fear_neut_diff \u00e2\u0080\u0093bcontr 0.5 0.5 \u00e2\u0080\u00931 emo_neut_diff \\\nThese lines test specific contrasts for each factor, averaged across the levels of the other factors.\n-aBcontr 1 \u00e2\u0080\u00931 : 1 cont_pat_diff_happ \u00e2\u0080\u0093aBcontr 1 \u00e2\u0080\u00931 : 2 cont_pat_diff_fear \u00e2\u0080\u0093aBcontr 1 \u00e2\u0080\u00931 : 3 cont_pat_diff_neut \\\n-Abcontr 1 : 1 \u00e2\u0080\u00931 0 happ_fear_diff_cont \u00e2\u0080\u0093Abcontr 2 : 1 \u00e2\u0080\u00931 0 happ_fear_diff_pat \\\nThese two lines test contrasts for one factor calculated within a specific level of the other factor.\nThere are abviously many more contrasts that could be specified than the ones we have here. Bear in mind that you should really only be looking at these contrasts if i) you have an apriori hypothesis about a specific contrast, ii) the main effect F-test for a given factor is significant and you want to know which factor level differences are driving the main effect, or ii) the interaction of two factors is significant and you need to know what differences are driving the interaction. Don\u2019t fall victim to a fishing expedition in which you test every single possible contrast, and possibly wind up with a catch of junk. If you must do exploratory analyses, then you should guard against Type I error by adopting a suitably more stringent threshold.\n-bucket anova\nFinally, we specify that all the results should be saved in a statistical bucket dataset called anova+tlrc.\nAt this stage you have completed a basic statistical analysis of fMRI data.\n<back to menu>\n12. Group analysis of individual statistical maps using regression\nIt is increasingly common for researchers to want to correlate one or more behavioural or physiological measures with a particular brain activation contrast across a group of subjects. For example, we might be interested in how negative-neutral BOLD contrast correlates with individual differences in some measure of anxiety (e.g. measures on an anxiety self report scale, or mean negative-neutral difference in the amplitude eyeblink startle). In this case, regression is an appropriate analysis technique.\nLet us assume that the negative-neutral contrast from a GLM performed on individual subject data is stored in sub-brick 4 of the GLM output bucket files, which are stored in a sub-directory as follows:\nsubjectN/-xydata 11 subject001/glm_out_5mm+tlrc\nwhere N is the subject number. Let's additional assume that we have an anxiety rating for each subject as follows:\nsubject    anxiety\n1             11\n2              8\n3              1\n4              3\n5              6\n6              4\n7             15\n8             11\nThen the command below could be used to run a voxelwise regression, using the anxiety ratings as predictors of the BOLD contrast:\n3dRegAna \u00e2\u0080\u0093rows 8 -cols 1\nThis first line specifies that we would like to perform a regression with 8 datasets (in this case subjects) and 1 predictor variable (in this case anxiety score)\n-xydata 11 subject001/glm_out_5mm+tlrc[4]\n-xydata 8 subject002/glm_out_5mm+tlrc[4]\n-xydata 1 subject003/glm_out_5mm+tlrc[4]\n-xydata 3 subject004/glm_out_5mm+tlrc[4]\n-xydata 6 subject005glm_out_5mm+tlrc[4]\n-xydata 4 subject006/glm_out_5mm+tlrc[4]\n-xydata 15 subject007/glm_out_5mm+tlrc[4]\n-xydata 11 subject008/glm_out_5mm+tlrc[4]\nThese lines specify the respective predictors (anxiety scores in this case) and corresponding \u201cdependent\u201d measures (BOLD contrast maps in this case). I enclose \u201cdependent\u201d in quotes here because although the BOLD contrast is treated as a dependent measure in the analysis, the analysis is correlational, so unless the predictor variables represent a directly manipulated quantity (usually not the case), we cannot interpret the results in causal terms.\nNow we need to specify a few more details about the model:\n-model 1:0\nThis sub-command often gets people confused. The purpose of this option is to specify the reduced and full models. Basically you can think of the full model as including all predictors of interest as well as predictors of no interest, whereas the reduced model only includes the predictors of no interest. In the current example we have 1 predictor of interest (anxiety score) and 1 predictor of no interest (the regression constant term, which we did not specify since it's included by default). Predictor variables are numbered sequentially from 0 (constant), 1 (first non-constant predictor), 2 (second non-constant predictor) etc. So in this example, we specify -model 1:0 which means the reduced model is composed of the 0th predictor (the constant), and the full model is composed of everything in the reduced model, plus the 1st non-constant predictor (the anxiety score). Further down the page there is an example of a multiple regression with non-constant predictors of interest and no interest, which you can look at to see how the model option is specified for such cases.\n-rmsmin 0\nThis command can be used to accept the constant-only model for voxels with a root mean square (RMS) value of less than the specified value, thus speeding up execution. If set to zero, the program will attempt to fit a full regression model to every voxel, which in many cases is the best option, since it's difficult to know beforehand what a reasonable RMS cutoff is, especially when analysing percent signal change contrast maps.\n-bucket 0 anxiety_regression_out\nThis specifies that we want the default output saved in the AFNI bucket dataset anxiety_regression_out+tlrc.HEAD/BRIK. This dataset will contain sub-bricks with the predictor parameter estimates (coefficients), associated t-tests, and an F-test of the full (versus reduced) model.\nNOTE: Do not make the mistake of thinking that the first coefficient and t-test pair refer to the 1st non-constant predictor. They do not. They refer to the constant term in the model (often, but not always, of no interest).\nLet's look at an example in which we have another predictor of interest, say a rating of depression, as well as a predictor of no interest, say the age of the subjects. We would set up the model as follows:\n3dRegAna \u00e2\u0080\u0093rows 8 -cols 3\nThis first line specifies that we would like to perform a regression with 8 datasets (in this case subjects) and 3 predictor variables (anxiety, depression, age)\n-xydata 11 5 32 subject001/glm_out_5mm+tlrc[4]\n-xydata 8 8 21 subject002/glm_out_5mm+tlrc[4]\n-xydata 1 4 43 subject003/glm_out_5mm+tlrc[4]\n.... and so on for the rest of the subjects\n-model 1 2:0 3\nHere we specify that the reduced model consists of the constant term and the 3rd non-constant predictor (age), while the full model additionally includes the axiety and depression scores (the 1st and 2nd non-constant predictors).\nThe rest of the command would be the same as before. Now in the output, the model F-test refers to how significantly anxiety and depression predict BOLD response, taking into account (i.e. covarying for) age. By including age in the model, we hopefully explain some variance that would otherwise be left unexplained and end up in the error term. Thus we can improve our sensitivity by including appropriate covariates. Note also though, that if age covaries with either of our predictors of interest, then we might reduce our sensitivity, a consequence of predictor collinearity.\nNOTE: All the usual considerations for regression analyses apply. In particular, no clusters showing significant correlation (even if strictly thresholded with corrected thresholds) should be trusted without extracting the individual subject contrast values from each cluster ( detailed here ) and closely examining scatter plots for outliers. Regression is very sensitive to outliers, particularly with the small N typically the case for fMRI studies.\nAt this stage you have completed a basic statistical analysis of fMRI data.\n<back to menu>\n13. Correction for multiple comparisons: Monte Carlo simulation\nOne major consideration when conducting voxelwise statistical tests is the sheer number of tests that are being performed. In such cases, Type I error inflation is a problem. A common approach to correct for this is to use a clustering method that tries to determine how big a contiguous cluster of voxels, each one significant at an uncorrected threshold of Pu, has to be to in order to be significant at a threshold Pc \u00e2\u0080\u0093 that is corrected for multiple comparisons. To determine how big such a cluster needs to be, there are 3 common methods:\n1. Guassian Random Field Theory: Uses the mathematical properties of random fields to derive a corrected cluster size.\n2. Permutation tests: Uses the data itself to create a null distribution of cluster sizes, from which the cluster size corresopnding to a desired corrected P can be read off.\n3. Monte Carlo simulation: Creates multiple simulated null datasets, and from them creates a distribution of cluster sizes, from which the cluster size corresponding to a desired corrected P can be read off.\nWe could go into a lot of detail about the relative merits of each method here, but that would be somewhat irrelevant for AFNI analysis, since the Monte Carlo method is the only one currently implemented in AFNI (other techniques might become available in the near future).\nIn order to carry out the Monte Carlo simulation, we need some basic information about the data itself, as well as the uncorrected Pu that we plan on using for creating clusters, and a few other parameters that control the clustering operation.\nFirst we need an estimate of how spatially smooth the noise in the dataset is. To estimate this, we use the program 3dFWHM:\n3dFWHM \u00e2\u0080\u0093dset err_ts+orig[0] \u00e2\u0080\u0093mask \u00e2\u0080\u00983dcalc( \u00e2\u0080\u0093a glm_out+orig[0] \u00e2\u0080\u0093expr step(a-500))\u2019\nHere we have used the residual time series from an individual subject GLM to estimate the spatial smoothness. We have masked the time series with the thresholded baseline estimate so as to exclude voxels from outside the brain. This command will return output like the following:\nGaussian filter widths: sigmax =\u00a0 1.72\u00a0\u00a0 FWHMx =\u00a0 4.05 sigmay =\u00a0 1.56\u00a0\u00a0 FWHMy =\u00a0 3.67 sigmaz =\u00a0 1.70\u00a0\u00a0 FWHMz =\u00a0 4.00\nSo for this dataset, our estimate would be that the full-width-at-half-maximum (FWHM) estimate of spatial smoothness is approximately 4mm. We could then do this for all our subjects and average the results. However, if we used spatial blurring on our statistical percent signal change estimates of more that 4mm, we should use the FWHM of the spatial blur that we applied. In the example above, we used a spatial blur of 5mm, so our estimate of spatial smoothness that we use in Monte Carlo simulations should be 5mm.\nThe next step is to decide upon the uncorrected Pu that we plan on using for creating clusters. This choice is fairly arbitrary. If we choose a fairly liberal voxelwise Pu, say Pu = 0.01, we will be able to detect large clusters of voxels that are activated at a fairly liberal threshold, but we will miss small clusters that are made up of highly significantly activated voxels. If, on the other hand, we use a conservative Pu, say Pu = 0.0001, we will be able to detect small clusters of highly activated voxels, but not larger clusters of less activated voxels. The final choice on a Pu depends on the size of clusters that we are looking for, as well as considerations of what our overall statistical power is likely to be.\nLet\u2019s say for the current example that we choose a Pu = 0.01.\nThe other parameter that we need to determine is what we want our cluster connection radius to be. This specifies how close two voxels need to be to one another in order to be considered contiguous. Usually we would use the voxel size of the datasets being analysed, in this case 2mm.\nSo now we can run the simulation:\nAlphaSim \u00e2\u0080\u0093mask glm_out_perc+tlrc[0] \u00e2\u0080\u0093fwhm 5 \u00e2\u0080\u0093rmm 2 \u00e2\u0080\u0093pthr 0.01 \u00e2\u0080\u0093iter 1000\nThe \u00e2\u0080\u0093iter 1000 specifies that we want to run 1000 simulations. This is usually enough. The program will take quite a few minutes to run, and then produce output like the following:\nCl Size\u00a0\u00a0\u00a0\u00a0 Frequency\u00a0\u00a0\u00a0 Cum Prop\u00a0\u00a0\u00a0\u00a0 p/Voxel\u00a0\u00a0 Max Freq\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Alpha 1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 229840\u00a0\u00a0\u00a0 0.328718\u00a0 0.01012002\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0\u00a0\u00a0\u00a0 1.000000 2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 108491\u00a0\u00a0\u00a0 0.483882\u00a0 0.00947785\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0\u00a0\u00a0\u00a0 1.000000 3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 66742\u00a0\u00a0\u00a0 0.579336\u00a0 0.00887161\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0\u00a0\u00a0\u00a0 1.000000 4\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 51187\u00a0\u00a0\u00a0 0.652544\u00a0 0.00831218\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a00\u00a0\u00a0\u00a0 1.000000 5\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 39186\u00a0\u00a0\u00a0 0.708588\u00a0 0.00774011\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0\u00a0\u00a0\u00a0 1.000000\nand so on. What you now need to do is read down the right-most column, which gives the corrected Pc for each minimum cluster size (left hand column).\n84\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4\u00a0\u00a0\u00a0 0.999920\u00a0 0.00001630\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4\u00a0\u00a0\u00a0 0.058000 85\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5\u00a0\u00a0\u00a0 0.999927\u00a0 0.00001536\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5\u00a0\u00a0\u00a0 0.054000 86\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1\u00a0\u00a0\u00a0 0.999929\u00a0 0.00001417\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1\u00a0\u00a0\u00a0 0.049000 87\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5\u00a0\u00a0\u00a0 0.999936\u00a0 0.00001393\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4\u00a0\u00a0\u00a0 0.048000\nSo from this we can see that according to Monte Carlo simulations, thresholding the statistical images with an uncorrected Pu = 0.01, and clustering with a cluster connection radius of 2mm, the resulting clusters need to be at least 86 voxels in size in order to achieve a corrected Pc < 0.05. We can now use this information to threshold and cluster our group analysis statistical maps, which is explained in the next section.\n<back to menu>\n14. Correction for multiple comparisons: Clustering\nWe can now use the information from Monte Carlo simulations to threshold and cluster our group analysis statistical maps, so as to get thresholded statistical images corrected for multiple comparisons. This we must do separately for each contrast or F-test that we are interested in. For the preceding example, imagine that we are interested in the F-test for the interaction of group by face. First we need to find out where this information is stored in the anova+tlrc bucket file:\n3dinfo anova+tlrc\nThis produces output such as:\n-- At sub-brick #0 'group:Inten' datum type is short:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 32767 [internal] [*\u00a0 0.000644032]\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 21.103 [scaled] -- At sub-brick #1 'group:F-stat' datum type is short:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2562 [internal] [*\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.01]\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 25.62 [scaled] statcode = fift;\u00a0 statpar = 1 14 -- At sub-brick #2 'face:Inten' datum type is short:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 32767 [internal] [*\u00a0 0.000571107]\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 18.7135 [scaled] -- At sub-brick #3 'face:F-stat' datum type is short:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3689 [internal] statcode = fift;\u00a0 statpar = 2 28 -- At sub-brick #4 'groupByFace:Inten' datum type is short:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 32767 [internal] [*\u00a0 0.000571007]\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 19.7125 [scaled] -- At sub-brick #5 'groupByFace:F-stat' datum type is short:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 to\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3489 [internal] statcode = fift;\u00a0 statpar = 2 28\nThis tells us that the sub-brick we want to threshold with is sub-brick 5. We can also keep the intensity sub-brick for the interaction, although this would make more sense for a t-test of a specific contrast, since the intensity sub-brick in that case contains the value of the contrast itself (e.g. percent signal change contrast between two conditions).\nBefore we run the cluster command, we need to determine the F-statistic that corresponds to a Pu of 0.01, for the degrees of freedom that we have for the interaction F-test. This can be read off from tables, calculated using Excel or an online calculator, or read off from the threshold slider in AFNI if viewing the interaction statistical map. In this case, the correct F-statistic is 5.45.\nNow we can use the 3dmerge command to perform the clustering:\n3dmerge \u00e2\u0080\u0093session . \u00e2\u0080\u0093prefix group_by_face_clust \u00e2\u0080\u00931thresh 5.45 -1clust 2 \u00e2\u0080\u009386 anova+tlrc[4,5] \n3dmerge \u00e2\u0080\u0093session . \u00e2\u0080\u0093prefix group_by_face_clust_order \u00e2\u0080\u00931thresh 5.45 -1clust_order 2 \u00e2\u0080\u009386 anova+tlrc[4,5]\nThe first command will identify clusters of voxels that exceed an F-value of 5.45, using a cluster connectivity radius of 2mm, and minimum cluster size of 86 voxels, using sub-brick 5 as the thresholding brick, saving the thresholded, clustered data in a new bucket dataset called group_by_face+tlrc. The second command will do the same thing, except all voxel intensities within a cluster will be replaced by the cluster size index (largest cluster=1, next largest=2, ...). This will be useful for extracting data from specific clusters later on. The reason why we specify the minimum cluster size as a negative number is that denotes cluster size in voxels. If we were to put a positive 86 there, that would denote 86mm3.\nSome of the other options in the command 3dmerge might be useful. In particular, you can play around with the \u00e2\u0080\u00931erode and \u00e2\u0080\u00931dilate options, which are useful in separating clusters that are joined by a narrow \u201cneck\u201d.\nWe would typically want to carry out the same basic operation on all the other F-tests and contrasts of interest. When this is done, it is possible to combine all the resulting clustered datasets into one big buckets file using the command 3dbucket.\nThis basically completes the voxelwise group analysis of data.\n<back to menu>\n15. Extracting cluster means from individual subjects\nOne remaining step that you might want to take is to extract each subject\u2019s individual percent signal change parameter estimates from the cluster regions identified in the preceding step, for entry into a statistics package or for graphing results. To do this, you can use the 3dmaskave command. Say, for example, that when looking at the face by group clusters identified in the preceding step, we were interested in a cluster in prefrontal cortex, which happens to be the 5th largest cluster in the group by face interaction dataset. Thus voxels in this cluster would have a value of 5 in the group_by_face_clust_order+tlrc dataset. We can thus use this to extract mean voxel percent signal change for individual subjects as follows:\n3dmaskave \u00e2\u0080\u0093mask group_by_face_clust_order+tlrc \u00e2\u0080\u0093mrange 5 5 control001/glm_out_5mm+tlrc[4]\nThis command would print on the screen the mean percent signal change for this cluster for control subject 1 for the happy face condition. We could repeat this command for the other face conditions, as well as for all the other subjects. The mean percent signal change values could then be used to create graphs with error bars etc.\nAlternatively, if want to output the mean percent signal change for each cluster, we could use the following command:\n3dROIstats -\u00e2\u0080\u0093mask group_by_face_clust_order+tlrc control001/glm_out_5mm+tlrc[4]\nThis would cycle through each cluster in group_by_face_clust_order+orig and output the mean for each one for control subject 1, face condition happy.\nBecause these commands simply print their output to the screen, you\u2019ll probably want to use them with the Linux redirection operators > and >> to save the output to a text file. Obviously these commands are probably best used in a script that cycles through subjects and conditions. For example, the following simple Python script will cycle through 6 subjects and extract the cluster means from 4 designated clusters and two conditions.\n#!/usr/bin/python # filename: extract_clusters.py # simple script to cycle through 6 subjects and extract the cluster means from 4 designated clusters and two conditions # means are first appended to a temporary text file, with subject number, cluster number, and condition brick # appended to a separate temporary text file. The two files are then combined using 1dcat to give a properly indexed # file which could be imported into a stats package or spreadsheet program import os,sys # set up study-specific directories and file names top_dir = '/study/my_study/analysis/' cluster_mask_file = top_dir+'group_by_face_clust_order+tlrc' subject_stats_filename = 'glm_out_5mm+tlrc' output_file = top_dir+'clust_means.txt' # specify the subjects, clusters and bricks that you want to extract subjects = ['001','002','004','008','009','010'] # these are the subjects (should be same as subject directory names) clusters = [2,5,6,8] # these are the cluster numbers that we want to extract condition_bricks = [4,6] # these are the bricks in the individual subject data files that contain the conditions of interest # check to see for existence of temporary files if os.isfile('tmp.txt') or os.isfile('tmp2.txt'): print 'You first need to delete or rename the file(s) tmp.txt and tmp2.txt' sys.exit() # check to see for existence of output file if os.isfile(output_file): print 'You first need to delete or rename the output file '+output_file sys.exit() # now loop through subjects, clusters and bricks to get data for subject in subjects: for cluster in clusters: for brick in condition_bricks: subject_dir = top_dir+subject+'/' data_file = subject_dir+subject_stats_filename+'['+str(brick)+']' command = '3dmaskave \u00e2\u0080\u0093mask '+cluster_mask_file+' -mrange '+str(cluster)+' '+str(cluster)+' '+data_file+' >> tmp.txt' print command os.system(command) command = 'echo '+subject+' '+str(cluster)+' '+str(brick)+' >> tmp2.txt' print command os.system(command) # create the output file with appropriate column headings command = 'echo subject cluster brick contrast > '+output_file print command os.system(command) # now combine the two temporary file and then delete them command = '1dcat tmp2.txt tmp.txt[0] >> '+output_file print command os.system(command) os.remove('tmp.txt') os.remove('tmp2.txt')\nAssuming the script is saved as extract_clusters.py, this script can easily be run by typing:\npython extract_clusters.py\n"}, {"score": 1077.3148, "uuid": "289feeb3-3c51-5ed9-830e-366f27cc0b38", "index": "cw12", "trec_id": "clueweb12-0400wb-09-20759", "target_hostname": "stommel.tamu.edu", "target_uri": "http://stommel.tamu.edu/~baum/graphics-graph-systems.html", "page_rank": 1.4571638e-09, "spam_rank": 90, "title": "Graphics and <em>Data</em> <em>Analysis</em> - Integrated Systems", "snippet": "It is also cross platform and <em>should</em> run on any platform where both <em>Python</em> and VTK are available (which is almost any *nix, Mac OSX <em>or</em> Windows). * modules <em>for</em> visualizing computational grids and scalar, vector and tensor <em>data</em>; * volume visualization of <em>data</em> via texture and ray cast mappers; An interactive", "explanation": null, "document": "Vis5D/Vis5D+\nVis5D is a software system that can be used to visualize both gridded data and irregularly located data. Sources for this data can come from numerical weather models, surface observations and other similar sources. Vis5D can work on data in the form of a five-dimensional rectangle. That is, the data are real numbers at each point of a \"grid\" which spans three space dimensions, one time dimension and a dimension for enumerating multiple physical variables. Of course, Vis5D works perfectly well on data sets with only one variable, one time step (i.e. no time dynamics) or one vertical level. However, your data grids should have at least two rows and columns. Vis5D can also work with irregularly spaced data which are stored as \"records\". Each record contains a geographic location, a time, and a set of variables which can contain either character or numerical data.\nA major feature of Vis5D is support for comparing multiple data sets. This extra data can be incorporated at run-time as a list of *.v5d files or imported at anytime after Vis5D is running. Data can be overlaid in the same 3-D display and/or viewed side-by-side spread sheet style. Data sets that are overlaid are aligned in space and time. In the spread sheet style, multiple displays can be linked. Once linked, the time steps from all data sets are merged and the controls of the linked displays are synchronized.\nThe Vis5D system includes the vis5d visualization program, several programs for managing and analyzing five-dimensional data grids, and instructions and sample source code for converting your data into its file format. We have included the Vis5D source code so you can modify it or write new programs. You can download the sample data sets from the LAMPS model and from Bob Schlesinger's thunderstorm model, so you can work through our examples. (We have also included a small dataset, hole.v5d, so you can verify that Vis5D works and try a couple of simple plots.)\nOver time, various enhanced versions of Vis5d have accumulated, which for one reason or another weren't folded into the main Vis5d tree. These forks unfortunately did not remain in sync with the main tree, nor were they coordinated with one another. Vis5d+ is a project that, with the original Vis5d authors' blessing, intends to be a central repository for third-party Vis5d enhancements in the future. We continue to communicate closely with the Vis5d authors, track changes in the main tree, and share bugfixes where possible.\nVisapult\nVisapult is an application and framework used for remote and distributed, high performance direct volume rendering. The Visapult application consists of two software components that work together in a pipelined fashion in order to perform volume rendering. The two software components consist of a viewer, which runs on your desktop machine, and which presents a GUI along with support for interactive transformations of the volume rendering. The other component, called the backEnd, runs on some other machine, and performs that tasks of data loading and partial rendering. The viewer, running on the desktop, is able to achieve interactive frame rates even with extremely large datasets due to its use of Image Based Rendering Accelerated Volume Rendering. The viewer requires OpenGL support on the display workstation, along with OpenRM Scene Graph, an Open Source scene graph distribution.\nEach of the backEnd and viewer are parallel programs. The backEnd is a distributed memory application that uses MPI as the parallel programming and execution framework. As such, the backEnd will run on any platform that supports MPI programs. The backEnd will read the raw scientific data using domain decomposition strategy that seeks to equally balance the amount of data to be read and processed by each PE. Each backEnd PE reads data and renders, using a software compositing algorithm, in parallel. The results of this activity are then sent to the viewer. For each backEnd PE, there exists a separate TCP communication channel to a corresponding execution thread within the viewer. Unlike the backEnd, the viewer is parallelized using pthreads, and is intended to be run on a single workstation. The viewer can benefit from multiple CPUs on any SMP platform, so best performance will occur when the viewer is run on an SMP platform, such as a multi-processor SGI, Sun or x86.\nVisIt\nisIt is a free interactive parallel visualization and graphical analysis tool for viewing scientific data on Unix and PC platforms. Users can quickly generate visualizations from their data, animate them through time, manipulate them, and save the resulting images for presentations. VisIt contains a rich set of visualization features so that you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. VisIt was designed to handle very large data set sizes in the terascale range and yet can also handle small data sets in the kilobyte range.\nViSta\nViSta, the Visual Statistics System, features statistical visualizations that are highly dynamic and very interactive. ViSta constructs very-high-interaction, dynamic graphics that show you multiple views of your data simultaneously. The graphics are designed to augment your visual intuition so that you can better understand your data.\nVolVis\nA volume visualization system that unites numerous visualization methods within a comprehensive visualization system, providing a flexible tool for the scientist and engineer as well as the visualization developer and researcher. It has been designed to supply a wide range of functionality with numerous methods provided within each functional component, to offer a user interface organized into functional components for ease of use, to allow new representations or algorithms to be easily added, to be portable, and to be freely available.\nThe components of VolVis include a file I/O component which can handle various file types including slice, image, function, and environment files. The object control component allows the user to control most object properties for system objects, e.g. position, orientation, color, texture, etc. The rendering component offers a variety of techniques ranging from a rough approximation of the final image to accurate rendering within a global illumination model, with each available rendering algorithm offering several levels of accuracy. The image control component facilitates the manipulation of images generated by VolVis. A navigation component allows the interactive control of object position and orientation. an animation component the creation of image sequences. A measurement component can be used to obtain quantitative information from the data models, e.g. surface and volume area. A filter component can be used to enhance features, smooth data, or reduce noise.\nVolVis is written in C and highly portable. It will run on most UNIX workstations support X/Motif, although some components require special hardware capabilities.\n[http://www.cs.sunysb.edu/~vislab/volvis_home.html]\nVPython\nThe VPython package includes the Python language with the IDLE interative development environment, a Python module called Visual that creates real-time 3-D output, and the Numeric module for fast array processing.\nThe Visual module can create 3D objects (such as spheres, curves, etc.) and position them in 3D space. Visual, running in a separate thread, automatically updates a 3D scene many times per second, to reflect the current positions of the objects. The programmer does not need to deal with display management, but can focus on the computational aspects of the program. The user can navigate in the 3D scene by using the mouse to zoom and rotate while the program is running. Visual supports full vector algebra.\nVtk\nThe Visualization Toolkit is a software system for 3-D graphics and visualization. It includes and C++ class library and a Tcl implementation based on the class library, and is designed on object-oriented principles. The graphics model used is at a higher level of abstraction than rendering libraries like openGL or pEX, which means it is easier to create useful graphics and visualization applications. In vtk applications can be written directly in C++ or in Tcl/Tk. It is a true visualization system that supports a wide variety of visualization algorithms including scalar, vector and tensor visualization and advanced modeling techniques like implicit modeling and polygon reduction. The rendering libraries supported are Sun's XGL, SGI's GL and OpenGL, HP's starbase and the freely available Mesa OpenGL renderer. The only pricey bit about this package is that the documentation is in a book that must be purchased from Prentice-Hall. Vtk should compile and run on most UNIX platforms, including Linux.\n[http://public.kitware.com/VTK/]\nWIP\nAn interactive package that can be used to produce high quality scientific graphical output. WIP is basically a command line user interface built on top of the PGPLOT graphics library package. It was developed by an astronomer for use by astronomers but it has much wider application possibilities.\nSalient features of WIP include the capability of reading two-dimensional images and automatically sense the image type (and thus simplifying the overlaying of images with different resolutions and spatial extents), an interpreted language with variables that allow arbitrarily difficult expressions to be evaluated internally, support for user defineable macros which can be defined and edited at run time or read from external files, and conditional commands and loop constructs that allow greater flexibility in the creation of complex plotting instructions.\nThe source code for WIP is available and will requires an ANSI-C compiler for installation. It has been successfully installed on Sun, VAX/VMS, Linux, Cray and CONVEX systems. It requires the prior installation of the PGPLOT package to function. A 150 page user's manual for WIP is available in Postscript format.\n[http://bima.astro.umd.edu/wip]\nXDataslice\nA color imaging and data analysis tool based on the X Window system. It was developed for the analysis of 3-D 32-bit floating point scientific data stored in NCSA HDF format. Features include color raster display of 2D slices from 3D datasets, display of actual data values in spreadsheet form, continuous and single-step animation of color raster images, tiling of multiple images in a single window, arbitrary slicing and dicing, 3-D visible volume rendering, and many more. The source code is available as well as binaries for Cray, Dec (3100 and Alpha), IBM, SGI, and Sun platforms. This was last updated in 1994.\n[ftp://ftp.ncsa.uiuc.edu/Unix/XDataSlice/]\nXFarbe\nA contouring program for iso-lines. It features high-quality nonlinear interpolation with bicubics on a rectangular grid, area filling, X Window and PostScript output, customization with a resource file, interative labeling of contour lines, interactive data probing for value and derivatives, placing of symbols according to information read from a file, exact location of extrema and saddle points, and computation of profiles.\nXFarbe reads an ASCII data file with a 2-D array, interpolates a bicubic representation of the data, computes isolines in vector form, and optionally fills the areas between the isolines with colors. Output can be to either an X display or a PostScript output file. Various attributes of the graph can be modified, e.g. contour levels, annotation strings, coordinates, colors, etc. Several additional options are also available.\n[http://www.fhi-berlin.mpg.de/grz/pub/xfarbe.html]\nxfig\nXfig is an interactive drawing tool which runs under X Window System Version 11 Release 4 (X11R4) or later, on most UNIX-compatible platforms. It is freeware. In xfig, figures may be drawn using objects such as circles, boxes, lines, spline curves, text, etc. It is also possible to import images in formats such as GIF, JPEG, EPSF (PostScript), etc. Those objects can be created, deleted, moved or modified. Attributes such as colors or line styles can be selected in various ways. For text, 35 fonts are available. Text can also include Latin-1 characters.\nXfig saves figures in its native Fig format, but they may be converted into various formats such as PostScript, GIF, JPEG, HP-GL, etc. xfig has facility to print figures to a PostScript printer, too. There are some applications which can produce output in the Fig format. For example, xfig doesn't have a facility to create graphs, but tools such as gnuplot or xgraph can create graphs and export them in Fig format. Even if your favorite application can't generate output for xfig, tools such as pstoedit or hp2xx may allow you to read and edit those figures with xfig. If you want to import images into the figure but you don't need to edit the image itself (like this example), it is also possible to import images in formats such as GIF, JPEG, EPSF (PostScript), etc.\nMost operation are performed using the mouse, but some operations may also be performed using keyboard accelerators (shortcuts). Use of a three-button mouse is recommended, but it is also possible to use a two-button mouse (if you have a two-button mouse and your X server doesn't emulate a three-button mouse, press the Meta (or Alt) key and right mouse button together to simulate mouse button 2). Normally, mouse buttons 1 to 3 are assigned to the left, middle, and right buttons respectively.\nXGobi\nXGobi is a data visualization system with state-of-the-art interactive and dynamic methods for the manipulation of views of data. It implements 2-D displays of projections of points and lines in high-dimensional spaces, as well as parallel coordinate displays and textual views thereof. Project tools include dotplots of single variables, plots of pairs of variables, 3-D data rotations, various grand tours, and interactive projection pursuit. Views of the data can be reshaped. Points can be labeled and brushed with glyphs and colons. Lines can be edited and colored. Several XGobi processes can be run simultaneously and linked for lableing, brushing and sharing of projections. Missing data are accomodated and their patterns can be examined; multiple imputations can be given to XGobi for rapid visual diagnostics. XGobi includes an extensive on-line help facility.\nXGobi can be integrated in other software systems, as has been done for the data analysis language S and the interactive multidimensional scaling program XGvis.\n[http://lib.stat.cmu.edu/general/XGobi/]\n"}, {"score": 1076.5645, "uuid": "b3a322be-ee37-5933-b079-682de8354f5d", "index": "cw12", "trec_id": "clueweb12-0400wb-09-20739", "target_hostname": "stommel.tamu.edu", "target_uri": "http://stommel.tamu.edu/%7Ebaum/graphics-graph-systems.html", "page_rank": 1.3118291e-09, "spam_rank": 91, "title": "Graphics and <em>Data</em> <em>Analysis</em> - Integrated Systems", "snippet": "It is also cross platform and <em>should</em> run on any platform where both <em>Python</em> and VTK are available (which is almost any *nix, Mac OSX <em>or</em> Windows). * modules <em>for</em> visualizing computational grids and scalar, vector and tensor <em>data</em>; * volume visualization of <em>data</em> via texture and ray cast mappers; An interactive", "explanation": null, "document": "Vis5D/Vis5D+\nVis5D is a software system that can be used to visualize both gridded data and irregularly located data. Sources for this data can come from numerical weather models, surface observations and other similar sources. Vis5D can work on data in the form of a five-dimensional rectangle. That is, the data are real numbers at each point of a \"grid\" which spans three space dimensions, one time dimension and a dimension for enumerating multiple physical variables. Of course, Vis5D works perfectly well on data sets with only one variable, one time step (i.e. no time dynamics) or one vertical level. However, your data grids should have at least two rows and columns. Vis5D can also work with irregularly spaced data which are stored as \"records\". Each record contains a geographic location, a time, and a set of variables which can contain either character or numerical data.\nA major feature of Vis5D is support for comparing multiple data sets. This extra data can be incorporated at run-time as a list of *.v5d files or imported at anytime after Vis5D is running. Data can be overlaid in the same 3-D display and/or viewed side-by-side spread sheet style. Data sets that are overlaid are aligned in space and time. In the spread sheet style, multiple displays can be linked. Once linked, the time steps from all data sets are merged and the controls of the linked displays are synchronized.\nThe Vis5D system includes the vis5d visualization program, several programs for managing and analyzing five-dimensional data grids, and instructions and sample source code for converting your data into its file format. We have included the Vis5D source code so you can modify it or write new programs. You can download the sample data sets from the LAMPS model and from Bob Schlesinger's thunderstorm model, so you can work through our examples. (We have also included a small dataset, hole.v5d, so you can verify that Vis5D works and try a couple of simple plots.)\nOver time, various enhanced versions of Vis5d have accumulated, which for one reason or another weren't folded into the main Vis5d tree. These forks unfortunately did not remain in sync with the main tree, nor were they coordinated with one another. Vis5d+ is a project that, with the original Vis5d authors' blessing, intends to be a central repository for third-party Vis5d enhancements in the future. We continue to communicate closely with the Vis5d authors, track changes in the main tree, and share bugfixes where possible.\nVisapult\nVisapult is an application and framework used for remote and distributed, high performance direct volume rendering. The Visapult application consists of two software components that work together in a pipelined fashion in order to perform volume rendering. The two software components consist of a viewer, which runs on your desktop machine, and which presents a GUI along with support for interactive transformations of the volume rendering. The other component, called the backEnd, runs on some other machine, and performs that tasks of data loading and partial rendering. The viewer, running on the desktop, is able to achieve interactive frame rates even with extremely large datasets due to its use of Image Based Rendering Accelerated Volume Rendering. The viewer requires OpenGL support on the display workstation, along with OpenRM Scene Graph, an Open Source scene graph distribution.\nEach of the backEnd and viewer are parallel programs. The backEnd is a distributed memory application that uses MPI as the parallel programming and execution framework. As such, the backEnd will run on any platform that supports MPI programs. The backEnd will read the raw scientific data using domain decomposition strategy that seeks to equally balance the amount of data to be read and processed by each PE. Each backEnd PE reads data and renders, using a software compositing algorithm, in parallel. The results of this activity are then sent to the viewer. For each backEnd PE, there exists a separate TCP communication channel to a corresponding execution thread within the viewer. Unlike the backEnd, the viewer is parallelized using pthreads, and is intended to be run on a single workstation. The viewer can benefit from multiple CPUs on any SMP platform, so best performance will occur when the viewer is run on an SMP platform, such as a multi-processor SGI, Sun or x86.\nVisIt\nisIt is a free interactive parallel visualization and graphical analysis tool for viewing scientific data on Unix and PC platforms. Users can quickly generate visualizations from their data, animate them through time, manipulate them, and save the resulting images for presentations. VisIt contains a rich set of visualization features so that you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. VisIt was designed to handle very large data set sizes in the terascale range and yet can also handle small data sets in the kilobyte range.\nViSta\nViSta, the Visual Statistics System, features statistical visualizations that are highly dynamic and very interactive. ViSta constructs very-high-interaction, dynamic graphics that show you multiple views of your data simultaneously. The graphics are designed to augment your visual intuition so that you can better understand your data.\nVolVis\nA volume visualization system that unites numerous visualization methods within a comprehensive visualization system, providing a flexible tool for the scientist and engineer as well as the visualization developer and researcher. It has been designed to supply a wide range of functionality with numerous methods provided within each functional component, to offer a user interface organized into functional components for ease of use, to allow new representations or algorithms to be easily added, to be portable, and to be freely available.\nThe components of VolVis include a file I/O component which can handle various file types including slice, image, function, and environment files. The object control component allows the user to control most object properties for system objects, e.g. position, orientation, color, texture, etc. The rendering component offers a variety of techniques ranging from a rough approximation of the final image to accurate rendering within a global illumination model, with each available rendering algorithm offering several levels of accuracy. The image control component facilitates the manipulation of images generated by VolVis. A navigation component allows the interactive control of object position and orientation. an animation component the creation of image sequences. A measurement component can be used to obtain quantitative information from the data models, e.g. surface and volume area. A filter component can be used to enhance features, smooth data, or reduce noise.\nVolVis is written in C and highly portable. It will run on most UNIX workstations support X/Motif, although some components require special hardware capabilities.\n[http://www.cs.sunysb.edu/~vislab/volvis_home.html]\nVPython\nThe VPython package includes the Python language with the IDLE interative development environment, a Python module called Visual that creates real-time 3-D output, and the Numeric module for fast array processing.\nThe Visual module can create 3D objects (such as spheres, curves, etc.) and position them in 3D space. Visual, running in a separate thread, automatically updates a 3D scene many times per second, to reflect the current positions of the objects. The programmer does not need to deal with display management, but can focus on the computational aspects of the program. The user can navigate in the 3D scene by using the mouse to zoom and rotate while the program is running. Visual supports full vector algebra.\nVtk\nThe Visualization Toolkit is a software system for 3-D graphics and visualization. It includes and C++ class library and a Tcl implementation based on the class library, and is designed on object-oriented principles. The graphics model used is at a higher level of abstraction than rendering libraries like openGL or pEX, which means it is easier to create useful graphics and visualization applications. In vtk applications can be written directly in C++ or in Tcl/Tk. It is a true visualization system that supports a wide variety of visualization algorithms including scalar, vector and tensor visualization and advanced modeling techniques like implicit modeling and polygon reduction. The rendering libraries supported are Sun's XGL, SGI's GL and OpenGL, HP's starbase and the freely available Mesa OpenGL renderer. The only pricey bit about this package is that the documentation is in a book that must be purchased from Prentice-Hall. Vtk should compile and run on most UNIX platforms, including Linux.\n[http://public.kitware.com/VTK/]\nWIP\nAn interactive package that can be used to produce high quality scientific graphical output. WIP is basically a command line user interface built on top of the PGPLOT graphics library package. It was developed by an astronomer for use by astronomers but it has much wider application possibilities.\nSalient features of WIP include the capability of reading two-dimensional images and automatically sense the image type (and thus simplifying the overlaying of images with different resolutions and spatial extents), an interpreted language with variables that allow arbitrarily difficult expressions to be evaluated internally, support for user defineable macros which can be defined and edited at run time or read from external files, and conditional commands and loop constructs that allow greater flexibility in the creation of complex plotting instructions.\nThe source code for WIP is available and will requires an ANSI-C compiler for installation. It has been successfully installed on Sun, VAX/VMS, Linux, Cray and CONVEX systems. It requires the prior installation of the PGPLOT package to function. A 150 page user's manual for WIP is available in Postscript format.\n[http://bima.astro.umd.edu/wip]\nXDataslice\nA color imaging and data analysis tool based on the X Window system. It was developed for the analysis of 3-D 32-bit floating point scientific data stored in NCSA HDF format. Features include color raster display of 2D slices from 3D datasets, display of actual data values in spreadsheet form, continuous and single-step animation of color raster images, tiling of multiple images in a single window, arbitrary slicing and dicing, 3-D visible volume rendering, and many more. The source code is available as well as binaries for Cray, Dec (3100 and Alpha), IBM, SGI, and Sun platforms. This was last updated in 1994.\n[ftp://ftp.ncsa.uiuc.edu/Unix/XDataSlice/]\nXFarbe\nA contouring program for iso-lines. It features high-quality nonlinear interpolation with bicubics on a rectangular grid, area filling, X Window and PostScript output, customization with a resource file, interative labeling of contour lines, interactive data probing for value and derivatives, placing of symbols according to information read from a file, exact location of extrema and saddle points, and computation of profiles.\nXFarbe reads an ASCII data file with a 2-D array, interpolates a bicubic representation of the data, computes isolines in vector form, and optionally fills the areas between the isolines with colors. Output can be to either an X display or a PostScript output file. Various attributes of the graph can be modified, e.g. contour levels, annotation strings, coordinates, colors, etc. Several additional options are also available.\n[http://www.fhi-berlin.mpg.de/grz/pub/xfarbe.html]\nxfig\nXfig is an interactive drawing tool which runs under X Window System Version 11 Release 4 (X11R4) or later, on most UNIX-compatible platforms. It is freeware. In xfig, figures may be drawn using objects such as circles, boxes, lines, spline curves, text, etc. It is also possible to import images in formats such as GIF, JPEG, EPSF (PostScript), etc. Those objects can be created, deleted, moved or modified. Attributes such as colors or line styles can be selected in various ways. For text, 35 fonts are available. Text can also include Latin-1 characters.\nXfig saves figures in its native Fig format, but they may be converted into various formats such as PostScript, GIF, JPEG, HP-GL, etc. xfig has facility to print figures to a PostScript printer, too. There are some applications which can produce output in the Fig format. For example, xfig doesn't have a facility to create graphs, but tools such as gnuplot or xgraph can create graphs and export them in Fig format. Even if your favorite application can't generate output for xfig, tools such as pstoedit or hp2xx may allow you to read and edit those figures with xfig. If you want to import images into the figure but you don't need to edit the image itself (like this example), it is also possible to import images in formats such as GIF, JPEG, EPSF (PostScript), etc.\nMost operation are performed using the mouse, but some operations may also be performed using keyboard accelerators (shortcuts). Use of a three-button mouse is recommended, but it is also possible to use a two-button mouse (if you have a two-button mouse and your X server doesn't emulate a three-button mouse, press the Meta (or Alt) key and right mouse button together to simulate mouse button 2). Normally, mouse buttons 1 to 3 are assigned to the left, middle, and right buttons respectively.\nXGobi\nXGobi is a data visualization system with state-of-the-art interactive and dynamic methods for the manipulation of views of data. It implements 2-D displays of projections of points and lines in high-dimensional spaces, as well as parallel coordinate displays and textual views thereof. Project tools include dotplots of single variables, plots of pairs of variables, 3-D data rotations, various grand tours, and interactive projection pursuit. Views of the data can be reshaped. Points can be labeled and brushed with glyphs and colons. Lines can be edited and colored. Several XGobi processes can be run simultaneously and linked for lableing, brushing and sharing of projections. Missing data are accomodated and their patterns can be examined; multiple imputations can be given to XGobi for rapid visual diagnostics. XGobi includes an extensive on-line help facility.\nXGobi can be integrated in other software systems, as has been done for the data analysis language S and the interactive multidimensional scaling program XGvis.\n[http://lib.stat.cmu.edu/general/XGobi/]\n"}, {"score": 1075.1266, "uuid": "904ae7db-b502-5760-baf9-232d731ada77", "index": "cw12", "trec_id": "clueweb12-1806wb-51-15993", "target_hostname": "diycomputerscience.com", "target_uri": "http://diycomputerscience.com/courses/course/introduction-to-python---using-learn-python-the-hard-way--v2-0/question/45", "page_rank": 1.2104312e-09, "spam_rank": 69, "title": "Introduction To <em>Python</em> - <em>Learn</em> <em>Python</em> the hard way v2.0 - Do It Yourself", "snippet": ". so here it of no use how ever <em>I</em> have tried the opening mode &#x27;a&#x27; <em>or</em> &#x27;a+&#x27; here the truncating file <em>should</em> happen.", "explanation": null, "document": "@sangita20\nNO!!!!\nTruncate would not work when file is opened in append mode, however it will work when file is open in write mode.\nHere is an example\nf = open(\"test\", \"w\") # file opened in write mode\nf.write(\"1\\n2\\n3\\n4\\n\") # some data written to file\nf.flush() # data flushed to disk\nf1 = open(\"test\") # open file in read mode to verify data is written to disk\nf1.read()\n'1\\n2\\n3\\n4\\n' # hmmmmm data is on disk\nf1.close()\nf.tell() # f is file opened in write mode, check its position\n8 # file position is 8\nf.seek(0) # seek to 0\nf.close() # close file opened in write mode\nopen(\"test\").read() # read file one more time\n'' # oops no data in file\nHere is what pydoc says about truncate\n| truncate(...) | truncate([size]) -> None. Truncate the file to at most size bytes. |\n| Size defaults to the current file position, as returned by tell().\nNOTE\ntruncate depends on current file position\nSince append mode changes the current file position to end of the file. It seems that, truncate on file opened in append does not work\nSince current file position of file opened in write mode can be changed, truncate would work on it.\nThanks a lot for this question.\nanswered Nov 10, 2011\nProvide an answer to this question.\nThis editor supports the markdown format. Please visit this page , If you are not familiar with Markdown.\nRecent Course Updates:\n"}, {"score": 1060.4343, "uuid": "53b8f6f8-0e97-5026-9e02-1a46138a6e15", "index": "cw12", "trec_id": "clueweb12-1904wb-76-27557", "target_hostname": "www.statisticalanalysisconsulting.com", "target_uri": "http://www.statisticalanalysisconsulting.com/sas-v-r-ease-of-learning/", "page_rank": 1.1892181e-09, "spam_rank": 84, "title": "<em>R</em>: Ease of learning | Statistical <em>Analysis</em> Consulting", "snippet": "(Note that <em>I</em> am only discussing ease of use <em>for</em> statistical <em>analysis</em> and <em>data</em> management necessary to do that <em>analysis</em>). Personally, <em>I</em> have found and continue to find SAS much easier to <em>learn</em>. There may be several reasons <em>for</em> this. First, <em>I</em> learned SAS before <em>I</em> learned <em>R</em>.", "explanation": null, "document": "By Peter Flom , April 19, 2010 10:01 am\nTwo days ago, I wrote an introduction to this series.\nToday, I will discuss ease of learning. Unlike the earlier post (and, I hope, most of the ones to come) this one is inherently subjective. \u201cEase of learning\u201d is not the same for everyone \u2013 indeed, one thing I\u2019d like to explore here and in the comments is why some people find SAS easier to learn, while others find R easier to learn. (Note that I am only discussing ease of use for statistical analysis and data management necessary to do that analysis).\nPersonally, I have found and continue to find SAS much easier to learn. There may be several reasons for this.\nHappenstantial reasons\nFirst, I learned SAS before I learned R. I\u2019ve been using SAS for nearly 20 years (scary thought!) and R for about 5. Second, I did a lot of learning of SAS in graduate school, where many of my professors gave us the programs they had written. I learn best by example. With R, though, I have mostly learned on my own. Third, I\u2019ve been to a lot of conferences about SAS, and none about R \u2013 purely because there have been SAS conferences that were convenient for me to go to, and there have not been such R conferences (although one is coming up this summer). Those three reasons might be lumped together into a \u201chappenstance\u201d group \u2013 None of them are inherently about the software.\nReasons that are due to my own traits\nNext, I consider some of my own traits that might relate to my feelings about ease of learning:\nFirst: I am not a programmer. I took one programming course way back when I was an undergraduate (in 1977). We learned a bit of ALGOL. But this was in the days of punchcards, and frequent machine outages, and freshman were on the bottom of the priority stack. I didn\u2019t learn that much (although I got an A) and some of what I learned was about binary arithmetic and AND gates and stuff. Not that useful for programming in R.\nSecond: I like voluminous help files. In a later post, I will discuss getting help, but there is little doubt that SAS documentation is lengthier than R documentation.\nReasons due to differences in the software\nFinally, some reasons that I think relate to differences in software.\nFirst, R is object oriented (amended per comments: R is a function language that is object oriented), SAS is more a procedural language (amended for clarity). I\u2019m not too sure about this, but I think that some people may have brains that prefer object oriented code, while others have brains that prefer procedural code. I find procedural code easier to understand. I also think people who are trained as programmers may find object oriented easier \u2013 but that is just a guess based on casual observation.\nSecond, all of SAS is written by one group of people. There are, to be sure, some inconsistencies in how things work, but there is an overall style. The base packages of R were also written by one group, but they are supplemented by a huge number of other packages, written by different people \u2013 often, these packages duplicate each other or base R in terms of their basic goals, but differ in how they approach getting to those goals.\nQuestions for commenters\nI\u2019d like this to be a conversation. So \u2026 to start us off:\n1. Do you find SAS easier or R easier?\n2. Why? Does it have to do with your background, or differences in the packages, or both?\n3. Do you prefer terse help files, or lengthy ones?\n4. What might make R easier to learn?\n5. What might make SAS easier to learn?\n23 Responses to \u201cSAS v. R: Ease of learning\u201d\nAlex\nApril 19, 2010 at 1:50 pm\nHi Peter,\nThank you for this post. I think another thing to consider is the helpfulness of the error messages in each. I have a much harder time understanding the error messages in R and then correcting my mistakes.\nPeter Flom\n"}, {"score": 1059.023, "uuid": "31755ed1-c9bc-5346-bc25-dc2b169403ed", "index": "cw12", "trec_id": "clueweb12-1905wb-00-02134", "target_hostname": "www.statisticalanalysisconsulting.com", "target_uri": "http://www.statisticalanalysisconsulting.com/sas-v-r-ease-of-learning/comment-page-1/", "page_rank": 1.1700305e-09, "spam_rank": 84, "title": "<em>R</em>: Ease of learning | Statistical <em>Analysis</em> Consulting", "snippet": "(Note that <em>I</em> am only discussing ease of use <em>for</em> statistical <em>analysis</em> and <em>data</em> management necessary to do that <em>analysis</em>). Personally, <em>I</em> have found and continue to find SAS much easier to <em>learn</em>. There may be several reasons <em>for</em> this. First, <em>I</em> learned SAS before <em>I</em> learned <em>R</em>.", "explanation": null, "document": "By Peter Flom , April 19, 2010 10:01 am\nTwo days ago, I wrote an introduction to this series.\nToday, I will discuss ease of learning. Unlike the earlier post (and, I hope, most of the ones to come) this one is inherently subjective. \u201cEase of learning\u201d is not the same for everyone \u2013 indeed, one thing I\u2019d like to explore here and in the comments is why some people find SAS easier to learn, while others find R easier to learn. (Note that I am only discussing ease of use for statistical analysis and data management necessary to do that analysis).\nPersonally, I have found and continue to find SAS much easier to learn. There may be several reasons for this.\nHappenstantial reasons\nFirst, I learned SAS before I learned R. I\u2019ve been using SAS for nearly 20 years (scary thought!) and R for about 5. Second, I did a lot of learning of SAS in graduate school, where many of my professors gave us the programs they had written. I learn best by example. With R, though, I have mostly learned on my own. Third, I\u2019ve been to a lot of conferences about SAS, and none about R \u2013 purely because there have been SAS conferences that were convenient for me to go to, and there have not been such R conferences (although one is coming up this summer). Those three reasons might be lumped together into a \u201chappenstance\u201d group \u2013 None of them are inherently about the software.\nReasons that are due to my own traits\nNext, I consider some of my own traits that might relate to my feelings about ease of learning:\nFirst: I am not a programmer. I took one programming course way back when I was an undergraduate (in 1977). We learned a bit of ALGOL. But this was in the days of punchcards, and frequent machine outages, and freshman were on the bottom of the priority stack. I didn\u2019t learn that much (although I got an A) and some of what I learned was about binary arithmetic and AND gates and stuff. Not that useful for programming in R.\nSecond: I like voluminous help files. In a later post, I will discuss getting help, but there is little doubt that SAS documentation is lengthier than R documentation.\nReasons due to differences in the software\nFinally, some reasons that I think relate to differences in software.\nFirst, R is object oriented (amended per comments: R is a function language that is object oriented), SAS is more a procedural language (amended for clarity). I\u2019m not too sure about this, but I think that some people may have brains that prefer object oriented code, while others have brains that prefer procedural code. I find procedural code easier to understand. I also think people who are trained as programmers may find object oriented easier \u2013 but that is just a guess based on casual observation.\nSecond, all of SAS is written by one group of people. There are, to be sure, some inconsistencies in how things work, but there is an overall style. The base packages of R were also written by one group, but they are supplemented by a huge number of other packages, written by different people \u2013 often, these packages duplicate each other or base R in terms of their basic goals, but differ in how they approach getting to those goals.\nQuestions for commenters\nI\u2019d like this to be a conversation. So \u2026 to start us off:\n1. Do you find SAS easier or R easier?\n2. Why? Does it have to do with your background, or differences in the packages, or both?\n3. Do you prefer terse help files, or lengthy ones?\n4. What might make R easier to learn?\n5. What might make SAS easier to learn?\n23 Responses to \u201cSAS v. R: Ease of learning\u201d\nAlex\nApril 19, 2010 at 1:50 pm\nHi Peter,\nThank you for this post. I think another thing to consider is the helpfulness of the error messages in each. I have a much harder time understanding the error messages in R and then correcting my mistakes.\nPeter Flom\n"}], [{"score": 1049.4219, "uuid": "b2b1aff8-16c9-5ee1-a6c9-e375069b784c", "index": "cw12", "trec_id": "clueweb12-1606wb-92-11207", "target_hostname": "www.rexx.com", "target_uri": "http://www.rexx.com/%7Edkuhlman/python_201/python_201.html", "page_rank": 1.1791456e-09, "spam_rank": 81, "title": "<em>Python</em> 201 -- (Slightly) Advanced <em>Python</em> Topics", "snippet": "See <em>Python</em> Library Reference: 4.2.1 Regular Expression Syntax <em>for</em> more. Because of the use of backslashes in patterns, you are usually better off defining regular expressions with raw strings, e.g. <em>r</em>&quot;abc&quot;. When a regular expression is to be used more than once, you <em>should</em> consider compiling it.", "explanation": null, "document": "Python 201 -- (Slightly) Advanced Python Topics\nPython 201 -- (Slightly) Advanced Python Topics\nDave Kuhlman\nFront Matter\nCopyright (c) 2003 Dave Kuhlman\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\nAbstract:\nThis document is a syllabus for a second course in Python programming. This course contains discussions of several advanced topics that are of interest to Python programmers.\n1. Python 201 -- (Slightly) Advanced Python Topics\nThis document is intended as notes for a course on (slightly) advanced Python topics.\nDefining a regular expression is to provide a sequence of characters, the pattern, that will match sequences of characters in a target.\nHere are several places to look for help:\nThe patterns or regular expressions can be defined as follows:\nLiteral characters must match exactly. For example, \"a\" matches \"a\".\nConcatenated patterns match concatenated targets. For example, \"ab\" (\"a\" followed by \"b\") matches \"ab\".\nAlternate patterns, separated by a vertical bar, match either of the alternative patterns. For example, \"(aaa)|(bbb)\" will match either \"aaa\" or \"bbb\".\nRepeating and optional items:\n\"abc*\" matches \"ab\" followed by zero or more occurances of \"c\", for example, \"ab\", \"abc\", \"abcc\", etc.\n\"abc+\" matches \"ab\" followed by one or more occurances of \"c\", for example, \"abc\", \"abcc\", etc, but not \"ab\".\n\"abc?\" matches \"ab\" followed by zero or one occurances of \"c\", for example, \"ab\" or \"abc\".\nSets of characters -- Characters and sequences of characters in square brackets form a set; a set matches any character in the set or range. For example, \"[abc]\" matches \"a\" or \"b\" or \"c\". And, for example, \"[_a-z0-9]\" matches an underscore or any lower-case letter or any digit.\nGroups -- Parentheses indicate a group with a pattern. For example, \"ab(cd)*ef\" is a pattern that matches \"ab\" followed by any number of occurances of \"cd\" followed by \"ef\", for example, \"abef\", \"abcdef\", \"abcdcdef\", etc.\nThere are special names for some sets of characters, for example \"\\d\" (any digit), \"\\w\" (any alphanumeric character), \"\\W\" (any non-alphanumeric character), etc. See Python Library Reference: 4.2.1 Regular Expression Syntax for more.\nBecause of the use of backslashes in patterns, you are usually better off defining regular expressions with raw strings, e.g. r\"abc\".\n2.2 Compiling regular expressions\nWhen a regular expression is to be used more than once, you should consider compiling it. For example:\nimport sys, re\n\npat = re.compile('aa[bc]*dd')\n\nwhile 1:\n    line = raw_input('Enter a line (\"q\" to quit):')\n    if line == 'q':\n        break\n    if pat.search(line):\n        print 'matched:', line\n    else:\n        print 'no match:', line\n"}, {"score": 1048.6906, "uuid": "8be406b7-c901-58c2-a304-e6a1a20fae64", "index": "cw12", "trec_id": "clueweb12-1606wb-37-10434", "target_hostname": "www.rexx.com", "target_uri": "http://www.rexx.com/~dkuhlman/python_201/python_201.html", "page_rank": 1.6354107e-09, "spam_rank": 81, "title": "<em>Python</em> 201 -- (Slightly) Advanced <em>Python</em> Topics", "snippet": "See <em>Python</em> Library Reference: 4.2.1 Regular Expression Syntax <em>for</em> more. Because of the use of backslashes in patterns, you are usually better off defining regular expressions with raw strings, e.g. <em>r</em>&quot;abc&quot;. When a regular expression is to be used more than once, you <em>should</em> consider compiling it.", "explanation": null, "document": "Python 201 -- (Slightly) Advanced Python Topics\nPython 201 -- (Slightly) Advanced Python Topics\nDave Kuhlman\nFront Matter\nCopyright (c) 2003 Dave Kuhlman\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\nAbstract:\nThis document is a syllabus for a second course in Python programming. This course contains discussions of several advanced topics that are of interest to Python programmers.\n1. Python 201 -- (Slightly) Advanced Python Topics\nThis document is intended as notes for a course on (slightly) advanced Python topics.\nDefining a regular expression is to provide a sequence of characters, the pattern, that will match sequences of characters in a target.\nHere are several places to look for help:\nThe patterns or regular expressions can be defined as follows:\nLiteral characters must match exactly. For example, \"a\" matches \"a\".\nConcatenated patterns match concatenated targets. For example, \"ab\" (\"a\" followed by \"b\") matches \"ab\".\nAlternate patterns, separated by a vertical bar, match either of the alternative patterns. For example, \"(aaa)|(bbb)\" will match either \"aaa\" or \"bbb\".\nRepeating and optional items:\n\"abc*\" matches \"ab\" followed by zero or more occurances of \"c\", for example, \"ab\", \"abc\", \"abcc\", etc.\n\"abc+\" matches \"ab\" followed by one or more occurances of \"c\", for example, \"abc\", \"abcc\", etc, but not \"ab\".\n\"abc?\" matches \"ab\" followed by zero or one occurances of \"c\", for example, \"ab\" or \"abc\".\nSets of characters -- Characters and sequences of characters in square brackets form a set; a set matches any character in the set or range. For example, \"[abc]\" matches \"a\" or \"b\" or \"c\". And, for example, \"[_a-z0-9]\" matches an underscore or any lower-case letter or any digit.\nGroups -- Parentheses indicate a group with a pattern. For example, \"ab(cd)*ef\" is a pattern that matches \"ab\" followed by any number of occurances of \"cd\" followed by \"ef\", for example, \"abef\", \"abcdef\", \"abcdcdef\", etc.\nThere are special names for some sets of characters, for example \"\\d\" (any digit), \"\\w\" (any alphanumeric character), \"\\W\" (any non-alphanumeric character), etc. See Python Library Reference: 4.2.1 Regular Expression Syntax for more.\nBecause of the use of backslashes in patterns, you are usually better off defining regular expressions with raw strings, e.g. r\"abc\".\n2.2 Compiling regular expressions\nWhen a regular expression is to be used more than once, you should consider compiling it. For example:\nimport sys, re\n\npat = re.compile('aa[bc]*dd')\n\nwhile 1:\n    line = raw_input('Enter a line (\"q\" to quit):')\n    if line == 'q':\n        break\n    if pat.search(line):\n        print 'matched:', line\n    else:\n        print 'no match:', line\n"}, {"score": 1048.5942, "uuid": "7adc7a2a-b7f4-5dff-95d7-29be6bdddf3b", "index": "cw12", "trec_id": "clueweb12-1606wb-93-27743", "target_hostname": "www.rexx.com", "target_uri": "http://www.rexx.com/~dkuhlman/python_201/", "page_rank": 1.1774839e-09, "spam_rank": 80, "title": "<em>Python</em> 201 -- (Slightly) Advanced <em>Python</em> Topics", "snippet": "See <em>Python</em> Library Reference: 4.2.1 Regular Expression Syntax <em>for</em> more. Because of the use of backslashes in patterns, you are usually better off defining regular expressions with raw strings, e.g. <em>r</em>&quot;abc&quot;. When a regular expression is to be used more than once, you <em>should</em> consider compiling it.", "explanation": null, "document": "Python 201 -- (Slightly) Advanced Python Topics\nPython 201 -- (Slightly) Advanced Python Topics\nDave Kuhlman\nFront Matter\nCopyright (c) 2003 Dave Kuhlman\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\nAbstract:\nThis document is a syllabus for a second course in Python programming. This course contains discussions of several advanced topics that are of interest to Python programmers.\n1. Python 201 -- (Slightly) Advanced Python Topics\nThis document is intended as notes for a course on (slightly) advanced Python topics.\nDefining a regular expression is to provide a sequence of characters, the pattern, that will match sequences of characters in a target.\nHere are several places to look for help:\nThe patterns or regular expressions can be defined as follows:\nLiteral characters must match exactly. For example, \"a\" matches \"a\".\nConcatenated patterns match concatenated targets. For example, \"ab\" (\"a\" followed by \"b\") matches \"ab\".\nAlternate patterns, separated by a vertical bar, match either of the alternative patterns. For example, \"(aaa)|(bbb)\" will match either \"aaa\" or \"bbb\".\nRepeating and optional items:\n\"abc*\" matches \"ab\" followed by zero or more occurances of \"c\", for example, \"ab\", \"abc\", \"abcc\", etc.\n\"abc+\" matches \"ab\" followed by one or more occurances of \"c\", for example, \"abc\", \"abcc\", etc, but not \"ab\".\n\"abc?\" matches \"ab\" followed by zero or one occurances of \"c\", for example, \"ab\" or \"abc\".\nSets of characters -- Characters and sequences of characters in square brackets form a set; a set matches any character in the set or range. For example, \"[abc]\" matches \"a\" or \"b\" or \"c\". And, for example, \"[_a-z0-9]\" matches an underscore or any lower-case letter or any digit.\nGroups -- Parentheses indicate a group with a pattern. For example, \"ab(cd)*ef\" is a pattern that matches \"ab\" followed by any number of occurances of \"cd\" followed by \"ef\", for example, \"abef\", \"abcdef\", \"abcdcdef\", etc.\nThere are special names for some sets of characters, for example \"\\d\" (any digit), \"\\w\" (any alphanumeric character), \"\\W\" (any non-alphanumeric character), etc. See Python Library Reference: 4.2.1 Regular Expression Syntax for more.\nBecause of the use of backslashes in patterns, you are usually better off defining regular expressions with raw strings, e.g. r\"abc\".\n2.2 Compiling regular expressions\nWhen a regular expression is to be used more than once, you should consider compiling it. For example:\nimport sys, re\n\npat = re.compile('aa[bc]*dd')\n\nwhile 1:\n    line = raw_input('Enter a line (\"q\" to quit):')\n    if line == 'q':\n        break\n    if pat.search(line):\n        print 'matched:', line\n    else:\n        print 'no match:', line\n"}, {"score": 1046.2516, "uuid": "b92bf231-683b-593e-b877-f7adac2fb300", "index": "cw12", "trec_id": "clueweb12-0406wb-47-25881", "target_hostname": "www.astro.cornell.edu", "target_uri": "http://www.astro.cornell.edu/staff/loredo/samsi/osx-install.html", "page_rank": 1.198777e-09, "spam_rank": 92, "title": "<em>Python</em> <em>for</em> Astrostatistics - Mac OS X Installation", "snippet": "If you&#x27;d like to work through the STScI <em>Data</em> <em>Analysis</em> With <em>Python</em> Tutorial, you <em>should</em> also install the DS9 image viewer (binaries <em>for</em> all major platforms are at the DS9 web site) and the following <em>Python</em> package from STScI, included in the tarball: Finally, copy the &quot;samsi-<em>for</em>-home.tar.gz&quot; file to your", "explanation": null, "document": "Python for Astrostatistics - Mac OS X Installation\nNote: You need to know the adminstrator/root password to install what you need. This is the same password OS X asks you for whenever you install or update a significant system component. The \"sudo\" command, invoked many times below, will produce a prompt asking you for it. This guide also presumes you are comfortable with use of the OS X Terminal application; these commands should be entered at a shell prompt in a Terminal window.\nSee the Python Statistical Computing Essentials page if you'd like to learn more about this software before or after installation.\nInstall Python for OS X\nOS X comes with a version of Python already installed. However, it is a rather minimalist installation, and it typically lags the current version by one minor version number (e.g, 2.3 vs. 2.4). You should install a separate version that is more recent and more feature-filled. It will not overwrite Apple's version, but it will be installed as the first \"python\" on your command path, so you will never encounter Apple's Python unless you go out of your way to use it.\nI recommend two options; I have installed the software mentioned below with both of them:\nMacPython \u0097 Mac Python developer Bob Ippolito hosts MacPython binary installers of this version of Python for both OS 10.3 and 10.4. The current version is 2.4.1, one \"microversion\" behind the current Python.org release. Microversions are bug-fix releases; in six years of Python programming I have yet to encounter a bug, so the minor version lag should not concern you.\nActivePython \u0097 The \"new kid on the block,\" ActiveState's ActivePython site is a binary installer that largely duplicates the MacPython installation (with the notable exception of omitting \"readline\" command line editing support due to license issues). But it includes a few extras, including an array of documentation (accessible via the \"Library\" menu in Help Viewer; you may need to logout and login again for the docs to get registered). It is maintained by ActiveState, a commercial developer tools company, but is provided for free. It is up-to-date with Python.org, at 2.4.2. You will really want readline capability (giving you cursor-based editing at the command prompt). The only way to get it is to grab an appropriate pre-built library from somewhere. Here is readline.so for Python 2.4 on Panther and readline.so for Python 2.4 on Tiger . Use the appropriate version to replace ActivePython's empty stub in this directory:\n/Library/Frameworks/Python.framework/Versions/2.4/lib/python2.4/lib-dynload/\nI can't guarantee this will work, but several users have reported success with this procedure. You will know it's working if, after starting the interactive Python interpreter with \"python\", you can use your arrow keys to edit an expression typed at the \">>>\" Python prompt.\nA quirk of OS X is that its window manager works quite differently than that of other platforms. As a result, if you run a Python program that requires GUI interaction (e.g., anything that plots with matplotlib), you should run it with the GUI-enabled \"pythonw\" command rather than \"python\". (ipython, installed below, always uses pythonw on the Mac.) The pythonw command is one of the \"extras\" you get with the Python distributions above (vs. Apple's Python, which has no cross-platform GUI support).\n\"python\" and \"pythonw\" get installed in /usr/local/bin; make sure this is earlier than /usr/bin (the location of Apple's Python) in your shell's PATH variable. There are a few other Python-based applications that get installed that you will want to access from the command line (e.g., idle, Python's IDE, and f2py, a Fortran-Python interface tool). To get easy access to these, you need to add the location of your new Python's binary directory to PATH. For example, the beginning of my PATH (from \"echo $PATH\" at the shell) is:\n/usr/local/bin:/System/Library/Frameworks/Python.framework/Versions/Current/bin:...\nThat second, long path is where MacPython and ActivePython install their binaries. Edit your .cshrc file (for tcsh shell) or .bashrc (for bash shell) to adjust your PATH accordingly.\nPrepare for Package Installation\nYou will need to be able to install Python packages from source; that is, you will run (simple!) commands that compile C, C++, and Fortran source code. You will also need some libraries the code must link against. You need to have Apple's Developer Tools installed to get gcc. A DVD with the Tools comes with OS X releases; the tools are also available as a free (but large!) download from Apple Developer Connection (you'll need to create a free account, then go to the download area and find the tools for your OS version). You also need Apple's X11 (Tiger users may have it already installed) and Tcl/TK Aqua (BI version) . Finally, you need an OS X version of the g77 Fortran compiler. Optionally, consider installing the FFTW2 Fourier transform library to accelerate FFTs. Chris Fonnesbeck's SciPy/Mac OS X Installation page explains where to get g77 and FFTW2.\nThe gcc compiler used by Apple's Developer Tools recently underwent a major version change, from 3.x to 4.x. For OS 10.4, the default gcc is a 4.x version. Users of a wide variety of software have reported problems due to bugs in early 4.x releases. The safest course is to change the default gcc to 3.x. Use \"gcc_select -l\" to list available compiler versions. Then use \"gcc_select #.#\" to set the active gcc compiler to version #.# (you may need administrator privileges to perform this operation; i.e., prefix it with \"sudo\"). Version 3.3 should be available and should work fine. Reportedly, very recent gcc-4.x versions are relatively bug-free, and you may have success with them (check Apple Developer Connection for the latest version). Further tips on this are at Chris's web page. You have been forewarned.\nThe matplotlib plotting package requires the FreeType (v. 2.1.7 or later), libpng , and zlib libraries. Panther users in particular will not have all of these unless they have been installed separately; I'm not sure of the situation for Tiger. Perhaps the most-used way to get them is as pre-built binaries via a package manager like Fink or DarwinPorts . (Here are some instructions using DarwinPorts specifically to get these libraries.) If you are using the popular gwTeX distribution of TeX for OS X , then you can use the i-Installer application you used to install TeX to install Freetype and perhaps the other libraries (use the i-Installer to see what's currently available). The libraries also build fine from source, using the standard \"./configure --prefix=/usr/local ; make ; make install\" installation sequence with minor variations. The Building R page gives instructions for libpng; this Pythonmac-SIG post describes the procedure for FreeType and libpng. Also, recently libpng was made available as an OS X binary from the libpng page linked above. I personally have used FreeType via i-Installer and the other libraries built from source.\nYou will soon be editing files containing Python source code. It will be handy to use an editor with a Python mode. There are many choices; some that I use on the Mac are xemacs (in an X11 xterm), jEdit , Eclipse with its pydev plugin (huge but feature-filled; good for organizing multi-file projects), and BBEdit (a very popular commercial text editor). If you are an emacs/xemacs fan, there is a useful Python mode for emacs. The IPython page hosts a widely-used, self-contained version: python-mode.el . You may also consider a newer, multi-file version that offers more functionality, just released by the python-mode project .\nInstall Packages for Scientific Computing\nFor several packages, you should be using a recent developer snapshot from the developer SVN or CVS archive, rather than the last official release, due to many recent innovations and bugfixes. At this point, OS 10.4 (Tiger) users are in luck. Chris hosts binary installers for the largest and most important packages we need. Install the numpy, scipy, numarray, and matplotlib packages from OSX at tichech.us (simple instructions are there). Then pick up the installation process below at \"ipython-0.7.0\". But read the comments at the end of the \"scipy\" entry, about suppressing import overwrite warning messages, and a recent FFT bug.\nFor OS 10.3 (Panther) users like myself, binaries may be available within a few days. But in light of the uncertainty, I've packaged all the releases in a single gzipped tarball so you can build the packages from source (time-consuming, but simple; and it's the computer that will take all the time). Download it and unpack it (\"tar zxf samsi-dist.tar.gz\") somewhere on your hard drive where you have substantial free space (300 MB or so). This will create a directory \"samsi-dist\" containing all the software. Then, in the following order, in a Terminal window, \"cd\" into the following directories and run the following install commands. The numbers in brackets indicate the [minutes:seconds] the command took on my old and slow 333 MHz PowerBook G3, to give you some idea of what to expect. Commands with no brackets run very quickly. You may delete each directory after the installation if you'd like to free the space (if you keep them, you can update the contents later via SVN or CVS and rebuild). You can safely ignore the many innocuous compiler warnings that will appear during some install procedures (they're warnings, not errors; real errors will abort the install process). The installations end with little fanfare; if there are no failure messages, the install went just fine.\nIf you have trouble with the installation, you can ask for help on one of the following email support lists. Please contact me for help only as a last resort, or if you think there is a mistake in these instructions; I will be in very limited email contact until the CASt/SAMSI School is over.\nPythonmac-sig \u0097 Discussion of Python on the Mac. Members have provided help for many of the packages here, and especially for general Python issues on OS X.\nscipy-user \u0097 For help with numpy and scipy\nmatplotlib-users \u0097 For help with matplotlib\nAstroPy (see the section near the bottom of this page) \u0097 A community of astronomers who use Python, including the developers of numarray, PyFITS, and numdisplay.\nSome packages come with test suites that you can run to verify the installation. I describe some below. I recommend you keep a separate shell open in which to run the tests. This will ensure that Python is finding the package installed in its global site-packages directory, and not in the distribution directory.\nNow to the package installations, by directory name under the unpacked distribution:\nnumpy\nsudo python setup.py install [5:28]\nIf you'd like to verify the installation, try this in your home directory (\">>>\" is the Python prompt; hit Ctl-D to quit Python at the end):\npython\n>>> import numpy\n>>> numpy.__version__\n'0.9.3.1882'\n>>> numpy.test(1,1)\nIt will print out a lot of information, including several lines of periods. Each \".\" indicates a passed test; failures will produce obvious and verbose messages. There may be a few warnings, errors or failures among the ~200 tests (they are being sorted out and won't affect our basic use). On my Mac all tests pass with this numpy release.\nscipy\nsudo python setup.py install [43:51]\nTest with:\npython\n>>> import scipy\n>>> scipy.__version__\n'0.4.4.1544'\n>>> scipy.test()\nFor a lengthier suite of tests, you may also try \"scipy.test(10)\". Around 1000 tests will be run; as above you can ignore the small number of possible failures for now (I get about 10).\nA few users have encountered a linking error when the installer attempts to build fftpack. If this happens, consult Chris Fonnesbeck's SciPy install page for a workaround.\nWhen you import scipy, it imports numpy. Both packages have FFT functions, so you will see a warning that SciPy is \"overwriting\" NumPy's fft and ifft functions. You can disregard this. If you'd like to suppress this import warning, set the SCIPY_IMPORT_VERBOSE environment variable to \"-1\" in your shell. For tcsh, use \"setenv SCIPY_IMPORT_VERBOSE -1\" and for bash, use \"export SCIPY_IMPORT_VERBOSE=-1\". You can use these at the command line, or stick them in your .cshrc or .bashrc file to make them \"permanent.\"\nOne last quirk. Recent revisions introduced a bug with SciPy's FFTs in at least some OS X installations; NumPy's FFTs remains solid. This will get sorted out soon; in the meantime, use numpy.fft and numpy.ifft in place of scipy.fft and scipy.ifft.\nnumarray-1.5.0\nsudo python setup.py install --gencode [10:16]\nTest with:\npython\n>>> import numarray.testall as testall\n>>> testall.test()\nmatplotlib-0.86\nsudo python setup.py build [33:18]\nsudo python setup.py install [0:55]\nTest it by going into the \"examples\" directory and trying to plot one of the examples. Since matplotlib displays plots in a GUI window, you need to use \"pythonw\" to run the examples in OS X. Try:\ncd examples\npythonw fill_demo.py\nIn the course of making your first plot, matplotlib will create a \".matplotlib\" directory in your home directory where it can access a startup profile and cache various information. You will likely see a bunch of warnings referring to obscure fonts it cannot find as it tries to build a font cache; you can safely ignore these! You should soon see a plot; when you close it, the Python command will terminate.\nAt this point, return to the install directory, where you'll find a default startup profile, \"matplotlibrc\". Copy this to your .matplotlib directory:\ncd ..\ncp matplotlibrc ~/.matplotlib\nUse your favorite text editor to edit the copy in your .matplotlib directory (not the original!). About 30 lines down from the top you should find two lines that look like this:\nbackend      : TkAgg     \nnumerix      : numpy  # numpy, Numeric or numarray\nMake sure they say \": TkAgg\" and \": numpy\" as shown above. These tell matplotlib to use an anti-aliasing Tcl/Tk backend for plotting, and to use numpy arrays.\nipython-0.7.0\nSince we'll be using ipython to draw plots with matplotlib, we need to build it with \"pythonw\" rather than \"python\"; this signals the installer to build a GUI-capable version. The ipython program is a Python script; tell the installer to put the script in /usr/local/bin so your shell will know where to find it.\nThe first time you run IPython (just type \"ipython\" at the shell), it will create a \".ipython\" directory in your home directory (and warn you about it). It installs a set of default startup profiles there that you may edit to customize how IPython behaves.\nsudo python setup.py install\nOne of IPython's profiles is intended for use with SciPy and is invoked with \"ipython -p scipy\". However, it tries do some \"magic\" tailored to the last official release and won't work with the latest version. So just use plain \"ipython\" for now, or write your own profile that automatically imports scipy.\nPyFITS\nsudo python setup.py install\nIf you'd like to work through the STScI Data Analysis With Python Tutorial, you should also install the DS9 image viewer (binaries for all major platforms are at the DS9 web site ) and the following Python package from STScI, included in the tarball:\nnumdisplay\nsudo python setup.py install\nSAMSI extras\nFinally, copy the \"samsi-for-home.tar.gz\" file to your home directory and unpack it there. It will create a \"samsi\" directory that includes a copy of the matplotlib examples directory (a good source of plotting recipes), and copies of some basic documentation: the official Python Tutorial and Standard Library docs, the IPython manual, the matplotlib tutorial, and the STScI Python Data Analysis Tutorial.\nMaintenance\nThe numpy, scipy, and numarray installations are from current SVN or CVS developer archive revisions. There will be solid official releases of all of these packages quite soon, including binary installers for several platforms; check the web sites (linked on the Essentials page) for news about official releases. If you are ambitious (courageous?) and wish to stay on the \"bleeding edge,\" do not delete the unpacked directories after installing the software. As you wish, update them via SVN or CVS as follows (execute these in your shell in the directory right above the package directory), and re-install the packages.\nNOTE: Before you reinstall a developer snapshot, which may have significant internal reorganization, you should manually remove all results of an earlier installation. To do this, delete the \"build\" folder within the package's source directory (the installer creates this to store files during the installation process). Also, delete the installed Python package in your Python's \"site-packages\" directory. This is a directory with the same name as the package, located here:\n/Library/Frameworks/Python.framework/Versions/2.4/lib/python2.4/site-packages\nnumpy from SVN (monitor at numpy SVN timeline ):\nsvn co http://svn.scipy.org/svn/numpy/trunk numpy\nscipy from SVN (monitor at scipy SVN timeline ):\nsvn co http://svn.scipy.org/svn/scipy/trunk scipy\ncvs -d:pserver:anonymous@cvs.sourceforge.net:/cvsroot/numpy login \ncvs -z3 -d:pserver:anonymous@cvs.sourceforge.net:/cvsroot/numpy co -P numarray\nmatplotlib from CVS:\ncvs -d:pserver:anonymous@cvs.sourceforge.net:/cvsroot/matplotlib login\ncvs -z3 -d:pserver:anonymous@cvs.sourceforge.net:/cvsroot/matplotlib co -P matplotlib\n10 January 2006 \u0097 Tom Loredo\n"}, {"score": 1028.0613, "uuid": "24cd101f-3d10-58c9-bda8-e8571d36a210", "index": "cw12", "trec_id": "clueweb12-0708wb-43-14121", "target_hostname": "zope.stackless.com", "target_uri": "http://zope.stackless.com/index_old.htm", "page_rank": 1.1700305e-09, "spam_rank": 66, "title": "Stackless <em>Python</em>", "snippet": "Actually it was true, and <em>I</em> had to change truth before <em>I</em> could continue. <em>For</em> further information, have a look into my paper <em>for</em> the 8th International <em>Python</em> Congress: Article in HTML format, as PDF file, Word 2000 document, <em>or</em> everything in a Zip compressed file.", "explanation": null, "document": "Mailing Lists\nSubscribe to the Stackless discussion or checkins list\n02-Mar-04: Bugfix Release While destructing tasklets, it can be necessary to create a new main tasklet just for the destruction. This did not work when an exception was pending. Added support for that to 2.2 and 2.3.\n01-Mar-04: Bugfix Release A while ago, we made tasklets garbage collected objects. By that, we introduced a bug, which acts like a time bomb. During garbage collection, the current tasklet is visited, when it has no valid frame in its frame field. A simple fix made sure that a tasklets frame is either NULL or valid. This patch was applied to both 2.3 and the (still) maintained 2.2 version.\nMany thanks to Seung Chan Lim for reporting this bug.\n24-Feb-04: Stackless Sprint in Berlin A sprint on Stackless Python will be done In Berlin, March 10-14. We are planning to work on auto-scheduling, more softswitching support, documentation, examples, Zope, refactoring Stackless into configurable layers, and more. See the announcement .\n24-Feb-04: Small corrections for Python 2.3.3 There were some pickling problems which caused IDLE to work no longer. This has been fixed. At the same time, an error in standard Python's cPickle showed up. Patch submitted.\n15-Feb-04: Stackless 3.0 for Python 2.3.3 is ready After a longer search for some final bug wihich applied to both Stackless for Python 2.2 and 2.3, I am releasing a so far final version of Stackless 3.0. There are a couple fo enhancements planned, of course. Some of them will be the theme of the upcoming Sprint on Stackless Python in March 2004\n29-Jan-04: Huge steps towards Python 2.3.3 What can I tell, things are just moving on very quickly. Bob is doing a very good job enhancing stuff and moving Stackless further towards Python 2.3.3. I'm very grateful for his tremendous efforts. A couple of people are actively discussing Stackless' future, interface changes and design goals on the Stackless list. People are sending money to support Stackless. This is great!\n25-Jan-04: Again towards Python 2.3.3\nBob Ippolito wrote the pickling code for the new iterator types in Python 2.3, today. Many thanks!\nI still have some problems with tasklet finalization which I don't understand These apply to both 2.2 and 2.3 . I had a working solution for 2.2, which I changed thoughtlessly, but since I also don't understand why that worked different, I have to carry on :-)\n19-Jan-04: Small fixes, on the way towards Python 2.3.3\nSome problems came up with the Mac OSX port. Bob Ippolito adjusted the registers to be saved during a stack switch, and it works fine, now.\nI spent the last weeks to develop a port to Python 2.3.3, which is almost ready now. The cvs tree is prepared. WHen the new version is done, I just have to change a symlink. People who would like to have access to all versions might drop me a note.\n08-Jan-04: Small fixes, heading towards CSP\nSamit Patel found a finalization bug that dumped core when channels were not empty on destruction. Bob Ippolito observed some incompatibilities on Mac OS X, and pointed me to some scripts which needed to be adjusted to the new structure. Tom Locke is building CSP support for Stackless. While doing so, he stumbled over related problems with tasklet finalization. Repaired these and actualized the binaries.\n06-Jan-04: A Huge Change in the Code Base\nFor the last two+ weeks, I was hacking on the internals of stackless, in order to make things cleaner, smaller, more natural and more complete.\nThis patch is a quite essential refactoring of the internals, enhancing functionality at the same time. On the other hand, semantics are almost not touched.\nThis finally became a redesign of a lot of the machinery. Especially, frames no longer have f_tstate, and much more important, frames no longer carry informationabout the C state. C state (as C stacks) is now a property of the tasklets.\nAll tasklets now have a c state by default.\nThere are trivial cstates and non-trivial cstates. They are distinguished by the nesting_level.\nA trivial cstate is a C state with nesting_level == 0. This cstate does not need to be saved, since it's context is well-known. It can be replaced by a number of trivial statements, which realize so-called soft switches, almost an order of magnitude faster.\nNon-trivial cstates are those created by C stack switching. Note that exactly these tasklets are not restartable after unpickling!\nAs a major enhancement, the system now keeps track of all tasklets which are using a non-trivial cstate, ensuring that all these tasklets will receive a kill() signal if they do not finish normally before thread termination.\nHere the check-in message:\n\"\"\" \nThis was by far the largest check-in since months!\nf_tstate variables are removed from frames. cstack objects are removed from frames.\nAll cstate is now kept in the tasklets. Tasklets with serious cstate are now killed\nautomatically on thread deallocation. Tasklet soft-switching is now secured against\nrepeated entry from \"outside\": A version variable is tracked, which makes sure that\n\"main\" is always left with the most recent version of initial_stub.\n\nHey, this was two weeks of fulltime work!\n\"\"\"\nPlease, give this code a heavy load of testing! With a few changes, this should be the code base for porting Stackless to Python 2.3.3. Especially, I'm planning to drop the version variable and use a reference to initial_stub, instead.\n18-Dec-03: Almost Last Update of this Website The transfer of this website is still in the works. I'm gradually moving things over. There was a lot of changes and enhancements to Stackless in the last months. I will try to extract a web page from the CVS logs.\nOne first thing you might want to try is a Demo of Stackless Zope . This is nothing fancy, the point is the completely different paradigm of web programming.\n02-Jul-03: Reorganizing the Website This website is going to be re-organized completely. Part of it will be moved into a Wiki, which will show up in a few days.\n27-Jun-03: Stackless again at EuroPython After another long period of silence, but being very vary busy to get Stackless 3.0 ready, I had a 90 minutes presentation at EuroPython 2003. The graphical highlight of the talk was my presentation of the EVE online game , which is now sold since a couple of weeks. Unfortunately there were a few hardware problems with my desktop, and I need to buy a better laptop for such opportunities. The software highlight was the new outstanding feature of Stackless 3.0: Thread Pickling is supported from now on and for all times.\nAgain a long break But don't worry. This time, I am preparing a huge overhaul of all of Stackless. There is a technology merge under the way which is very hard to do: Combine the elegance and small footprint of Stackless 1.0 with the power of Stackless 2.0. This is again a complete re-implementation of Stackless, re-introducing ideas from the first implementation, but without continuations. Tasklets and channels will stay the basic building blocks of Stackless. There is almost no change in the user interface: Stackless 3.0 is just better, smaller, faster and more versatile, but software should run almost unchanged.\n15-Oct-02: Ported to Python 2.2.2 One day after the release of Python 2.2.2, Stackless has been ported to it. I hope to be quicker next time. There were a couple of changes to typeobject.c which were backported from 2.3, and I had to make many adjustments (basically thowing stuff away). My changes to the type implementation are now very small and will be re-submitted as a proposed patch.\nThe PC binaries have been upgraded to 2.2.2 and include _tkinter as well.\n10-Oct-02: Scheduler Object introduced In order to gather more flexibility for deriving your own scheduler, I provide a scheduler object, which can be overridden by the user. The primary reason for this is the very different scheduler implementation of IronPort, which needs to be supported. Right now, the object is almost empty, but it will be filled with methods as the porting progresses.\n07-Oct-02: Support for PPC Unix and S390 Thanks to Gustavo Niemeyer for his additional platform support. I'm thinking to start a porting contest, in order to get all the rest of platforms prepared for Stackless :-)\n15-Sep-02: Support for Sparc and Cygwin On July, 16, Gerd Woetzel sent me a patch that supports Sun Sparc. It took this long to get back to it, due to all the other changes. Many thanks, Gerd! Stackless now supports:\nWindows, Mac OS X, Linux-X86, Cygwin, SunOS Sparc with gcc.\nSorry, multiple-channel waits had to wait for this to get ready. Will come rather soon now...\n15-Sep-02: Stackless as External The Stackless core code and the stackless module application are now split apart. Gettig the Linux build to work again took me quite some days, but now everything is in good shape. Stackless with tasklets and channels is now an external module. The PC binaries now include stackless.pyd as extension module.\n13-Sep-02: Stackless with Flextype In order to be able to have an efficient, clean way for inheritance from my internal types, I invented flextype.c, which supports virtual method tables in type objects. This leads to very efficient virtual methods with no time penalty when a method is not overridden. Virtual method resolution is done when a class is instantiated. The source tree is now split into many small files, and the project is reorganized quite much.\n02-Sep-02: New Beta with raise_exception Quite a lot has happened. Tasklets support now raise_exception, whether they are waiting on a channel or not. A lot of changes have been made. The whole schedule() implementation is re-worked, and the tasklet flag handling as well. Please try it out. The PC binaries have been updated. A complete rework of the channel protocol, together with multiple channel waits, will appear in a few days.\n25-Aug-02: Stackless Beta with PC Binaries Stackless has been improved reasonably in the last weeks. Please get the latest binaries and try it out.\n01-Jul-02: Stackless Ported to Power PC Thanks to Just van Rossum, who gave me an account on his PPC machine, I was able to finalize the PPC port of Stackless. It seems to work great. Please give it some testing and send me feedback. Note that I have no idea about PowerPC assembly. I just used what I learned from ICON's swtich implementation, and from Armin Rigo's patch for the x86-Unix version.\n26-Jun-02: Stackless At EuroPython After hacking on the pickler stuff until the morning of this first EuroPython day, I was quite sure that this will work. I used the last three hours before my talk to throw some 20 slides together, and was able to show first results during the talk. It worked out great. The conference was also a big success, with over 250 attendees, and many interesting talks, a lot of Python VIPs...\n23-Jun-02: Thread Pickling? I got an email from Arman Bostani, concerning the thread pickler. Trying to answer his email, I happened to fall into hacking mode and begun to implement what is necessary to pickle threads which have ctsack objects attached to frames. I ended up with doing an analysis of compiler map files, source/code/assembly listing, instead of preparing my slides for EuroPython.\n21-Jun-02: TCL problem again solved so far Jeff corrected some of his own bugs. It still crashes when one thread uses slicing, but it works with a single thread. Fortunately, the Tcl developers informed me that they are planning to remove the stack references completely in Tcl 2.4, so we decided to finalize the solution when this version is available.\n17-Jun-02: TCL problems again Jeff Senn reported crashes with Tcl and multiple (real) threads, when running the interpreter loop in a seperate thread. I could not really track it down. Jeff also used a patched Stackless with his own scheduler built in, so there were quite a few new variables in the play.\n26-May-02: TCL problem solved so far After lots of debugging and hair-pulling, it seems to be solved. The simple idea is to block the stack slicing feature in the context of tkinter calls. Waiting for user feedback, if this solution is complete.\n22-May-02: TCL becoming a problem During my stay at IronPort, I happend to step on some problems with tkinter. Tcl/Tk does keep global data on the C stack and expects it to be accessible during recursions into the Python/Tcl interpreter. This is really bad. When being \"stackless\", stack slices are removed at every n (=8 or so) recursion levels.\n18-May-02: Limbo Dancing works fine! Yet some more thinking for weeks, and a few days of implementation. Now, the first implementation of channels is available. It isn't complete, but works just fine. The stackless module now supports to create channels by function channel(). A channel has the methods send() and receive(). Try it yourself! You might either want to build your own Stackless, checking out the module stackless from :pserver:anonymous@stackless.com:/home/cvs , or, for Windows users, just take the pre-built binary , extract it into some directory, and start Python or PythonWin from there. I will for sure build an installer again, after I have got some feedback.\nThis implementation tries to follow the concepts of the OCCAM/ALEF/LIMBO languages by supporting the channel concept as the central communication/blocking/control feature. Tasks do not communicate directly, but through channels.\nYou might have guessed it: Stackless will try to implement/support Hoare's CSP, finally. But before, I have to understand more of it. This will happen in the next few weeks.\nThis release is an alpha release. There are most probably a lot of omissions. Some are obvious:\n- There isn't yet a scheduler, like there was one for the uthread module. This is trivial to implement and will be added in almost no time.\n- There isn't yet an ALT/PRI ALT construct like in OCCAM/ALEF/LIMBO. This is most sophisticated and very hard to implement correctly! I will try to find out how to do this the best way, during my stay at IronPort , from May 18th to May 29. If somebody from the US would like to contact me by phone, please use the opportunity and call me via IronPort . I am employed by this company.\n- You shouldn't use channels from the interpreter prompt, but in scripts, only. The machinery doesn't check for changed interactive sessions yet and will most likely crash, soon. Despite of that, it seems to work nicely with PythonWin and extensions.\nApril 02: Stackless goes Limbo After the thinking pause, I am now busy implementing the new design and writing some paper about it. I decided to adopt the concepts of Inferno 's programming language Limbo . You might want to read about their concepts in Brian W. Kernighan's A Descent into Limbo . To give a quick idea, just this: Tasklets are scheduled preemptively. All data and control logic between tasklets is performed by channels. Tasklet synchronisation and blocking is performed on reads and writes via a channel.\n16-Mar-02: Hired for Stackless Stackless got the best sponsorship ever: I was hired part-time by IronPort , a company near San Francisco with a great messaging product that is based upon Stackless! It is a special delight to be in the same company with Sam Rushing. I'm working for them in Germany, coming over from time to time. Really great!\n06-Mar-02: Thread Pickler It is incredible! Arman Bostani from TwinSun told me that they have implemented a thread pickler! Their only reason to use the old stackless was its freedom from the C stack, and they implemented in fact such an animal. They are going to donate it to the Open Source. The big drawback: My new version is no longer C stack independant. I promised them to do everything necessary to make this work with the new Stackless, too.\nMarch 02: I need a pause for thinking After some discussions on py-dev and a lot of private emails, I became quite undecided how to continue the system design. I wasn't able to continue coding until I was completely sure what to code.\n24-Feb-02: Further Simplification There is now only one single stack switching function left, which contains just four lines of assembly code. Recursion elimination has been mapped onto two internal transfer calls.\n22-Feb-02: 1,000,000 Switches in 0.6 seconds I was stunned when I measured switching speed on an AMD Athlon XP 1500+ machine. A python script switched tasklets 1,000,000 times. The usual Python call overhead was included. I measured this without switching and got about 0.3 seconds. That means: Context switching by an automated, internal scheduler without the interpreter overhead comes at the same cost as a Python function call.\n22-Feb-02: 100,000 Tasklets in 80 MB Right now I'm designing tasklets, which are the interface objects for coroutines, microthreads, however you name them. A tasklet wraps a chain of Python frames and a piece of the C stack -- the piece that will be needed to restart the tasklet. Today, I tested a Win32 release build and created 100,000 tasklets. The application's size grew to 80 MB. That is: A tasklet doesn't cost much more than 800 bytes.\n14-Feb-02: O'Reilly Network: Stackless Reincarnate Please read what a nice article Steven Figgins created again from an interview.\n21-Jan-02: Stackless is in CVS. I created a CVS repository on tismer.com and checked a full Python 2.2 source tree in, with the vendor tag python_2_2. The module name is, guess, \"stackless\". Here the login data:\n:pserver:anonymous@stackless.com:/home/cvs\nAdditionally, there is a mailing list which fills your inbox with all my patches: http://www.stackless.com/mailman/listinfo/stackless-checkins/\n19-Jan-02 Stackless Python is DEAD! Long live Stackless Python\nHere the message that I sent out, declaring the end and the beginning of a new era . Stackless now breaks its primary rule, never to touch the C stack. Without this rule, we get the best implementation ever, with unlimited switching. Stackless will not make it into the core after this change, but who cares. The maintenance work is from now on a cake walk.\nA long, long break. Is Stackless dead? It was, nearly. An attempt to port to 2.1 didn't work out. I got bored to have to continously try to catch up with the rasant development of Python, for a Stackless that would probably never get complete, and that would never make it into the Python distribution, causing me to maintain this incomprehensible code for life.\n14-May-01: Sourcecode Debug update: Some minor changes to the project files, ceval.c and continuationmodule.c, in order to support a DEBUG build. There is also a \"pyexe20\" project supplied for building the debug Python shell. On behalf of Aaron Brancotti from First Thought , Italy.\n04-May-01: My Apologies: This bugfix is ready since two weeks and I didn't upload it. Sorry, this shall not happen again.\nStackless Python 2.1 will be available, soon. I already have an internal version, but I want to to some harder changes to the kernel before releasing the code.\n18-Apr-01: Probably the last bugfix for 2.0: Bernd Rinn reported of a memory leak in March. It took weeks to nail that bug down. For heaven's sake, I wanted to get the 2.1 port done, but fixing this bug was of higher priority.\n06-Mar-01: The Award is there: Thanks to all who voted for me. I got the trophy on the reception of SPAM9. Pictures will be added, soon.\n23-Feb-01: Yet another bugfix: Changjune Kim reported a funny bug in interactive mode. This is solved, the bug was due to an unitialized error variable. People who don't like to download the source again need to do just this: Insert \"err = 0;\" after line 1477 of ceval.c:\ncase PRINT_EXPR:\n"}, {"score": 1025.0637, "uuid": "f17ffd47-ce67-5661-b566-072715881290", "index": "cw12", "trec_id": "clueweb12-0008wb-85-29202", "target_hostname": "www.adass.org", "target_uri": "http://www.adass.org/adass/proceedings/adass99/O3-03/", "page_rank": 1.4622974e-09, "spam_rank": 98, "title": "A New CL <em>for</em> IRAF Based On <em>Python</em>", "snippet": "Fitzpatrick, M. 1997, in ASP Conf. 145, Astronomical <em>Data</em> <em>Analysis</em> Software and Systems VII, ed. <em>R</em>. Albrecht, <em>R</em>. N. A. Shaw, <em>R</em>. A. White, <em>R</em>.", "explanation": null, "document": "Greenfield, P. & White, R. L. 2000, in ASP Conf. Ser., Vol. 216, Astronomical Data Analysis Software and Systems IX, eds. N. Manset, C. Veillet, D. Crabtree (San Francisco: ASP), 59\nA New CL for IRAF Based On Python\nP. Greenfield, R. L. White\nSpace Telescope Science Institute, 3700 San Martin Dr., Baltimore, MD 21218\nAbstract:\nWe have developed a new CL for IRAF to allow us to develop scripts with capabilities not easily obtained in the current IRAF CL. The new CL is based on Python, a powerful, freely available object-oriented scripting language with an large and growing user base. We can run virtually all existing IRAF tasks with full functionality including interactive graphics and image display. Since there is a module for Python that provides IDL-like array manipulation, we expect to eventually combine the strengths of the IRAF and IDL environments into one system.\n1. Introduction\nWe have long felt there was an important role for a more powerful Command Language (CL) for IRAF. A suitably powerful CL could become the data analysis software language of first resort, one that both astronomers and programmers would consider using first and where writing applications in a traditional compiled language such as C, Fortran or SPP would be infrequently required. Nevertheless, when necessary, it should be easy to incorporate compiled code into the CL either as a task, or by calling newly added functions within a CL program. Furthermore, the CL should be useful not only as a tool for astronomical data reduction, but also for many other tasks including accessing databases, the web, and text processing. Since we have a large investment in existing IRAF tasks it is essential that a new CL provide full access to existing IRAF task functionality in a style that current IRAF users will find comfortable.\nDeveloping a new scripting language that satisfies these general requirements is not easy. We decided at the outset that the only practical way of developing a new IRAF CL was to adapt an existing scripting language to the task. By choosing suitably we would automatically get the wished-for attributes for free (except for the ability to run IRAF tasks).\nThe above requirements and the attributes of the existing CL that we wished to preserve resulted in a more specific list of requirements for an acceptable scripting language.\nA powerful but clear programming language,\nSupport for object-oriented programming,\nAccess to a wide base of libraries (built-in and external),\nEasy access to code written in standard languages such as Fortran, C, C++, and Java,\nIDL-like data access and image manipulation capabilities,\nEasy access to GUI facilities,\nOpen Source and freely distributable,\nPortable,\nLarge user base and good prospects for long term support.\nThe popularity of IDL in the astronomical community (Shaw et al. 1999) has demonstrated that it is possible to write significant astronomical data analysis applications using an array-based interactive language. Having such a capability in a scripting language would enable it to serve for more than just scripting IRAF tasks; it would also be a language in which actual data analysis algorithms could be implemented directly.\nOur requirement for object-oriented features was born out of conviction that such a capability would make a new CL environment significantly more flexible and extendible.\nThe requirement for a large user base stems from a natural desire to develop using software that has a large community of people supporting it. The larger the community involved, the more talent and resources are available to support the language itself as well as libraries it uses. An orphaned language presents the problem of providing support for the language and its libraries with limited resources and expertise.\n2. Why Python?\nOne reason we chose Python as the basis for our new CL is because it satisfied these requirements. But what about its competitors? While Python is a reasonably well known scripting language, its user base is smaller than that of two better known scripting languages, namely Tcl and Perl. Nevertheless, we believe that the following characteristics dictated that Python be chosen over Tcl and Perl for our purposes.\nFirst, we felt that readability was an essential requirement. Since we expect that the new CL will be used more by astronomers than by professional software developers, we need a language that is both quick to learn and comparatively easy to read and understand for those who cannot devote large amounts of time to becoming an expert with it. In that regard we judged Python to be far superior to Perl. Python is also better for writing object-oriented programs since it was designed to be an object-oriented language from the start--though one can write procedural programs just as easily.\nWhile Tcl is generally more readable than Perl, its support for numeric types is weak. It is not a language we considered a natural choice for developing numerical algorithms. It also is not as well suited as Python for object-oriented programming.\nArray-based numeric operations are not part of the base Python system. But there is an extension module (Numeric) which does implement an efficient array-based functionality that can provide many of the features found in IDL and Matlab.\nAlthough Python's user community is smaller than that of Perl or Tcl, it is substantial and it is continuing to grow rapidly. It has been adopted by major organizations including Hewlett Packard, Red Hat, Industrial Light and Magic, and many others. While no language's future is guaranteed, we expect that Python will be well supported for many years to come.\n3. Technical Challenges\nWith the adoption of Python, the primary technical challenge was reduced to being able to run IRAF tasks with full functionality. One condition that we set was that we should not require any changes in the IRAF system proper. The new CL must coexist with the existing system and CL.\nTask Control and Execution. To run IRAF tasks, we must emulate IRAF task control and communications faithfully enough that tasks ``believe'' they are communicating with the old IRAF CL. IRAF tasks run as connected subprocesses and multiplex several I/O channels onto the executable's stdin and stdout channels. We were able to adapt an existing module written in Python to start and communicate with an IRAF connected process, and we wrote Python code to handle the multiplexing/demultiplexing of IRAF I/O between Python and the IRAF task.\nTraversing IRAF Packages and Interpreting Parameter Files. To make tasks easy to use requires being able to interpret the IRAF package structure from package CL scripts (and thus being able to determine what packages are available, what tasks are contained within, and where the executables are located). We wished not to maintain any parallel configuration information but, rather, to use the information contained within IRAF itself. IRAF parameter files are an integral part of the IRAF system; any CL that wishes to use IRAF tasks must be able to interpret IRAF parameters in the same way the IRAF CL does. This includes handling typing issues, range or value checking and various other parameter attributes.\nGraphics and Image Display. The existing IRAF CL manages the graphics of IRAF tasks. Retaining graphics functionality requires being able to render the graphics stream from the task and being able to handle IRAF CL gcur mode for interactive graphics. We implemented a Python/IRAF graphics kernel using Tkinter and OpenGL libraries. While image display does not go through the CL, imcur interactions do, so it is necessary for a CL to be able to interact with the image display server (e.g., ximtool or saoimage). This capability was provided by wrapping the NOAO Client Display Library (Fitzpatrick 1997) with SWIG (Simplified Wrapper Interface Generator, Beazley 2000).\nCL Emulation. Without CL emulation, a number of IRAF tasks that exist as CL scripts will not be accessible in the new CL. The IRAF CL has a complicated syntax and semantics, and emulating it is not trivial. Nevertheless, we have written a CL compiler that converts CL procedures to Python using one of the Python parsing modules (SPARK, Aycock 1998).\nFamiliar User Interface. While Python syntax is one that most programmers will find very straightforward, it is not one that all existing CL users will find convenient. We have provided a front end to the Python interpreter that allows the use of IRAF command mode syntax as well as standard mode syntax for interactive use.\nWe have found Python to be both very powerful and productive. It has been one of those rare instances where we have accomplished more than we set out to do with significantly less effort than expected. We have implemented all of the above capabilities in Python itself. The only occasions we have had to resort to using C have been for Xlib manipulations not available in Tk (such as focus or cursor manipulations) and to wrap the Client Display Library. Speed has been an issue only during initialization on slow workstations.\n4. Future Plans\nWe see the development of a new CL--which we've dubbed ``Pyraf''--as having four phases. The first (completed) demonstrates that it is possible to run IRAF tasks with full functionality. The second makes Pyraf usable as a scripting language for IRAF. This involves supplying some utility functions and settling on public interfaces to IRAF tasks and environment. We have nearly completed this phase. Phase 3 makes Pyraf usable as an interactive environment for the typical astronomer. This includes supporting IRAF CL command mode syntax, providing a parameter editor (a prototype exists, see De La Pena 2000) and OS type utilities (e.g., dir, chdir, page, etc.), and logging. This phase will form the basis of our beta release (expected in December 1999).\nThe final phase makes Pyraf usable for IDL-style analysis and programming. While there is existing functionality that allows much of this, we feel it is not quite mature enough to meet the expectations of most astronomers, particularly those who currently use IDL. Some improvements are needed to the Numerics module to fill holes in functionality and to make it more memory efficient. A full-featured, easy-to-use plotting package is needed (there is good progress in the Python community on that front). There is also work proceeding on enhancing the FITS I/O module (Barrett & Bridgman 2000). An additional year will be needed to bring Pyraf to the point of being a practical alternative to IDL as a data analysis language.\nWe are already beginning to use Python for new STScI tools. Initial projects include a GUI front end for Synphot, a revised dither package, and an ACS display tool. The first release is expected in the summer of 2000.\nReferences\nAycock, J. 1998, Proc. 7th International Python Conference, 100\nBarrett, P. & Bridgman, T. 2000, this volume, 67\nBeazley, D. 2000, this volume, 49\nDe La Pena, M. 2000, this volume, 63\nFitzpatrick, M. 1997, in ASP Conf. Ser., Vol.\u00a0145, Astronomical Data Analysis Software and Systems VII, ed.\u00a0R. Albrecht, R.\u00a0N.\u00a0Hook, & H.\u00a0A.\u00a0Bushouse (San Francisco: ASP), 200\nShaw, R. A. White, R. L. & Greenfield, P. 1999, this volume, 24\n"}, {"score": 1012.6029, "uuid": "5304a86e-3879-5650-89be-2c5c4c6fe5d6", "index": "cw12", "trec_id": "clueweb12-1314wb-49-25640", "target_hostname": "www.r-bloggers.com", "target_uri": "http://www.r-bloggers.com/online-resources-for-learning-r/", "page_rank": 1.1700305e-09, "spam_rank": 72, "title": "Online Resources <em>for</em> Learning <em>R</em> | (<em>R</em> news &amp; tutorials)", "snippet": "You can subscribe <em>for</em> e-mail updates: And get updates to your Facebook: If you are an <em>R</em> blogger yourself you are invited to add your own <em>R</em> content feed to this site (Non-English <em>R</em> bloggers <em>should</em> add themselves- here) ggplot2 <em>r</em>-project graphics programming rstats <em>R</em> Language Books visualization <em>Data</em> events", "explanation": null, "document": "Online Resources for Learning R\nDecember 1, 2010\n(This article was first published on Josh Paulson's Blog \u00bb R , and kindly contributed to R-bloggers)\nOnline classes are an easy and convenient way to learn more about a topic of interest. Not surprisingly, there are a variety of online resources, free and otherwise, to learn more about R. From online graduate classes, to the more \u201clearn at your own pace\u201d approach, here are some resources I have found useful:\nProgramming R \u2013 Beginner to advanced resources for the R programming language\nStatistics.com \u00e2\u0080\u0093 Variety of courses including beginner and advance topics in R\nProgramming in R \u00e2\u0080\u0093 Intermediate level programming\nR graphics \u00e2\u0080\u0093 Learn to fully customize plots and graphics\nggplot2 \u00e2\u0080\u0093 In depth course taught by Dr. Hadley Wickham\nUniversity of Washington \u00e2\u0080\u0093 Certificate in Computational Finance with R Programming\nCarnegie Mellon \u2013 Open and Free Courses including teaching Statistics with R\nPenn State \u00e2\u0080\u0093 Online Learning through the Department of Statistics\nTo leave a comment for the author, please follow the link and comment on his blog: Josh Paulson's Blog \u00bb R .\n"}, {"score": 1000.61, "uuid": "14b07ee4-ca9e-5064-b3e9-7d25e7674488", "index": "cw12", "trec_id": "clueweb12-1018wb-75-04075", "target_hostname": "microsoftcambridge.com", "target_uri": "http://microsoftcambridge.com/Events/TestinginPython/tabid/232/Default.aspx", "page_rank": 1.3739125e-09, "spam_rank": 78, "title": "November <em>Python</em> Meetup: Testing in <em>Python</em>", "snippet": "* Location: Microsoft New England <em>R</em>&amp;D Center, One Memorial Drive, Cambridge, MA 02142 * Audience: <em>Python</em> programmers, <em>or</em> even the Py-Curious. * Description: Tayeb Al Karim will present about Testing in <em>Python</em>.", "explanation": null, "document": "Location: Microsoft New England R&D Center, One Memorial Drive, Cambridge, MA 02142\nTime: 7pm\nAudience: Python programmers, or even the Py-Curious.\nDescription: Tayeb Al Karim will present about Testing in Python. Tayeb is a Software Engineer in Test at Google, and will cover a range of topics from the basics of testing your Python code, continuous builds, and how to improve testability with techniques like dependency injection. No matter what type of code you are writing, testing is a must, but many developers have doubts about how to proceed. Whether you are just test-curious or a grizzled expert, it should be an enlightening evening. (Registration not required)\n"}, {"score": 989.1865, "uuid": "916abe70-7c4e-5390-9527-ff0078b5a3db", "index": "cw12", "trec_id": "clueweb12-1111wb-64-01525", "target_hostname": "computinged.wordpress.com", "target_uri": "http://computinged.wordpress.com/tag/python/", "page_rank": 3.067879e-09, "spam_rank": 75, "title": "<em>Python</em> \u00ab Computing Education Blog", "snippet": "<em>I</em> still strongly believe that the first course in computing <em>should</em> not be a course in software engineering. Students <em>should</em> not have to <em>learn</em> the discipline of creating code <em>for</em> others, while just starting to make sense of the big ideas of computing. ", "explanation": null, "document": "Predictions on Future CS1\u00a0Languages\nA recent article in InfoWorld on up-and-coming languages got me thinking about the future of CS1 languages. \u00a0They went on at some length about Python , which I think most people consider to be the up-and-coming CS1 language.\nThere seems to be two sorts of people who love Python: those who hate brackets, and scientists. The former helped create the language by building a version of Perl that is easier to read and not as chock-full of opening and closing brackets as a C descendant. Fast-forward several years, and the solution was good enough to be the first language available on Googles AppEngine \u2014 a clear indication Python has the kind of structure that makes it easy to scale in the cloud, one of the biggest challenges for enterprise-grade computing.Python\u2019s popularity in scientific labs is a bit hard to explain, given that, unlike Stephen Wolframs Mathematica for mathematicians, the language never offered any data structures or elements explicitly tuned to meet the needs of scientists. Python creator Guido von Rossum believes Python caught on in the labs because \u201cscientists often need to improvise when trying to interpret results, so they are drawn to dynamic languages which allow them to work very quickly and see results almost immediately.\u201d\nvia 7 programming languages on the rise | Developer World \u2013 InfoWorld .\nThere have only really been three \u201cCS1 languages,\u201d the way that I\u2019m using the term: Pascal, C++, and Java. \u00a0All three programming languages were used in a large (over 50%) percentage of CS1 (intro CS for CS majors in post-secondary education in the US, and AP in high school) classes. \u00a0All three were AP CS languages.\nPascal at one point was probably in over 80-90% CS1 courses. \u00a0Not everyone jumped immediately to C++, but C++ was in the majority of CS1 classes. \u00a0I know that because, when our Java MediaComp book came out, our publisher said that Java had just pulled even with C++ in terms of percent of the market \u2014 that means C++ had to have been in lots of classes. \u00a0 Java is the dominant language in CS1 classes today, but it\u2019s declining. \u00a0Python\u2019s market share is rapidly growing, 40% per year the last three years . \u00a0While it\u2019s not clear that the new AP CS nor the AP CS Level A would ever adopt Python, Python might still gain the plurality of all CS1 languages. \u00a0I doubt that any language will ever gain more than 30-40% of the CS1 market again \u2014 there are (and will be) too many options for CS1 languages, and too many entrenched interests. \u00a0Faculty will stick with one, and may skip a plurality, e.g., I\u2019ve talked to teachers at schools where they stuck with C++ but now are switching to Python.\nI have two specific predictions to make about future CS1 languages, based on observations of the last three and the likely fourth.\nAll future CS1 languages will be in common use in industry.\nNo language will gain a plurality of CS1 courses unless it existed at the time of the last transition.\nThe transition from Pascal to C++ led to the greatest spike in AP CS Level A tests taken in Georgia. \u00a0Until 2010, that was largest number of AP CS exams taken in Georgia. \u00a0The transition from C++ to Java had nowhere near that kind of impact on the test numbers in Georgia. \u00a0What might have led to so much more interest in the Pascal -> C++ transition? \u00a0Pascal was a language that was not (perceived to be) common in industry, while C++ was. \u00a0I don\u2019t think that people perceived such a huge difference between C++ and Java. \u00a0I believe that the sense that C++ was vocationally useful, was approved of by industry, had a huge positive impact on student interest in the test.\nIn this blog, we have often touched on the tension between vocational and academic interests in computer science classes. \u00a0Vocational most often wins, especially in the majority schools. \u00a0The elite schools might play with BYOB Scratch in their intro courses (but notice \u2014 even at Harvard and Berkeley, it\u2019s for the non-majors, not for those who will major in CS), and community colleges might use Alice to ease the transition into programming, but the vast majority of schools in the middle value industry-approval too much to adopt a pedagogical language for their CS majors.\nThe implication of the first prediction is that, if Scratch or Alice are ever adopted for the new AP CS , only schools on the edges of the distribution will give CS major credit for it, because most schools will not adopt a CS1 language that isn\u2019t useful for programming in industry. \u00a0That isn\u2019t necessarily a bad thing for the new AP CS \u2014 to succeed, schools must agree to give some credit for it, not necessarily CS major. \u00a0Another implication is, if my prediction holds true, Scheme will never gain a plurality in CS1 courses.\nThe second implication is based on an observation of the timing of the four languages. \u00a0Each existed as the previous was adopted for the AP CS Level A, which is a reasonable point at which to claim that the language had reached plurality. \u00a0C++ existed ( since 1983 ) when the AP CS Level A was started in Pascal ( 1988, I think ). C++ was adopted in 2001 , and Java came out in 1995 . \u00a0AP CS Level A shifted to Java in 2003, and Python 1.0 came out in 1989, with Python 2.0 (the one receiving the most interest) in 2000 . \u00a0It takes a lot of time to develop that industry use, and to build up the sense that the new language may be worth the pain in shifting.\nThe implication is that, whatever the next CS1 language will be (after Python), it exists today, as Python reaches plurality. \u00a0Maybe Ruby , or Scala \u2013more likely Ruby, given the greater industry penetration. \u00a0Any language that we might invent for CS1 must wait for the next iteration. \u00a0Scratch, Alice, and Kodu are unlikely to ever become CS1 languages, because it is unlikely that industry will adopt them. \u00a0Few professional programmers will get their jobs due to their expertise in Scratch, Alice, or Kodu. That absolutely should not matter to CS1 instructors . \u00a0But it does.\nMulti-lingual Pedagogical Programming Environment:\u00a0Pyjama\nDoug Blank at Bryn Mawr is looking for people to try out and help with his new editor/shell IDE, Pyjama ( http://PyjamaProject.org ). Pyjama is built on Microsoft\u2019s Dynamic Language Runtime. Languages in Pyjama can share data, functions, and objects.\nPyjama currently comes with support for four languages: Python, Ruby, Scheme, and Dinah (\u201ca new language prototype similar to the Alice interface\u201d). \u00a0There are YouTube videos available about using the shell and the editor . The current tool is written in IronPython with Gtk# for the GUI. \u00a0It runs under Mono for Mac and Linux.\nDoug is looking for folks to help out with Pyjama. \u00a0Explicitly, he wants this to be an educator\u2019s tool, written for and by educators. Doug told me about his reasons for Pyjama in an email:\nThe larger goal of Pyjama is to make it so that educators can easily\u00a0switch between programming languages, or switch contexts and topics. For\u00a0example, if you would like to do Alice 3D programming, but you would\u00a0rather do it in Python, then you could. Or if you would like to use an\u00a0Alice interface to control robots, then you could. In addition, if you\u00a0would like to create a new language (or language environment, like Processing or Scratch ) then you could. Or if you would like to create a\u00a0new module (say in Scientific Methods) then that module will instantly be\u00a0available to all of the Pyjama languages.\nWe (teachers) need to be in control of everything in the academic\u00a0environment\u2026 If Java isn\u2019t the right language, let\u2019s build our own. If\u00a0IDLE and Python doesn\u2019t work just right, let\u2019s alter it to suit our\u00a0needs, not just use what is available.\nThe First Language-Independent, Valid Measure of CS1 Knowledge: Allison Tew\u00a0Defends\nAllison Elliott Tew has been working for five years to be able to figure out how we can compare different approaches to teaching CS1. \u00a0As Alan Kay noted in his comments to my recent previous post on computing education research , there are lots of factors, like who is taking the class and what they\u2019re doing in the class. \u00a0But to make a fair comparison in terms of the inputs, we need a stable measure of the output. \u00a0Allison made a pass in 2005 , but became worried when she couldn\u2019t replicate her results in later semesters. \u00a0She decided that the problem was that we had no scientific tool that we could rely on to measure CS1 knowledge. \u00a0We have had no way of measuring what students learn in CS1, in a way that was independent of language or approach, that was reliable and valid. \u00a0Allison set out to create one.\nAllison defends this week. \u00a0She took a huge gamble \u2014 at the end of her dissertation work, she collected two multiple choice question exams from each of 952 subjects. \u00a0If you get that wrong, you can\u2019t really try again.\nShe doesn\u2019t need to. She won.\nHer dissertation had three main questions.\n(1) How do you do this? \u00a0All the standard educational assessment methods involve comparing new methods to old methods in order to validate them. \u00a0How do you bootstrap a new test when one has never been created before? \u00a0She developed a multi-step process for validating her exam, and she carefully defined the range of the test using a combination of text analysis and curriculum standards .\n(2) Can you use pseudo-code to make the test language-independent? \u00a0First, she developed 3 open-ended versions of her test in MATLAB, Python, and Java, then had subjects take those. \u00a0By analyzing those, she was able to find three distractors (wrong answers) for every question that covered the top three wrong answers in each language \u2014 which by itself was pretty amazing. \u00a0I wouldn\u2019t have guessed that the same mistakes would be made in all three languages.\nThen she developed her pseudo-code test. \u00a0She ran subjects through two sessions (counter-balanced). In one session, they took the test in their \u201cnative\u201d language (whatever their CS1 was in), and in another (a week later, to avoid learning effects), the pseudo-code version.\nThe pseudo-code and native language tests were strongly correlated. \u00a0The social scientists say that, in this kind of comparison, a correlation statistic r over 0.37 is considered the same test. \u00a0She beat that on every language.\nNotice that the Python correlation was only .415. \u00a0She then split out the Python CS1 with only CS majors, from the one with mostly non-majors. \u00a0That\u2019s the .615 vs. the .372 \u2014 CS majors will always beat non-majors. \u00a0One of her hypotheses was that this transfer from native code to pseudo-code would work best for the best students. \u00a0She found that that was true. \u00a0She split her subjects into quartiles and the top quartile was significantly different than the third, the third from the second, and so on. \u00a0I think that this is really important for all those folks who might say, \u201cOh sure, your students did badly. \u00a0Our students would rock that exam!\u201d \u00a0( As I mentioned , the average score on the pseudo-code test was 33.78%, and 48.61% on the \u201cnative\u201d language test.) \u00a0Excellent! \u00a0Allison\u2019s test works even better as a proxy test for really good students. Do show us better results, then publish it and tell us how you did it!\n(3) Then comes the validity argument \u2014 is this testing really testing what\u2019s important? \u00a0Is it a good test? \u00a0Like I said, she had a multi-step process. First, she had a panel of experts review her test for reasonableness of coverage. Second, she did think-alouds with 12 students to make sure that they were reading the exam the way she intended. \u00a0Third, she ran IRT analysis to show that her problems were reasonable. \u00a0Finally, she correlated performance on her pseudo-code test (FCS1) with the final exam grades. \u00a0That one is the big test for me \u2014 is this test measuring what we think is important, across two universities and four different classes? \u00a0Another highly significant set of correlations, but it\u2019s this scatterplot that really tells the story for me.\nNext, Allison defends, and takes a job as a post-doc at University of British Columbia. \u00a0She plans to make her exam available for other researchers to use \u2014 in comparison of CS1 approaches and languages. \u00a0Want to know if your new Python class is leading to the same learning as your old Java class? \u00a0This is your test! \u00a0But she\u2019ll never post it for free on the Internet. \u00a0If there\u2019s any chance that a student has seen the problems first, the argument for validity fails. \u00a0So, she\u2019ll be carefully controlling access to the test.\nAllison\u2019s work is a big deal. \u00a0We need it in our \u201c Georgia Computes! \u201d work, as do our teachers. \u00a0As we change our approaches to broaden participation, we need to show that learning isn\u2019t impacted. \u00a0In general, we need it in computing education research. \u00a0We finally have a yardstick by which we can start comparing learning. \u00a0This isn\u2019t the final and end-all assessment. \u00a0For example, there are no objects in this test, and we don\u2019t know if it\u2019ll be valid for graphical languages. \u00a0But it\u2019s the first test like this, and that\u2019s a big step. I hope that others will follow the trail Allison made so that we end up with lots of great learning measures in computing education research.\nGrowth of Python in\u00a0CS1\nI just got a report from Tracy Dunkelberger of Pearson on the state of the Python CS1 market. \u00a0The market size is estimated to be about 20,300 students per year, up 45% since last year. \u00a0The market has had around 40% gains for each of the last three years. \u00a0She also shared with me some market share data which she asked me not to share further \u2014 I think it\u2019s sufficient to say that our Media Computation book isn\u2019t the top book in the market, but is doing well.\nProviding social infrastructure for Open\u00a0Courseware\nMy colleague Ashwin Ram was one of the founders of OpenStudy , which aims to be a social network aimed at supporting student learning, e.g., through online study groups.\u00a0 It\u2019s just been announced that the MIT OpenCourseware initiative is going to partner with OpenStudy, so that students viewing the OCW material might work together to support learning (including for MIT\u2019s Python course).\u00a0 This is an exciting and important idea, to provide an infrastructure for learning beyond the raw content provided by OCW.\nOCW has partnered with OpenStudy to offer an online study group for this course. 6.00 Introduction to Computer Science and Programming and two other courses (18.01 Single Variable Calculus, and 21F.101 Chinese I) have been selected for this pilot project. We need your feedback to determine whether more study groups should be offered for OCW courses.\nThese study groups are not moderated or managed by OCW, and you cannot earn credit for participating in them. To participate, you will need to register with OpenStudy or log in with your Facebook account.\nAbout OpenStudy\nOpenStudy is social study network for students to ask questions, give help, collaborate and meet others. Founded by professors and students from Georgia Tech and Emory University, and funded by the National Science Foundation and the Georgia Research Alliance, OpenStudy believes that students can teach other students through collaborative learning.\n"}, {"score": 984.9395, "uuid": "e585e0b3-cfcb-5e2a-bc8f-ba7a50966d54", "index": "cw12", "trec_id": "clueweb12-1609wb-77-07427", "target_hostname": "www.muhuk.com", "target_uri": "http://www.muhuk.com/tag/python/", "page_rank": 2.2815216e-09, "spam_rank": 81, "title": "<em>python</em> \u00ab muhuk.com", "snippet": "Being a <em>Python</em> person myself, <em>I</em> think the best low-level language to be proficient <em>for</em> me is C. Many other high-level languages have C interfaces. So investing the time to <em>learn</em> C <em>should</em> pay off one way <em>or</em> the other. Aren\u2019t we doing a lot of web programming these days?", "explanation": null, "document": "Developing Reusable Django Apps\nWednesday, January 13th, 2010\nDjango app structure is an implementation of seperation of concerns . It is slightly different than what you can find in other MVC frameworks. The stack is split vertically, not horizontally. And then the app is split horizontally within, i.e. models, views, templates etc are in their seperate modules/packages/directories. This vertical splitting allows you to collect all ingredients of one functionality in your project in one place.\nI think apps1 are one of the strong points of Django. A selling point if you like. There is a great ecosystem of apps, you can find an app for almost anything posssible with Python. And Python is kick-ass when it comes to library wealth. But there is another major advantage of apps when they\u2019re done right; a sane code base. Here is a slide from the Django in the Real World presentation by Jacob Kaplan-Moss:\nThe fivefold path\nDo one thing, and do it well.\nDon\u2019t be afraid of multiple apps.\nWrite for \ufb02exibility.\nBuild to distribute.\nExtend carefully.\nI will focus on flexibility and interoperability of apps in this post. But before we proceed I would like to emphasize the first bullet point in the slide. Because the scope of your app plays a big role in its flexibility and interoperability. Apps should be small enough to easily understand and integrate (into a project). Many times I have moved away from an otherwise good app because of its many dependencies and/or excessive features. On the other hand apps should be big enough to allow for different configurations and allow extension without modifying their code. Do one thing, and do it well.\nTake django-tagging for example; it\u2019s 1.3 KLOC but it does tagging and nothing else. There are no dependencies other than Django, you can add tags to any model without modifying the model source, a tag can be associated with any type of model and tagging hides the gory details from you\u2026 In short; finding the right size is important. This is why tagging is the tagging app for Django.\nBuilding For Reuse\nGeneral advice is \u201ceven project specific apps should be reusable\u201c. Slapping the same app onto another project is not the only advantage. In fact it may not be possible if you are not in the habit of upgrading your whole project to recent versions of Django. The main advantage as I have said before is sanity. I prefer Django to other web frameworks/environments because it provides a civilized way of development. Let\u2019s accept it; web programming is not a particularly interesting, exciting or intellectually rewarding field. You write the same piece of code over and over. And worst of all the challanges you face are actually a result of either the underlying system was designed by morons or you are trying to use it for something it\u2019s not intended to be used. So it is only natural that web programmers feel they\u2019re rusting. Django eases the pain. If you stick to certain conventions serenity will follow as well.\nNaturally the framework does most of the work regarding app flexibility and interoperability. Take URLs for instance include('myapp.urls') and you are good to go. You don\u2019t have to bind views one by one. Is it inflexible? Who said urls.py can only contain a hardcoded list of URLs. You can do anything that is possible with Python. You can generate different urlpatterns based on a setting for instance.\nIt is relatively easy and straightforward to reuse and extend forms and views (both function based and class based). Models are a little harder to get right though. You should always think of the most difficult situation which is you can\u2019t touch either app\u2019s code. Registration pattern of admin app provides a good solution here. You can register a third party model to another third party app in just a few lines.\nYou don\u2019t need to write lots of code to get the flexibility and interoperability. Well designed apps make good use of settings.py for example. Why should the project developer wrap a view when a single line assignment would do the job? Supplying good templatetags and template snipplets (includes) is another way to make things easy for app consumers.\nSignals provide a great way to propagate the events generated from your app. Even though they are one way2, signals are extremely powerful. Any number of observers can connect to a signal and you can send a signal anywhere in your code. Literally. It is even possible your app suppying a signal and then another app sending it3.\nThere are many more ways to tame your app to be reusable. It all starts with your determination and discipline. Just like documentation, testing and maintaining a software generally. I will write more about reusable apps.\n1: The word application is used both for a web application and a Django application. To avoid confusion I always use app to indicate the latter.\n2: Signals don\u2019t have return values. But you can use a callback AFAIK.\n3: I can\u2019t think of an example this would be useful, but still\u2026\n"}], [{"score": 983.87225, "uuid": "62dd8b1c-69af-590c-be12-109f7895f03e", "index": "cw12", "trec_id": "clueweb12-1113wb-84-05041", "target_hostname": "computinged.wordpress.com", "target_uri": "https://computinged.wordpress.com/tag/python/", "page_rank": 1.3696191e-09, "spam_rank": 75, "title": "<em>Python</em> \u00ab Computing Education Blog", "snippet": "<em>I</em> still strongly believe that the first course in computing <em>should</em> not be a course in software engineering. Students <em>should</em> not have to <em>learn</em> the discipline of creating code <em>for</em> others, while just starting to make sense of the big ideas of computing. ", "explanation": null, "document": "Predictions on Future CS1\u00a0Languages\nA recent article in InfoWorld on up-and-coming languages got me thinking about the future of CS1 languages. \u00a0They went on at some length about Python , which I think most people consider to be the up-and-coming CS1 language.\nThere seems to be two sorts of people who love Python: those who hate brackets, and scientists. The former helped create the language by building a version of Perl that is easier to read and not as chock-full of opening and closing brackets as a C descendant. Fast-forward several years, and the solution was good enough to be the first language available on Googles AppEngine \u2014 a clear indication Python has the kind of structure that makes it easy to scale in the cloud, one of the biggest challenges for enterprise-grade computing.Python\u2019s popularity in scientific labs is a bit hard to explain, given that, unlike Stephen Wolframs Mathematica for mathematicians, the language never offered any data structures or elements explicitly tuned to meet the needs of scientists. Python creator Guido von Rossum believes Python caught on in the labs because \u201cscientists often need to improvise when trying to interpret results, so they are drawn to dynamic languages which allow them to work very quickly and see results almost immediately.\u201d\nvia 7 programming languages on the rise | Developer World \u2013 InfoWorld .\nThere have only really been three \u201cCS1 languages,\u201d the way that I\u2019m using the term: Pascal, C++, and Java. \u00a0All three programming languages were used in a large (over 50%) percentage of CS1 (intro CS for CS majors in post-secondary education in the US, and AP in high school) classes. \u00a0All three were AP CS languages.\nPascal at one point was probably in over 80-90% CS1 courses. \u00a0Not everyone jumped immediately to C++, but C++ was in the majority of CS1 classes. \u00a0I know that because, when our Java MediaComp book came out, our publisher said that Java had just pulled even with C++ in terms of percent of the market \u2014 that means C++ had to have been in lots of classes. \u00a0 Java is the dominant language in CS1 classes today, but it\u2019s declining. \u00a0Python\u2019s market share is rapidly growing, 40% per year the last three years . \u00a0While it\u2019s not clear that the new AP CS nor the AP CS Level A would ever adopt Python, Python might still gain the plurality of all CS1 languages. \u00a0I doubt that any language will ever gain more than 30-40% of the CS1 market again \u2014 there are (and will be) too many options for CS1 languages, and too many entrenched interests. \u00a0Faculty will stick with one, and may skip a plurality, e.g., I\u2019ve talked to teachers at schools where they stuck with C++ but now are switching to Python.\nI have two specific predictions to make about future CS1 languages, based on observations of the last three and the likely fourth.\nAll future CS1 languages will be in common use in industry.\nNo language will gain a plurality of CS1 courses unless it existed at the time of the last transition.\nThe transition from Pascal to C++ led to the greatest spike in AP CS Level A tests taken in Georgia. \u00a0Until 2010, that was largest number of AP CS exams taken in Georgia. \u00a0The transition from C++ to Java had nowhere near that kind of impact on the test numbers in Georgia. \u00a0What might have led to so much more interest in the Pascal -> C++ transition? \u00a0Pascal was a language that was not (perceived to be) common in industry, while C++ was. \u00a0I don\u2019t think that people perceived such a huge difference between C++ and Java. \u00a0I believe that the sense that C++ was vocationally useful, was approved of by industry, had a huge positive impact on student interest in the test.\nIn this blog, we have often touched on the tension between vocational and academic interests in computer science classes. \u00a0Vocational most often wins, especially in the majority schools. \u00a0The elite schools might play with BYOB Scratch in their intro courses (but notice \u2014 even at Harvard and Berkeley, it\u2019s for the non-majors, not for those who will major in CS), and community colleges might use Alice to ease the transition into programming, but the vast majority of schools in the middle value industry-approval too much to adopt a pedagogical language for their CS majors.\nThe implication of the first prediction is that, if Scratch or Alice are ever adopted for the new AP CS , only schools on the edges of the distribution will give CS major credit for it, because most schools will not adopt a CS1 language that isn\u2019t useful for programming in industry. \u00a0That isn\u2019t necessarily a bad thing for the new AP CS \u2014 to succeed, schools must agree to give some credit for it, not necessarily CS major. \u00a0Another implication is, if my prediction holds true, Scheme will never gain a plurality in CS1 courses.\nThe second implication is based on an observation of the timing of the four languages. \u00a0Each existed as the previous was adopted for the AP CS Level A, which is a reasonable point at which to claim that the language had reached plurality. \u00a0C++ existed ( since 1983 ) when the AP CS Level A was started in Pascal ( 1988, I think ). C++ was adopted in 2001 , and Java came out in 1995 . \u00a0AP CS Level A shifted to Java in 2003, and Python 1.0 came out in 1989, with Python 2.0 (the one receiving the most interest) in 2000 . \u00a0It takes a lot of time to develop that industry use, and to build up the sense that the new language may be worth the pain in shifting.\nThe implication is that, whatever the next CS1 language will be (after Python), it exists today, as Python reaches plurality. \u00a0Maybe Ruby , or Scala \u2013more likely Ruby, given the greater industry penetration. \u00a0Any language that we might invent for CS1 must wait for the next iteration. \u00a0Scratch, Alice, and Kodu are unlikely to ever become CS1 languages, because it is unlikely that industry will adopt them. \u00a0Few professional programmers will get their jobs due to their expertise in Scratch, Alice, or Kodu. That absolutely should not matter to CS1 instructors . \u00a0But it does.\nMulti-lingual Pedagogical Programming Environment:\u00a0Pyjama\nDoug Blank at Bryn Mawr is looking for people to try out and help with his new editor/shell IDE, Pyjama ( http://PyjamaProject.org ). Pyjama is built on Microsoft\u2019s Dynamic Language Runtime. Languages in Pyjama can share data, functions, and objects.\nPyjama currently comes with support for four languages: Python, Ruby, Scheme, and Dinah (\u201ca new language prototype similar to the Alice interface\u201d). \u00a0There are YouTube videos available about using the shell and the editor . The current tool is written in IronPython with Gtk# for the GUI. \u00a0It runs under Mono for Mac and Linux.\nDoug is looking for folks to help out with Pyjama. \u00a0Explicitly, he wants this to be an educator\u2019s tool, written for and by educators. Doug told me about his reasons for Pyjama in an email:\nThe larger goal of Pyjama is to make it so that educators can easily\u00a0switch between programming languages, or switch contexts and topics. For\u00a0example, if you would like to do Alice 3D programming, but you would\u00a0rather do it in Python, then you could. Or if you would like to use an\u00a0Alice interface to control robots, then you could. In addition, if you\u00a0would like to create a new language (or language environment, like Processing or Scratch ) then you could. Or if you would like to create a\u00a0new module (say in Scientific Methods) then that module will instantly be\u00a0available to all of the Pyjama languages.\nWe (teachers) need to be in control of everything in the academic\u00a0environment\u2026 If Java isn\u2019t the right language, let\u2019s build our own. If\u00a0IDLE and Python doesn\u2019t work just right, let\u2019s alter it to suit our\u00a0needs, not just use what is available.\nThe First Language-Independent, Valid Measure of CS1 Knowledge: Allison Tew\u00a0Defends\nAllison Elliott Tew has been working for five years to be able to figure out how we can compare different approaches to teaching CS1. \u00a0As Alan Kay noted in his comments to my recent previous post on computing education research , there are lots of factors, like who is taking the class and what they\u2019re doing in the class. \u00a0But to make a fair comparison in terms of the inputs, we need a stable measure of the output. \u00a0Allison made a pass in 2005 , but became worried when she couldn\u2019t replicate her results in later semesters. \u00a0She decided that the problem was that we had no scientific tool that we could rely on to measure CS1 knowledge. \u00a0We have had no way of measuring what students learn in CS1, in a way that was independent of language or approach, that was reliable and valid. \u00a0Allison set out to create one.\nAllison defends this week. \u00a0She took a huge gamble \u2014 at the end of her dissertation work, she collected two multiple choice question exams from each of 952 subjects. \u00a0If you get that wrong, you can\u2019t really try again.\nShe doesn\u2019t need to. She won.\nHer dissertation had three main questions.\n(1) How do you do this? \u00a0All the standard educational assessment methods involve comparing new methods to old methods in order to validate them. \u00a0How do you bootstrap a new test when one has never been created before? \u00a0She developed a multi-step process for validating her exam, and she carefully defined the range of the test using a combination of text analysis and curriculum standards .\n(2) Can you use pseudo-code to make the test language-independent? \u00a0First, she developed 3 open-ended versions of her test in MATLAB, Python, and Java, then had subjects take those. \u00a0By analyzing those, she was able to find three distractors (wrong answers) for every question that covered the top three wrong answers in each language \u2014 which by itself was pretty amazing. \u00a0I wouldn\u2019t have guessed that the same mistakes would be made in all three languages.\nThen she developed her pseudo-code test. \u00a0She ran subjects through two sessions (counter-balanced). In one session, they took the test in their \u201cnative\u201d language (whatever their CS1 was in), and in another (a week later, to avoid learning effects), the pseudo-code version.\nThe pseudo-code and native language tests were strongly correlated. \u00a0The social scientists say that, in this kind of comparison, a correlation statistic r over 0.37 is considered the same test. \u00a0She beat that on every language.\nNotice that the Python correlation was only .415. \u00a0She then split out the Python CS1 with only CS majors, from the one with mostly non-majors. \u00a0That\u2019s the .615 vs. the .372 \u2014 CS majors will always beat non-majors. \u00a0One of her hypotheses was that this transfer from native code to pseudo-code would work best for the best students. \u00a0She found that that was true. \u00a0She split her subjects into quartiles and the top quartile was significantly different than the third, the third from the second, and so on. \u00a0I think that this is really important for all those folks who might say, \u201cOh sure, your students did badly. \u00a0Our students would rock that exam!\u201d \u00a0( As I mentioned , the average score on the pseudo-code test was 33.78%, and 48.61% on the \u201cnative\u201d language test.) \u00a0Excellent! \u00a0Allison\u2019s test works even better as a proxy test for really good students. Do show us better results, then publish it and tell us how you did it!\n(3) Then comes the validity argument \u2014 is this testing really testing what\u2019s important? \u00a0Is it a good test? \u00a0Like I said, she had a multi-step process. First, she had a panel of experts review her test for reasonableness of coverage. Second, she did think-alouds with 12 students to make sure that they were reading the exam the way she intended. \u00a0Third, she ran IRT analysis to show that her problems were reasonable. \u00a0Finally, she correlated performance on her pseudo-code test (FCS1) with the final exam grades. \u00a0That one is the big test for me \u2014 is this test measuring what we think is important, across two universities and four different classes? \u00a0Another highly significant set of correlations, but it\u2019s this scatterplot that really tells the story for me.\nNext, Allison defends, and takes a job as a post-doc at University of British Columbia. \u00a0She plans to make her exam available for other researchers to use \u2014 in comparison of CS1 approaches and languages. \u00a0Want to know if your new Python class is leading to the same learning as your old Java class? \u00a0This is your test! \u00a0But she\u2019ll never post it for free on the Internet. \u00a0If there\u2019s any chance that a student has seen the problems first, the argument for validity fails. \u00a0So, she\u2019ll be carefully controlling access to the test.\nAllison\u2019s work is a big deal. \u00a0We need it in our \u201c Georgia Computes! \u201d work, as do our teachers. \u00a0As we change our approaches to broaden participation, we need to show that learning isn\u2019t impacted. \u00a0In general, we need it in computing education research. \u00a0We finally have a yardstick by which we can start comparing learning. \u00a0This isn\u2019t the final and end-all assessment. \u00a0For example, there are no objects in this test, and we don\u2019t know if it\u2019ll be valid for graphical languages. \u00a0But it\u2019s the first test like this, and that\u2019s a big step. I hope that others will follow the trail Allison made so that we end up with lots of great learning measures in computing education research.\nGrowth of Python in\u00a0CS1\nI just got a report from Tracy Dunkelberger of Pearson on the state of the Python CS1 market. \u00a0The market size is estimated to be about 20,300 students per year, up 45% since last year. \u00a0The market has had around 40% gains for each of the last three years. \u00a0She also shared with me some market share data which she asked me not to share further \u2014 I think it\u2019s sufficient to say that our Media Computation book isn\u2019t the top book in the market, but is doing well.\nProviding social infrastructure for Open\u00a0Courseware\nMy colleague Ashwin Ram was one of the founders of OpenStudy , which aims to be a social network aimed at supporting student learning, e.g., through online study groups.\u00a0 It\u2019s just been announced that the MIT OpenCourseware initiative is going to partner with OpenStudy, so that students viewing the OCW material might work together to support learning (including for MIT\u2019s Python course).\u00a0 This is an exciting and important idea, to provide an infrastructure for learning beyond the raw content provided by OCW.\nOCW has partnered with OpenStudy to offer an online study group for this course. 6.00 Introduction to Computer Science and Programming and two other courses (18.01 Single Variable Calculus, and 21F.101 Chinese I) have been selected for this pilot project. We need your feedback to determine whether more study groups should be offered for OCW courses.\nThese study groups are not moderated or managed by OCW, and you cannot earn credit for participating in them. To participate, you will need to register with OpenStudy or log in with your Facebook account.\nAbout OpenStudy\nOpenStudy is social study network for students to ask questions, give help, collaborate and meet others. Founded by professors and students from Georgia Tech and Emory University, and funded by the National Science Foundation and the Georgia Research Alliance, OpenStudy believes that students can teach other students through collaborative learning.\n"}, {"score": 981.5685, "uuid": "c44cb2cf-6edf-5b36-8db8-b39494cf043b", "index": "cw12", "trec_id": "clueweb12-0300tw-66-03421", "target_hostname": "www.packtpub.com", "target_uri": "http://www.packtpub.com/statistical-analysis-with-r-beginners-guide/book", "page_rank": 1.1864841e-09, "spam_rank": 85, "title": "Statistical <em>Analysis</em> with <em>R</em> | Packt Publishing Technical &amp; IT Book and", "snippet": "If you are a <em>data</em> analyst, business <em>or</em> information technology professional, student, educator, researcher, <em>or</em> anyone else who wants to <em>learn</em> to analyze the <em>data</em> effectively then this book is <em>for</em> you. No prior experience with <em>R</em> is necessary.", "explanation": null, "document": "Reviews\nDownloads\nAn easy introduction for people who are new to R, with plenty of strong examples for you to work through\nThis book will take you on a journey to learn R as the strategist for an ancient Chinese kingdom!\nA step by step guide to understand R, its benefits, and how to use it to maximize the impact of your data analysis\nA practical guide to conduct and communicate your data analysis with R in the most effective manner\nBook Details\nPaperback : 300 pages [ 235mm x 191mm ]\nRelease Date : October 2010\n"}, {"score": 979.4678, "uuid": "99089580-11b6-523d-a3f7-176e60841acb", "index": "cw12", "trec_id": "clueweb12-0815wb-93-26467", "target_hostname": "coopunit.forestry.uga.edu", "target_uri": "http://coopunit.forestry.uga.edu/FORS8390/Labs/Documents/python_html", "page_rank": 1.1936839e-09, "spam_rank": 80, "title": "Using <em>Python</em> to Create Input Files", "snippet": "Shaping raw <em>data</em> into the appropriate input file <em>for</em> <em>analysis</em> via MARK, CAPTURE, and other programs can be time consuming and fussy. It is often useful to write a short program to do this automatically, especially when more than one (<em>or</em> one very complex) input file is required.", "explanation": null, "document": "Using Python to Create Input Files\nShaping raw data into the appropriate input file for analysis via MARK, CAPTURE, and other programs can be time consuming and fussy. It is often useful to write a short program to do this automatically, especially when more than one (or one very complex) input file is required. Generically, this function is called data parsing. Several freely available programming languages can be used to perform data parsing, including:\nAwk: A special-purpose language for data formatting.\nR: A public license clone of S-Plus. Can manipulate data, analyse, and chart all in one place.\nPython: An object-oriented programming language, easy to use, with lots of useful modules for database access, numerical methods, etc.\nPerl: A scripting language, similar and older than Python. Powerful, but hard to learn.\nRuby: A very new language, also similar to Python.\nAll of these programs run on almost every computer platform available. I will illustrate how to set up and use Python, as an example.\nThe two most common desktop platforms Linux and Windows. If you a re a Windows user, you need to grab a couple of installation packages to get started:\nA current release of Python itself ( download )\nWindows extensions, for database access ( download )\nYou can install these programs in-place, by telling Windows to run the install file, rather than save them to disk.\nAfter installation, in your programs menu, you should see a folder containing links to all of the Python executables, including a workplace shell, called \"pythonwin\". This provides an interactive window, plus a text editor for writing and debugging files.\nHere are two sample scripts showing how to construct input files from data in a relational database. In this case, MySQL was used, but the methods for other databases, such as Access, Postgresql and Oracle, are similar:\nNesting success : Input file for Program MARK in encounter history format. Contains records for Warbling Vireos in Southern B.C., ath three different study sites.\nBand recovery : Input file using the Brownie parameterization for analysis by Program MARK. Uses command-line arguments to select appropriate records.\nAfter you have installed Python and downloaded one of these sample files, you should be able to right-click a file in your file manager, and select \"edit\". This will bring up the program in the python development environment. Feel free to modify, use and redistribute these files as you like.\nIt is always handy to have a command reference nearby. The help files for Python are excellent, and the manual inculdes a tutorial. There you will also find references for the syntax to connect to your favorite Windows database using ODBC. If you need help setting up ODBC data sources on your computer, send me an email , and I will give you a hand.\nLast updated 27 Feb 2012\n"}, {"score": 978.5862, "uuid": "e6ca9a51-b44b-5525-a882-200f8b53b65c", "index": "cw12", "trec_id": "clueweb12-0814wb-81-19147", "target_hostname": "coopunit.forestry.uga.edu", "target_uri": "http://coopunit.forestry.uga.edu/FORS8390/Documents/Labs/python_html", "page_rank": 1.1936839e-09, "spam_rank": 80, "title": "Using <em>Python</em> to Create Input Files", "snippet": "Shaping raw <em>data</em> into the appropriate input file <em>for</em> <em>analysis</em> via MARK, CAPTURE, and other programs can be time consuming and fussy. It is often useful to write a short program to do this automatically, especially when more than one (<em>or</em> one very complex) input file is required.", "explanation": null, "document": "Using Python to Create Input Files\nShaping raw data into the appropriate input file for analysis via MARK, CAPTURE, and other programs can be time consuming and fussy. It is often useful to write a short program to do this automatically, especially when more than one (or one very complex) input file is required. Generically, this function is called data parsing. Several freely available programming languages can be used to perform data parsing, including:\nAwk: A special-purpose language for data formatting.\nR: A public license clone of S-Plus. Can manipulate data, analyse, and chart all in one place.\nPython: An object-oriented programming language, easy to use, with lots of useful modules for database access, numerical methods, etc.\nPerl: A scripting language, similar and older than Python. Powerful, but hard to learn.\nRuby: A very new language, also similar to Python.\nAll of these programs run on almost every computer platform available. I will illustrate how to set up and use Python, as an example.\nThe two most common desktop platforms Linux and Windows. If you a re a Windows user, you need to grab a couple of installation packages to get started:\nA current release of Python itself ( download )\nWindows extensions, for database access ( download )\nYou can install these programs in-place, by telling Windows to run the install file, rather than save them to disk.\nAfter installation, in your programs menu, you should see a folder containing links to all of the Python executables, including a workplace shell, called \"pythonwin\". This provides an interactive window, plus a text editor for writing and debugging files.\nHere are two sample scripts showing how to construct input files from data in a relational database. In this case, MySQL was used, but the methods for other databases, such as Access, Postgresql and Oracle, are similar:\nNesting success : Input file for Program MARK in encounter history format. Contains records for Warbling Vireos in Southern B.C., ath three different study sites.\nBand recovery : Input file using the Brownie parameterization for analysis by Program MARK. Uses command-line arguments to select appropriate records.\nAfter you have installed Python and downloaded one of these sample files, you should be able to right-click a file in your file manager, and select \"edit\". This will bring up the program in the python development environment. Feel free to modify, use and redistribute these files as you like.\nIt is always handy to have a command reference nearby. The help files for Python are excellent, and the manual inculdes a tutorial. There you will also find references for the syntax to connect to your favorite Windows database using ODBC. If you need help setting up ODBC data sources on your computer, send me an email , and I will give you a hand.\nLast updated 27 Feb 2012\n"}, {"score": 978.4226, "uuid": "37932d61-43f9-5844-b516-4dc2e755a7f8", "index": "cw12", "trec_id": "clueweb12-0813wb-91-14005", "target_hostname": "coopunit.forestry.uga.edu", "target_uri": "http://coopunit.forestry.uga.edu/FORS8390/Labs/python_html", "page_rank": 1.1951341e-09, "spam_rank": 80, "title": "Using <em>Python</em> to Create Input Files", "snippet": "Shaping raw <em>data</em> into the appropriate input file <em>for</em> <em>analysis</em> via MARK, CAPTURE, and other programs can be time consuming and fussy. It is often useful to write a short program to do this automatically, especially when more than one (<em>or</em> one very complex) input file is required.", "explanation": null, "document": "Using Python to Create Input Files\nShaping raw data into the appropriate input file for analysis via MARK, CAPTURE, and other programs can be time consuming and fussy. It is often useful to write a short program to do this automatically, especially when more than one (or one very complex) input file is required. Generically, this function is called data parsing. Several freely available programming languages can be used to perform data parsing, including:\nAwk: A special-purpose language for data formatting.\nR: A public license clone of S-Plus. Can manipulate data, analyse, and chart all in one place.\nPython: An object-oriented programming language, easy to use, with lots of useful modules for database access, numerical methods, etc.\nPerl: A scripting language, similar and older than Python. Powerful, but hard to learn.\nRuby: A very new language, also similar to Python.\nAll of these programs run on almost every computer platform available. I will illustrate how to set up and use Python, as an example.\nThe two most common desktop platforms Linux and Windows. If you a re a Windows user, you need to grab a couple of installation packages to get started:\nA current release of Python itself ( download )\nWindows extensions, for database access ( download )\nYou can install these programs in-place, by telling Windows to run the install file, rather than save them to disk.\nAfter installation, in your programs menu, you should see a folder containing links to all of the Python executables, including a workplace shell, called \"pythonwin\". This provides an interactive window, plus a text editor for writing and debugging files.\nHere are two sample scripts showing how to construct input files from data in a relational database. In this case, MySQL was used, but the methods for other databases, such as Access, Postgresql and Oracle, are similar:\nNesting success : Input file for Program MARK in encounter history format. Contains records for Warbling Vireos in Southern B.C., ath three different study sites.\nBand recovery : Input file using the Brownie parameterization for analysis by Program MARK. Uses command-line arguments to select appropriate records.\nAfter you have installed Python and downloaded one of these sample files, you should be able to right-click a file in your file manager, and select \"edit\". This will bring up the program in the python development environment. Feel free to modify, use and redistribute these files as you like.\nIt is always handy to have a command reference nearby. The help files for Python are excellent, and the manual inculdes a tutorial. There you will also find references for the syntax to connect to your favorite Windows database using ODBC. If you need help setting up ODBC data sources on your computer, send me an email , and I will give you a hand.\nLast updated 27 Feb 2012\n"}, {"score": 978.06116, "uuid": "3c1ff492-1f3a-5113-bb92-314ef34f86c4", "index": "cw12", "trec_id": "clueweb12-0814wb-60-35713", "target_hostname": "coopunit.forestry.uga.edu", "target_uri": "http://coopunit.forestry.uga.edu/FORS8390/Documents/Documents/python_html", "page_rank": 1.2022853e-09, "spam_rank": 80, "title": "Using <em>Python</em> to Create Input Files", "snippet": "Shaping raw <em>data</em> into the appropriate input file <em>for</em> <em>analysis</em> via MARK, CAPTURE, and other programs can be time consuming and fussy. It is often useful to write a short program to do this automatically, especially when more than one (<em>or</em> one very complex) input file is required.", "explanation": null, "document": "Using Python to Create Input Files\nShaping raw data into the appropriate input file for analysis via MARK, CAPTURE, and other programs can be time consuming and fussy. It is often useful to write a short program to do this automatically, especially when more than one (or one very complex) input file is required. Generically, this function is called data parsing. Several freely available programming languages can be used to perform data parsing, including:\nAwk: A special-purpose language for data formatting.\nR: A public license clone of S-Plus. Can manipulate data, analyse, and chart all in one place.\nPython: An object-oriented programming language, easy to use, with lots of useful modules for database access, numerical methods, etc.\nPerl: A scripting language, similar and older than Python. Powerful, but hard to learn.\nRuby: A very new language, also similar to Python.\nAll of these programs run on almost every computer platform available. I will illustrate how to set up and use Python, as an example.\nThe two most common desktop platforms Linux and Windows. If you a re a Windows user, you need to grab a couple of installation packages to get started:\nA current release of Python itself ( download )\nWindows extensions, for database access ( download )\nYou can install these programs in-place, by telling Windows to run the install file, rather than save them to disk.\nAfter installation, in your programs menu, you should see a folder containing links to all of the Python executables, including a workplace shell, called \"pythonwin\". This provides an interactive window, plus a text editor for writing and debugging files.\nHere are two sample scripts showing how to construct input files from data in a relational database. In this case, MySQL was used, but the methods for other databases, such as Access, Postgresql and Oracle, are similar:\nNesting success : Input file for Program MARK in encounter history format. Contains records for Warbling Vireos in Southern B.C., ath three different study sites.\nBand recovery : Input file using the Brownie parameterization for analysis by Program MARK. Uses command-line arguments to select appropriate records.\nAfter you have installed Python and downloaded one of these sample files, you should be able to right-click a file in your file manager, and select \"edit\". This will bring up the program in the python development environment. Feel free to modify, use and redistribute these files as you like.\nIt is always handy to have a command reference nearby. The help files for Python are excellent, and the manual inculdes a tutorial. There you will also find references for the syntax to connect to your favorite Windows database using ODBC. If you need help setting up ODBC data sources on your computer, send me an email , and I will give you a hand.\nLast updated 27 Feb 2012\n"}, {"score": 977.9318, "uuid": "dd004d27-4d42-5bfe-89f4-191b432eb6d3", "index": "cw12", "trec_id": "clueweb12-0813wb-46-26190", "target_hostname": "coopunit.forestry.uga.edu", "target_uri": "http://coopunit.forestry.uga.edu/FORS8390/Documents/python_html", "page_rank": 1.1951341e-09, "spam_rank": 80, "title": "Using <em>Python</em> to Create Input Files", "snippet": "Shaping raw <em>data</em> into the appropriate input file <em>for</em> <em>analysis</em> via MARK, CAPTURE, and other programs can be time consuming and fussy. It is often useful to write a short program to do this automatically, especially when more than one (<em>or</em> one very complex) input file is required.", "explanation": null, "document": "Using Python to Create Input Files\nShaping raw data into the appropriate input file for analysis via MARK, CAPTURE, and other programs can be time consuming and fussy. It is often useful to write a short program to do this automatically, especially when more than one (or one very complex) input file is required. Generically, this function is called data parsing. Several freely available programming languages can be used to perform data parsing, including:\nAwk: A special-purpose language for data formatting.\nR: A public license clone of S-Plus. Can manipulate data, analyse, and chart all in one place.\nPython: An object-oriented programming language, easy to use, with lots of useful modules for database access, numerical methods, etc.\nPerl: A scripting language, similar and older than Python. Powerful, but hard to learn.\nRuby: A very new language, also similar to Python.\nAll of these programs run on almost every computer platform available. I will illustrate how to set up and use Python, as an example.\nThe two most common desktop platforms Linux and Windows. If you a re a Windows user, you need to grab a couple of installation packages to get started:\nA current release of Python itself ( download )\nWindows extensions, for database access ( download )\nYou can install these programs in-place, by telling Windows to run the install file, rather than save them to disk.\nAfter installation, in your programs menu, you should see a folder containing links to all of the Python executables, including a workplace shell, called \"pythonwin\". This provides an interactive window, plus a text editor for writing and debugging files.\nHere are two sample scripts showing how to construct input files from data in a relational database. In this case, MySQL was used, but the methods for other databases, such as Access, Postgresql and Oracle, are similar:\nNesting success : Input file for Program MARK in encounter history format. Contains records for Warbling Vireos in Southern B.C., ath three different study sites.\nBand recovery : Input file using the Brownie parameterization for analysis by Program MARK. Uses command-line arguments to select appropriate records.\nAfter you have installed Python and downloaded one of these sample files, you should be able to right-click a file in your file manager, and select \"edit\". This will bring up the program in the python development environment. Feel free to modify, use and redistribute these files as you like.\nIt is always handy to have a command reference nearby. The help files for Python are excellent, and the manual inculdes a tutorial. There you will also find references for the syntax to connect to your favorite Windows database using ODBC. If you need help setting up ODBC data sources on your computer, send me an email , and I will give you a hand.\nLast updated 27 Feb 2012\n"}, {"score": 976.5224, "uuid": "af1194e6-ea95-5bed-9b82-0f2903bfa553", "index": "cw12", "trec_id": "clueweb12-0813wb-22-01634", "target_hostname": "coopunit.forestry.uga.edu", "target_uri": "http://coopunit.forestry.uga.edu/FORS8390/python_html", "page_rank": 1.3407371e-09, "spam_rank": 80, "title": "Using <em>Python</em> to Create Input Files", "snippet": "Shaping raw <em>data</em> into the appropriate input file <em>for</em> <em>analysis</em> via MARK, CAPTURE, and other programs can be time consuming and fussy. It is often useful to write a short program to do this automatically, especially when more than one (<em>or</em> one very complex) input file is required.", "explanation": null, "document": "Using Python to Create Input Files\nShaping raw data into the appropriate input file for analysis via MARK, CAPTURE, and other programs can be time consuming and fussy. It is often useful to write a short program to do this automatically, especially when more than one (or one very complex) input file is required. Generically, this function is called data parsing. Several freely available programming languages can be used to perform data parsing, including:\nAwk: A special-purpose language for data formatting.\nR: A public license clone of S-Plus. Can manipulate data, analyse, and chart all in one place.\nPython: An object-oriented programming language, easy to use, with lots of useful modules for database access, numerical methods, etc.\nPerl: A scripting language, similar and older than Python. Powerful, but hard to learn.\nRuby: A very new language, also similar to Python.\nAll of these programs run on almost every computer platform available. I will illustrate how to set up and use Python, as an example.\nThe two most common desktop platforms Linux and Windows. If you a re a Windows user, you need to grab a couple of installation packages to get started:\nA current release of Python itself ( download )\nWindows extensions, for database access ( download )\nYou can install these programs in-place, by telling Windows to run the install file, rather than save them to disk.\nAfter installation, in your programs menu, you should see a folder containing links to all of the Python executables, including a workplace shell, called \"pythonwin\". This provides an interactive window, plus a text editor for writing and debugging files.\nHere are two sample scripts showing how to construct input files from data in a relational database. In this case, MySQL was used, but the methods for other databases, such as Access, Postgresql and Oracle, are similar:\nNesting success : Input file for Program MARK in encounter history format. Contains records for Warbling Vireos in Southern B.C., ath three different study sites.\nBand recovery : Input file using the Brownie parameterization for analysis by Program MARK. Uses command-line arguments to select appropriate records.\nAfter you have installed Python and downloaded one of these sample files, you should be able to right-click a file in your file manager, and select \"edit\". This will bring up the program in the python development environment. Feel free to modify, use and redistribute these files as you like.\nIt is always handy to have a command reference nearby. The help files for Python are excellent, and the manual inculdes a tutorial. There you will also find references for the syntax to connect to your favorite Windows database using ODBC. If you need help setting up ODBC data sources on your computer, send me an email , and I will give you a hand.\nLast updated 27 Feb 2012\n"}, {"score": 976.37714, "uuid": "92368666-cc74-5828-8cf3-342667ee0825", "index": "cw12", "trec_id": "clueweb12-0816wb-15-31637", "target_hostname": "coopunit.forestry.uga.edu", "target_uri": "http://coopunit.forestry.uga.edu/FORS8390/Labs/Labs/python_html", "page_rank": 1.1936839e-09, "spam_rank": 80, "title": "Using <em>Python</em> to Create Input Files", "snippet": "Shaping raw <em>data</em> into the appropriate input file <em>for</em> <em>analysis</em> via MARK, CAPTURE, and other programs can be time consuming and fussy. It is often useful to write a short program to do this automatically, especially when more than one (<em>or</em> one very complex) input file is required.", "explanation": null, "document": "Using Python to Create Input Files\nShaping raw data into the appropriate input file for analysis via MARK, CAPTURE, and other programs can be time consuming and fussy. It is often useful to write a short program to do this automatically, especially when more than one (or one very complex) input file is required. Generically, this function is called data parsing. Several freely available programming languages can be used to perform data parsing, including:\nAwk: A special-purpose language for data formatting.\nR: A public license clone of S-Plus. Can manipulate data, analyse, and chart all in one place.\nPython: An object-oriented programming language, easy to use, with lots of useful modules for database access, numerical methods, etc.\nPerl: A scripting language, similar and older than Python. Powerful, but hard to learn.\nRuby: A very new language, also similar to Python.\nAll of these programs run on almost every computer platform available. I will illustrate how to set up and use Python, as an example.\nThe two most common desktop platforms Linux and Windows. If you a re a Windows user, you need to grab a couple of installation packages to get started:\nA current release of Python itself ( download )\nWindows extensions, for database access ( download )\nYou can install these programs in-place, by telling Windows to run the install file, rather than save them to disk.\nAfter installation, in your programs menu, you should see a folder containing links to all of the Python executables, including a workplace shell, called \"pythonwin\". This provides an interactive window, plus a text editor for writing and debugging files.\nHere are two sample scripts showing how to construct input files from data in a relational database. In this case, MySQL was used, but the methods for other databases, such as Access, Postgresql and Oracle, are similar:\nNesting success : Input file for Program MARK in encounter history format. Contains records for Warbling Vireos in Southern B.C., ath three different study sites.\nBand recovery : Input file using the Brownie parameterization for analysis by Program MARK. Uses command-line arguments to select appropriate records.\nAfter you have installed Python and downloaded one of these sample files, you should be able to right-click a file in your file manager, and select \"edit\". This will bring up the program in the python development environment. Feel free to modify, use and redistribute these files as you like.\nIt is always handy to have a command reference nearby. The help files for Python are excellent, and the manual inculdes a tutorial. There you will also find references for the syntax to connect to your favorite Windows database using ODBC. If you need help setting up ODBC data sources on your computer, send me an email , and I will give you a hand.\nLast updated 27 Feb 2012\n"}, {"score": 975.60156, "uuid": "ecd89839-caaf-5ff8-8b3f-e6a118964fc6", "index": "cw12", "trec_id": "clueweb12-0304wb-70-24678", "target_hostname": "www.python.org", "target_uri": "http://www.python.org/about/success/astra/", "page_rank": 7.844649e-09, "spam_rank": 94, "title": "<em>Python</em> Success Stories", "snippet": "<em>For</em> example, a molecule&#x27;s mass depends on its composition. The older Drone code maintained these dependencies manually with a set of &#x27;if&#x27; statements that specified which prediction routines <em>should</em> be called, and in which order, during execution of an <em>analysis</em>.", "explanation": null, "document": "http://www.astrazeneca.com/\nSummary:\nAstraZeneca, one of the world's leading pharmaceutical companies, uses Python to reduce costs and increase productivity in the drug identification process.\nLogo:\nIntroduction\nAstraZeneca is one of the world's leading pharmaceutical companies. With over 54,000 employees world-wide, it provides innovative, effective medicines designed to fight cancer, provide pain control, heal infection, and fight diseases of the cardiovascular, central nervous, gastrointestinal, and respiratory systems.\nFinding a new drug often takes over a decade and more than $800 million. A big problem early in the process is identifying those candidates more likely to be good drugs from the vast universe of possible molecules.\nComputational chemists have developed many techniques to predict molecular properties. These can be used to evaluate the likelihood that a molecule will be stable in the stomach (for pills that are swallowed), and that it can travel through the blood stream, cross the cell membrane, and eventually be broken down and eliminated, all without being too toxic to the body.\nIf these computational techniques were good enough there would be no need to do actual experiments. But today's computer models cannot fully characterize a molecule's behavior in the body, nor replace the intuition of a skilled pharmaceutical chemist. Real molecules must still be tested in the laboratory to see how they react.\nTo save time and money on laboratory work, experimental chemists use computational models to narrow the field of good drug candidates, while also verifying that the candidates to be tested are not simple variations of each other's basic chemical structure.\nProcess Improvements Needed\nMuch of the work on drug identification actually takes place through collaboration between many research groups scattered around the world. As part of this process, experimental chemists send a list of compounds to the computational chemist, who works on the data set and sends back the results.\nHistorically, experimental chemists were forced to rely on computational chemists and other staff to run computer predictions. Each prediction technique required running a separate program, some commercial and others developed in-house by different groups around the company, and each program had its own set of inputs, options, configurations, and failure behaviors. An experimental chemist usually didn't have the training to work with them, which meant that the computational chemists were forced to take time out of their work on developing new techniques to run routine models.\nIn 2000, AstraZeneca wanted to improve this process so that experimental chemists could make better computational predictions on their own, and so that the research of the computational chemists could progress at a faster rate, and make its way into the lab more quickly.\nPierre Bruneau, a Principal Scientist at AstraZeneca, had worked on this problem while at Zeneca, which merged to form AstraZeneca. He developed a web-based interface called H2X, named after the allied navigation systems used during the second world war. H2X was based on an in-house molecular property calculator called Drone. This system used a Perl script which computed some of the simpler molecular properties by calling the appropriate prediction program, usually through a wrapper written in Perl, csh, or a domain specific control language.\nPython Chosen\nH2X using Drone was a successful experiment and it was used by many people. In 2001 AstraZeneca decided to develop it further and brought in Andrew Dalke as a consultant, to improve the back-end code by making it more robust, extensible, and maintainable. Andrew, a well-known advocate for Python in computational chemistry and biology, convinced the group that Python was the appropriate language for the next generation back-end, which was named PyDrone.\nPython was chosen for this work because it is one of the best languages available for physical scientists, that is, for people who do not have a computer science background. Many other powerful and expressive high level languages exist, including Perl, Lisp, Scheme, Ruby, CAML, and Haskell. Of all these, Python is one of the few that is based on research into usability and the factors that make a programming language easy to learn and use. Yet Python was also designed to solve real-world problems faced by an expert programmer. The result is a language that scales well from small scripts written by a chemist to large packages written by a software developer.\nPython's Error Handling Improves Robustness\nThe first iteration of PyDrone refactored the existing Perl code into more appropriate functions, classes, and modules while translating the code base into Python. Refactoring the Perl code without moving to Python would have produced comparable architectural results, but Python's explicit error handling and stronger type checking helped to considerably improve the code's robustness.\nThe current version of PyDrone uses about 20 different external binaries and scripts to predict various molecular properties. When an external program works correctly, then the output is easy to parse into the desired results. However, these programs don't always work correctly, are not fully documented, and it's often hard to determine all the possible failure cases from the outside. To compensate for this, the PyDrone developers wrote tests to anticipate as many error cases as possible, but it was impossible to rule out additional unexpected error cases after deployment.\nFrom experience dealing with this issue first in Perl (Drone) and then in Python (PyDrone), we found that Python is better at catching many types of errors and at managing unexpected problems in a deployed application. This is a result of the way in which the two languages approach error handling in general. For example, Perl's I/O routines are quiet and failures must be checked explicitly, usually with the \"or die\" idiom. A conscientious programmer will always add those, but they take up space, are easy to forget, and hard to test. In contrast to this, Python functions are noisy and almost always raise an exception automatically when there is a problem in code.\nAfter rewriting in Python, we initially thought this noisy behavior was a nuisance because Python kept raising exceptions and stopping where the old Perl code had kept on going. However, we soon found that nearly every exception indicated a previously undetected error case for which we needed to add new error handling code. Python was helping us find problem spots and preventing us from letting silent errors into our data.\nOne example of an error case that Python uncovered for us was caused by an external prediction program that would usually return a numerical error code but in some cases was found to return the string \"error\" instead. In Perl, strings and numbers are converted automatically, for example the string \"123\" becomes the integer 123. Unfortunately, Perl also converts non-numerical strings, such as \"error\", to 0. As a result of this, Drone was treating \"error\" as a successful return value, leading to incorrect results. Python's stronger typing uncovered the error as soon as the rare case that caused it was executed.\nAnother way in which Python helped us to improve our code was by its inclusion of a complete stack traceback with each exception report. We found this very useful in helping us understand the source of a problem without needing to run a debugger or add extra code instrumentation. This feature of Python was particularly helpful in remote debugging of rare cases. Andrew is in the United States and Pierre is in France. When an error occurred, Pierre's email with the traceback often contained enough information to pinpoint and fix the problem.\nAdding Powerful Extensibility with Python\nThe next stage in PyDrone's development was to improve its extensibility. Some molecular properties depend on other properties. For example, a molecule's mass depends on its composition. The older Drone code maintained these dependencies manually with a set of 'if' statements that specified which prediction routines should be called, and in which order, during execution of an analysis. In this approach, adding new dependencies soon led to a combinatorial nightmare.\nTo solve this problem in Python, we developed a simple rule base which acts like a Python dictionary. It contains a data cache and a mapping from property name to prediction function. If a requested property name (the dictionary key) is in the cache, we reuse it. Otherwise, we find the associated function, call it to compute the value, store the result in the cache, and return it. The functions themselves are given a reference to the Properties manager so they can recursively request any additionally needed dependencies. To add a new prediction we register the new function in the function table -- and let the functions themselves handle the dependencies. The cache is needed because some properties are expensive to compute or are needed by many other properties.\nArchitecture of the Property Manager Zoom in\nThe resulting new architecture made a simple but profound difference to the project. We now have a single system that can accommodate all current and future prediction methods, that computes only the minimum needed to yield the requested results, and that is easy to understand and maintain. Before we built it in Python, several people in the company had argued it impossible to build such a system at all.\nThe Benefits of Python's Type System\nThe PyDrone architecture could have been implemented in many languages, but Python's dynamic typing made it much easier to build our Property manager. Some molecular properties are numbers, others strings, or lists and dictionaries, and still others are class instances. A statically typed language would have required extra hassle to allow a mixture of return types to be inserted into the Property manager. Even Perl, which is also dynamically typed, would have required some way to distinguish between references to a $scalar, %hash, or @list. In Python it just worked, and we could mix the data types of the keys in the Property manager dictionary without any extra effort at all. Yet, as described above, Python does at the same time provide sufficient data type checking to find many kinds of common type mismatch errors.\nOne of the factors that made our Property manager so successful was that Python lets user-defined types emulate the behavior of built-in types. Our Property manager acts very much like a lookup table that maps property name to value, so we designed it to emulate a Python dictionary. In Python, this is done by implementing specific special methods such as __getitem__(), __setitem__(), and so forth. By emulating a dictionary, nearly every other Python function that operates on a dictionary would work with our manager. It also made the code easier to understand and debug because the syntax and point-of-call usage fit naturally with what people expect.\nPython facilitated our Property manager implementation in other ways as well. One PyDrone feature that had been requested by users was the ability to describe a new prediction using an equation based on existing properties. For example, an equation might look like:\n0.34 + (0.78*CLOGP) - (0.018*HBA) - (0.015*HB_TOT) - (0.11*MM_HADCA) - (0.017*MM_QON) + (0.012*VDW_POL_AREA)\nwhere the variables are keys in the Property manager. This was quite easy to implement in Python, and we would be hard pressed to find a language that makes it any easier. Python's mathematical expressions are almost identical to the standard form used in the sciences, so we could use Python's \"eval\" statement to parse and evaluate the user-defined expressions. Because our Property manager acts like a Python dictionary, it could (at least in theory) be provided directly to the eval statement as the locals dictionary used for variable lookup during expression evaluation.\nAs it turned out, for performance reasons, the eval() implementation in Python accepts only built-in dictionary types and not an emulated mapping type, so we had to engage in some extra trickery to make our on-demand dependency system work with equations. Nevertheless, the entire implementation was quite easy.\nResults\nPyDrone took about 3 months of development time, another 3 months of QA, and 3 weeks of documentation time to produce about 5,600 lines of finished Python code.\nOverall PyDrone has been a wonderful success for AstraZeneca. As a result of using Python, we were able to quickly and easily develop a great tool that is both very simple to use and that adapts well to new prediction methods.\nThe biggest problem we've had with Python is that relatively few people at AstraZeneca use it for development. The IT group prefers either Perl (systems people) or Java (architecture people) so we occasionally get requests to rewrite parts of the project in one of those languages. Even so, we have found developers are interested in learning Python, especially when they see comparisons of development time and effort, resulting code size, and other metrics.\nAbout the Authors\nScott Boyer is a Principal Scientist in the Enabling Science and Technology section of AstraZeneca Discovery R&D, M\u00f6lndal, Sweden. Scott received his Ph.D. from the University of Colorado, Boulder and has worked at both Pfizer and AstraZeneca in experimental mass spectrometry and NMR associated with establishing optimal 'Drug-Like Properties'. About four years ago he made the transition to the 'dark side' of computational chemistry and now heads the internal project to bring more modelling tools to bench chemists on all 10 Discovery research sites in AstraZeneca.\nAndrew Dalke is the founder of Dalke Scientific Software, LLC, a software consulting and contract programming shop located in Santa Fe, New Mexico, USA. Andrew has been developing computational chemistry and biology software since 1992. His main focus is combining usability design and software engineering to develop software tools that scientists both use and enjoy. It's no wonder he likes Python so much.\nPierre Bruneau is a Principal Scientist in the Cancer and Infection Research Area of AstraZeneca Discovery, Reims, France. After studying chemistry at the Ecole Nationale Sup\u00e9rieure de Chimie de Strasbourg, he initially joined Organon R&D and then AstraZeneca (formerly ICI Pharma and then Zeneca) at Reims, France. After several years acting as a Medicinal Chemist, Pierre now heads the local physical chemistry and computer group of the French lab, while maintaining an interest in developing methods and tools for predicting physico-chemical properties and establishing structure-activity relationships of potential drug molecules.\n"}], [{"score": 975.14966, "uuid": "a5bdd04f-ccc8-5aa6-80a7-1bee60c9224a", "index": "cw12", "trec_id": "clueweb12-0801wb-14-06770", "target_hostname": "python.org", "target_uri": "http://python.org/about/success/astra/", "page_rank": 2.022707e-09, "spam_rank": 96, "title": "<em>Python</em> Success Stories", "snippet": "<em>For</em> example, a molecule&#x27;s mass depends on its composition. The older Drone code maintained these dependencies manually with a set of &#x27;if&#x27; statements that specified which prediction routines <em>should</em> be called, and in which order, during execution of an <em>analysis</em>.", "explanation": null, "document": "http://www.astrazeneca.com/\nSummary:\nAstraZeneca, one of the world's leading pharmaceutical companies, uses Python to reduce costs and increase productivity in the drug identification process.\nLogo:\nIntroduction\nAstraZeneca is one of the world's leading pharmaceutical companies. With over 54,000 employees world-wide, it provides innovative, effective medicines designed to fight cancer, provide pain control, heal infection, and fight diseases of the cardiovascular, central nervous, gastrointestinal, and respiratory systems.\nFinding a new drug often takes over a decade and more than $800 million. A big problem early in the process is identifying those candidates more likely to be good drugs from the vast universe of possible molecules.\nComputational chemists have developed many techniques to predict molecular properties. These can be used to evaluate the likelihood that a molecule will be stable in the stomach (for pills that are swallowed), and that it can travel through the blood stream, cross the cell membrane, and eventually be broken down and eliminated, all without being too toxic to the body.\nIf these computational techniques were good enough there would be no need to do actual experiments. But today's computer models cannot fully characterize a molecule's behavior in the body, nor replace the intuition of a skilled pharmaceutical chemist. Real molecules must still be tested in the laboratory to see how they react.\nTo save time and money on laboratory work, experimental chemists use computational models to narrow the field of good drug candidates, while also verifying that the candidates to be tested are not simple variations of each other's basic chemical structure.\nProcess Improvements Needed\nMuch of the work on drug identification actually takes place through collaboration between many research groups scattered around the world. As part of this process, experimental chemists send a list of compounds to the computational chemist, who works on the data set and sends back the results.\nHistorically, experimental chemists were forced to rely on computational chemists and other staff to run computer predictions. Each prediction technique required running a separate program, some commercial and others developed in-house by different groups around the company, and each program had its own set of inputs, options, configurations, and failure behaviors. An experimental chemist usually didn't have the training to work with them, which meant that the computational chemists were forced to take time out of their work on developing new techniques to run routine models.\nIn 2000, AstraZeneca wanted to improve this process so that experimental chemists could make better computational predictions on their own, and so that the research of the computational chemists could progress at a faster rate, and make its way into the lab more quickly.\nPierre Bruneau, a Principal Scientist at AstraZeneca, had worked on this problem while at Zeneca, which merged to form AstraZeneca. He developed a web-based interface called H2X, named after the allied navigation systems used during the second world war. H2X was based on an in-house molecular property calculator called Drone. This system used a Perl script which computed some of the simpler molecular properties by calling the appropriate prediction program, usually through a wrapper written in Perl, csh, or a domain specific control language.\nPython Chosen\nH2X using Drone was a successful experiment and it was used by many people. In 2001 AstraZeneca decided to develop it further and brought in Andrew Dalke as a consultant, to improve the back-end code by making it more robust, extensible, and maintainable. Andrew, a well-known advocate for Python in computational chemistry and biology, convinced the group that Python was the appropriate language for the next generation back-end, which was named PyDrone.\nPython was chosen for this work because it is one of the best languages available for physical scientists, that is, for people who do not have a computer science background. Many other powerful and expressive high level languages exist, including Perl, Lisp, Scheme, Ruby, CAML, and Haskell. Of all these, Python is one of the few that is based on research into usability and the factors that make a programming language easy to learn and use. Yet Python was also designed to solve real-world problems faced by an expert programmer. The result is a language that scales well from small scripts written by a chemist to large packages written by a software developer.\nPython's Error Handling Improves Robustness\nThe first iteration of PyDrone refactored the existing Perl code into more appropriate functions, classes, and modules while translating the code base into Python. Refactoring the Perl code without moving to Python would have produced comparable architectural results, but Python's explicit error handling and stronger type checking helped to considerably improve the code's robustness.\nThe current version of PyDrone uses about 20 different external binaries and scripts to predict various molecular properties. When an external program works correctly, then the output is easy to parse into the desired results. However, these programs don't always work correctly, are not fully documented, and it's often hard to determine all the possible failure cases from the outside. To compensate for this, the PyDrone developers wrote tests to anticipate as many error cases as possible, but it was impossible to rule out additional unexpected error cases after deployment.\nFrom experience dealing with this issue first in Perl (Drone) and then in Python (PyDrone), we found that Python is better at catching many types of errors and at managing unexpected problems in a deployed application. This is a result of the way in which the two languages approach error handling in general. For example, Perl's I/O routines are quiet and failures must be checked explicitly, usually with the \"or die\" idiom. A conscientious programmer will always add those, but they take up space, are easy to forget, and hard to test. In contrast to this, Python functions are noisy and almost always raise an exception automatically when there is a problem in code.\nAfter rewriting in Python, we initially thought this noisy behavior was a nuisance because Python kept raising exceptions and stopping where the old Perl code had kept on going. However, we soon found that nearly every exception indicated a previously undetected error case for which we needed to add new error handling code. Python was helping us find problem spots and preventing us from letting silent errors into our data.\nOne example of an error case that Python uncovered for us was caused by an external prediction program that would usually return a numerical error code but in some cases was found to return the string \"error\" instead. In Perl, strings and numbers are converted automatically, for example the string \"123\" becomes the integer 123. Unfortunately, Perl also converts non-numerical strings, such as \"error\", to 0. As a result of this, Drone was treating \"error\" as a successful return value, leading to incorrect results. Python's stronger typing uncovered the error as soon as the rare case that caused it was executed.\nAnother way in which Python helped us to improve our code was by its inclusion of a complete stack traceback with each exception report. We found this very useful in helping us understand the source of a problem without needing to run a debugger or add extra code instrumentation. This feature of Python was particularly helpful in remote debugging of rare cases. Andrew is in the United States and Pierre is in France. When an error occurred, Pierre's email with the traceback often contained enough information to pinpoint and fix the problem.\nAdding Powerful Extensibility with Python\nThe next stage in PyDrone's development was to improve its extensibility. Some molecular properties depend on other properties. For example, a molecule's mass depends on its composition. The older Drone code maintained these dependencies manually with a set of 'if' statements that specified which prediction routines should be called, and in which order, during execution of an analysis. In this approach, adding new dependencies soon led to a combinatorial nightmare.\nTo solve this problem in Python, we developed a simple rule base which acts like a Python dictionary. It contains a data cache and a mapping from property name to prediction function. If a requested property name (the dictionary key) is in the cache, we reuse it. Otherwise, we find the associated function, call it to compute the value, store the result in the cache, and return it. The functions themselves are given a reference to the Properties manager so they can recursively request any additionally needed dependencies. To add a new prediction we register the new function in the function table -- and let the functions themselves handle the dependencies. The cache is needed because some properties are expensive to compute or are needed by many other properties.\nArchitecture of the Property Manager Zoom in\nThe resulting new architecture made a simple but profound difference to the project. We now have a single system that can accommodate all current and future prediction methods, that computes only the minimum needed to yield the requested results, and that is easy to understand and maintain. Before we built it in Python, several people in the company had argued it impossible to build such a system at all.\nThe Benefits of Python's Type System\nThe PyDrone architecture could have been implemented in many languages, but Python's dynamic typing made it much easier to build our Property manager. Some molecular properties are numbers, others strings, or lists and dictionaries, and still others are class instances. A statically typed language would have required extra hassle to allow a mixture of return types to be inserted into the Property manager. Even Perl, which is also dynamically typed, would have required some way to distinguish between references to a $scalar, %hash, or @list. In Python it just worked, and we could mix the data types of the keys in the Property manager dictionary without any extra effort at all. Yet, as described above, Python does at the same time provide sufficient data type checking to find many kinds of common type mismatch errors.\nOne of the factors that made our Property manager so successful was that Python lets user-defined types emulate the behavior of built-in types. Our Property manager acts very much like a lookup table that maps property name to value, so we designed it to emulate a Python dictionary. In Python, this is done by implementing specific special methods such as __getitem__(), __setitem__(), and so forth. By emulating a dictionary, nearly every other Python function that operates on a dictionary would work with our manager. It also made the code easier to understand and debug because the syntax and point-of-call usage fit naturally with what people expect.\nPython facilitated our Property manager implementation in other ways as well. One PyDrone feature that had been requested by users was the ability to describe a new prediction using an equation based on existing properties. For example, an equation might look like:\n0.34 + (0.78*CLOGP) - (0.018*HBA) - (0.015*HB_TOT) - (0.11*MM_HADCA) - (0.017*MM_QON) + (0.012*VDW_POL_AREA)\nwhere the variables are keys in the Property manager. This was quite easy to implement in Python, and we would be hard pressed to find a language that makes it any easier. Python's mathematical expressions are almost identical to the standard form used in the sciences, so we could use Python's \"eval\" statement to parse and evaluate the user-defined expressions. Because our Property manager acts like a Python dictionary, it could (at least in theory) be provided directly to the eval statement as the locals dictionary used for variable lookup during expression evaluation.\nAs it turned out, for performance reasons, the eval() implementation in Python accepts only built-in dictionary types and not an emulated mapping type, so we had to engage in some extra trickery to make our on-demand dependency system work with equations. Nevertheless, the entire implementation was quite easy.\nResults\nPyDrone took about 3 months of development time, another 3 months of QA, and 3 weeks of documentation time to produce about 5,600 lines of finished Python code.\nOverall PyDrone has been a wonderful success for AstraZeneca. As a result of using Python, we were able to quickly and easily develop a great tool that is both very simple to use and that adapts well to new prediction methods.\nThe biggest problem we've had with Python is that relatively few people at AstraZeneca use it for development. The IT group prefers either Perl (systems people) or Java (architecture people) so we occasionally get requests to rewrite parts of the project in one of those languages. Even so, we have found developers are interested in learning Python, especially when they see comparisons of development time and effort, resulting code size, and other metrics.\nAbout the Authors\nScott Boyer is a Principal Scientist in the Enabling Science and Technology section of AstraZeneca Discovery R&D, M\u00f6lndal, Sweden. Scott received his Ph.D. from the University of Colorado, Boulder and has worked at both Pfizer and AstraZeneca in experimental mass spectrometry and NMR associated with establishing optimal 'Drug-Like Properties'. About four years ago he made the transition to the 'dark side' of computational chemistry and now heads the internal project to bring more modelling tools to bench chemists on all 10 Discovery research sites in AstraZeneca.\nAndrew Dalke is the founder of Dalke Scientific Software, LLC, a software consulting and contract programming shop located in Santa Fe, New Mexico, USA. Andrew has been developing computational chemistry and biology software since 1992. His main focus is combining usability design and software engineering to develop software tools that scientists both use and enjoy. It's no wonder he likes Python so much.\nPierre Bruneau is a Principal Scientist in the Cancer and Infection Research Area of AstraZeneca Discovery, Reims, France. After studying chemistry at the Ecole Nationale Sup\u00e9rieure de Chimie de Strasbourg, he initially joined Organon R&D and then AstraZeneca (formerly ICI Pharma and then Zeneca) at Reims, France. After several years acting as a Medicinal Chemist, Pierre now heads the local physical chemistry and computer group of the French lab, while maintaining an interest in developing methods and tools for predicting physico-chemical properties and establishing structure-activity relationships of potential drug molecules.\n"}, {"score": 932.80493, "uuid": "6e264b96-c53f-586f-a333-689cacc40a68", "index": "cw12", "trec_id": "clueweb12-0802wb-41-01586", "target_hostname": "python.org", "target_uri": "http://python.org/community/jobs/", "page_rank": 2.5531757e-09, "spam_rank": 96, "title": "<em>Python</em> Job Board", "snippet": "We are looking <em>for</em> talented and passionate software engineers to design and build our <em>data</em> management systems, develop advanced <em>analysis</em> tools to leverage this <em>data</em> <em>for</em> agricultural management, and create web-based front-ends <em>for</em> <em>data</em> <em>analysis</em> and visualization.", "explanation": null}, {"score": 966.331, "uuid": "d75f127f-5e79-52b3-860d-e9ce10580536", "index": "cw12", "trec_id": "clueweb12-1200tw-54-03681", "target_hostname": "datadrivenjournalism.net", "target_uri": "http://datadrivenjournalism.net/news_and_analysis/Data_journalism_at_NPR_Interview_with_Matt_Stiles", "page_rank": 1.194334e-09, "spam_rank": 85, "title": "<em>Data</em> journalism at NPR: Interview with Matt Stiles | News &amp; <em>Analysis</em> |", "snippet": "<em>I</em> have a MacBook that runs Windows 7. <em>I</em> have the basic CAR suite (Excel&#x2F;Access, ArcGIS, SPSS, etc.) but also plenty of open-source tools, such as <em>R</em> <em>for</em> visualization <em>or</em> MySQL&#x2F;Postgres <em>for</em> databases. <em>I</em> use Coda and Text Mate <em>for</em> coding. <em>I</em> use BBEdit and <em>Python</em> <em>for</em> text manipulation.", "explanation": null, "document": "rss print msg\nOriginally published by Alexander Howard on O'Reilly Radar on 6 March 2012. This article is republished with permission.\nAround the globe, the bond between data and journalism is growing stronger . In an age of big data, the growing importance of data journalism lies in the ability of its practitioners to provide context, clarity and, perhaps most important, find truth in the expanding amount of digital content in the world. In that context, data journalism has profound importance for society .\nTo learn more about the people who are doing this work and, in some cases, building the newsroom stack for the 21st century, I conducted a series of email interviews during the 2012 NICAR Conference.\nMatt Stiles ( @Stiles ), a data journalist based in Washington, D.C., maintains a popular Daily Visualization blog . Our interview follows.\nWhere do you work now? What is a day in your life like?\nI work at NPR, where I oversee data journalism on the State Impact project, a local-national partnership between us and member stations. My typical day always begins with a morning \"scrum\" meeting among the D.C. team as part of our agile development process. I spend time acquiring and analyzing data throughout each data, and I typically work directly with reporters, training them on software and data visualization techniques. I also spend time planning news apps and interactives, a process that requires close consultation with reporters, designers and developers.\nHow did you get started in data journalism? Did you get any special degrees or certificates?\nNo special training or certificates, though I did attend three NICAR boot camps (databases, mapping, statistics) over the years.\nDid you have any mentors? Who? What were the most important resources they shared with you?\nI have several mentors, both on the reporting side and the data side. For data, I wouldn't be where I am today without the help of two people: Chase Davis and Jennifer LaFleur. Jen got me interested early, and has helped me with formal and informal training over the years. Chase helped me with day-to-day questions when we worked together at the Houston Chronicle.\nWhat does your personal data journalism \"stack\" look like? What tools could you not live without?\nI have a MacBook that runs Windows 7. I have the basic CAR suite (Excel/Access, ArcGIS, SPSS, etc.) but also plenty of open-source tools, such as R for visualization or MySQL/Postgres for databases. I use Coda and Text Mate for coding. I use BBEdit and Python for text manipulation. I also couldn't live without Photoshop and Illustrator for cleaning up graphics.\nWhat data journalism project are you the most proud of working on or creating?\nI'm most proud of the online data library I created (and others have since expanded) at The Texas Tribune , but we're building some sweet apps at NPR. That's only going to expand now that we've created a national news apps team, which I'm joining soon.\nWhere do you turn to keep your skills updated or learn new things?\nI read blogs, subscribe to email lists and attend lots of conferences for inspiration. There's no silver bullet. If you love this stuff, you'll keep up.\nWhy are data journalism and \"news apps\" important, in the context of the contemporary digital environment for information?\nMore and more information is coming at us every day. The deluge is so vast. Data journalism at its core is important because it's about facts, not anecdotes.\nApps are important because Americans are already savvy data consumers, even if they don't know it. We must get them thinking -- or, even better, not thinking -- about news consumption in the same way they think about syncing their iPads or booking flights on Priceline or purchasing items on eBay. These are all \"apps\" that are familiar to many people. Interactive news should be, too.\nThis interview has been edited and condensed for clarity.\n"}, {"score": 955.32874, "uuid": "c5feea76-296c-581c-ba46-c61db996918f", "index": "cw12", "trec_id": "clueweb12-0210wb-98-14235", "target_hostname": "www.stat.washington.edu", "target_uri": "http://www.stat.washington.edu/~hoytak/_sources/blog/whypython.txt", "page_rank": 1.1737921e-09, "spam_rank": 87, "title": "_why-<em>python</em>: 10 Reasons <em>Python</em> Rocks <em>for</em> Research (And a Few Reasons it", "snippet": "<em>R</em> is beautiful <em>for</em> interactive <em>data</em> <em>analysis</em>, and its open library of statistical packages is amazing. However, the language design can be unnatural, and even maddening, <em>for</em> larger development projects.", "explanation": null, "document": ".. _why-python: 10 Reasons Python Rocks for Research (And a Few Reasons it Doesn't) =================================================================== The following is an account of my own experience with Python. Because that experience has been so positive, it is an unabashed attempt to promote the use of Python for general scientific research and development. About four years ago, I dropped MATLAB in favor of Python as my primary language for coding research projects. This article is a personal account of how rewarding I have found that experience. As I describe in the next sections, the variety and quality of Python's features has spoiled me. Even in small scripts, I now rely on Python's numerous :ref:`data structures\n`, :ref:`classes\n`, the :ref:`flexible function calling syntax\n`, an extensive kitchen-sink-included :ref:`standard library\n`, great :ref:`scientific libraries\n`, and outstanding documentation. To clarify, I am not advocating *just* Python as the perfect scientific programming environment; I am advocating Python plus a handful of mature 3rd-party open source libraries, namely `Numpy/Scipy`_ for numerical operations, Cython_ for low-level optimization, IPython_ for interactive work, and MatPlotLib_ for plotting. Later, I describe these and others in more detail, but I introduce these four here so I can weave discussion of them throughout this article. Given these libraries, many features in MATLAB that enable one to quickly write code for machine learning and artificial intelligence -- my primary area of research -- are essentially a small subset of those found in Python. After a day learning Python, I was able to still use most of the matrix tricks I had learned in MATLAB, but also utilize more powerful data structures and design patterns when needed. Holistic Language Design ------------------------ I once believed that the perfect language for research was one that allowed concise and direct translation from notepad scribblings to code. On the surface, this is reasonable. The more barriers between generating ideas and trying them out, the slower research progresses. In other words, the less one has to think about the actual coding, the better. I now believe, however, that this attitude is misguided. MATLAB's language design is focused on matrix and linear algebra operations; for turning such equations into one-liners, it is pretty much unsurpassed. However, move beyond these operations and it often becomes an exercise in frustration. R is beautiful for interactive data analysis, and its open library of statistical packages is amazing. However, the language design can be unnatural, and even maddening, for larger development projects. While Mathematica_ is perfect for interactive work with pure math, it is not intended for general purpose coding. The problem with the \"perfect match\" approach is that you lose generalizability very quickly. When the criteria for language design is too narrow, you inevitably choose excellence for one application over greatness for many. This is why universities have graduate programs in computer language design -- navigating the pros and cons of various design decisions is extremely difficult to get right. The extensive use of Python in everything from system administration and website design to numerical number-crunching shows that it has, indeed, hit the sweet spot. In fact, I've anecdotally observed that becoming better at R leads to skill at interacting with data, becoming better at MATLAB leads to skill at quick-and-dirty scripting, but becoming better at Python leads to genuine programming skill. Practically, in my line of work, the downside is that some matrix operators that are expressable using syntactical constructs in MATLAB become function calls (e.g. ``x = solve(A, y)`` instead of ``x = A \\ y``). In exchange for this extra verbosity -- which I have not found problematic -- one gains incredible flexibility and a language that is natural for everything from automating system processes to scientific research. The coder doesn't have to switch to another language when writing non-scientific code, and allows one to easily leverage other libraries (e.g. databases) for scientific research. Furthermore, Python allows one to easily leverage object oriented and functional design patterns. Just as different problems call for different ways of thinking, so also different problems call for different programming paradigms. There is no doubt that a linear, procedural style is natural for many scientific problems. However, an object oriented style that builds on classes having internal functionality and external behavior is a perfect design pattern for others. For this, classes in Python are full-featured and practical. Functional programming, which builds on the power of iterators and functions-as-variables, makes many programming solutions concise and intuitive. Brilliantly, in Python, everything can be passed around as an object, including functions, class definitions, and modules. Iterators are a key language component and Python comes with a full-featured iterator library. While it doesn't go as far in any of these categories as flagship paradigm languages such as Java or Haskell, it does allow one to use some very practical tools from these paradigms. These features combine to make the language very flexible for problem solving, one key reason for its popularity. Readability ----------- To reiterate a recurrent point, Python's syntax is *very* well thought out. Unlike many scripting languages (e.g. Perl), readability was a primary consideration when Python's syntax was designed. In fact, the joke is that turning pseudocode into correct Python code is a matter of correct indentation. This readability has a number of beneficial effects. Guido van Rossum, Python's original author, writes: *This emphasis on readability is no accident. As an object-oriented language, Python aims to encourage the creation of reusable code. Even if we all wrote perfect documentation all of the time, code can hardly be considered reusable if it's not readable. Many of Python's features, in addition to its use of indentation, conspire to make Python code highly readable.* [#readability]_ In addition, I've found it encourages collaboration, and not just by lowers the barrier to contributing to an open source Python project. If you can easily discuss your code with others in your office, the result can be better code and better coders. As two examples of this, consider the following code snippet:: def classify(values, boundary=0): \"Classifies values as being below (False) or above (True) a boundary.\" return [(True if v > boundary else False) for v in values] # Call the above function classify(my_values, boundary=0.5) Let me list three aspects of this code. First, it is a small, self-contained function that only requires three lines to define, including documentation (the string following the function). Second, a default argument for the boundary is specified in a way that is instantly readable (and yes, that does show up when using Sphinx for automatic documentation). Third, the list processing syntax is designed to be readable. Even if you are not used to reading Python code, it is easy to parse this code -- a new list is defined and returned from the list ``values`` using ``True`` if a particular value ``v`` is above ``boundary`` and ``False`` otherwise. Finally, when calling functions, Python allows named arguments -- this universally promotes clarity and reduces stupid bookkeeping bugs, particularly with functions requiring more than one or two arguments. Permit me to contrast these features with MATLAB. With MATLAB, globally available functions are put in separate files, discouraging the use of smaller functions and -- in practice -- often promotes cut-and-paste programming, the bane of debugging. Default arguments are a pain, requiring conditional coding to set unspecified arguments. Finally, specifying arguments by name when calling is not an option, though one popular but artificial construct -- alternating names and values in an argument list -- allows this to some extent. Balance of High Level and Low Level Programming ----------------------------------------------- The ease of balancing high-level programming with low-level optimization is a particular strong point of Python code. Python code is meant to be as high level as reasonable -- I've heard that in writing similar algorithms, on average you would write six lines of C/C++ code for every line of Python. However, as with most high-level languages, you often sacrifice code speed for programming speed. One sensible approach around this is to deal with higher level objects -- such as matrices and arrays -- and optimize operations on these objects to make the program acceptably fast. This is MATLAB's approach and is one of the keys to its success; it is also natural with Python. In this context, speeding code up means vectorizing your algorithm to work with arrays of numbers instead of with single numbers, thus reducing the overhead of the language when array operations are optimized. Abstractions such as this are absolutely essential for good scientific coding. Focusing on higher-level operations over higher-level data types generally leads to massive gains in coding speed and coding accuracy. Python's extension type system seamlessly allows libraries to be designed around this idea. Numpy's array type is a great examples. However, existing abstractions are not always enough when you're developing new algorithms or coding up new ideas. For example, vectorizing code through the use of arrays is powerful but limited. In many cases, operations really need loops, recursion, or other coding structures that are extremely efficient in optimized, compiled machine code but are not in most interpreted languages. As variables in many interpreted languages are not statically typed, the code can't easily be compiled into optimized machine code. In the scientific context, Cython_ provides the perfect balance between the two by allowing either. Cython_ works by first translating Python code into equivalent C code that runs the Python interpreted through the Python C API. It then uses a C compiler to create a shared library that can be loaded as a Python module. Generally, this module is functionally equivalent to the original Python module and usually runs marginally faster. The advantage, however, is that Cython_ allows one to statically type variables -- e.g. ``cdef int i`` declares ``i`` to be an integer. This gives massive speedups, as typed variables are now treated using low-level types rather than Python variables. With these annotations, your \"Python\" code can be as fast as C -- while requiring very little actual knowledge of C. Practically, a few type declarations can give you incredible speedups. For example, suppose you have the following code:: def foo(A): for i in range(A.shape[0]): for j in range(A.shape[1]): A[i,j] += i*j where ``A`` is a 2d NumPy array. This code uses interpreted loops and thus runs fairly slowly. However, add type information and use Cython:: def cyfoo(ndarray[double, ndim=2] A): cdef size_t i, j for i in range(A.shape[0]): for j in range(A.shape[1]): A[i,j] += i*j Cython translates necessary Python operations into calls to the Python C-API, but the looping and array indexing operations are turned into low level C code. For a 1000 x 1000 array, on my 2.4 GHz laptop, the Python version takes 1.67 seconds, while the Cython version takes only 3.67 milliseconds (a vectorized version of the above using an outer product took 15.1 ms). A general rule of thumb is that your program spends 80% of its time running 20% of the code. Thus a good strategy for efficient coding is to write everything, profile your code, and optimize the parts that need it. Python's profilers_ are great, and Cython_ allows you to do the latter step with *minimal* effort. Language Interoperability ------------------------- As a side affect of its universality, Python excels at gluing other languages together. One can call MATLAB functions from Python (through the MATLAB engine) using MLabWrap_, easing transitions from MATLAB to Python. Need to use that linear regression package in R? RPy_ puts it at your fingertips. Have fast FORTRAN code for a particular numerical algorithm? F2py_ will effortless generate a wrapper. Have general C or C++ libraries you want to call? Ctypes_, Cython_, or SWIG_ are three ways to easily interface to it (my favorite is Cython_). Now, if only all these were two way streets... Documentation System -------------------- Brilliantly, Python incorporates module, class, function, and method documentation directly into the language itself. In essence, there are two levels of comments -- programming level comments (start with ``#``) that are ignored by the compiler, and documentation comments that are specified by a `doc string`_ after the function or method name. These documentation strings add tags to the methods which are accessible by anyone using an interactive Python shell or by automatic documentation generators. The beauty of Python's system becomes apparent when using Sphinx_, a documentation generation system originally built for the Python language documentation. To allow sufficient presentation flexibility, it allows `reStructuredText`_ directives, a simple, readable markup language that is becoming widely used in code documentation. Sphinx works easily with embedded doc-strings, but it is useful beyond documentation -- for example, my personal website, my course webpages when I teach, my code documentation sites, and, of course, Python's main website are generated using Sphinx. One helpful feature for scientific programming is the ability to put LaTeX equations and plots directly in code documentation. For example, if you write:: .. math:: \\Gamma(z) = \\int_0^\\infty x^{z-1}e^{-x}\\,dx in the doc string, it is rendered in the webpage as .. math:: \\Gamma(z) = \\int_0^\\infty x^{z-1}e^{-x}\\,dx Including plots is easy. The following doc-string code:: .. plot:: import numpy as np import matplotlib.pyplot as plt x = np.linspace(0,2, 1000) plt.figure() plt.plot(x, np.sqrt(x), label = r\"Skiing: $\\sqrt{x}$\") plt.plot(x, x**2, label = r\"Snowboarding: $x^2$\") plt.title(\"Learning Curves for Snowboarding and Skiing\") plt.xlabel(\"Time\") ; plt.ylabel(\"Skill\") plt.legend(loc='upper left') plt.show() gives .. plot:: import numpy as np import matplotlib.pyplot as plt x = np.linspace(0,2, 1000) plt.figure() plt.plot(x, np.sqrt(x), label = r\"Skiing: $\\sqrt{x}$\") plt.plot(x, x**2, label = r\"Snowboarding: $x^2$\") plt.title(\"Learning Curves for Snowboarding and Skiing\") plt.xlabel(\"Time\") ; plt.ylabel(\"Skill\") plt.legend(loc='upper left') plt.show() In essence, this enables not only comments about the code, but also comments about the science and research behind your code, to be interwoven into the coding file. Hierarchical Module System -------------------------- Python uses `modular programming`_, a popular system that naturally organizes functions and classes into hierarchical namespaces. Each Python file defines a module. Classes, functions, or variables that are defined in or imported into that file show up in that module's namespace. Importing a module either creates a local dictionary holding that module's objects, pulls some of the module's objects into the local namespace. For example, ``import hashlib`` binds ``hashlib.md5`` to hashlib's md5 checksum function; alternately, ``from hashlib import md5`` binds ``md5`` to this function. This helps programming namespaces to follow a hierarchical organization. On the coding end, a Python file defines a module. Similarly, a directory containing an ``__init__.py`` Python file is treated the same way, files in that directory can define submodules, and so on. Thus the code is arranged in a hierarchical structure for both the programmer and the user. Permit me a short rant about MATLAB to help illustrate why this is a great feature. In MATLAB, all functions are declared in the global namespace, with names determined by filenames in the current path variable. However, this discourages code reusability by making the programmer do extra work keeping disparate program components separate. In other words, without a hierarchical structure to the program, it's difficult to extract and reuse specific functionality. Second, programmers must either give their functions long names, essentially doing what a decent hierarchical system inherently does, or risk namespace conflicts which can be difficult to resolve and result in subtle errors. While this may help one to throw something together quickly, it is a horrible system from a programming language perspective. Data Structures --------------- Good programming requires having and using the correct data structures for your algorithm. This is almost universally under-emphasized in research-oriented coding. While proof-of-concept code often doesn't need optimal data structures, such code causes problems when used in production. This often -- though it's rarely stated or known explicitly -- limits the scalability of a lot of existing code. Furthermore, when such features are not natural in a language's design, coders often avoid them and fail to learn and use good design patterns. Python has lists_, tuples_, sets_, dictionaries_, strings_, threadsafe queues_, and `many other types\n`_ built-in. Lists_ hold arbitrary data objects and can be sliced, indexed, joined, split, and used as stacks_. Sets_ hold unordered, unique items. Dictionaries_ map from a unique key to anything and form the real basis of the language. Heaps are available as operations on top of lists (similar to the C++ STL heaps). Add in NumPy, and one has an n-dimensional array structure that supports optimized and flexible broadcasting and matrix operations. Add in SciPy, and you have sparse matrices, kd-trees, image objects, time-series, and more. Available Libraries ------------------- Python has an *impressive* `standard library`_ packaged with Python. Its philosophy is `batteries-included\n`_, and a standard Python distribution comes with built-in database functionality, a variety of data persistence features, routines for interfacing with the operating system, website interfacing, email and networking tools, data compression support, cryptography, xml support, regular expressions, unit testing, multithreading, and much more. In short, if I want to take a break from writing a bunch of matrix manipulation code and automate an operating system task, I don't have to switch languages. Numerous libraries provide the needed functionality for scientific . The following is a list of the ones I use regularly and find to be well-tested and mature: `NumPy/SciPy`_: This pair of libraries provide array and matrix structures, linear algebra routines, numerical optimization, random number generation, statistics routines, differential equation modeling, Fourier transforms and signal processing, image processing, sparse and masked arrays, spatial computation, and numerous other mathematical routines. Together, they cover most of MATLAB's basic functionality and parts of many of the toolkits, and include support for reading and writing MATLAB files. Additionally, they now have great documentation (vastly improved from a few years ago) and a very active community. IPython_: One of the best things in Python is IPython, an enhanced interactive Python shell that makes debugging, profiling code, interactive plotting. It supports tab completion on objects, integrated debugging, module finding, and more -- essentially, it does almost everything you'd expect a command line programming interface to do. Additionally, Cython_: Referenced earlier, Cython is a painless way of embedding compiled, optimized bits of code in a larger Python program. SQLAlchemy_: SQLAlchemy_ makes leveraging the power of a database incredibly simple and intuitive. It is essentially a wrapper around an SQL database. You build queries using intuitive operators, then it generates the SQL, queries the database, and returns an iterator over the results. Combining it with sqlite_ -- embedded in Python's standard library -- allows one to leverage databases for scientific work with impressive ease. And, if you tell sqlite to build its database in memory, you've got another powerful data structure. To slightly plagiarize xkcd, SQLAlchemy makes databases fun again. PyTables_: PyTables is a great way of managing large amounts of data in an organized, reliable, and efficient fashion. It optimizes resources, automatically transferring data between disk and memory as needed. It also supports on-the-fly (DE)compression and works seamlessly with NumPy arrays. PyQt_: For writing user interfaces in C++, I recommend it is, in my experience, difficult to beat QT_. PyQt_ brings the ease of QT_ to Python_. And I do mean ease -- using the interactive `QT designer`_, I've build a reasonably complex GUI-driven scientific application with only a few dozen lines of custom GUI code. The entire thing was done in a few days. The code is cross-platform over Linux, Mac OS X, and Windows. If you need to develop a front end to your data framework, and don't mind the license (GPL for PyQT_, LGPL for QT), this is, in my experience, the easiest way to do so. TreeDict_: Without proper foresight and planning, larger research projects are particularly prone to the second law of thermodynamics: over time, the organization of parameters, options, data, and results becomes increasingly random. TreeDict_ is a Python data structure I designed to fight this. It stores hierarchical collections of parameters, variables, or data, and supports splicing, joining, copying, hashing, and other operations over tree structures. The hierarchical structure promotes organization that naturally tracks the conceptual divisions in the program -- for example, a single file can define all parameters while reflecting the structure of the rest of the code. Sage_: Sage_ doesn't really fit on this list as it packages many of the above packages into a single framework for mathematical research. It aims to be a complete solution to scientific programming, and it incorporates over a hundred open source scientific libraries. It builds on these with a great notebook concept that can really streamline the thought process and help organize general research. As an added bonus, it has an online interface for trying it out. As a complete package, I recommend newcomers to scientific Python programming try Sage_ first; it does a great job of unifying available tools in a consistent presentation. `Enthought Python Distribution`_: Also packaging these many libraries into a complete package for scientific computing, the `Enthought Python Distribution`_ is distributed by a company that contributes heavily to developing and maintaining these libraries. While there are commercial support options, it is free for academic use. Testing Framework ----------------- I do not feel comfortable releasing code without an accompanying suite of tests. This attitude, of course, reflects practical programmer wisdom; code that is guaranteed to function a certain way -- as encapsulated in these unit tests -- is reusable and dependable. While packaging test code with does not always equate with code quality, there is a strong correlation. Unfortunately, the research community does not often emphasize writing proper test code, due partly to that emphasis being directed, understandably, towards technique, theory, and publication. But this is exactly why a no-boilerplate, practical and solid testing framework and simple testing constructs like assert statements are so important. Python provides a built-in, low barrier-to-entry testing framework that encourages good test coverage by making the fastest workflow, including debugging time, involve writing test cases. In this way, Python again distinguishes itself from its competitors for scientific code. Downsides --------- No persuasive essay is complete without an honest presentation of the counterpoints, and indeed several can be made here. In fact, many of my arguments invite a counterargument -- with so many options available at every corner, where does one start? Having to make decisions at each turn could paralyze productivity. For most applications, wouldn't a language with a rigid but usually adequate style -- like MATLAB -- be better? While one can certainly use a no-flair scripting style in Python, I agree with this argument, at least to a certain extent. However, the situation is not uniformly bad -- rather, it's a bit like learning to ski versus learning to snowboard. The first day or two learning to snowboarding is always horrid, while one can pick up basic skiing quite quickly. However, fast-forward a few weeks, and while the snowboarder is perfecting impressive tricks, the skier is still working on not doing the splits. An exaggerated analogy, perhaps, but the principle still holds: investment in Python yields impressive rewards, but be prepared for a small investment in learning to leverage its power. The other downside with using Python for general scientific coding is the current landscape of conventions and available resources. Since MATLAB is so common in many fields, it is often conventional to publish open research code in MATLAB (except in some areas of mathematics, where Python is more common on account of Sage_; or in statistics, where R is the lingua franca). While MLabWrap_ makes this fairly workable, it does means that a Python programmer may need to work with both languages and possess a MATLAB license. Anyone considering a switch should be aware of this potential inconvenience; however, there seems to be a strong movement within scientific research towards Python -- largely for the reasons outlined here. A Complete Programming Solution ------------------------------- In summary, and reiterating my point that Python is a complete programming solution, I mention three additional points, each of which would make a great final thought. First, it is open source and `completely free\n`_, even for commercial use, as are many of the key scientific libraries. Second, it `runs natively on Windows, Mac OS, linux, and others\n`_, as does its standard library and the third party libraries I've mentioned here. Third, it fits quick scripting and large development projects equally well. A quick perusal of some `success stories\n`_ on Python's website showcases the diversity of environments in which Python provides a scalable, well-supported, and complete programming solution for research and scientific coding. However, the best closing thought is due to Randall Monroe, the author of xkcd_: \"Programming is fun again!\" Footnotes --------- .. [#readability] http://www.python.org/doc/essays/foreword/ .. include:: ../references.rst\n"}, {"score": 954.67676, "uuid": "8504e1d4-d842-5b5c-b036-e30426d7c6cb", "index": "cw12", "trec_id": "clueweb12-1200tw-24-15512", "target_hostname": "www.cio.com.au", "target_uri": "http://www.cio.com.au/article/419782/8_cool_tools_data_analysis_visualization_presentation/?utm_source=twitterfeed&utm_medium=twitter", "page_rank": 1.1700305e-09, "spam_rank": 72, "title": "8 cool tools <em>for</em> <em>data</em> <em>analysis</em>, visualization and presentation - software", "snippet": "At this year&#x27;s conference, <em>I</em> learned about other free (<em>or</em> at least inexpensive) tools <em>for</em> <em>data</em> <em>analysis</em> and presentation. Want to see all the tools from last year and 2012? <em>For</em> quick reference, check out our chart listing all 30 free <em>data</em> visualization and <em>analysis</em> tools.", "explanation": null, "document": "Comments\nReporters wrangle all sorts of data, from analyzing property tax valuations to mapping fatal accidents -- and, here at Computerworld, for stories about IT salaries and H-1B visas . In fact, tools used by data-crunching journalists are generally useful for a wide range of other, non-journalistic tasks -- and that includes software that's been specifically designed for newsroom use. And, given the generally thrifty culture of your average newsroom, these tools often have the added appeal of little or no cost.\nI came back from last year's National Institute for Computer-Assisted Reporting (NICAR) conference with 22 free tools for data visualization and analysis -- most of which are still popular and worth a look. At this year's conference, I learned about other free (or at least inexpensive) tools for data analysis and presentation.\nWant to see all the tools from last year and 2012?\nFor quick reference, check out our chart listing all 30 free data visualization and analysis tools.\nLike that previous group of 22 tools, these range from easy enough for a beginner (i.e., anyone who can do rudimentary spreadsheet data entry) to expert (requiring hands-on coding). Here are eight of the best:\nCSVKit\nWhat it does: This utility suite from GitHub has a host of Unix-like command-line tools for importing, analyzing and reformatting comma-separated data files.\nWhat's cool: Sure, you could pull your file into Excel to examine it, but CSVKit makes it quick and easy to preview, slice and summarize.\nFor example, you can see all your column headers in a list -- which is handy for super-wide, many-column files -- and then just pull data from a few of those columns. In addition to inputting CSV files, it can import several fixed-width file formats -- for example, there are libraries available for the specific fixed-width formats used by the Census Bureau and Federal Elections Commission.\nTwo simple commands will generate a data structure that can, in turn, be used by several SQL database formats ( Mr. Data Converter handles only MySQL). The SQL code will create a table, inferring the proper data type for each field as well as the insert commands for adding data to the table.\nThe Unix-like interface will be familiar to anyone who has worked on a *nix system, and makes it easy to save multiple frequently used commands in a batch file.\nDrawbacks: Working on a command line means learning new text commands (not to mention the likely risk of typing errors), which might not be worthwhile unless you work with CSV files fairly often. Also, be advised that this tool suite is written in Python, so Windows users will need that installed on their system as well.\nSkill level: Expert\nRuns on: Any Windows, Mac or Linux system with Python installed.\nLearn more: The documentation includes an easy-to-follow tutorial . There's also a brief introductory slide presentation that was given at the NICAR conference last month.\nRelated tools: Google Refine is a desktop application that can do some rudimentary file analysis as well as its core task of data cleaning; and The R Project for Statistical Computing can do more powerful statistical analysis on CSV and other files.\nDataTables\nWhat it does: This popular jQuery plug-in (which was designed and created by Allan Jardine) creates sortable, searchable HTML tables from a variety of data sources -- say, an existing, static HTML table, a JavaScript array, JSON or server-side SQL.\nWhat's cool: In addition to sortable tables, results can be searched in real time (results are narrowed further with each search-entry keystroke).\nDrawbacks: Search capability is fairly basic and cannot be narrowed by column or by using wildcard or Boolean searches.\nSkill level: Expert\nRuns on: JavaScript-enabled Web browsers\nLearn more: Numerous examples on the DataTables site show many ways to use this plug-in.\nFreeDive\nWhat it does: This alpha project from the Knight Digital Media Center at UC Berkeley turns a Google Docs spreadsheet into an interactive, sortable database that can be posted on the Web.\nWhat's cool: In addition to text searching, you can include numerical range-based sliders. Usage is free. End users can easily create their own databases from spreadsheets without writing code.\nFreeDive's chief current attraction is the ability to create databases without programming; however, freeDive source code will be posted and available for use once the project is more mature. That could appeal to IT departments seeking a way to offer this type of service in-house, allowing end users to turn a Google Doc into a filterable, sortable Web database using the Google Visualization API, Google Query Language, JavaScript and jQuery -- without needing to manually generate that code.\nDrawbacks: My test application ran into some intermittent problems; for example, it wouldn't display my data list when using the \"show all records\" button. This is an alpha project, and should be treated as such.\nIn addition, the current iteration limits spreadsheets to 10 columns and a single sheet. One column must have numbers, so this won't work for text-only information. The search widget is currently limited to a few specific choices of fields to search, although this might increase as the project matures. (A paid service like Caspio would offer more customization.) The nine-step wizard might get cumbersome after frequent use.\nSkill level: Advanced beginner.\nRuns on: Current Web browsers\nLearn more: The freeDive site includes several video tutorials at the bottom of the home page as well as test data to try out the wizard .\nRelated tools: Caspio is a well-established commercial alternative. For a JavaScript alternative with more control over the table created from a Google Docs spreadsheet, you might want to investigate Tabletop , which makes a Google Docs spreadsheet accessible to JavaScript code.\nHighcharts JS\nWhat it does: This JavaScript library from Highsoft Solutions provides an easy way to create professional-looking interactive charts for the Web. JQuery , Mootools or Prototype required.\nWhat's cool: With Highcharts, users can mouse over items for more details; they can also click on items in the chart legend to turn them on and off. There are many different chart types available, from basic line, bar, column and area charts to zoomable time series; each comes with six stylesheet options. Little customization is needed to get a sleek-looking chart -- and charts will display on iOS and Android devices as well as on desktop browsers.\nDrawbacks: Highcharts, like Google Maps, does have a distinctive look, so you may want to customize the Highcharts stylesheets so your visualizations don't look like numerous other Highcharts on the Web. While charts displayed fine for me on an Android phone, they weren't interactive (they were on an iPad ).\nAnd unlike most JavaScript/jQuery libraries, Highcharts is free only for non-commercial use, although a site-wide license for many companies costs only $80. (The cost jumps to $300 per developer seat in some cases -- for example, if charts are customized for individual users.) Rendering can be slow in some older browsers (notably Internet Explorer 6 and 7).\nSkill level: Intermediate to Expert.\nRuns on: Web browsers\nLearn more: The Highcharts demo gallery includes easy-to-view source code; the documentation explains other options.\nRelated tools: Google Chart Tools create static image charts and graphs or more interactive JavaScript-based visualizations; there are also JavaScript libraries such as Protovis and the JavaScript InfoVis Toolkit . Exhibit is an MIT Simile Project spinoff designed for presenting data on the Web with filtering, sorting and interactive capabilities.\nMr. Data Converter\nWhat it does: How often do you have data in one format -- while your application needs it in another? New York Times interactive graphics editor Shan Carter ran into this situation often enough that he coded a tool that converts comma- or tab-delimited data into nine different formats. It's available as either a service on the Web or an open source tool.\nWhat's cool: Mr. Data Converter can generate XML, JSON, ASP/VBScript or basic HTML table formatting as well as arrays in PHP, Python (as a dictionary) and Ruby. It will even generate MySQL code to create a table (guessing at field formats based on the data) and insert your data. If your data is in an Excel spreadsheet, you don't need to save it as a CSV or TSV; you can just copy and paste it into the tool.\nDrawbacks: Only CSV or TSV formats can be input, as well as copying and pasting in data from Excel.\nSkill level: Beginner\nRuns on: JavaScript-enabled Web browsers\nLearn more: You can follow Mr. Data Converter on Twitter at @mrdataconverter.\nRelated tools: Data Wrangler is a Web-based tool that reformats data to your specifications.\nPanda Project\nWhat it does: Panda is less about analyzing or presenting data than finding it amidst the pile of standalone spreadsheets scattered around an organization. It was specifically designed for newsrooms, but could be used by any organization where individuals collect information on their desktops that would be worth sharing. Billed as a \"newsroom appliance,\" users can upload CSV or Excel files to Panda and then search across all available data sets or a within a single file.\nWhat's cool: Panda makes it simple to give others access to information that's been sitting on individuals' hard drives in different stand-alone spreadsheets. Even non-technical users can easily upload and search data. Search is extremely fast, using ApacheSolr .\nDrawbacks: Queries are basic -- you can't specify a particular column/field to search, so a search for \"Washington\" would bring back items containing both the place and a person's name. The required hosting platform is quite specific, requiring Ubuntu 11.1. (Panda's developers have created an Amazon Community Image with the required server setup for hosting on Amazon Web Services EC2.)\nSkill level: Beginner (Advanced Beginner for administration)\nRuns on: Must be hosted on Amazon EC2 or a server running Ubuntu 11.10. Clients can use any Web browser.\nLearn more: Panda documentation , still in the works, gives basics on setup, configuration and use. Nieman Journalism Lab has some background on the project , which was funded by a $150,000 Knight News Challenge grant.\nPowerPivot\nWhat it does: This free plugin from Microsoft allows Excel 2010 to handle massively large data sets much more efficiently than the basic version of Excel does. It also lets Excel act like a relational database by adding the capacity to truly join columns in different tables instead of relying on Excel's somewhat cumbersome VLOOKUP command. PowerPivot includes its own formula language, Data Analysis Expressions (DAX), which has a similar syntax to Excel's conventional formulas.\nWhat's cool: PowerPivot can handle millions of records -- data sets that would usually grind PowerPivot-less Excel to a halt. And by joining tables, you can make more \"intelligent\" pivot tables and charts to explore and visualize large data sets with Excel's point-and-click interface.\nDrawbacks: This is limited to Excel 2010 on Windows systems. Also, SQL jocks might prefer using a true relational database for multi-table data in order to build complex data queries.\nSkill level: Intermediate\nRuns on: Excel 2010 on Windows only.\nLearn more: There are links to demos and videos on the PowerPivot main page , as well as an introductory tutorial on Microsoft's TechNet.\nRelated tools: Zoho Reports can take data from various file formats and turn it into charts, tables and pivot tables.\nWeave\nWhat it does: This general-purpose visualization platform allows creation of interactive dashboards with multiple, related visualizations -- for example, a bar chart, scatter plot and map. The open-source project was created by the University of Massachusetts at Lowell in partnership with a consortium of government agencies and is still in beta.\nWhat's cool: The visualizations are slick and highly interactive; clicking an area in one visualization also affects others in the dashboard. The platform includes powerful statistical analysis capabilities. Users can create their own visualizations on a Weave-based Web system, or save and alter the tools and appearances of visualizations that have been publicly shared by others.\nDrawbacks: Requires Flash for end-user viewing. It's currently somewhat difficult to install, although a one-click install is scheduled for this summer. And because it's so powerful, some users say that implementations must consider how to winnow down functionality so as not to overwhelm end users.\nSkill level: Intermediate for those just creating visualizations; Expert for those implementing a Weave system.\nRuns on: Flash-enabled browsers. Server requires a Java servlet container (Tomcat or Glassfish, MySQL or PostgreSQL, Linux and Adobe Flex 3.6 SDK).\nLearn more: The Weave site includes demos, videos and a user guide . For more examples of visualizations that can be built using a Weave platform, see one planner's MetroBoston DataCommon gallery . In addition, I wrote more detailed Computerworld coverage of Weave following a presentation at Northeastern University.\nRelated tools: Tableau Public is a robust general-purpose visualization platform.\nAlso see: 22 free tools for data visualization and analysis (April 20, 2011) and Chart and image gallery: 30 free tools for data visualization and analysis .\nSharon Machlis is online managing editor at Computerworld. Her e-mail address is smachlis@computerworld.com. You can follow her on Twitter @sharon000.\nJoin the CIO Australia group on LinkedIn . The group is open to CIOs, IT Directors, COOs, CTOs and senior IT managers.\n"}, {"score": 946.0128, "uuid": "5af71531-b4ed-5240-a251-5f2c72a3c4e5", "index": "cw12", "trec_id": "clueweb12-1200tw-26-10619", "target_hostname": "www.cio.com.au", "target_uri": "http://www.cio.com.au/article/419782/8_cool_tools_data_analysis_visualization_presentation/", "page_rank": 1.5210402e-09, "spam_rank": 74, "title": "8 cool tools <em>for</em> <em>data</em> <em>analysis</em>, visualization and presentation - software", "snippet": "At this year&#x27;s conference, <em>I</em> learned about other free (<em>or</em> at least inexpensive) tools <em>for</em> <em>data</em> <em>analysis</em> and presentation. Want to see all the tools from last year and 2012? <em>For</em> quick reference, check out our chart listing all 30 free <em>data</em> visualization and <em>analysis</em> tools.", "explanation": null, "document": "Comments\nReporters wrangle all sorts of data, from analyzing property tax valuations to mapping fatal accidents -- and, here at Computerworld, for stories about IT salaries and H-1B visas . In fact, tools used by data-crunching journalists are generally useful for a wide range of other, non-journalistic tasks -- and that includes software that's been specifically designed for newsroom use. And, given the generally thrifty culture of your average newsroom, these tools often have the added appeal of little or no cost.\nI came back from last year's National Institute for Computer-Assisted Reporting (NICAR) conference with 22 free tools for data visualization and analysis -- most of which are still popular and worth a look. At this year's conference, I learned about other free (or at least inexpensive) tools for data analysis and presentation.\nWant to see all the tools from last year and 2012?\nFor quick reference, check out our chart listing all 30 free data visualization and analysis tools.\nLike that previous group of 22 tools, these range from easy enough for a beginner (i.e., anyone who can do rudimentary spreadsheet data entry) to expert (requiring hands-on coding). Here are eight of the best:\nCSVKit\nWhat it does: This utility suite from GitHub has a host of Unix-like command-line tools for importing, analyzing and reformatting comma-separated data files.\nWhat's cool: Sure, you could pull your file into Excel to examine it, but CSVKit makes it quick and easy to preview, slice and summarize.\nFor example, you can see all your column headers in a list -- which is handy for super-wide, many-column files -- and then just pull data from a few of those columns. In addition to inputting CSV files, it can import several fixed-width file formats -- for example, there are libraries available for the specific fixed-width formats used by the Census Bureau and Federal Elections Commission.\nTwo simple commands will generate a data structure that can, in turn, be used by several SQL database formats ( Mr. Data Converter handles only MySQL). The SQL code will create a table, inferring the proper data type for each field as well as the insert commands for adding data to the table.\nThe Unix-like interface will be familiar to anyone who has worked on a *nix system, and makes it easy to save multiple frequently used commands in a batch file.\nDrawbacks: Working on a command line means learning new text commands (not to mention the likely risk of typing errors), which might not be worthwhile unless you work with CSV files fairly often. Also, be advised that this tool suite is written in Python, so Windows users will need that installed on their system as well.\nSkill level: Expert\nRuns on: Any Windows, Mac or Linux system with Python installed.\nLearn more: The documentation includes an easy-to-follow tutorial . There's also a brief introductory slide presentation that was given at the NICAR conference last month.\nRelated tools: Google Refine is a desktop application that can do some rudimentary file analysis as well as its core task of data cleaning; and The R Project for Statistical Computing can do more powerful statistical analysis on CSV and other files.\nDataTables\nWhat it does: This popular jQuery plug-in (which was designed and created by Allan Jardine) creates sortable, searchable HTML tables from a variety of data sources -- say, an existing, static HTML table, a JavaScript array, JSON or server-side SQL.\nWhat's cool: In addition to sortable tables, results can be searched in real time (results are narrowed further with each search-entry keystroke).\nDrawbacks: Search capability is fairly basic and cannot be narrowed by column or by using wildcard or Boolean searches.\nSkill level: Expert\nRuns on: JavaScript-enabled Web browsers\nLearn more: Numerous examples on the DataTables site show many ways to use this plug-in.\nFreeDive\nWhat it does: This alpha project from the Knight Digital Media Center at UC Berkeley turns a Google Docs spreadsheet into an interactive, sortable database that can be posted on the Web.\nWhat's cool: In addition to text searching, you can include numerical range-based sliders. Usage is free. End users can easily create their own databases from spreadsheets without writing code.\nFreeDive's chief current attraction is the ability to create databases without programming; however, freeDive source code will be posted and available for use once the project is more mature. That could appeal to IT departments seeking a way to offer this type of service in-house, allowing end users to turn a Google Doc into a filterable, sortable Web database using the Google Visualization API, Google Query Language, JavaScript and jQuery -- without needing to manually generate that code.\nDrawbacks: My test application ran into some intermittent problems; for example, it wouldn't display my data list when using the \"show all records\" button. This is an alpha project, and should be treated as such.\nIn addition, the current iteration limits spreadsheets to 10 columns and a single sheet. One column must have numbers, so this won't work for text-only information. The search widget is currently limited to a few specific choices of fields to search, although this might increase as the project matures. (A paid service like Caspio would offer more customization.) The nine-step wizard might get cumbersome after frequent use.\nSkill level: Advanced beginner.\nRuns on: Current Web browsers\nLearn more: The freeDive site includes several video tutorials at the bottom of the home page as well as test data to try out the wizard .\nRelated tools: Caspio is a well-established commercial alternative. For a JavaScript alternative with more control over the table created from a Google Docs spreadsheet, you might want to investigate Tabletop , which makes a Google Docs spreadsheet accessible to JavaScript code.\nHighcharts JS\nWhat it does: This JavaScript library from Highsoft Solutions provides an easy way to create professional-looking interactive charts for the Web. JQuery , Mootools or Prototype required.\nWhat's cool: With Highcharts, users can mouse over items for more details; they can also click on items in the chart legend to turn them on and off. There are many different chart types available, from basic line, bar, column and area charts to zoomable time series; each comes with six stylesheet options. Little customization is needed to get a sleek-looking chart -- and charts will display on iOS and Android devices as well as on desktop browsers.\nDrawbacks: Highcharts, like Google Maps, does have a distinctive look, so you may want to customize the Highcharts stylesheets so your visualizations don't look like numerous other Highcharts on the Web. While charts displayed fine for me on an Android phone, they weren't interactive (they were on an iPad ).\nAnd unlike most JavaScript/jQuery libraries, Highcharts is free only for non-commercial use, although a site-wide license for many companies costs only $80. (The cost jumps to $300 per developer seat in some cases -- for example, if charts are customized for individual users.) Rendering can be slow in some older browsers (notably Internet Explorer 6 and 7).\nSkill level: Intermediate to Expert.\nRuns on: Web browsers\nLearn more: The Highcharts demo gallery includes easy-to-view source code; the documentation explains other options.\nRelated tools: Google Chart Tools create static image charts and graphs or more interactive JavaScript-based visualizations; there are also JavaScript libraries such as Protovis and the JavaScript InfoVis Toolkit . Exhibit is an MIT Simile Project spinoff designed for presenting data on the Web with filtering, sorting and interactive capabilities.\nMr. Data Converter\nWhat it does: How often do you have data in one format -- while your application needs it in another? New York Times interactive graphics editor Shan Carter ran into this situation often enough that he coded a tool that converts comma- or tab-delimited data into nine different formats. It's available as either a service on the Web or an open source tool.\nWhat's cool: Mr. Data Converter can generate XML, JSON, ASP/VBScript or basic HTML table formatting as well as arrays in PHP, Python (as a dictionary) and Ruby. It will even generate MySQL code to create a table (guessing at field formats based on the data) and insert your data. If your data is in an Excel spreadsheet, you don't need to save it as a CSV or TSV; you can just copy and paste it into the tool.\nDrawbacks: Only CSV or TSV formats can be input, as well as copying and pasting in data from Excel.\nSkill level: Beginner\nRuns on: JavaScript-enabled Web browsers\nLearn more: You can follow Mr. Data Converter on Twitter at @mrdataconverter.\nRelated tools: Data Wrangler is a Web-based tool that reformats data to your specifications.\nPanda Project\nWhat it does: Panda is less about analyzing or presenting data than finding it amidst the pile of standalone spreadsheets scattered around an organization. It was specifically designed for newsrooms, but could be used by any organization where individuals collect information on their desktops that would be worth sharing. Billed as a \"newsroom appliance,\" users can upload CSV or Excel files to Panda and then search across all available data sets or a within a single file.\nWhat's cool: Panda makes it simple to give others access to information that's been sitting on individuals' hard drives in different stand-alone spreadsheets. Even non-technical users can easily upload and search data. Search is extremely fast, using ApacheSolr .\nDrawbacks: Queries are basic -- you can't specify a particular column/field to search, so a search for \"Washington\" would bring back items containing both the place and a person's name. The required hosting platform is quite specific, requiring Ubuntu 11.1. (Panda's developers have created an Amazon Community Image with the required server setup for hosting on Amazon Web Services EC2.)\nSkill level: Beginner (Advanced Beginner for administration)\nRuns on: Must be hosted on Amazon EC2 or a server running Ubuntu 11.10. Clients can use any Web browser.\nLearn more: Panda documentation , still in the works, gives basics on setup, configuration and use. Nieman Journalism Lab has some background on the project , which was funded by a $150,000 Knight News Challenge grant.\nPowerPivot\nWhat it does: This free plugin from Microsoft allows Excel 2010 to handle massively large data sets much more efficiently than the basic version of Excel does. It also lets Excel act like a relational database by adding the capacity to truly join columns in different tables instead of relying on Excel's somewhat cumbersome VLOOKUP command. PowerPivot includes its own formula language, Data Analysis Expressions (DAX), which has a similar syntax to Excel's conventional formulas.\nWhat's cool: PowerPivot can handle millions of records -- data sets that would usually grind PowerPivot-less Excel to a halt. And by joining tables, you can make more \"intelligent\" pivot tables and charts to explore and visualize large data sets with Excel's point-and-click interface.\nDrawbacks: This is limited to Excel 2010 on Windows systems. Also, SQL jocks might prefer using a true relational database for multi-table data in order to build complex data queries.\nSkill level: Intermediate\nRuns on: Excel 2010 on Windows only.\nLearn more: There are links to demos and videos on the PowerPivot main page , as well as an introductory tutorial on Microsoft's TechNet.\nRelated tools: Zoho Reports can take data from various file formats and turn it into charts, tables and pivot tables.\nWeave\nWhat it does: This general-purpose visualization platform allows creation of interactive dashboards with multiple, related visualizations -- for example, a bar chart, scatter plot and map. The open-source project was created by the University of Massachusetts at Lowell in partnership with a consortium of government agencies and is still in beta.\nWhat's cool: The visualizations are slick and highly interactive; clicking an area in one visualization also affects others in the dashboard. The platform includes powerful statistical analysis capabilities. Users can create their own visualizations on a Weave-based Web system, or save and alter the tools and appearances of visualizations that have been publicly shared by others.\nDrawbacks: Requires Flash for end-user viewing. It's currently somewhat difficult to install, although a one-click install is scheduled for this summer. And because it's so powerful, some users say that implementations must consider how to winnow down functionality so as not to overwhelm end users.\nSkill level: Intermediate for those just creating visualizations; Expert for those implementing a Weave system.\nRuns on: Flash-enabled browsers. Server requires a Java servlet container (Tomcat or Glassfish, MySQL or PostgreSQL, Linux and Adobe Flex 3.6 SDK).\nLearn more: The Weave site includes demos, videos and a user guide . For more examples of visualizations that can be built using a Weave platform, see one planner's MetroBoston DataCommon gallery . In addition, I wrote more detailed Computerworld coverage of Weave following a presentation at Northeastern University.\nRelated tools: Tableau Public is a robust general-purpose visualization platform.\nAlso see: 22 free tools for data visualization and analysis (April 20, 2011) and Chart and image gallery: 30 free tools for data visualization and analysis .\nSharon Machlis is online managing editor at Computerworld. Her e-mail address is smachlis@computerworld.com. You can follow her on Twitter @sharon000.\nJoin the CIO Australia group on LinkedIn . The group is open to CIOs, IT Directors, COOs, CTOs and senior IT managers.\n"}, {"score": 943.9863, "uuid": "b9dcd48a-3127-5ebe-b012-57dbcf6ddaf6", "index": "cw12", "trec_id": "clueweb12-1802wb-10-16955", "target_hostname": "archlinux.me", "target_uri": "http://archlinux.me/dusty/tag/python/", "page_rank": 1.7589001e-09, "spam_rank": 74, "title": "Dusty&#x27;s Diverse Domain \u00bb <em>python</em>", "snippet": "<em>I</em> don\u2019t have exact numbers, but <em>for</em> \u201chistorical reasons\u201d, a non-<em>python</em> file containing Base64 encoded <em>data</em> was given a .py extension. When excluding this file from the line count, around half a million lines of actual <em>Python</em> code exist, and about a quarter of these are tests.", "explanation": null, "document": "Pyjaco in a real app: Todos with local storage\nJanuary 8, 2012, 10:15 pm\nI didn\u2019t get the memo, but there appears to be a movement to demonstrate emerging web technologies with a simple todo list application, much as hello world is used to introduce programming languages.\nIn my last post , I introduced using jQuery with Pyjaco , the PYthon JAvascript COmpiler. Since then, I\u2019ve made several contributions to the project and have been involved in diverse discussions with Pyjaco developers regarding the current and future status of the project. This post goes further by acting as a tutorial for writing a basic todos app using Pyjaco.\nPyjaco is alpha software. It is hard to write valid code, and harder to debug. I\u2019ve managed to both lock up Firefox and hard crash it while using the Pyjaco library.\nOn the positive side, Pyjaco is under active, rapid development. The head developer, Christian Iversen is extremely responsive to both questions about Pyjaco, and to code contributions. This is a project with a lot of potential, and I see it as the current best bet for Python programmers hoping to avoid javascript one day in the future.\nIn spite of the hiccups, it is possible to generate a working javascript app using just Pyjaco. Here\u2019s how.\nLet\u2019s start:\nmkdir pyjados cd pyjados\nvirtualenv2 venv --distribute --no-site-packages source venv/bin/activate\npip install git+https://buchuki@github.com/buchuki/pyjaco.git@run_script\nFirst we create a directory to work in and install a virtualenv. Pyjaco does not currently work with python 3, so in Arch Linux, I use the virtualenv2 command. We then activate the virtualenv and install the pyjaco package. Here I am installing from my personal fork, as it contains some changes for generating the built-in standard library that have not yet been merged upstream. You should normally install directly from chrivers\u2019s git repository using pip install git+git://github.com/chrivers/pyjaco.git.\nNow let\u2019s create a basic HTML 5 page with jQuery loaded:\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>PyJaco Todo List Example</title>\n    <script type=\"text/javascript\" src=\"http://code.jquery.com/jquery-1.6.4.min.js\"></script>\n  </head>\n  <body>\n    <h1>PyJaco Todo List Example</h1>\n  </body>\n</html>\nWe can load this in our web browser using a file:// URL. This is the only HTML page in our app, and it can be refreshed to load our changes as we work.\nPyjaco doesn\u2019t simply translate Python into Javascript. Rather, it creates a basic standard library of Python-like objects that are utilized in the compiled javascript code. I wasn\u2019t too keen on this idea when I first heard it, as it introduces a dependency that currently weighs in at 65K before minification and compression. While this is not a terribly heavy library, there are efforts under way to shrink the builtins or to dynamically generate it to contain only those builtins that your code actually touches. At any rate, we need to ensure this library is available to our code. First we generate the library:\npyjs.py --builtins=generate --output=py-builtins.js\npyjs.py is the name of the pyjaco command. It is expected to be renamed to pyjaco in the future. The --builtins=generate option tells pyjaco to generate the standard library, while the --output flag provides the filename for the new library file:\n$ ls index.html  py-builtins.js  venv\nWe then need to load this library in the head of our html file. Let\u2019s also load the future pyjados.js script at this time:\n<head>\n    <title>PyJaco Todo List Example</title>\n    <script type=\"text/javascript\" src=\"http://code.jquery.com/jquery-1.6.4.min.js\"></script>\n    <script type=\"text/javascript\" src=\"py-builtins.js\"></script>\n    <script type=\"text/javascript\" src=\"pyjados.js\"></script>\n  </head>\nNow, before we start coding the Python file that will be compiled to Javascript, I want to discuss what I consider to be the most confusing aspect of Pyjaco development. There are basically two types of variables in Pyjaco, Javascript variables, and Python variables. Javascript variables refer to \u201cnormal\u201d variables that you would call in Javascript. These include alert, window, document and the like, as well as variables in third-party Javascript libraries, such as the ubiquitous jQuery. Further, any attributes on those objects are also Javascript variables, and the return value of any methods will also be Javascript variables.\nPython variables, on the other hand, refer to any variables that you define in your Python source code. If you create a dict or a list, for example, it will be compiled to a list or dict object from the standard library we just generated. In the compiled script, of course these Python variables are represented by Javascript objects, but from the point of view of a Pyjaco coder, it is important to keep the two types of files separate. Almost all the bugs I have encountered in my Pyjaco code have been caused by confusing the two types of variables.\nThe distinction between python and javascript variables introduces a couple of complications to writing Pyjaco compatible python code. First we need to flag all of our Javascript variables using a decorator on methods that access them. Second, we need to explicitly convert our variables between Javascript and Python any time we access one from the other. I\u2019m told that this conversion can \u2014 and one day will \u2014 be done automatically by the pyjaco multiplexer, but in the meantime, we need to make it explicit. We do this by using two javascript functions supplied with the standard library we just generated, appropriately named js() and py(). You will see examples of these shortly.\nWhen I finally figured out the distinction, my first thought was, \u201cok, let\u2019s prefer to always work with python variables.\u201d Therefore, in my initialization code, I tried jQ=py(jQuery). Unfortunately, jQuery is a rather large object, and the py function apparently recursively converts all attributes from javascript to python. I ended up with a stack overflow.\nNow, let\u2019s create our first python code and watch it compile to Javascript. Name the file pyjados.py:\ndef setup(): print \"Pyjados Hello World\" jQuery(js(setup));\nFirst we write a python function named setup. This function is a python object. jQuery is a javascript object that expects a javascript object as input. Therefore, we wrap setup in a js() call and pass the result into the jQuery function. jQuery will now run setup when document.ready is fired.\nNow we compile the code using the following command inside our activated virtualenv:\npyjs.py --watch pyjados.py --output pyjados.js\nYou\u2019ll notice the command doesn\u2019t exit. That is the --watch option at work. If you now make a change to pyjados.py and save it, it will automatically recompile it. The output file pyjados.js is regenerated each time. This is the file we included in our html file. So now, open that html file in a web browser using a file:// url. Make sure the Javascript console is displayed and reload the page. You should see the words \u201cPyjados Hello World\u201d printed on the console. Pyjaco automatically compiles print statements into console.log output.\nBefore we start implementing our Todo list, let\u2019s look at an example of accessing a javascript variable inside a function. Change setup.py to utilize alert, as follows:\n@JSVar(\"alert\") def setup():\n    alert(js(\"Pyjados Hello Alert\") jQuery(js(setup));\nDid you look closely at that code? There is a missing close bracket on the alert line. You\u2019ll see the syntax error in your console where pyjs.py is watching the compiled code. Add the bracket and let it automatically recompile itself:\n@JSVar(\"alert\") def setup():\n    alert(js(\"Pyjados Hello Alert\")) jQuery(js(setup));\nLet\u2019s analyze this snippet. First, notice how we told the compiler that alert is a Javascript variable when used inside setup(). This is a bit odd, since the JSVar decorator is never actually imported into the namespace. This is a bit of magic in the Pyjaco compiler, just pretend it has been imported.\nSecond, notice that since alert has been flagged as a JSVar, it must accept a Javascript variable. However, the string \u201cPyjados Hello Alert\u201d is a Python variable. Therefore, we convert it using js() as we pass it into the alert call.\nNow let\u2019s prepare to create some working todo-list code. Start by adding a form for submitting todos and a list to render the todos to the html body:\n<body>\n    <h1>PyJaco Todo List Example</h1>\n    <form id=\"add_todo_form\">\n      <input type=\"text\" id=\"add_box\" placeholder=\"Add Todo\", autofocus=\"autofocus\">\n      <button id=\"add_button\">Add Todo</button>\n    </form>\n    <ul id=\"todo_items\"></ul>\n  </body>\nNothing too exciting here. Note the ids on the elements, since we\u2019ll be utilizing these from Pyjaco using jQuery selectors.\nNow back into the python file. Let\u2019s create a class to manage the various todo elements:\nclass TodosApp:\n    @JSVar(\"jQuery\", \"js_add_form\") def __init__(self):\n        js_add_form = jQuery(js(\"#add_todo_form\")) js_add_form.submit(js(self.add_todo)) def add_todo(self, event): print \"form submitted\" return js(False) def setup():\n    todo_app = TodosApp() jQuery(js(setup));\nThe __init__ function hooks up the form\u2019s submit button to a method on the object. Notice that we need to flag not just jQuery, but also js_add_form as a javascript variable. Pyjaco does not (currently) know that a javascript variable is returned when calling a method on an existing javascript variable. I like to add the js_ prefix to variable names to help remind myself that this is a javascript variable.\nIn an ideal world, we could convert this variable to a Python variable using py(), but as noted earlier, calling py on a jQuery object results in a stack overflow or browser crash.\nAlso pay attention to the way we wrap the self.add_todo method name in a js() call when we pass it into the submit handler. The submit method is a javascript function expecting a javascript object.\nThe def add_todo method has its single parameter flagged as a @JSVar, since the method is being called internally by jQuery when the event occurs. We also wrap the False return value (to prevent event propogation on the submit handler) in a js() call so that jQuery recognizes it as a javascript false rather than a (true) object named False.\nTry the code. Ensure the compiler recompiled it, and reload the html file. Enter some characters into the text box and use the Enter key or the Add Todo button to submit the form. The words form submitted should be displayed in the javascript console.\nNow let\u2019s actually store and render a newly added todo. The todos are stored in memory in a python dict object. Initialize this object by adding the following two lines of code to the end of __init__:\nself.todos = {} self.next_id = 1\nAnd rewrite add_todo as follows as well as a new method named render\n@JSVar(\"event\", \"js_add_box\") def add_todo(self, js_event):\n        js_add_box = jQuery(js(\"#add_box\")) self.todos[self.next_id] = py(js_add_box.val()) js_add_box.val('') js_add_box.focus() self.next_id += 1 self.render() return js(False) @JSVar(\"js_todo_items\") def render(self):\n        js_todo_items = jQuery(js(\"#todo_items\")) js_todo_items.html(\"\") for id, todo in sorted(self.todos.items()):\n            js_todo_items.append(js('<li>%s</li>' % (id, todo)))\nNote that the todos dict is a Python object, so when we insert the value of the js_add_box into it, we must convert it from a javascript object using py(). Also note how, because we are writing in a python function, manipulating the python value self.next_id requires no conversion, and calling the python function self.render is also clean.\nIn the render function itself, I think it\u2019s pretty cool that string formatting using % is supported by pyjaco (as an aside, the str.format method introduced in python 2.6 is not yet available) and that the python sorted() function is available. Note also how we can loop over items() on the self.todos dictionary just as if we were using a normal python dictionary.\nNow let\u2019s add the ability to complete todos. Let\u2019s start by adding a template string as a class variable, and use that string inside the render function. This illustrates that pyjaco supports class variables:\nclass TodosApp:\n    list_item_template = \"\"\"<li>\n    %(text)s\n    </li>\"\"\"\nand we change the for loop in render to:\nfor id, todo in sorted(self.todos.items()):\n            js_todo_items.append(js(TodosApp.list_item_template % { \"id\": id, \"text\": todo}))\nReload the page again and notice how checkboxes have been displayed beside each todo. The next step is to make clicking these boxes actually complete the todos. We add a couple lines to our __init__ method to connect a live click event to the checkbox items, which now looks like this:\n@JSVar(\"jQuery\", \"js_add_form\", \"js_checkbox\") def __init__(self):\n        js_add_form = jQuery(js(\"#add_todo_form\")) js_add_form.submit(js(self.add_todo)) js_checkbox = jQuery(js(\"input[type=checkbox]\")) js_checkbox.live(\"click\", js(self.complete_todo)) self.todos = {} self.next_id = 1\nDon\u2019t forget to add js_checkbox to the JSVar decorator.\nThe complete_todo method looks like this:\n@JSVar(\"event\", \"jQuery\", \"todo_item\") def complete_todo(self, event):\n        todo_item = jQuery(event.target).parent() id = int(py(todo_item.attr(\"id\"))[5:]) del self.todos[id] todo_item.delay(1500).fadeOut(\"slow\")\nThe first line is using exclusively javascript arguments, and returns the <li> element containing the checkbox that was clicked. The id = line converts the javascript string id attribute of this element (which looks like \u201ctodo_5\u201c, as defined in list_item_template) into the python integer id of the todo. The remaining lines simply remove that todo from the internal list and from the DOM, after a 1.5 second delay.\nIn fact, we now have a fully functional todo list that allows adding todos and checking them off. Now, as a bonus, let\u2019s try hooking this up to the HTML 5 localStorage object so that the list is maintained across page reloads. We start by adding a store() method to our class:\n@JSVar(\"localStorage\", \"JSON\") def store(self):\n        localStorage.setItem(\"todolist\", JSON.stringify(js(self.todos)))\nThe main line of code is easiest to read from the inside out. First we convert the self.todos dict to a normal javascript object using the js() function. Then we call JSON.stringify on this object to create a string suitable for insertion into localStorage.\nNow add this call to the end of the two methods that manipulate the todo list, add_todo and complete_todo:\nself.store()\n.\nRefresh the page, add a couple todos, and inspect the localStorage object in your console. You should see the stringified dict in the todolist value.\nNow all we have to do is ensure the self.todos dict is loaded from localStorage when the app is initialized. Add the following to the end of the __init__ method (make sure to add js_stored_todos to the JSVars decorator):\njs_stored_todos = localStorage.getItem(\"todolist\") if js_stored_todos:\n            stored_dict = dict(py(JSON.parse(js_stored_todos))) self.todos = dict([(int(i), stored_dict[i]) for i in stored_dict.keys()]) self.next_id = max(self.todos.keys()) + 1 self.render()\nNote that calling py() on the output of JSON.parse creates a python object, not a python dict. The code is therefore wrapped in a call to dict(), which converts the object to a dictionary.\nUnfortunately, the resultant dict contains keys that are strings, whereas our original dict used integer keys. So a pure-python list comprehension is used to convert the dictionary to one with integer keys. This line is a bit hard to read, but I wanted to include it to demonstrate that Pyjaco can parse list comprehensions. Finally, we set self.next_id using the python max() call, which Pyjaco also automatically translates into javascript.\nTry it out. Load the pyjados HTML file, add some todos, check a few of them off, then close and reload the web browser. Your todos will be stored!\nI hope you\u2019ve enjoyed this introduction to Pyjaco. It is a nice tool with a lot of potential. Currently, I find writing Pyjaco code to be approximately equally tedious to writing Javascript code. However, I feel that as I learn the ins and outs of Pyjaco, and as the developers continue to refine and improve the compiler, Pyjaco may one day be a perfectly viable alternative to writing pure Javascript or to the rather too Ruby-esque, but otherwise excellent Coffeescript.\n"}, {"score": 937.219, "uuid": "5c203e3f-8807-563b-aea2-b1c39fdaef73", "index": "cw12", "trec_id": "clueweb12-1200wb-81-06816", "target_hostname": "pythonconquerstheuniverse.wordpress.com", "target_uri": "http://pythonconquerstheuniverse.wordpress.com/page/2/", "page_rank": 1.1748761e-09, "spam_rank": 79, "title": "<em>Python</em> Conquers The Universe", "snippet": "<em>For</em> a number of years <em>I</em> did quite a lot of teaching \u2014 computer systems <em>analysis</em> and design, and also <em>data</em> modeling and database design. During the classes the students and <em>I</em> constructed a lot of diagrams of various kinds.", "explanation": null, "document": "Filed under: Moving to Python 3 \u2014 Steve Ferg @ 1:20 pm\nWith the improved support for Unicode in Python3, more and more folks will be working with languages (Arabic, Hebrew, etc.) that read right-to-left rather than left-to-right. So more and more folks will have a need to reverse a string.\nUnfortunately, Python doesn\u2019t have a built-in function, nor do string objects have a built-in method, to do what they will want.\u00a0 The obvious techniques don\u2019t work. This:\ntry:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 print(1)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 s = \"a b c\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 s = reverse(s)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 print(s)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 except Exception as e:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 print(e)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 try:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 print(2)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 s = \"a b c\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 s = reversed(s)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 print(s)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 except Exception as e:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 print(e)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 try:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 print(3)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 s = \"a b c\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 s.reverse()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 print(s)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 except Exception as e:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 print(e)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 try:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 print(4)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 s = \"a b c\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 s.reversed()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 print(s)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 except Exception as e:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 print(e)\nproduces this output\n1\n        name 'reverse' is not defined\n        2\n        <reversed object at 0x00BAB5F0>\n        3\n        'str' object has no attribute 'reverse'\n        4\n        'str' object has no attribute 'reversed'\nFortunately, the solution is not too difficult. A little one-line function will do the trick.\nI call the function \u201crev\u201d rather than \u201creverse\u201d on the chance that Python will eventually acquire its own builtin function named \u201creverse\u201d.\ndef rev(s): return s[::-1]\nIn a comment, Michael Watkins has noted another possible implementation of the \u201crev\u201d function.\ndef rev(s): ''.join(reversed(s))\ntry:\n            print(5)\n            s = \"a b c\"\n            s = rev(s)\n            print(s)\n        except Exception as e:\n            print(e)\nproduces\n"}, {"score": 936.1007, "uuid": "68543694-391b-54ff-ae9d-4b4ced254c85", "index": "cw12", "trec_id": "clueweb12-0301wb-56-34104", "target_hostname": "www.python.org", "target_uri": "http://www.python.org/community/jobs/", "page_rank": 1.0962415e-08, "spam_rank": 91, "title": "<em>Python</em> Job Board", "snippet": "We are looking <em>for</em> talented and passionate software engineers to design and build our <em>data</em> management systems, develop advanced <em>analysis</em> tools to leverage this <em>data</em> <em>for</em> agricultural management, and create web-based front-ends <em>for</em> <em>data</em> <em>analysis</em> and visualization.", "explanation": null}, {"score": 930.7033, "uuid": "a8be8232-fb3e-56f4-a6fc-75a2e146f3e5", "index": "cw12", "trec_id": "clueweb12-0809wb-69-20500", "target_hostname": "www.haidongji.com", "target_uri": "http://www.haidongji.com/category/technology/python/", "page_rank": 1.1700305e-09, "spam_rank": 69, "title": "The Ji Village News \u00bb <em>Python</em>", "snippet": "In analyzing and building up the grammar&#x2F;structure of the text <em>for</em> <em>Python</em>, <em>I</em> opted <em>for</em> line by line <em>analysis</em> and coding. <em>I</em> think this brings clarity and is easier to read.", "explanation": null, "document": "January 5, 2011 @ \u00b7 Filed under Linux , Python , SQLServer , Technology , Windows\nCommand line utility to search for objects based on names. Search results will be displayed in well formatted tabular format, left-justified.\n1. pyodbc installed;\n2. Use trusted authentication by default. Find the relevant code to adjust to login/password as needed;\n3. -w for wild card search. Default is exact name match, case insensitive in almost all cases, depending on your SQL Server configuration;\n4. If -d is not specified, all databases in instance will be searched, minus the ones that one not accessible at the moment. To specify database(s) you want to search into, use -d followed by database names. Separate them with comma if more than one;\n5. Use -S to specify instance;\n6. Good sample code for command line parameters processing and pyodbc usage;\n7. Won\u2019t work with SQL Server 2000 or lower.\nExample 1: python objectSearch.py myobject -S instance1\nResult: Database objects that are named myobject in instance1 will be displayed\nExample 2: python objectSearch.py -S server1 -w -d db1,db2 myobject\nResult: Database objects that has \u201cmyobject\u201d in db1 and db2 in server1 will be displayed\nimport pyodbc, argparse\ndef pp(cursor, data=None, check_row_lengths=True):\n    if not data:\n        data = cursor.fetchall( )\n    names = [  ]\n    lengths = [  ]\n    rules = [  ]\n    for col, field_description in enumerate(cursor.description):\n        field_name = field_description[0]\n        names.append(field_name)\n        field_length = field_description[2] or 12\n        field_length = max(field_length, len(field_name))\n        if check_row_lengths:\n            # double-check field length, if it's unreliable\n            data_length = max([ len(str(row[col])) for row in data ])\n            field_length = max(field_length, data_length)\n        lengths.append(field_length)\n        rules.append('-' * field_length)\n    format = \" \".join([\"%%-%ss\" % l for l in lengths])\n    result = [ format % tuple(names), format % tuple(rules) ]\n    for row in data:\n        result.append(format % tuple(row))\n    return \"\\n\".join(result)\ndef objectMatch(wildcardSearch, objectNamePattern, dbName, cn):\n\tif wildcardSearch:\n\t\tsql = \"select '%s' as dbName, s.name schema_name, o.name, o.type from %s.sys.objects o inner join %s.sys.schemas s on o.schema_id = s.schema_id where o.name like '%%%s%%\\' order by dbname, schema_name, o.name\" % (dbName, dbName, dbName, objectNamePattern)\n\t\tcursor = cn.cursor()\n\t\tcursor.execute(sql)\n\t\trows = cursor.fetchall()\n\t\tif len(rows) > 0:\n\t\t\tprint pp(cursor, rows, 1)\n\telse:\n\t\tsql = \"select '%s' as dbName, s.name schema_name, o.name, o.type from %s.sys.objects o inner join %s.sys.schemas s on o.schema_id = s.schema_id where o.name = '%s' order by dbname, schema_name, o.name\" % (dbName, dbName, dbName, objectNamePattern)\n\t\tcursor = cn.cursor()\n\t\tcursor.execute(sql)\n\t\trows = cursor.fetchall()\n\t\tif len(rows) > 0:\n\t\t\tprint pp(cursor, rows, 1)\nparser = argparse.ArgumentParser(description='SQL Server object search. Case insensitive')\nparser.add_argument('-S', '--server', help='Instance you wish to connect to. myInstance is the default if not specified', dest='instance', default='myInstance')\nparser.add_argument('-w', help='Wildcard search indicator. If specified, LIKE clause will be used for pattern matching. Otherwise it will look for an exact match. Exact match is default', action=\"store_true\", dest='wild', default=False)\nparser.add_argument('-d', '--database', help='Database(s) you want to search in. If more than one, separate them by comma only. It will search all databases by default', action=\"store\", dest=\"dbs\")\nparser.add_argument('objectNamePattern', help='Object name pattern you want to search for', action=\"store\")\nargList = parser.parse_args()\ntry:\n\tcn = pyodbc.connect(\"DRIVER={SQL Server};SERVER=%s;DATABASE=master;Trusted_Connection=yes\" % argList.instance)\nexcept:\n\tprint \"Couldn't connect to %s. It is down or you might have had a typo.\" % argList.instance\nif argList.dbs is None:\n\tcursor = cn.cursor()\n\tcursor.execute(\"select name from sys.databases where state = 0\")\n\trows = cursor.fetchall()\n\tfor row in rows:\n\t\tobjectMatch(argList.wild, argList.objectNamePattern, row.name, cn)\nelse:\n\tdbs = argList.dbs.split(',')\n\tfor db in dbs:\n\t\tobjectMatch(argList.wild, argList.objectNamePattern, db, cn)\nQuestions on Text processing with Python\nDecember 31, 2010 @ \u00b7 Filed under Python , Technology\nNo need to waste time on proving the importance of text processing, I suppose. Here is an automation use case I had in mind when I started my search: sucking out all domain\\login inside a block of text.\nYes, I can build my own regular expressions, and I have done that in the past. But, another use case is log file processing: SQL Server, Apache, MySQL, and such. Therefore an existing module that is easy to code, read and maintain is better than me code everything. In the end, regular expression is still going to be used, but a layer of abstraction will help productivity and maintainability.\nI came across 3 modules: pyparsing , SimpleParse , and NLTK . I am curious to hear your opinions on those 3 modules, or if you have suggestions other than the 3 mentioned here:\n1. How easy/difficult to learn those modules? I haven\u2019t tried SimpleParse or NLTK yet, but I have tried pyparsing, which looks easy to pick up and the author, Paul McGuire, is very helpful. NTLK might be an overkill for what I do, at first glance.\n2. What about performance? In most of my use cases, this is probably not that important, but I\u2019ve come across comments on StackOverflow saying that pyparsing does not perform very well when text volume is large.\n3. What about support and ongoing development? Like I mentioned earlier, the author behind pyparsing seems to be very active in answering questions and incorporating new ideas.\nThanks in advance for any insights and Happy New Year!\nPS, here are 2 solutions to get domain\\login out with pyparsing that Paul helpfully provided when I asked:\nfrom pyparsing import *\ngrammar = Combine(Word(alphanums) + \"\\\\\" + Word(alphanums))\nmatches = grammar.searchString(\"jwfoleow fjlwowe\\jfoew lwfweolwfo\\jofweojw lifewijowe\")\nfor m in matches:\n\tprint m[0]\n########\nfor toks,start,end in grammar.scanString(\"jwfoleow fjlwowe\\jfoew lwfweolwfo\\jofweojw lifewijowe\"):\n\tprint \"%d:%d:%s\" % (start,end,toks[0])\n"}], [{"score": 929.802, "uuid": "d67c1047-baa2-5d04-96bb-de70914dbf03", "index": "cw12", "trec_id": "clueweb12-1903wb-94-01538", "target_hostname": "www.oak-tree.us", "target_uri": "http://www.oak-tree.us/blog/index.php/2009/11/08/r-stats-part1", "page_rank": 1.2086127e-09, "spam_rank": 67, "title": "Apolitically Incorrect \u00bb Statistics With <em>R</em> \u2013 Part 1: An Old Dog <em>Learns</em>", "snippet": "Statistics With <em>R</em> \u2013 Part 1: An Old Dog <em>Learns</em> New Computing Tricks When doing math <em>or</em> numerical <em>analysis</em>, the knowledge of the technique is far too often tied to the tool performing the calculation. ", "explanation": null, "document": "Statistics With R \u2013 Part 1: An Old Dog Learns New Computing Tricks\nRob Oakes | November 8, 2009 10:21 pm\nWhen doing math or numerical analysis, the knowledge of the technique is far too often tied to the tool performing the calculation.\u00a0 Consider an engineer whose understanding of the Fast Fourier transformation is inseparably tied to the fft function in Matlab.\u00a0 Of course this hypothetical engineer understands what the results mean (more or less) but may not be able to duplicate his analysis if Matlab were taken away.\nIn most cases, it is likely that no deeper understanding will be required.\u00a0 But what happens if the computer makes a mistake?\u00a0 Or the program becomes unavailable?\u00a0 Both situations are entirely possible.\u00a0 Computer algorithms aren\u2019t perfect and occasionally arrive at results make little sense; and hardware has been known to fail.\nWhen the engineer understands how the computer arrived at the answer, however, he can recognize, understand, and ultimately correct those cases where the results are unexpected.\u00a0 This is an important reality check that can prevent costly disasters later down the line.\u00a0 Or, if the hardware is unavailable, he can use an alternative tool or software package to duplicate the analysis.\nBut while such a situation can arise with any type of numerical software, it\u2019s most likely to happen to users of a statistical package.\u00a0 I find this extremely ironic since a proper understanding of statistics is essential to live in the modern world.\u00a0 (Much more so than an understanding of the Fast Fourier transform, at any rate.)\u00a0 The rules of probability, the normal curve, correlation, and multivariate statistics can have a direct impact on how we live our lives.\u00a0 They are used in making important decisions in finance, medicine, science and government.\u00a0 A misunderstanding of stats and the methods of science (from which statistics is inseparable), underlies the most divisive issues of our day: abortion, stem cell research, and global warming.\nMoreover, neither side has a monopoly on ignorance or misunderstanding.\u00a0 People fail to distinguish between correlation and causality, or insist in using the word \u201caverage\u201d as a slur.\u00a0 Nearly as bad are those that \u2013 like the hypothetical engineer described above \u2013 only understand statistics within the narrow context of their stats package.\u00a0 Casual statisticians are nearly as dangerous as the wholly uninformed.\nThe Statistical Package for the Social Sciences (SPSS) , is one of the biggest perpetrators of this crisis.\u00a0 Which is hugely ironic, because I happen to love SPSS.\u00a0 SPSS is probably the first statistical package that has placed advanced statistical methods within the grasp of the novice user.\u00a0 I\u2019ve been a happy user for nearly a decade (ever since I was introduced to the program in high school).\u00a0 But there is no doubt that I\u2019ve come to understand statistics within the context of SPSS and its GUI.\nPlease don\u2019t misunderstand me, I have a pretty good grasp of basic statistics.\u00a0 I can sling probability with the best of them and take relish in describing when to use the Fischer Exact test instead of a Chi-Square; but advanced statistics are a completely different matter.\u00a0 Advanced stats scare me.\u00a0 I can certainly use these more complicated methods.\u00a0 I\u2019ve analyzed and written about multi-variate models and even ventured into Analysis of Variance (ANOVA).\u00a0 But I have to rely on SPSS and the aid of my institution\u2019s biostatistician to help me recognize when there is a problem.\nWhich is why, in a time of tight budgets, losing the institution\u2019s SPSS license has been a crushing blow to my productivity.\u00a0 (Whoever made that decision should be hauled out and shot!)\u00a0 Because I don\u2019t have my statistics software any more, there are certain aspects of my job that are much more difficult to do.\u00a0 And unfortunately, there is only logical conclusion to draw: I\u2019ve become a victim of the statistical ease of SPSS.\nOpen Source Alternatives\nI went through a similar experience about a year ago .\u00a0 At the time, I had become increasingly frustrated with the restrictions, licensing fees, and limitations of the Matlab technical computing language.\u00a0 After one particularly infuriating meeting, I decided that I had had enough and was going to do something about it.\u00a0 In the months that followed, I spoke with friends and colleagues, and experimented with every alternative I could get my hands on.\u00a0 I looked at Octave (the \u201cOpen Source Matlab\u201d) and Ruby, before eventually settling on a combination of Python and PyQt to meet my needs.\u00a0 The result of these changes has been tremendously positive.\u00a0 Python is both easier to use and far more powerful than Matlab could ever hope to be.\u00a0 Not only am I happier and more productive, but so are those who work with me.\nIt is, therefore, logical that when I lost my statistical language of choice that I would look to open source to provide an alternative.\u00a0 Fortunately, the Open Source community delivers not one alternative to SPSS, but two: Gnu PSPP and R.\nGnu PSPP\nAs the name implies, PSPP has one simple goal: to clone SPSS in every way that matters.\u00a0 It can perform descriptive statistics, T-tests, linear regression and non-parametric tests.\u00a0 It has an easy to easy to use and relatively intuitive GUI.\u00a0 It can use SPSS syntax and read SPSS data files.\u00a0 It supports an obscene number of variables and cases (about a billion).\u00a0 It interoperates with Gnumeric and OpenOffice.\u00a0 Finally, it\u2019s fast.\nAside from its horribly ugly icon, PSPP would appear to deliver exactly what I want and need.\u00a0 Except, you might have noticed that this article is titled \u201cStatistics with R\u201d, not \u201cStatistics with PSPP\u201d.\u00a0 Obviously, I chose to go with the second alternative.\u00a0 But why?\nPSPP works as advertised.\u00a0 I found it able to deal with nearly all of the old SPSS data files and syntax that I threw its way.\u00a0 But, the program suffers from the problem of all clones everywhere: it\u2019s greatest aspiration is to be a copy of something else.\u00a0 That is to say, it seeks to be \u201cGood Enough\u201d,\u00a0 and therein lies the problem, I don\u2019t want a tool that is good enough .\u00a0 I want to use excellent software, even if it\u2019s different or requires me to learn new things.\u00a0 Even if I have to pay for it.\nI\u2019m not trying to pick on or be unfair to PSPP.\u00a0 It meets an important need in the free software landscape.\u00a0 It just doesn\u2019t fit in my with my desires or preferences very well.\nThe R Statistical Project\nThis is where R steps into the picture.\u00a0 Whereas PSPP is \u201caimed at statisticians, social scientists and students requiring fast convenient analysis of sampled data (emphasis added)\u201d, R is the software that most statisticians actually use.\u00a0 When I contacted the statistician at my institution to ask, \u201cWhat statistical software should I use?\u00a0 I\u2019m looking at R and PSPP.\u201d\nHe responded, \u201cOh that\u2019s easy.\u00a0 Use R.\u00a0 There will be a learning curve, but it\u2019s much more powerful and capable than even SPSS or SAS.\u201d\nAs I\u2019ve started to explore the feature set and available modules, it readily becomes apparent as to why.\u00a0 R is a huge language.\u00a0 There are thousands of packages that cover every type of statistics I\u2019ve ever heard of, and many more I haven\u2019t.\nEven better, people have gone to great lengths to incorporate R into other tools.\u00a0 It has a set of excellent python bindings and interoperates very well with LyX and LaTeX.\u00a0 As just a single example, using the Sweave document class, you can use R to easily embed code in reports and other documents that need to be updated on a very frequent basis.\u00a0 This allows for these publications to be generated on demand with the most recent data.\u00a0 The only other place I\u2019ve seen the equal to this feature is within the proprietary universe of Microsoft Office and SQL Server.\nEasing Into R\nIndeed, if R can be said to have a major weakness, it would be that it is too full featured and capable. Particularly for someone who is a statistical novice.\u00a0 The sheer number of packages and options available is absolutely overwhelming.\u00a0 Moreover, the reference material is distributed in nature.\u00a0 Like other open source tools, you can find answers to your questions; but you need to be intelligent about how you ask them.\nAs I\u2019ve explored R, there have been quite a few painful moments.\u00a0 This isn\u2019t because R is more difficult than SPSS or SAS, but rather because it is tremendously different.\u00a0 As an example, consider the differences in how the programs work with data.\nBoth SPSS and SAS use one main data structure, the data set.\u00a0 A data set can be thought of as a big spreadsheet where the variable names are kept in columns and the individual observations are kept in rows (SPSS calls them cases).\u00a0 In contrast, R uses may types of data structures.\u00a0 It has an a two dimensional array that is similar to the data set, but it is possible to use one dimensional arrays of data (similar to vectors), or three dimensional arrays (which might contain extremely complex data).\u00a0 The added options raise very complicated questions: What sorts of statistical calculations are done on a three dimensional arrays?\u00a0 Why are they necessary?\u00a0 How do I need to code my data so that I can take advantage of R\u2019s advanced features?\nThose aren\u2019t issues that a user of SPSS or SAS even needs to consider.\u00a0 But with R, they present themselves before you even begin to use the program; and there is no centralized source of information to help you figure out the answers.\u00a0 The result of too many options and too little information is unproductive agony.\nSeries Introduction\nWhich is why I decided to write this series.\u00a0 R does some spectacular things, it is excellent software.\u00a0 But there are some things you need to know before using it.\u00a0 Here are just a few examples:\nWhat user interfaces are available for R and which should you use?\nHow can you use R to summarize data and do basic comparisons?\nHow does R handle moderately advanced statistics like one-way ANOVA and non-parametric tests?\u00a0 What about mult-variate statistics and regression analysis?\nHow does R work with other programs?\u00a0 How can you format your output into publication quality figures?\nWhat support does LyX and LaTeX offer for users of R?\nThe purposes of these articles are to address the concerns of the novice statistician or scientist.\u00a0 I will try and avoid jargon and other indecipherable terms.\u00a0 I will ensure that the examples are interesting and relevant.\u00a0 But most importantly, I will try and to help build a deeper statistical foundation.\u00a0 I know exactly what it feels like to be the \u201chypothetical\u201d engineer who has become too reliant on his tools.\u00a0 As long as the tool is nearby, you\u2019re fine.\u00a0 But when that tool is taken from you, be prepared for a world of hurt.\nUntil fairly recently, I\u2019ve been in that world of hurt.\u00a0 You might just say that these articles are my way of explaining how I got out.\nSimilar Posts:\n"}, {"score": 927.5188, "uuid": "960824be-cc14-5e40-a6d6-2b69f6308de3", "index": "cw12", "trec_id": "clueweb12-1114wb-64-06586", "target_hostname": "blog.apps.chicagotribune.com", "target_uri": "http://blog.apps.chicagotribune.com/category/python/", "page_rank": 4.4509334e-09, "spam_rank": 84, "title": "<em>Python</em> \u00ab News Apps Blog", "snippet": "We found one implementation of dot distribution in <em>Python</em>, which we extended into a module <em>for</em> reuse. Englewood (named after an ailing Chicago neighborhood that the newspaper writes many sad stories about) uses the <em>Python</em> bindings <em>for</em> GDAL to load <em>data</em> from PostGIS <em>or</em> shapefile.", "explanation": null, "document": "with 16 comments\nBetween Brian, Joe, and myself there hasn\u2019t been a time in the last six months where at least one of us wasn\u2019t working with census data.\nBack in February we attacked the less-detailed (redistricting) data for print and the web . In April, May, and June we contributed to a joint effort with an esteemed cadre of news nerds to develop census.ire.org , a site intended to make it easier for journalists to report from census data. And to prepare for this recent release, we even spent a week hacking near-complete prototype maps using data that the census had already released, Kings County, New York.\nWe learned hard lessons about the scale and nuance of the census in the last few months, and along the way, further built out our toolkit for making maps. Last week the Census Bureau released detailed (summary file) data for Illinois, and we used our new tools to produce a couple of maps we\u2019re pretty excited about:\n2010 Census: Children less than five years old (shown above)\nThese maps demonstrate a map style we haven\u2019t attempted before: dot density mapping. Dot maps let us represent multi-variate data more richly than choropleth maps\u2013for example, illustrating variation in race and population density simultaneously. We were inspired in this effort by Bill Rankin\u2019s Radical Cartography project and Dennis McClendon\u2019s race map for the Encyclopedia of Chicago .\nMany of the tools needed to create the maps we wanted didn\u2019t exist. Using the fantastic TileMill as our starting point, we began to build a toolkit.\nInvar\nInvar automates the generation of map tiles, and the deployment of tiles to S3. It is the first and least glamorous of the tools we created, but crucially, it\u2019s very, very fast. Fast!\nThe first time we ever tried to create our own tileset, it took hours to render and twice as long to deploy. Thanks to invar\u2019s parallelizing these tasks, we can now produce a map in minutes and deploy it just as fast. In fact, we now deploy our maps to four separate S3 buckets so that we can take advantage of Leaflet \u2018s support for round-robining tile requests to multiple subdomains . Fast!\nEnglewood\nNext we needed to distribute dots across geographies. We found one implementation of dot distribution in Python , which we extended into a module for reuse.\nEnglewood (named after an ailing Chicago neighborhood that the newspaper writes many sad stories about) uses the Python bindings for GDAL to load data from PostGIS or shapefile. It scatters points within each feature and then writes the points out to a table or new shapefile.\nA small snippet of Python is required to configure Englewood. The following code renders the dots for our map of children less than five from a database. (A demo using shapefiles can be found in the repository ):\n#!/usr/bin/env python\r\n\r\nfrom englewood import DotDensityPlotter \r\n\r\ndef get_data(feature):\r\n    \"\"\"\r\n    This function is called for each feature Englewood processes and needs to return a\r\n    dictionary of classes, with a number assigned to each. Englewood will divide this\r\n    number by a \"dots_per\" value set below and create that many dots for that class\r\n    within the geography.\r\n    \"\"\"\r\n    return {\r\n        'hispanic': feature.GetFieldAsInteger(feature.GetFieldIndex('hispanic_under5')),\r\n        'black': feature.GetFieldAsInteger(feature.GetFieldIndex('black_under5')),\r\n        'asian': feature.GetFieldAsInteger(feature.GetFieldIndex('asian_under5')),\r\n        'nhwhite': feature.GetFieldAsInteger(feature.GetFieldIndex('nhwhite_under5'))\r\n    }\r\n\r\n# Example argument values passed into the DotDensityPlotter\r\n# In this case features are read from a PostGIS table (under_5_by_race_blocks_shapes)...\r\nsource = 'PG:dbname=chicagocensus host=localhost'\r\nsource_layer = 'under_5_by_race_blocks_shapes'\r\n# ...and written into another PostGIS table (under_five_dots)\r\ndest_driver = 'PostgreSQL'\r\ndest = 'PG:dbname=chicagocensus host=localhost'\r\ndest_layer = 'under_five_dots'\r\nget_data_callback = get_data\r\ndots_per = 1\r\n\r\ndots = DotDensityPlotter(source, source_layer, dest_driver, dest, dest_layer, get_data_callback, dots_per)\r\ndots.plot()\nDeployment\nA fast and stable process is useless if you can\u2019t repeat it. We\u2019ve built out a fabric configuration which allows us to make these maps in the quickest and most efficient way possible. Among other things, it allows us to keep some configuration (such as a bounding box) in a per-map YAML file. It parses this file and handles passing the correct arguments to invar for rendering and deployment. Perhaps most exciting, if you\u2019re using the new TileMill 0.4 (available for OSX or Ubuntu) it can completely automate the production of Wax interactivity grids , such as we used to do the highlighting in our recent maps.\nVia Crayonsman (CC BY-SA 3.0)\nStyling dots\nCreating dot density maps created new challenges with regards to styling. Brian tried numerous approaches to color and size the dots, but ultimately we settled on a few principles that worked pretty well:\nUse a dark, sparse base-layer (we used a custom-styled Google Maps layer, but would like to move to an Open Street Map base-layer in the future).\nMake your dots to stand out brightly. Try the fluorescent colors from the palette of Crayola crayons .\nPlay with transparency\u2013you may want to take advantage of the effect of overlapping transparent dots.\nMake Dots scale on zoom.\nWhenever possible, use one dot per individual. It\u2019ll make for a more interesting map.\nHere is the style we settled on:\n#under-five {\r\n  marker-fill:#f00;\r\n  marker-line-width:0;\r\n  marker-allow-overlap: true;\r\n  marker-opacity:1;\r\n  [group=\"asian\"] {marker-fill:#FF496C;}\r\n  [group=\"black\"] {marker-fill:#76FF7A;}\r\n  [group=\"hispanic\"] {marker-fill:#FFCF48;}\r\n  [group=\"nhwhite\"] {marker-fill:#7366BD;}\r\n  \r\n  [zoom=9] {marker-height:.2;}\r\n  [zoom=10] {marker-height:.3;}\r\n  [zoom=11] {marker-height:.5; marker-opacity:.7;}\r\n  [zoom=12] {marker-height:.8; marker-opacity:.7;}\r\n  [zoom=13] {marker-height:1; marker-opacity:.8;}\r\n  [zoom=14] {marker-height:1.5; marker-opacity:.8;}\r\n}\nWrapping up\nAlthough I\u2019ve linked to a number of projects and code snippets in this post, you may find it useful to see a complete project. This week, with Illinois under our belt, I decided to apply the same methodology to my side-project, Hack Tyler . I produced a map of race in Smith County, Texas (related blog post ). Part of Hack Tyler\u2019s modus operandi is developing in a completely transparent manner. As a result, you can see complete examples of both our backend and client-side mapping rigs in the following projects:\nhacktyler-census\nWe hope that we\u2019ve pressed the envelope a bit with these new maps. Someone said that this was the year cartographers retake the internet. I hope that\u2019s true. Its about time that online maps were more than just shading boxes.\nWritten by Christopher Groskopf\n"}, {"score": 927.0237, "uuid": "9e12d9b6-ff15-59ce-8d47-510e989d7810", "index": "cw12", "trec_id": "clueweb12-0813wb-92-14727", "target_hostname": "www.eriksmartt.com", "target_uri": "http://www.eriksmartt.com/blog/archives/tag/python", "page_rank": 4.172195e-09, "spam_rank": 66, "title": "<em>python</em> \u00ab eriksmartt.com&#x2F;blog", "snippet": "(Note: This means they have better <em>Python</em> performance then <em>Python</em> <em>for</em> S60 is seeing on current S60 phones.) o James Tauber, \u201cReinventing the wheel is great if your goal is to <em>learn</em> more about the wheel.\u201d o Jonathan Ellis, \u201cWhen you control the whole stack you can innovate faster.\u201d", "explanation": null, "document": "Reading \u201cThe Definitive Guide to Django\u201d; Verdict: A solid learning reference for a beginning/intermediate Django user\nLast week I received a review-copy of the new \u201c The Definitive Guide to Django \u201d book from Apress . I hadn\u2019t planned on buying the book since it seemed a little too beginner-focused; but I agreed to give it an honest reading, so I happily dove in with an \u201cit\u2019s Python , of course I\u2019m going to like it\u201d attitude.\nBackground\nThe book was written by Adrian Holovaty and Jacob Kaplan-Moss , the creators and \u201cBenevolent Dictators\u201d of the Django Web Framework. It was Holovaty and Kaplan-Moss\u2019 first book, and, I believe, meant to be the first Django book to market. The book was drafted online; open to peer-review and community feedback; and ultimately published under the GNU Free Documentation License.\nFrom the get-go, the print edition had a few inherent market challenges to face: First, the entire book is available online, for free, at: < http://www.djangobook.com/ >. Second, in many ways the book is a re-hash of the docs available at < http://www.djangoproject.com/documentation/ >, which are also free. Third, the book covers Django 0.96, not SVN. (0.96 is technically the latest-snapshot release, but a lot has changed since 0.96.) And finally, the $45 MSRP could be seen as a little steep for what is effectively a printed copy of a free, online book.\nThe print experience\nDiving in, the book takes the reader through the basic installation process, provides a brief background on how the framework came to be (and why you want one), then steps through the major features (ie., the template system, ORM, URLconfs, generic views, etc.) It\u2019s what you\u2019d expect from a technical reference \u2014 no fluff, and straight to the details. There are plenty of code snippets to learn from, and the sidebar notes tend to be insightful.\nSince it wasn\u2019t new material for me, the book was a fairly quick read; but the experience of reading Django documentation in book-form was actually quite fascinating. There\u2019s something about settling into a comfortable chair with a book, pen, and highlighter that you just can\u2019t get with online documentation. Perhaps it was just a little more noticeable given the material. When I read the Django docs online, I tend to skim over them while trying to solve a problem. I use them as a reference more then a learning tool, and it\u2019s usually while actively coding, thus my brain is partially distracted with whatever it is I\u2019m building.\nWith a physical book, you can unplug, step away from the computer, and give the material your undivided attention. This isolation from distraction results in a much deeper understanding of the text. This is the real the value of the printed book \u2014 it\u2019s an opportunity to digest online documentation in an environment more conducive to learning and retention.\nMy general take-aways and observations\nThe book definitely has a beginner/intermediate feel to it, but only in the sense of a beginner Django user \u2014 not a beginner Web developer or Python programmer. I\u2019m curious how well the book is received by folks who are beginners at Django and dynamic Web development since the text brings up a lot of complex topics in Web development that aren\u2019t really explained. (Ex., database administration, server clustering, manipulating HTTP headers, etc.)\nThe breadth of the book is impressive, but in some ways, the book really feeds you through a firehose, so to speak. It throws a lot of new concepts at the reader and doesn\u2019t always explain why you\u2019d need to know them, or how you might use them in the real world. For someone deploying a site with Django, it will be good to know that all these features are available, but it might take awhile before they need to use them (if ever.)\nThe book does touch on some of the more advanced Django features (like extending the template system and writing custom middleware), which was nice, but some topics are reserved for the appendix and get limited coverage (ex., model managers and \u2018Q\u2019 queries.) Others, like the Sites Framework, are given good exposure, but not so much that the reader is left with a clear picture on when to use them and what their limitations are.\nThe forms processing chapter was a bit lighter then what I was hoping for \u2014 especially given that the current newforms documentation still trends toward \u201cread the source code.\u201d It provides enough to start using newforms if your form needs are pretty basic, but doesn\u2019t address creating your own widgets, or any of the fun stuff you can do once you start dynamically generating and manipulating newforms objects.\nIt might have been nicer if the examples in the book were a little more tied together, perhaps all focused on building a single example project and showing how the various features are used in real-world applications. (The example of the book-publisher\u2019s app was a reoccurring theme, but not so strongly that each chapter applied it\u2019s new learnings to it.)\nThe Deploying Django: \u201cGoing Big\u201d sub-section provides a nice infrastructure graphics for how high-traffic systems might be setup, but once you get to the point of being \u201cbig\u201d, you need to architect for it, and that\u2019s really outside of the scope of this book. For this section, it might have been nice to reference other resources on scaling infrastructure, and perhaps pointing out some of the ways that Django can be optimized for performance and horizontal scaling. (For example, one of the Django projects we put into production at work will happily support 1,200 requests/second, but the database layer and session middleware have been reworked a bit, and the content caching approach is a little different then the standard Django offering.)\nOn the more positive side, even as someone who\u2019s been using Django for some time, I still learned a few new tricks, and I was reminded of a few features that I could be taking better advantage of. (And when you do this stuff professionally, every shortcut and productivity gain has monetary value \u2014 avoiding even a half-hour of debugging pays for the cost of this book.)\nThis book would make a fantastic read for a back-end developer joining a project that is already using Django. I normally tell new developers to go through the Python Tutorial at < http://python.org/doc/tut/ > if they\u2019re new to Python, then to complete the Django Tutorials at < http://www.djangoproject.com/documentation/ > before trying to grok any in-progress Django project. Now I have a third reference (though I might still suggest that they walk through the tutorials first, so that they have some context when reading the book. Otherwise, there are just too many new concepts to do a straight read-through and still grasp it all.)\nSummary\nThe market needed a good Django book, and this one delivered a solid reference for the framework. Arguably, it\u2019s not really a \u201cBeginner\u2019s Guide to Django\u201d, but hopefully it covers enough of the basics that future books can focus on best practices and more advanced techniques. (On a related note, there\u2019s apparently an upcoming \u201cPractical Django Projects\u201d book, also from Apress, that will focus more on building \u201creusable Django applications from start to finish\u201d. This might actually make for a better beginner\u2019s book, depending on how it turns out. [Via The B-List: Speaking and writing ].)\nThe million-dollar question then, is \u201cShould you buy this book?\u201d My answer ended up being a bit more positive then I expected, but there are two parts: First, if you\u2019re a front-end developer only, you don\u2019t need this book. You can just read Chapter 4: The Django Template System online, and then use the \u201c Django Templates: Guide for HTML authors \u201d section of the online docs as a reference. For back-end developers, the story is different. If you\u2019re going to just \u201cread it while you hack\u201d, then you might as well just read it online; but if you\u2019re serious about building applications with Django (especially if you\u2019re new to it), then you should consider the book and investing the time to step away from the computer and really let yourself get into it. Unless you are an active contributor to Django (which I\u2019m not, just to be clear), the odds are pretty good that you\u2019ll learn something new, even if you\u2019re already using Django today.\n"}, {"score": 925.89264, "uuid": "a4d6dfa4-99ab-5d34-8527-32bc306c8500", "index": "cw12", "trec_id": "clueweb12-1500tw-37-11708", "target_hostname": "www.seo.com", "target_uri": "http://www.seo.com/blog/tutorial-google-webmaster-tools-data-windows-python/", "page_rank": 1.2752859e-09, "spam_rank": 70, "title": "Tutorial: How to Get Google Webmaster Tools <em>Data</em> on Windows With <em>Python</em>", "snippet": "Now you can easily pull a report <em>for</em> any site you have access to, without editing the <em>Python</em> file every time! <em>I</em> will follow up with instructions <em>for</em> automating this every month <em>for</em> compiling historic <em>data</em>, as Google Webmaster Tools only provides the past 30 days of <em>data</em>.", "explanation": null, "document": "Tweet\nThe Search Query report in Google Webmaster tools is more important than ever, with the ominous (not provided) mask hiding 25%-40% of referring keyword traffic in Google Analytics. Google recently made WMT data available through an open source Python Library, making it easy to transfer that data straight into Google Docs or to your desktop, but setup and configuration isn\u2019t easy for most.\nTo make it a little easier to access that data, I\u2019ll walk you through the beginners steps to get search queries from Google Webmaster Tools using the Google Data Python Library on Windows. Expect this to take at least 15 minutes.\n1. Download and install Python\nFirst, download the Windows Installer from the Python.org official download page . At the time of writing, you\u2019ll want the \u201cPython 2.7.2 Windows Installer (Windows binary \u2014 does not include source).\u201d If you have a 64-bit machine, make sure you select the \u201cX86-64 Installer.\u201d Once downloaded, open it up and follow the installation wizard. I recommend choosing default settings if you\u2019re a beginner but feel free to customize if you know what you\u2019re doing.\nInstalling with default settings will install Python in the C:\\Python27 directory. You don\u2019t need to touch anything in there yet.\n2.\u00a0Download and extract the\u00a0Google Data APIs Client Library for Python.\nDownload the latest Google Data API Client Library in .zip or .tar.gz format (recommended if you use WinRar or 7zip), depending on your preference, and extract the contents to the C:\\Python27 folder.\nThat will place the zipped \u201cgdata-2.0.16\u2033 folder in your C:\\Python27 folder, as follows: C:\\Python27\\gdata-2.0.16\\ , which should contain the following contents:\nIMPORTANT: Now, you need to copy ALL the contents of the gdata-2.0.16 folder and paste them straight into the Python27 folder. You will probably get a popup saying that the README file already exists \u2013 you can \u00a0replace it, or skip it, you don\u2019t really need either unless you enjoy the literary merits of README files.\nRead more about the\u00a0Google Data APIs Python Client Library.\n3. Install the Google Data API Client Library\nNow it gets fun \u2013 it\u2019s time to fire up your Windows Command Prompt by finding it in your \u201cAccessories\u201d folder, or by pressing Windows+R, typing \u201ccmd\u201d and hitting \u201cEnter.\u201d\nNavigate into the C:\\Python27 folder by typing \u201ccd \\Python27\u201d without quotes.\n\u201ccd\u201d is short for \u201cchange directory,\u201d which is how to move from one folder (directory) to another from the command line.\nNow that you\u2019re in the Python27 folder, type \u201cpython ./setup.py install\u201d without quotes, which will install the client library. If you start seeing hundreds of scrolling lines, you\u2019ve done everything correctly to this point.\n(optional) Now you can start playing around with the sample scripts that came with the Data API Library \u2013 here is some information on how to do that .\n4. Create a \u2018wmt\u2019 folder, and set up scripts\nCreate a folder in the Python27 folder called wmt, so that it can be found at C:\\Python27\\wmt\nDownload and place\u00a0the downloader.py\u00a0script in the new wmt folder (either copy the code from that page into a new file and save it as downloader.py, or right click the \u201cview raw file\u201d link on that page and save it to the folder).\nDownload and place the example-create-spreadsheet.py script to the wmt folder,\u00a0and right click the file and select \u201cEdit with IDLE\u201d\nFind the highlighted fields and replace them with your Webmaster Tools user credentials and website, then save (Ctrl+s):\nemail\nwebsite\n5. Open the command line and start getting Search Query reports\nNow that C:\\Python27\\wmt\\example-create-spreadsheet.py has been updated with your website of choice, along with your Webmaster Tools username and password, open your Command Line prompt, (make sure you\u2019re still working in the C:\\Python27 directory), and type in this command:\npython ./wmt/example-create-spreadsheet.py\nAfter several seconds you will see a message that says \u201cSpreadsheet now accessible online at: https://docs.google.com/spreadsheet/\u2026.,\u201d and you can access the CSV file at that URL or by logging into Google Docs!\nThat\u2019s it!\nIf you would rather download it directly to your desktop rather than transferring the data to Google Docs,\u00a0use example-simple-download.py instead.\n6. Handy tip\nThe example-create-spreadsheet.py code\u00a0provided for Google is great if you consistently pull search query data from only one website, but if\u00a0you\u2019re like me and you need to easily pull reports for multiple websites, I recommend downloading my modified version: sq-report.py (right click->Save as\u2026). Everything is the same except for the website field, which you can now call from the command line like so:\npython ./wmt/sq-report.py http://www.seo.com/\nReplace http://www.seo.com/ with any website your Webmaster Tools account has access to \u2013 and don\u2019t forget to edit the sq-report.py file with your username and password.\nNow you can easily pull a report for any site you have access to, without editing the Python file every time!\n7. Conclusion\nI will follow up with instructions for automating this every month for compiling historic data, as Google Webmaster Tools only provides the past 30 days of data.\nI hope this is helpful for the average Windows user who may find it trick to install and work with Python code, let me know how it goes and if you run into problems, contact me on Twitter or in the comments so I can help you troubleshoot.\n"}, {"score": 924.33295, "uuid": "16be84dd-f6d8-59c2-96e0-8e24c10077cb", "index": "cw12", "trec_id": "clueweb12-1500tw-39-04561", "target_hostname": "www.seo.com", "target_uri": "http://www.seo.com/blog/tutorial-google-webmaster-tools-data-windows-python/?utm_source=twitterfeed&utm_medium=twitter", "page_rank": 1.1700305e-09, "spam_rank": 71, "title": "Tutorial: How to Get Google Webmaster Tools <em>Data</em> on Windows With <em>Python</em>", "snippet": "Now you can easily pull a report <em>for</em> any site you have access to, without editing the <em>Python</em> file every time! <em>I</em> will follow up with instructions <em>for</em> automating this every month <em>for</em> compiling historic <em>data</em>, as Google Webmaster Tools only provides the past 30 days of <em>data</em>.", "explanation": null, "document": "Tweet\nThe Search Query report in Google Webmaster tools is more important than ever, with the ominous (not provided) mask hiding 25%-40% of referring keyword traffic in Google Analytics. Google recently made WMT data available through an open source Python Library, making it easy to transfer that data straight into Google Docs or to your desktop, but setup and configuration isn\u2019t easy for most.\nTo make it a little easier to access that data, I\u2019ll walk you through the beginners steps to get search queries from Google Webmaster Tools using the Google Data Python Library on Windows. Expect this to take at least 15 minutes.\n1. Download and install Python\nFirst, download the Windows Installer from the Python.org official download page . At the time of writing, you\u2019ll want the \u201cPython 2.7.2 Windows Installer (Windows binary \u2014 does not include source).\u201d If you have a 64-bit machine, make sure you select the \u201cX86-64 Installer.\u201d Once downloaded, open it up and follow the installation wizard. I recommend choosing default settings if you\u2019re a beginner but feel free to customize if you know what you\u2019re doing.\nInstalling with default settings will install Python in the C:\\Python27 directory. You don\u2019t need to touch anything in there yet.\n2.\u00a0Download and extract the\u00a0Google Data APIs Client Library for Python.\nDownload the latest Google Data API Client Library in .zip or .tar.gz format (recommended if you use WinRar or 7zip), depending on your preference, and extract the contents to the C:\\Python27 folder.\nThat will place the zipped \u201cgdata-2.0.16\u2033 folder in your C:\\Python27 folder, as follows: C:\\Python27\\gdata-2.0.16\\ , which should contain the following contents:\nIMPORTANT: Now, you need to copy ALL the contents of the gdata-2.0.16 folder and paste them straight into the Python27 folder. You will probably get a popup saying that the README file already exists \u2013 you can \u00a0replace it, or skip it, you don\u2019t really need either unless you enjoy the literary merits of README files.\nRead more about the\u00a0Google Data APIs Python Client Library.\n3. Install the Google Data API Client Library\nNow it gets fun \u2013 it\u2019s time to fire up your Windows Command Prompt by finding it in your \u201cAccessories\u201d folder, or by pressing Windows+R, typing \u201ccmd\u201d and hitting \u201cEnter.\u201d\nNavigate into the C:\\Python27 folder by typing \u201ccd \\Python27\u201d without quotes.\n\u201ccd\u201d is short for \u201cchange directory,\u201d which is how to move from one folder (directory) to another from the command line.\nNow that you\u2019re in the Python27 folder, type \u201cpython ./setup.py install\u201d without quotes, which will install the client library. If you start seeing hundreds of scrolling lines, you\u2019ve done everything correctly to this point.\n(optional) Now you can start playing around with the sample scripts that came with the Data API Library \u2013 here is some information on how to do that .\n4. Create a \u2018wmt\u2019 folder, and set up scripts\nCreate a folder in the Python27 folder called wmt, so that it can be found at C:\\Python27\\wmt\nDownload and place\u00a0the downloader.py\u00a0script in the new wmt folder (either copy the code from that page into a new file and save it as downloader.py, or right click the \u201cview raw file\u201d link on that page and save it to the folder).\nDownload and place the example-create-spreadsheet.py script to the wmt folder,\u00a0and right click the file and select \u201cEdit with IDLE\u201d\nFind the highlighted fields and replace them with your Webmaster Tools user credentials and website, then save (Ctrl+s):\nemail\nwebsite\n5. Open the command line and start getting Search Query reports\nNow that C:\\Python27\\wmt\\example-create-spreadsheet.py has been updated with your website of choice, along with your Webmaster Tools username and password, open your Command Line prompt, (make sure you\u2019re still working in the C:\\Python27 directory), and type in this command:\npython ./wmt/example-create-spreadsheet.py\nAfter several seconds you will see a message that says \u201cSpreadsheet now accessible online at: https://docs.google.com/spreadsheet/\u2026.,\u201d and you can access the CSV file at that URL or by logging into Google Docs!\nThat\u2019s it!\nIf you would rather download it directly to your desktop rather than transferring the data to Google Docs,\u00a0use example-simple-download.py instead.\n6. Handy tip\nThe example-create-spreadsheet.py code\u00a0provided for Google is great if you consistently pull search query data from only one website, but if\u00a0you\u2019re like me and you need to easily pull reports for multiple websites, I recommend downloading my modified version: sq-report.py (right click->Save as\u2026). Everything is the same except for the website field, which you can now call from the command line like so:\npython ./wmt/sq-report.py http://www.seo.com/\nReplace http://www.seo.com/ with any website your Webmaster Tools account has access to \u2013 and don\u2019t forget to edit the sq-report.py file with your username and password.\nNow you can easily pull a report for any site you have access to, without editing the Python file every time!\n7. Conclusion\nI will follow up with instructions for automating this every month for compiling historic data, as Google Webmaster Tools only provides the past 30 days of data.\nI hope this is helpful for the average Windows user who may find it trick to install and work with Python code, let me know how it goes and if you run into problems, contact me on Twitter or in the comments so I can help you troubleshoot.\n"}, {"score": 923.5455, "uuid": "63fc810d-7a5d-588f-b9ec-b683dd483885", "index": "cw12", "trec_id": "clueweb12-1500tw-39-04562", "target_hostname": "www.seo.com", "target_uri": "http://www.seo.com/blog/tutorial-google-webmaster-tools-data-windows-python/?utm_source=twitter&utm_medium=social&utm_campaign=blog", "page_rank": 1.1700305e-09, "spam_rank": 70, "title": "Tutorial: How to Get Google Webmaster Tools <em>Data</em> on Windows With <em>Python</em>", "snippet": "Now you can easily pull a report <em>for</em> any site you have access to, without editing the <em>Python</em> file every time! <em>I</em> will follow up with instructions <em>for</em> automating this every month <em>for</em> compiling historic <em>data</em>, as Google Webmaster Tools only provides the past 30 days of <em>data</em>.", "explanation": null, "document": "Tweet\nThe Search Query report in Google Webmaster tools is more important than ever, with the ominous (not provided) mask hiding 25%-40% of referring keyword traffic in Google Analytics. Google recently made WMT data available through an open source Python Library, making it easy to transfer that data straight into Google Docs or to your desktop, but setup and configuration isn\u2019t easy for most.\nTo make it a little easier to access that data, I\u2019ll walk you through the beginners steps to get search queries from Google Webmaster Tools using the Google Data Python Library on Windows. Expect this to take at least 15 minutes.\n1. Download and install Python\nFirst, download the Windows Installer from the Python.org official download page . At the time of writing, you\u2019ll want the \u201cPython 2.7.2 Windows Installer (Windows binary \u2014 does not include source).\u201d If you have a 64-bit machine, make sure you select the \u201cX86-64 Installer.\u201d Once downloaded, open it up and follow the installation wizard. I recommend choosing default settings if you\u2019re a beginner but feel free to customize if you know what you\u2019re doing.\nInstalling with default settings will install Python in the C:\\Python27 directory. You don\u2019t need to touch anything in there yet.\n2.\u00a0Download and extract the\u00a0Google Data APIs Client Library for Python.\nDownload the latest Google Data API Client Library in .zip or .tar.gz format (recommended if you use WinRar or 7zip), depending on your preference, and extract the contents to the C:\\Python27 folder.\nThat will place the zipped \u201cgdata-2.0.16\u2033 folder in your C:\\Python27 folder, as follows: C:\\Python27\\gdata-2.0.16\\ , which should contain the following contents:\nIMPORTANT: Now, you need to copy ALL the contents of the gdata-2.0.16 folder and paste them straight into the Python27 folder. You will probably get a popup saying that the README file already exists \u2013 you can \u00a0replace it, or skip it, you don\u2019t really need either unless you enjoy the literary merits of README files.\nRead more about the\u00a0Google Data APIs Python Client Library.\n3. Install the Google Data API Client Library\nNow it gets fun \u2013 it\u2019s time to fire up your Windows Command Prompt by finding it in your \u201cAccessories\u201d folder, or by pressing Windows+R, typing \u201ccmd\u201d and hitting \u201cEnter.\u201d\nNavigate into the C:\\Python27 folder by typing \u201ccd \\Python27\u201d without quotes.\n\u201ccd\u201d is short for \u201cchange directory,\u201d which is how to move from one folder (directory) to another from the command line.\nNow that you\u2019re in the Python27 folder, type \u201cpython ./setup.py install\u201d without quotes, which will install the client library. If you start seeing hundreds of scrolling lines, you\u2019ve done everything correctly to this point.\n(optional) Now you can start playing around with the sample scripts that came with the Data API Library \u2013 here is some information on how to do that .\n4. Create a \u2018wmt\u2019 folder, and set up scripts\nCreate a folder in the Python27 folder called wmt, so that it can be found at C:\\Python27\\wmt\nDownload and place\u00a0the downloader.py\u00a0script in the new wmt folder (either copy the code from that page into a new file and save it as downloader.py, or right click the \u201cview raw file\u201d link on that page and save it to the folder).\nDownload and place the example-create-spreadsheet.py script to the wmt folder,\u00a0and right click the file and select \u201cEdit with IDLE\u201d\nFind the highlighted fields and replace them with your Webmaster Tools user credentials and website, then save (Ctrl+s):\nemail\nwebsite\n5. Open the command line and start getting Search Query reports\nNow that C:\\Python27\\wmt\\example-create-spreadsheet.py has been updated with your website of choice, along with your Webmaster Tools username and password, open your Command Line prompt, (make sure you\u2019re still working in the C:\\Python27 directory), and type in this command:\npython ./wmt/example-create-spreadsheet.py\nAfter several seconds you will see a message that says \u201cSpreadsheet now accessible online at: https://docs.google.com/spreadsheet/\u2026.,\u201d and you can access the CSV file at that URL or by logging into Google Docs!\nThat\u2019s it!\nIf you would rather download it directly to your desktop rather than transferring the data to Google Docs,\u00a0use example-simple-download.py instead.\n6. Handy tip\nThe example-create-spreadsheet.py code\u00a0provided for Google is great if you consistently pull search query data from only one website, but if\u00a0you\u2019re like me and you need to easily pull reports for multiple websites, I recommend downloading my modified version: sq-report.py (right click->Save as\u2026). Everything is the same except for the website field, which you can now call from the command line like so:\npython ./wmt/sq-report.py http://www.seo.com/\nReplace http://www.seo.com/ with any website your Webmaster Tools account has access to \u2013 and don\u2019t forget to edit the sq-report.py file with your username and password.\nNow you can easily pull a report for any site you have access to, without editing the Python file every time!\n7. Conclusion\nI will follow up with instructions for automating this every month for compiling historic data, as Google Webmaster Tools only provides the past 30 days of data.\nI hope this is helpful for the average Windows user who may find it trick to install and work with Python code, let me know how it goes and if you run into problems, contact me on Twitter or in the comments so I can help you troubleshoot.\n"}, {"score": 918.3926, "uuid": "7f8e7d7c-9d02-5164-a593-d9bf6202cb79", "index": "cw12", "trec_id": "clueweb12-1315wb-18-29406", "target_hostname": "www.r-bloggers.com", "target_uri": "http://www.r-bloggers.com/tag/r-programming/", "page_rank": 2.5626683e-09, "spam_rank": 74, "title": "<em>R</em> Programming (<em>R</em> news &amp; tutorials)", "snippet": "You can subscribe <em>for</em> e-mail updates: And get updates to your Facebook: If you are an <em>R</em> blogger yourself you are invited to add your own <em>R</em> content feed to this site (Non-English <em>R</em> bloggers <em>should</em> add themselves- here) ggplot2 <em>r</em>-project graphics programming rstats <em>R</em> Language Books visualization <em>Data</em> events", "explanation": null, "document": "By Rob Kabacoff\nA common task when analyzing multi-group designs is obtaining descriptive statistics for various cells\u00c2\u00a0and cell combinations. There are many functions that can help you accomplish this, including aggregate() and by() in the base installation, summaryBy() in the doBy package, and \u2026 Continue reading \u2192\nPage 1 of 6 1 2 3 4 5 ... 6 \u00bb\nTop 7 articles of the week\n"}, {"score": 916.8831, "uuid": "7e611462-eb34-5365-8408-2dcf4beb8362", "index": "cw12", "trec_id": "clueweb12-0611wb-57-15414", "target_hostname": "www.python.jp", "target_uri": "http://www.python.jp/doc/release/whatsnew/2.5.html", "page_rank": 1.2390308e-09, "spam_rank": 82, "title": "What\u2019s New in <em>Python</em> 2.5 \u2014 <em>Python</em> 2.7ja1 documentation", "snippet": "The decision was made to not require parentheses in the <em>Python</em> language\u2019s grammar, but as a matter of style <em>I</em> think you <em>should</em> always use them.", "explanation": null, "document": "An explanation of coroutines from a Perl point of view, written by Dan Sugalski.\nPEP 343: The \u2018with\u2019 statement \u00b6\nThe \u2018 with \u2018 statement clarifies code that previously would use try...finally blocks to ensure that clean-up code is executed. In this section, I\u2019ll discuss the statement as it will commonly be used. In the next section, I\u2019ll examine the implementation details and show how to write objects for use with this statement.\nThe \u2018 with \u2018 statement is a new control-flow structure whose basic structure is:\nwith expression [as variable]:\n    with-block\nThe expression is evaluated, and it should result in an object that supports the context management protocol (that is, has __enter__() and __exit__() methods.\nThe object\u2019s __enter__() is called before with-block is executed and therefore can run set-up code. It also may return a value that is bound to the name variable, if given. (Note carefully that variable is not assigned the result of expression.)\nAfter execution of the with-block is finished, the object\u2019s __exit__() method is called, even if the block raised an exception, and can therefore run clean-up code.\nTo enable the statement in Python 2.5, you need to add the following directive to your module:\nfrom __future__ import with_statement\nThe statement will always be enabled in Python 2.6.\nSome standard Python objects now support the context management protocol and can be used with the \u2018 with \u2018 statement. File objects are one example:\nwith open('/etc/passwd', 'r') as f: for line in f: print line ... more processing code ...\nAfter this statement has executed, the file object in f will have been automatically closed, even if the for loop raised an exception part- way through the block.\n\u00e3\u0083\u008e\u00e3\u0083\u00bc\u00e3\u0083\u0088\nIn this case, f is the same object created by open() , because file.__enter__() returns self.\nThe threading module\u2019s locks and condition variables also support the \u2018 with \u2018 statement:\nlock = threading.Lock() with lock: # Critical section of code ...\nThe lock is acquired before the block is executed and always released once the block is complete.\nThe new localcontext() function in the decimal module makes it easy to save and restore the current decimal context, which encapsulates the desired precision and rounding characteristics for computations:\nfrom decimal import Decimal, Context, localcontext # Displays with default precision of 28 digits v = Decimal('578') print v.sqrt() with localcontext(Context(prec=16)): # All code in this block uses a precision of 16 digits. # The original context is restored on exiting the block. print v.sqrt()\nWriting Context Managers \u00b6\nUnder the hood, the \u2018 with \u2018 statement is fairly complicated. Most people will only use \u2018 with \u2018 in company with existing objects and don\u2019t need to know these details, so you can skip the rest of this section if you like. Authors of new objects will need to understand the details of the underlying implementation and should keep reading.\nA high-level explanation of the context management protocol is:\nThe expression is evaluated and should result in an object called a \u201ccontext manager\u201d. The context manager must have __enter__() and __exit__() methods.\nThe context manager\u2019s __enter__() method is called. The value returned is assigned to VAR. If no 'as VAR' clause is present, the value is simply discarded.\nThe code in BLOCK is executed.\nIf BLOCK raises an exception, the __exit__(type, value, traceback)() is called with the exception details, the same values returned by sys.exc_info() . The method\u2019s return value controls whether the exception is re-raised: any false value re-raises the exception, and True will result in suppressing it. You\u2019ll only rarely want to suppress the exception, because if you do the author of the code containing the \u2018 with \u2018 statement will never realize anything went wrong.\nIf BLOCK didn\u2019t raise an exception, the __exit__() method is still called, but type, value, and traceback are all None.\nLet\u2019s think through an example. I won\u2019t present detailed code but will only sketch the methods necessary for a database that supports transactions.\n(For people unfamiliar with database terminology: a set of changes to the database are grouped into a transaction. Transactions can be either committed, meaning that all the changes are written into the database, or rolled back, meaning that the changes are all discarded and the database is unchanged. See any database textbook for more information.)\nLet\u2019s assume there\u2019s an object representing a database connection. Our goal will be to let the user write code like this:\ndb_connection = DatabaseConnection() with db_connection as cursor: cursor.execute('insert into ...') cursor.execute('delete from ...') # ... more operations ...\nThe transaction should be committed if the code in the block runs flawlessly or rolled back if there\u2019s an exception. Here\u2019s the basic interface for DatabaseConnection that I\u2019ll assume:\nclass DatabaseConnection: # Database interface def cursor (self): \"Returns a cursor object and starts a new transaction\" def commit (self): \"Commits current transaction\" def rollback (self): \"Rolls back current transaction\"\nThe __enter__() method is pretty easy, having only to start a new transaction. For this application the resulting cursor object would be a useful result, so the method will return it. The user can then add as cursor to their \u2018 with \u2018 statement to bind the cursor to a variable name.\nclass DatabaseConnection: ... def __enter__ (self): # Code to start a new transaction cursor = self.cursor() return cursor\nThe __exit__() method is the most complicated because it\u2019s where most of the work has to be done. The method has to check if an exception occurred. If there was no exception, the transaction is committed. The transaction is rolled back if there was an exception.\nIn the code below, execution will just fall off the end of the function, returning the default value of None. None is false, so the exception will be re-raised automatically. If you wished, you could be more explicit and add a return statement at the marked location.\nclass DatabaseConnection: ... def __exit__ (self, type, value, tb): if tb is None: # No exception, so commit self.commit() else: # Exception occurred, so rollback. self.rollback() # return False\nThe contextlib module \u00b6\nThe new contextlib module provides some functions and a decorator that are useful for writing objects for use with the \u2018 with \u2018 statement.\nThe decorator is called contextmanager(), and lets you write a single generator function instead of defining a new class. The generator should yield exactly one value. The code up to the yield will be executed as the __enter__() method, and the value yielded will be the method\u2019s return value that will get bound to the variable in the \u2018 with \u2018 statement\u2019s as clause, if any. The code after the yield will be executed in the __exit__() method. Any exception raised in the block will be raised by the yield statement.\nOur database example from the previous section could be written using this decorator as:\nfrom contextlib import contextmanager @contextmanager def db_transaction (connection): cursor = connection.cursor() try: yield cursor except: connection.rollback() raise else: connection.commit() db = DatabaseConnection() with db_transaction(db) as cursor: ...\nThe contextlib module also has a nested(mgr1, mgr2, ...)() function that combines a number of context managers so you don\u2019t need to write nested \u2018 with \u2018 statements. In this example, the single \u2018 with \u2018 statement both starts a database transaction and acquires a thread lock:\nlock = threading.Lock() with nested (db_transaction(db), lock) as (cursor, locked): ...\nFinally, the closing(object)() function returns object so that it can be bound to a variable, and calls object.close at the end of the block.\nimport urllib, sys from contextlib import closing with closing(urllib.urlopen('http://www.yahoo.com')) as f: for line in f: sys.stdout.write(line)\n\u00e5\u008f\u0082\u00e8\u0080\u0083\nPEP 343 - The \u201cwith\u201d statement\nPEP written by Guido van Rossum and Nick Coghlan; implemented by Mike Bland, Guido van Rossum, and Neal Norwitz. The PEP shows the code generated for a \u2018 with \u2018 statement, which can be helpful in learning how the statement works.\nThe documentation for the contextlib module.\nPEP 352: Exceptions as New-Style Classes \u00b6\nException classes can now be new-style classes, not just classic classes, and the built-in Exception class and all the standard built-in exceptions ( NameError , ValueError , etc.) are now new-style classes.\nThe inheritance hierarchy for exceptions has been rearranged a bit. In 2.5, the inheritance relationships are:\nBaseException       # New in Python 2.5\n|- KeyboardInterrupt\n|- SystemExit\n|- Exception\n   |- (all other current built-in exceptions)\nThis rearrangement was done because people often want to catch all exceptions that indicate program errors. KeyboardInterrupt and SystemExit aren\u2019t errors, though, and usually represent an explicit action such as the user hitting Control-C or code calling sys.exit() . A bare except: will catch all exceptions, so you commonly need to list KeyboardInterrupt and SystemExit in order to re-raise them. The usual pattern is:\ntry:\n    ...\nexcept (KeyboardInterrupt, SystemExit):\n    raise\nexcept:\n    # Log error...\n    # Continue running program...\nIn Python 2.5, you can now write except Exception to achieve the same result, catching all the exceptions that usually indicate errors but leaving KeyboardInterrupt and SystemExit alone. As in previous versions, a bare except: still catches all exceptions.\nThe goal for Python 3.0 is to require any class raised as an exception to derive from BaseException or some descendant of BaseException , and future releases in the Python 2.x series may begin to enforce this constraint. Therefore, I suggest you begin making all your exception classes derive from Exception now. It\u2019s been suggested that the bare except: form should be removed in Python 3.0, but Guido van Rossum hasn\u2019t decided whether to do this or not.\nRaising of strings as exceptions, as in the statement raise \"Error occurred\", is deprecated in Python 2.5 and will trigger a warning. The aim is to be able to remove the string-exception feature in a few releases.\n\u00e5\u008f\u0082\u00e8\u0080\u0083\nPEP 352 - Required Superclass for Exceptions\nPEP written by Brett Cannon and Guido van Rossum; implemented by Brett Cannon.\nPEP 353: Using ssize_t as the index type \u00b6\nA wide-ranging change to Python\u2019s C API, using a new Py_ssize_t type definition instead of int, will permit the interpreter to handle more data on 64-bit platforms. This change doesn\u2019t affect Python\u2019s capacity on 32-bit platforms.\nVarious pieces of the Python interpreter used C\u2019s int type to store sizes or counts; for example, the number of items in a list or tuple were stored in an int. The C compilers for most 64-bit platforms still define int as a 32-bit type, so that meant that lists could only hold up to 2**31 - 1 = 2147483647 items. (There are actually a few different programming models that 64-bit C compilers can use \u2013 see http://www.unix.org/version2/whatsnew/lp64_wp.html for a discussion \u2013 but the most commonly available model leaves int as 32 bits.)\nA limit of 2147483647 items doesn\u2019t really matter on a 32-bit platform because you\u2019ll run out of memory before hitting the length limit. Each list item requires space for a pointer, which is 4 bytes, plus space for a PyObject representing the item. 2147483647*4 is already more bytes than a 32-bit address space can contain.\nIt\u2019s possible to address that much memory on a 64-bit platform, however. The pointers for a list that size would only require 16 GiB of space, so it\u2019s not unreasonable that Python programmers might construct lists that large. Therefore, the Python interpreter had to be changed to use some type other than int, and this will be a 64-bit type on 64-bit platforms. The change will cause incompatibilities on 64-bit machines, so it was deemed worth making the transition now, while the number of 64-bit users is still relatively small. (In 5 or 10 years, we may all be on 64-bit machines, and the transition would be more painful then.)\nThis change most strongly affects authors of C extension modules. Python strings and container types such as lists and tuples now use Py_ssize_t to store their size. Functions such as PyList_Size() now return Py_ssize_t. Code in extension modules may therefore need to have some variables changed to Py_ssize_t.\nThe PyArg_ParseTuple() and Py_BuildValue() functions have a new conversion code, n, for Py_ssize_t. PyArg_ParseTuple() \u2018s s# and t# still output int by default, but you can define the macro PY_SSIZE_T_CLEAN before including Python.h to make them return Py_ssize_t.\nPEP 353 has a section on conversion guidelines that extension authors should read to learn about supporting 64-bit platforms.\n\u00e5\u008f\u0082\u00e8\u0080\u0083\nPEP 353 - Using ssize_t as the index type\nPEP written and implemented by Martin von L\u00f6wis.\nPEP 357: The \u2018__index__\u2019 method \u00b6\nThe NumPy developers had a problem that could only be solved by adding a new special method, __index__() . When using slice notation, as in [start:stop:step], the values of the start, stop, and step indexes must all be either integers or long integers. NumPy defines a variety of specialized integer types corresponding to unsigned and signed integers of 8, 16, 32, and 64 bits, but there was no way to signal that these types could be used as slice indexes.\nSlicing can\u2019t just use the existing __int__() method because that method is also used to implement coercion to integers. If slicing used __int__() , floating-point numbers would also become legal slice indexes and that\u2019s clearly an undesirable behaviour.\nInstead, a new special method called __index__() was added. It takes no arguments and returns an integer giving the slice index to use. For example:\nclass C: def __index__ (self): return self.value\nThe return value must be either a Python integer or long integer. The interpreter will check that the type returned is correct, and raises a TypeError if this requirement isn\u2019t met.\nA corresponding nb_index slot was added to the C-level PyNumberMethods structure to let C extensions implement this protocol. PyNumber_Index(obj)() can be used in extension code to call the __index__() function and retrieve its result.\n\u00e5\u008f\u0082\u00e8\u0080\u0083\nPEP 357 - Allowing Any Object to be Used for Slicing\nPEP written and implemented by Travis Oliphant.\nOther Language Changes \u00b6\nHere are all of the changes that Python 2.5 makes to the core Python language.\nThe dict type has a new hook for letting subclasses provide a default value when a key isn\u2019t contained in the dictionary. When a key isn\u2019t found, the dictionary\u2019s __missing__(key)() method will be called. This hook is used to implement the new defaultdict class in the collections module. The following example defines a dictionary that returns zero for any missing key:\nclass zerodict (dict): def __missing__ (self, key): return 0 d = zerodict({1:1, 2:2}) print d[1], d[2] # Prints 1, 2 print d[3], d[4] # Prints 0, 0\nBoth 8-bit and Unicode strings have new partition(sep)() and rpartition(sep)() methods that simplify a common use case.\nThe find(S)() method is often used to get an index which is then used to slice the string and obtain the pieces that are before and after the separator. partition(sep)() condenses this pattern into a single method call that returns a 3-tuple containing the substring before the separator, the separator itself, and the substring after the separator. If the separator isn\u2019t found, the first element of the tuple is the entire string and the other two elements are empty. rpartition(sep)() also returns a 3-tuple but starts searching from the end of the string; the r stands for \u2018reverse\u2019.\nSome examples:\n>>> ('http://www.python.org').partition('://') ('http', '://', 'www.python.org') >>> ('file:/usr/share/doc/index.html').partition('://') ('file:/usr/share/doc/index.html', '', '') >>> (u'Subject: a quick question').partition(':') (u'Subject', u':', u' a quick question') >>> 'www.python.org'.rpartition('.') ('www.python', '.', 'org') >>> 'www.python.org'.rpartition(':') ('', '', 'www.python.org')\n(Implemented by Fredrik Lundh following a suggestion by Raymond Hettinger.)\nThe startswith() and endswith() methods of string types now accept tuples of strings to check for.\ndef is_image_file (filename): return filename.endswith(('.gif', '.jpg', '.tiff'))\n(Implemented by Georg Brandl following a suggestion by Tom Lynn.)\nThe min() and max() built-in functions gained a key keyword parameter analogous to the key argument for sort(). This parameter supplies a function that takes a single argument and is called for every value in the list; min() / max() will return the element with the smallest/largest return value from this function. For example, to find the longest string in a list, you can do:\nL = ['medium', 'longest', 'short'] # Prints 'longest' print max(L, key=len) # Prints 'short', because lexicographically 'short' has the largest value print max(L)\n(Contributed by Steven Bethard and Raymond Hettinger.)\nTwo new built-in functions, any() and all() , evaluate whether an iterator contains any true or false values. any() returns True if any value returned by the iterator is true; otherwise it will return False . all() returns True only if all of the values returned by the iterator evaluate as true. (Suggested by Guido van Rossum, and implemented by Raymond Hettinger.)\nThe result of a class\u2019s __hash__() method can now be either a long integer or a regular integer. If a long integer is returned, the hash of that value is taken. In earlier versions the hash value was required to be a regular integer, but in 2.5 the id() built-in was changed to always return non-negative numbers, and users often seem to use id(self) in __hash__() methods (though this is discouraged).\nASCII is now the default encoding for modules. It\u2019s now a syntax error if a module contains string literals with 8-bit characters but doesn\u2019t have an encoding declaration. In Python 2.4 this triggered a warning, not a syntax error. See PEP 263 for how to declare a module\u2019s encoding; for example, you might add a line like this near the top of the source file:\n# -*- coding: latin1 -*-\nA new warning, UnicodeWarning, is triggered when you attempt to compare a Unicode string and an 8-bit string that can\u2019t be converted to Unicode using the default ASCII encoding. The result of the comparison is false:\n>>> chr(128) == unichr(128) # Can't convert chr(128) to Unicode __main__:1: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal False >>> chr(127) == unichr(127) # chr(127) can be converted True\nPreviously this would raise a UnicodeDecodeError exception, but in 2.5 this could result in puzzling problems when accessing a dictionary. If you looked up unichr(128) and chr(128) was being used as a key, you\u2019d get a UnicodeDecodeError exception. Other changes in 2.5 resulted in this exception being raised instead of suppressed by the code in dictobject.c that implements dictionaries.\nRaising an exception for such a comparison is strictly correct, but the change might have broken code, so instead UnicodeWarning was introduced.\n(Implemented by Marc-Andr\u00e9 Lemburg.)\nOne error that Python programmers sometimes make is forgetting to include an __init__.py module in a package directory. Debugging this mistake can be confusing, and usually requires running Python with the -v switch to log all the paths searched. In Python 2.5, a new ImportWarning warning is triggered when an import would have picked up a directory as a package but no __init__.py was found. This warning is silently ignored by default; provide the -Wd option when running the Python executable to display the warning message. (Implemented by Thomas Wouters.)\nThe list of base classes in a class definition can now be empty. As an example, this is now legal:\nclass C(): pass\n(Implemented by Brett Cannon.)\nInteractive Interpreter Changes \u00b6\nIn the interactive interpreter, quit and exit have long been strings so that new users get a somewhat helpful message when they try to quit:\n>>> quit 'Use Ctrl-D (i.e. EOF) to exit.'\nIn Python 2.5, quit and exit are now objects that still produce string representations of themselves, but are also callable. Newbies who try quit() or exit() will now exit the interpreter as they expect. (Implemented by Georg Brandl.)\nThe Python executable now accepts the standard long options --help and --version ; on Windows, it also accepts the /? option for displaying a help message. (Implemented by Georg Brandl.)\nOptimizations \u00b6\nSeveral of the optimizations were developed at the NeedForSpeed sprint, an event held in Reykjavik, Iceland, from May 21\u201328 2006. The sprint focused on speed enhancements to the CPython implementation and was funded by EWT LLC with local support from CCP Games. Those optimizations added at this sprint are specially marked in the following list.\nWhen they were introduced in Python 2.4, the built-in set and frozenset types were built on top of Python\u2019s dictionary type. In 2.5 the internal data structure has been customized for implementing sets, and as a result sets will use a third less memory and are somewhat faster. (Implemented by Raymond Hettinger.)\nThe speed of some Unicode operations, such as finding substrings, string splitting, and character map encoding and decoding, has been improved. (Substring search and splitting improvements were added by Fredrik Lundh and Andrew Dalke at the NeedForSpeed sprint. Character maps were improved by Walter D\u00f6rwald and Martin von L\u00f6wis.)\nThe long(str, base)() function is now faster on long digit strings because fewer intermediate results are calculated. The peak is for strings of around 800\u20131000 digits where the function is 6 times faster. (Contributed by Alan McIntyre and committed at the NeedForSpeed sprint.)\nIt\u2019s now illegal to mix iterating over a file with for line in file and calling the file object\u2019s read()/ readline() /readlines() methods. Iteration uses an internal buffer and the read*() methods don\u2019t use that buffer. Instead they would return the data following the buffer, causing the data to appear out of order. Mixing iteration and these methods will now trigger a ValueError from the read*() method. (Implemented by Thomas Wouters.)\nThe struct module now compiles structure format strings into an internal representation and caches this representation, yielding a 20% speedup. (Contributed by Bob Ippolito at the NeedForSpeed sprint.)\nThe re module got a 1 or 2% speedup by switching to Python\u2019s allocator functions instead of the system\u2019s malloc() and free(). (Contributed by Jack Diederich at the NeedForSpeed sprint.)\nThe code generator\u2019s peephole optimizer now performs simple constant folding in expressions. If you write something like a = 2+3, the code generator will do the arithmetic and produce code corresponding to a = 5. (Proposed and implemented by Raymond Hettinger.)\nFunction calls are now faster because code objects now keep the most recently finished frame (a \u201czombie frame\u201d) in an internal field of the code object, reusing it the next time the code object is invoked. (Original patch by Michael Hudson, modified by Armin Rigo and Richard Jones; committed at the NeedForSpeed sprint.) Frame objects are also slightly smaller, which may improve cache locality and reduce memory usage a bit. (Contributed by Neal Norwitz.)\nPython\u2019s built-in exceptions are now new-style classes, a change that speeds up instantiation considerably. Exception handling in Python 2.5 is therefore about 30% faster than in 2.4. (Contributed by Richard Jones, Georg Brandl and Sean Reifschneider at the NeedForSpeed sprint.)\nImporting now caches the paths tried, recording whether they exist or not so that the interpreter makes fewer open() and stat() calls on startup. (Contributed by Martin von L\u00f6wis and Georg Brandl.)\nNew, Improved, and Removed Modules \u00b6\nThe standard library received many enhancements and bug fixes in Python 2.5. Here\u2019s a partial list of the most notable changes, sorted alphabetically by module name. Consult the Misc/NEWS file in the source tree for a more complete list of changes, or look through the SVN logs for all the details.\nThe audioop module now supports the a-LAW encoding, and the code for u-LAW encoding has been improved. (Contributed by Lars Immisch.)\nThe codecs module gained support for incremental codecs. The codec.lookup() function now returns a CodecInfo instance instead of a tuple. CodecInfo instances behave like a 4-tuple to preserve backward compatibility but also have the attributes encode, decode, incrementalencoder, incrementaldecoder, streamwriter, and streamreader. Incremental codecs can receive input and produce output in multiple chunks; the output is the same as if the entire input was fed to the non-incremental codec. See the codecs module documentation for details. (Designed and implemented by Walter D\u00f6rwald.)\nThe collections module gained a new type, defaultdict, that subclasses the standard dict type. The new type mostly behaves like a dictionary but constructs a default value when a key isn\u2019t present, automatically adding it to the dictionary for the requested key value.\nThe first argument to defaultdict\u2018s constructor is a factory function that gets called whenever a key is requested but not found. This factory function receives no arguments, so you can use built-in type constructors such as list() or int() . For example, you can make an index of words based on their initial letter like this:\nwords = \"\"\"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura che la diritta via era smarrita\"\"\".lower().split() index = defaultdict(list) for w in words: init_letter = w[0] index[init_letter].append(w)\nPrinting index results in the following output:\ndefaultdict(<type 'list'>, {'c': ['cammin', 'che'], 'e': ['era'],\n        'd': ['del', 'di', 'diritta'], 'm': ['mezzo', 'mi'],\n        'l': ['la'], 'o': ['oscura'], 'n': ['nel', 'nostra'],\n        'p': ['per'], 's': ['selva', 'smarrita'],\n        'r': ['ritrovai'], 'u': ['una'], 'v': ['vita', 'via']}\n(Contributed by Guido van Rossum.)\nThe deque double-ended queue type supplied by the collections module now has a remove(value)() method that removes the first occurrence of value in the queue, raising ValueError if the value isn\u2019t found. (Contributed by Raymond Hettinger.)\nNew module: The contextlib module contains helper functions for use with the new \u2018 with \u2018 statement. See section The contextlib module for more about this module.\nNew module: The cProfile module is a C implementation of the existing profile module that has much lower overhead. The module\u2019s interface is the same as profile : you run cProfile.run('main()') to profile a function, can save profile data to a file, etc. It\u2019s not yet known if the Hotshot profiler, which is also written in C but doesn\u2019t match the profile module\u2019s interface, will continue to be maintained in future versions of Python. (Contributed by Armin Rigo.)\nAlso, the pstats module for analyzing the data measured by the profiler now supports directing the output to any file object by supplying a stream argument to the Stats constructor. (Contributed by Skip Montanaro.)\nThe csv module, which parses files in comma-separated value format, received several enhancements and a number of bugfixes. You can now set the maximum size in bytes of a field by calling the csv.field_size_limit(new_limit)() function; omitting the new_limit argument will return the currently-set limit. The reader class now has a line_num attribute that counts the number of physical lines read from the source; records can span multiple physical lines, so line_num is not the same as the number of records read.\nThe CSV parser is now stricter about multi-line quoted fields. Previously, if a line ended within a quoted field without a terminating newline character, a newline would be inserted into the returned field. This behavior caused problems when reading files that contained carriage return characters within fields, so the code was changed to return the field without inserting newlines. As a consequence, if newlines embedded within fields are important, the input should be split into lines in a manner that preserves the newline characters.\n(Contributed by Skip Montanaro and Andrew McNamara.)\nThe datetime class in the datetime module now has a strptime(string, format)() method for parsing date strings, contributed by Josh Spoerri. It uses the same format characters as time.strptime() and time.strftime() :\nfrom datetime import datetime ts = datetime.strptime('10:13:15 2006-03-07', '%H:%M:%S %Y-%m-%d')\nThe SequenceMatcher.get_matching_blocks() method in the difflib module now guarantees to return a minimal list of blocks describing matching subsequences. Previously, the algorithm would occasionally break a block of matching elements into two list entries. (Enhancement by Tim Peters.)\nThe doctest module gained a SKIP option that keeps an example from being executed at all. This is intended for code snippets that are usage examples intended for the reader and aren\u2019t actually test cases.\nAn encoding parameter was added to the testfile() function and the DocFileSuite class to specify the file\u2019s encoding. This makes it easier to use non-ASCII characters in tests contained within a docstring. (Contributed by Bjorn Tillenius.)\nThe email package has been updated to version 4.0. (Contributed by Barry Warsaw.)\nThe fileinput module was made more flexible. Unicode filenames are now supported, and a mode parameter that defaults to \"r\" was added to the input() function to allow opening files in binary or universal-newline mode. Another new parameter, openhook, lets you use a function other than open() to open the input files. Once you\u2019re iterating over the set of files, the FileInput object\u2019s new fileno() returns the file descriptor for the currently opened file. (Contributed by Georg Brandl.)\nIn the gc module, the new get_count() function returns a 3-tuple containing the current collection counts for the three GC generations. This is accounting information for the garbage collector; when these counts reach a specified threshold, a garbage collection sweep will be made. The existing gc.collect() function now takes an optional generation argument of 0, 1, or 2 to specify which generation to collect. (Contributed by Barry Warsaw.)\nThe nsmallest() and nlargest() functions in the heapq module now support a key keyword parameter similar to the one provided by the min() / max() functions and the sort() methods. For example:\n>>> import heapq >>> L = [\"short\", 'medium', 'longest', 'longer still'] >>> heapq.nsmallest(2, L) # Return two lowest elements, lexicographically ['longer still', 'longest'] >>> heapq.nsmallest(2, L, key=len) # Return two shortest elements ['short', 'medium']\n(Contributed by Raymond Hettinger.)\nThe itertools.islice() function now accepts None for the start and step arguments. This makes it more compatible with the attributes of slice objects, so that you can now write the following:\ns = slice(5) # Create slice object itertools.islice(iterable, s.start, s.stop, s.step)\n(Contributed by Raymond Hettinger.)\nThe format() function in the locale module has been modified and two new functions were added, format_string() and currency().\nThe format() function\u2019s val parameter could previously be a string as long as no more than one %char specifier appeared; now the parameter must be exactly one %char specifier with no surrounding text. An optional monetary parameter was also added which, if True, will use the locale\u2019s rules for formatting currency in placing a separator between groups of three digits.\nTo format strings with multiple %char specifiers, use the new format_string() function that works like format() but also supports mixing %char specifiers with arbitrary text.\nA new currency() function was also added that formats a number according to the current locale\u2019s settings.\n(Contributed by Georg Brandl.)\nThe mailbox module underwent a massive rewrite to add the capability to modify mailboxes in addition to reading them. A new set of classes that include mbox, MH, and Maildir are used to read mailboxes, and have an add(message)() method to add messages, remove(key)() to remove messages, and lock()/unlock() to lock/unlock the mailbox. The following example converts a maildir-format mailbox into an mbox-format one:\nimport mailbox # 'factory=None' uses email.Message.Message as the class representing # individual messages. src = mailbox.Maildir('maildir', factory=None) dest = mailbox.mbox('/tmp/mbox') for msg in src: dest.add(msg)\n(Contributed by Gregory K. Johnson. Funding was provided by Google\u2019s 2005 Summer of Code.)\nNew module: the msilib module allows creating Microsoft Installer .msi files and CAB files. Some support for reading the .msi database is also included. (Contributed by Martin von L\u00f6wis.)\nThe nis module now supports accessing domains other than the system default domain by supplying a domain argument to the nis.match() and nis.maps() functions. (Contributed by Ben Bell.)\nThe operator module\u2019s itemgetter() and attrgetter() functions now support multiple fields. A call such as operator.attrgetter('a', 'b') will return a function that retrieves the a and b attributes. Combining this new feature with the sort() method\u2019s key parameter lets you easily sort lists using multiple fields. (Contributed by Raymond Hettinger.)\nThe optparse module was updated to version 1.5.1 of the Optik library. The OptionParser class gained an epilog attribute, a string that will be printed after the help message, and a destroy() method to break reference cycles created by the object. (Contributed by Greg Ward.)\nThe os module underwent several changes. The stat_float_times variable now defaults to true, meaning that os.stat() will now return time values as floats. (This doesn\u2019t necessarily mean that os.stat() will return times that are precise to fractions of a second; not all systems support such precision.)\nConstants named os.SEEK_SET , os.SEEK_CUR , and os.SEEK_END have been added; these are the parameters to the os.lseek() function. Two new constants for locking are os.O_SHLOCK and os.O_EXLOCK .\nTwo new functions, wait3() and wait4(), were added. They\u2019re similar the waitpid() function which waits for a child process to exit and returns a tuple of the process ID and its exit status, but wait3() and wait4() return additional information. wait3() doesn\u2019t take a process ID as input, so it waits for any child process to exit and returns a 3-tuple of process-id, exit-status, resource-usage as returned from the resource.getrusage() function. wait4(pid)() does take a process ID. (Contributed by Chad J. Schroeder.)\nOn FreeBSD, the os.stat() function now returns times with nanosecond resolution, and the returned object now has st_gen and st_birthtime. The st_flags member is also available, if the platform supports it. (Contributed by Antti Louko and Diego Petten\u00f2.)\nThe Python debugger provided by the pdb module can now store lists of commands to execute when a breakpoint is reached and execution stops. Once breakpoint #1 has been created, enter commands 1 and enter a series of commands to be executed, finishing the list with end. The command list can include commands that resume execution, such as continue or next. (Contributed by Gr\u00e9goire Dooms.)\nThe pickle and cPickle modules no longer accept a return value of None from the __reduce__() method; the method must return a tuple of arguments instead. The ability to return None was deprecated in Python 2.4, so this completes the removal of the feature.\nThe pkgutil module, containing various utility functions for finding packages, was enhanced to support PEP 302\u2019s import hooks and now also works for packages stored in ZIP-format archives. (Contributed by Phillip J. Eby.)\nThe pybench benchmark suite by Marc-Andr\u00e9 Lemburg is now included in the Tools/pybench directory. The pybench suite is an improvement on the commonly used pystone.py program because pybench provides a more detailed measurement of the interpreter\u2019s speed. It times particular operations such as function calls, tuple slicing, method lookups, and numeric operations, instead of performing many different operations and reducing the result to a single number as pystone.py does.\nThe pyexpat module now uses version 2.0 of the Expat parser. (Contributed by Trent Mick.)\nThe Queue class provided by the Queue module gained two new methods. join() blocks until all items in the queue have been retrieved and all processing work on the items have been completed. Worker threads call the other new method, task_done(), to signal that processing for an item has been completed. (Contributed by Raymond Hettinger.)\nThe old regex and regsub modules, which have been deprecated ever since Python 2.0, have finally been deleted. Other deleted modules: statcache, tzparse, whrandom.\nAlso deleted: the lib-old directory, which includes ancient modules such as dircmp and ni, was removed. lib-old wasn\u2019t on the default sys.path, so unless your programs explicitly added the directory to sys.path, this removal shouldn\u2019t affect your code.\nThe rlcompleter module is no longer dependent on importing the readline module and therefore now works on non-Unix platforms. (Patch from Robert Kiendl.)\nThe SimpleXMLRPCServer and DocXMLRPCServer classes now have a rpc_paths attribute that constrains XML-RPC operations to a limited set of URL paths; the default is to allow only '/' and '/RPC2'. Setting rpc_paths to None or an empty tuple disables this path checking.\nThe socket module now supports AF_NETLINK sockets on Linux, thanks to a patch from Philippe Biondi. Netlink sockets are a Linux-specific mechanism for communications between a user-space process and kernel code; an introductory article about them is at http://www.linuxjournal.com/article/7356 . In Python code, netlink addresses are represented as a tuple of 2 integers, (pid, group_mask).\nTwo new methods on socket objects, recv_into(buffer)() and recvfrom_into(buffer)(), store the received data in an object that supports the buffer protocol instead of returning the data as a string. This means you can put the data directly into an array or a memory-mapped file.\nSocket objects also gained getfamily(), gettype(), and getproto() accessor methods to retrieve the family, type, and protocol values for the socket.\nNew module: the spwd module provides functions for accessing the shadow password database on systems that support shadow passwords.\nThe struct is now faster because it compiles format strings into Struct objects with pack() and unpack() methods. This is similar to how the re module lets you create compiled regular expression objects. You can still use the module-level pack() and unpack() functions; they\u2019ll create Struct objects and cache them. Or you can use Struct instances directly:\ns = struct.Struct('ih3s') data = s.pack(1972, 187, 'abc') year, number, name = s.unpack(data)\nYou can also pack and unpack data to and from buffer objects directly using the pack_into(buffer, offset, v1, v2, ...)() and unpack_from(buffer, offset)() methods. This lets you store data directly into an array or a memory- mapped file.\n(Struct objects were implemented by Bob Ippolito at the NeedForSpeed sprint. Support for buffer objects was added by Martin Blais, also at the NeedForSpeed sprint.)\nThe Python developers switched from CVS to Subversion during the 2.5 development process. Information about the exact build version is available as the sys.subversion variable, a 3-tuple of (interpreter-name, branch-name, revision-range). For example, at the time of writing my copy of 2.5 was reporting ('CPython', 'trunk', '45313:45315').\nThis information is also available to C extensions via the Py_GetBuildInfo() function that returns a string of build information like this: \"trunk:45355:45356M, Apr 13 2006, 07:42:19\". (Contributed by Barry Warsaw.)\nAnother new function, sys._current_frames() , returns the current stack frames for all running threads as a dictionary mapping thread identifiers to the topmost stack frame currently active in that thread at the time the function is called. (Contributed by Tim Peters.)\nThe TarFile class in the tarfile module now has an extractall() method that extracts all members from the archive into the current working directory. It\u2019s also possible to set a different directory as the extraction target, and to unpack only a subset of the archive\u2019s members.\nThe compression used for a tarfile opened in stream mode can now be autodetected using the mode 'r|*'. (Contributed by Lars Gust\u00e4bel.)\nThe threading module now lets you set the stack size used when new threads are created. The stack_size([*size*])() function returns the currently configured stack size, and supplying the optional size parameter sets a new value. Not all platforms support changing the stack size, but Windows, POSIX threading, and OS/2 all do. (Contributed by Andrew MacIntyre.)\nThe unicodedata module has been updated to use version 4.1.0 of the Unicode character database. Version 3.2.0 is required by some specifications, so it\u2019s still available as unicodedata.ucd_3_2_0 .\nNew module: the uuid module generates universally unique identifiers (UUIDs) according to RFC 4122 . The RFC defines several different UUID versions that are generated from a starting string, from system properties, or purely randomly. This module contains a UUID class and functions named uuid1(), uuid3(), uuid4(), and uuid5() to generate different versions of UUID. (Version 2 UUIDs are not specified in RFC 4122 and are not supported by this module.)\n>>> import uuid >>> # make a UUID based on the host ID and current time >>> uuid.uuid1() UUID('a8098c1a-f86e-11da-bd1a-00112444be1e') >>> # make a UUID using an MD5 hash of a namespace UUID and a name >>> uuid.uuid3(uuid.NAMESPACE_DNS, 'python.org') UUID('6fa459ea-ee8a-3ca4-894e-db77e160355e') >>> # make a random UUID >>> uuid.uuid4() UUID('16fd2706-8baf-433b-82eb-8c7fada847da') >>> # make a UUID using a SHA-1 hash of a namespace UUID and a name >>> uuid.uuid5(uuid.NAMESPACE_DNS, 'python.org') UUID('886313e1-3b8a-5372-9b90-0c9aee199e5d')\n(Contributed by Ka-Ping Yee.)\nThe weakref module\u2019s WeakKeyDictionary and WeakValueDictionary types gained new methods for iterating over the weak references contained in the dictionary. iterkeyrefs() and keyrefs() methods were added to WeakKeyDictionary, and itervaluerefs() and valuerefs() were added to WeakValueDictionary. (Contributed by Fred L. Drake, Jr.)\nThe webbrowser module received a number of enhancements. It\u2019s now usable as a script with python -m webbrowser, taking a URL as the argument; there are a number of switches to control the behaviour (-n for a new browser window, -t for a new tab). New module-level functions, open_new() and open_new_tab(), were added to support this. The module\u2019s open() function supports an additional feature, an autoraise parameter that signals whether to raise the open window when possible. A number of additional browsers were added to the supported list such as Firefox, Opera, Konqueror, and elinks. (Contributed by Oleg Broytmann and Georg Brandl.)\nThe xmlrpclib module now supports returning datetime objects for the XML-RPC date type. Supply use_datetime=True to the loads() function or the Unmarshaller class to enable this feature. (Contributed by Skip Montanaro.)\nThe zipfile module now supports the ZIP64 version of the format, meaning that a .zip archive can now be larger than 4 GiB and can contain individual files larger than 4 GiB. (Contributed by Ronald Oussoren.)\nThe zlib module\u2019s Compress and Decompress objects now support a copy() method that makes a copy of the object\u2019s internal state and returns a new Compress or Decompress object. (Contributed by Chris AtLee.)\nThe ctypes package \u00b6\nThe ctypes package, written by Thomas Heller, has been added to the standard library. ctypes lets you call arbitrary functions in shared libraries or DLLs. Long-time users may remember the dl module, which provides functions for loading shared libraries and calling functions in them. The ctypes package is much fancier.\nTo load a shared library or DLL, you must create an instance of the CDLL class and provide the name or path of the shared library or DLL. Once that\u2019s done, you can call arbitrary functions by accessing them as attributes of the CDLL object.\nimport ctypes libc = ctypes.CDLL('libc.so.6') result = libc.printf(\"Line of output\\n\")\nType constructors for the various C types are provided: c_int(), c_float(), c_double(), c_char_p() (equivalent to char *), and so forth. Unlike Python\u2019s types, the C versions are all mutable; you can assign to their value attribute to change the wrapped value. Python integers and strings will be automatically converted to the corresponding C types, but for other types you must call the correct type constructor. (And I mean must; getting it wrong will often result in the interpreter crashing with a segmentation fault.)\nYou shouldn\u2019t use c_char_p() with a Python string when the C function will be modifying the memory area, because Python strings are supposed to be immutable; breaking this rule will cause puzzling bugs. When you need a modifiable memory area, use create_string_buffer():\ns = \"this is a string\" buf = ctypes.create_string_buffer(s) libc.strfry(buf)\nC functions are assumed to return integers, but you can set the restype attribute of the function object to change this:\n>>> libc.atof('2.71828') -1783957616 >>> libc.atof.restype = ctypes.c_double >>> libc.atof('2.71828') 2.71828\nctypes also provides a wrapper for Python\u2019s C API as the ctypes.pythonapi object. This object does not release the global interpreter lock before calling a function, because the lock must be held when calling into the interpreter\u2019s code. There\u2019s a py_object() type constructor that will create a PyObject * pointer. A simple usage:\nimport ctypes d = {} ctypes.pythonapi.PyObject_SetItem(ctypes.py_object(d), ctypes.py_object(\"abc\"), ctypes.py_object(1)) # d is now {'abc', 1}.\nDon\u2019t forget to use py_object(); if it\u2019s omitted you end up with a segmentation fault.\nctypes has been around for a while, but people still write and distribution hand-coded extension modules because you can\u2019t rely on ctypes being present. Perhaps developers will begin to write Python wrappers atop a library accessed through ctypes instead of extension modules, now that ctypes is included with core Python.\n\u00e5\u008f\u0082\u00e8\u0080\u0083\nThe ctypes web page, with a tutorial, reference, and FAQ.\nThe documentation for the ctypes module.\nThe ElementTree package \u00b6\nA subset of Fredrik Lundh\u2019s ElementTree library for processing XML has been added to the standard library as xml.etree. The available modules are ElementTree, ElementPath, and ElementInclude from ElementTree 1.2.6. The cElementTree accelerator module is also included.\nThe rest of this section will provide a brief overview of using ElementTree. Full documentation for ElementTree is available at http://effbot.org/zone/element-index.htm .\nElementTree represents an XML document as a tree of element nodes. The text content of the document is stored as the text and tail attributes of (This is one of the major differences between ElementTree and the Document Object Model; in the DOM there are many different types of node, including TextNode.)\nThe most commonly used parsing function is parse(), that takes either a string (assumed to contain a filename) or a file-like object and returns an ElementTree instance:\nfrom xml.etree import ElementTree as ET tree = ET.parse('ex-1.xml') feed = urllib.urlopen( 'http://planet.python.org/rss10.xml') tree = ET.parse(feed)\nOnce you have an ElementTree instance, you can call its getroot() method to get the root Element node.\nThere\u2019s also an XML() function that takes a string literal and returns an Element node (not an ElementTree). This function provides a tidy way to incorporate XML fragments, approaching the convenience of an XML literal:\nsvg = ET.XML(\"\"\"<svg width=\"10px\" version=\"1.0\"> </svg>\"\"\") svg.set('height', '320px') svg.append(elem1)\nEach XML element supports some dictionary-like and some list-like access methods. Dictionary-like operations are used to access attribute values, and list-like operations are used to access child nodes.\nOperation\n"}, {"score": 915.1693, "uuid": "d0a8e026-8c8e-55cb-a439-5424e928c871", "index": "cw12", "trec_id": "clueweb12-0706wb-04-32399", "target_hostname": "www.wingware.com", "target_uri": "http://www.wingware.com/psupport/python-manual/3.2/whatsnew/2.5.html", "page_rank": 1.2017487e-09, "spam_rank": 71, "title": "What\u2019s New in <em>Python</em> 2.5 \u2014 <em>Python</em> v3.2.1 documentation", "snippet": "The decision was made to not require parentheses in the <em>Python</em> language\u2019s grammar, but as a matter of style <em>I</em> think you <em>should</em> always use them.", "explanation": null, "document": "An explanation of coroutines from a Perl point of view, written by Dan Sugalski.\nPEP 343: The \u2018with\u2019 statement \u00b6\nThe \u2018 with \u2018 statement clarifies code that previously would use try...finally blocks to ensure that clean-up code is executed. In this section, I\u2019ll discuss the statement as it will commonly be used. In the next section, I\u2019ll examine the implementation details and show how to write objects for use with this statement.\nThe \u2018 with \u2018 statement is a new control-flow structure whose basic structure is:\nwith expression [as variable]: with-block\nThe expression is evaluated, and it should result in an object that supports the context management protocol (that is, has __enter__() and __exit__() methods.\nThe object\u2019s __enter__() is called before with-block is executed and therefore can run set-up code. It also may return a value that is bound to the name variable, if given. (Note carefully that variable is not assigned the result of expression.)\nAfter execution of the with-block is finished, the object\u2019s __exit__() method is called, even if the block raised an exception, and can therefore run clean-up code.\nTo enable the statement in Python 2.5, you need to add the following directive to your module:\nfrom __future__ import with_statement\nThe statement will always be enabled in Python 2.6.\nSome standard Python objects now support the context management protocol and can be used with the \u2018 with \u2018 statement. File objects are one example:\nwith open('/etc/passwd', 'r') as f: for line in f: print line ... more processing code ...\nAfter this statement has executed, the file object in f will have been automatically closed, even if the for loop raised an exception part- way through the block.\nNote\nIn this case, f is the same object created by open() , because file.__enter__() returns self.\nThe threading module\u2019s locks and condition variables also support the \u2018 with \u2018 statement:\nlock = threading.Lock() with lock: # Critical section of code ...\nThe lock is acquired before the block is executed and always released once the block is complete.\nThe new localcontext() function in the decimal module makes it easy to save and restore the current decimal context, which encapsulates the desired precision and rounding characteristics for computations:\nfrom decimal import Decimal, Context, localcontext # Displays with default precision of 28 digits v = Decimal('578') print v.sqrt() with localcontext(Context(prec=16)): # All code in this block uses a precision of 16 digits. # The original context is restored on exiting the block. print v.sqrt()\nWriting Context Managers \u00b6\nUnder the hood, the \u2018 with \u2018 statement is fairly complicated. Most people will only use \u2018 with \u2018 in company with existing objects and don\u2019t need to know these details, so you can skip the rest of this section if you like. Authors of new objects will need to understand the details of the underlying implementation and should keep reading.\nA high-level explanation of the context management protocol is:\nThe expression is evaluated and should result in an object called a \u201ccontext manager\u201d. The context manager must have __enter__() and __exit__() methods.\nThe context manager\u2019s __enter__() method is called. The value returned is assigned to VAR. If no 'as VAR' clause is present, the value is simply discarded.\nThe code in BLOCK is executed.\nIf BLOCK raises an exception, the __exit__(type, value, traceback)() is called with the exception details, the same values returned by sys.exc_info() . The method\u2019s return value controls whether the exception is re-raised: any false value re-raises the exception, and True will result in suppressing it. You\u2019ll only rarely want to suppress the exception, because if you do the author of the code containing the \u2018 with \u2018 statement will never realize anything went wrong.\nIf BLOCK didn\u2019t raise an exception, the __exit__() method is still called, but type, value, and traceback are all None.\nLet\u2019s think through an example. I won\u2019t present detailed code but will only sketch the methods necessary for a database that supports transactions.\n(For people unfamiliar with database terminology: a set of changes to the database are grouped into a transaction. Transactions can be either committed, meaning that all the changes are written into the database, or rolled back, meaning that the changes are all discarded and the database is unchanged. See any database textbook for more information.)\nLet\u2019s assume there\u2019s an object representing a database connection. Our goal will be to let the user write code like this:\ndb_connection = DatabaseConnection() with db_connection as cursor: cursor.execute('insert into ...') cursor.execute('delete from ...') # ... more operations ...\nThe transaction should be committed if the code in the block runs flawlessly or rolled back if there\u2019s an exception. Here\u2019s the basic interface for DatabaseConnection that I\u2019ll assume:\nclass DatabaseConnection: # Database interface def cursor (self): \"Returns a cursor object and starts a new transaction\" def commit (self): \"Commits current transaction\" def rollback (self): \"Rolls back current transaction\"\nThe __enter__() method is pretty easy, having only to start a new transaction. For this application the resulting cursor object would be a useful result, so the method will return it. The user can then add as cursor to their \u2018 with \u2018 statement to bind the cursor to a variable name.\nclass DatabaseConnection: ... def __enter__ (self): # Code to start a new transaction cursor = self.cursor() return cursor\nThe __exit__() method is the most complicated because it\u2019s where most of the work has to be done. The method has to check if an exception occurred. If there was no exception, the transaction is committed. The transaction is rolled back if there was an exception.\nIn the code below, execution will just fall off the end of the function, returning the default value of None. None is false, so the exception will be re-raised automatically. If you wished, you could be more explicit and add a return statement at the marked location.\nclass DatabaseConnection: ... def __exit__ (self, type, value, tb): if tb is None: # No exception, so commit self.commit() else: # Exception occurred, so rollback. self.rollback() # return False\nThe contextlib module \u00b6\nThe new contextlib module provides some functions and a decorator that are useful for writing objects for use with the \u2018 with \u2018 statement.\nThe decorator is called contextmanager(), and lets you write a single generator function instead of defining a new class. The generator should yield exactly one value. The code up to the yield will be executed as the __enter__() method, and the value yielded will be the method\u2019s return value that will get bound to the variable in the \u2018 with \u2018 statement\u2019s as clause, if any. The code after the yield will be executed in the __exit__() method. Any exception raised in the block will be raised by the yield statement.\nOur database example from the previous section could be written using this decorator as:\nfrom contextlib import contextmanager @contextmanager def db_transaction (connection): cursor = connection.cursor() try: yield cursor except: connection.rollback() raise else: connection.commit() db = DatabaseConnection() with db_transaction(db) as cursor: ...\nThe contextlib module also has a nested(mgr1, mgr2, ...)() function that combines a number of context managers so you don\u2019t need to write nested \u2018 with \u2018 statements. In this example, the single \u2018 with \u2018 statement both starts a database transaction and acquires a thread lock:\nlock = threading.Lock() with nested (db_transaction(db), lock) as (cursor, locked): ...\nFinally, the closing(object)() function returns object so that it can be bound to a variable, and calls object.close at the end of the block.\nimport urllib, sys from contextlib import closing with closing(urllib.urlopen('http://www.yahoo.com')) as f: for line in f: sys.stdout.write(line)\nSee also\nPEP 343 - The \u201cwith\u201d statement\nPEP written by Guido van Rossum and Nick Coghlan; implemented by Mike Bland, Guido van Rossum, and Neal Norwitz. The PEP shows the code generated for a \u2018 with \u2018 statement, which can be helpful in learning how the statement works.\nThe documentation for the contextlib module.\nPEP 352: Exceptions as New-Style Classes \u00b6\nException classes can now be new-style classes, not just classic classes, and the built-in Exception class and all the standard built-in exceptions ( NameError , ValueError , etc.) are now new-style classes.\nThe inheritance hierarchy for exceptions has been rearranged a bit. In 2.5, the inheritance relationships are:\nBaseException # New in Python 2.5 |- KeyboardInterrupt |- SystemExit |- Exception |- (all other current built-in exceptions)\nThis rearrangement was done because people often want to catch all exceptions that indicate program errors. KeyboardInterrupt and SystemExit aren\u2019t errors, though, and usually represent an explicit action such as the user hitting Control-C or code calling sys.exit() . A bare except: will catch all exceptions, so you commonly need to list KeyboardInterrupt and SystemExit in order to re-raise them. The usual pattern is:\ntry: ... except (KeyboardInterrupt, SystemExit): raise except: # Log error... # Continue running program...\nIn Python 2.5, you can now write except Exception to achieve the same result, catching all the exceptions that usually indicate errors but leaving KeyboardInterrupt and SystemExit alone. As in previous versions, a bare except: still catches all exceptions.\nThe goal for Python 3.0 is to require any class raised as an exception to derive from BaseException or some descendant of BaseException , and future releases in the Python 2.x series may begin to enforce this constraint. Therefore, I suggest you begin making all your exception classes derive from Exception now. It\u2019s been suggested that the bare except: form should be removed in Python 3.0, but Guido van Rossum hasn\u2019t decided whether to do this or not.\nRaising of strings as exceptions, as in the statement raise \"Error occurred\", is deprecated in Python 2.5 and will trigger a warning. The aim is to be able to remove the string-exception feature in a few releases.\nSee also\nPEP 352 - Required Superclass for Exceptions\nPEP written by Brett Cannon and Guido van Rossum; implemented by Brett Cannon.\nPEP 353: Using ssize_t as the index type \u00b6\nA wide-ranging change to Python\u2019s C API, using a new Py_ssize_t type definition instead of int, will permit the interpreter to handle more data on 64-bit platforms. This change doesn\u2019t affect Python\u2019s capacity on 32-bit platforms.\nVarious pieces of the Python interpreter used C\u2019s int type to store sizes or counts; for example, the number of items in a list or tuple were stored in an int. The C compilers for most 64-bit platforms still define int as a 32-bit type, so that meant that lists could only hold up to 2**31 - 1 = 2147483647 items. (There are actually a few different programming models that 64-bit C compilers can use \u2013 see http://www.unix.org/version2/whatsnew/lp64_wp.html for a discussion \u2013 but the most commonly available model leaves int as 32 bits.)\nA limit of 2147483647 items doesn\u2019t really matter on a 32-bit platform because you\u2019ll run out of memory before hitting the length limit. Each list item requires space for a pointer, which is 4 bytes, plus space for a PyObject representing the item. 2147483647*4 is already more bytes than a 32-bit address space can contain.\nIt\u2019s possible to address that much memory on a 64-bit platform, however. The pointers for a list that size would only require 16 GiB of space, so it\u2019s not unreasonable that Python programmers might construct lists that large. Therefore, the Python interpreter had to be changed to use some type other than int, and this will be a 64-bit type on 64-bit platforms. The change will cause incompatibilities on 64-bit machines, so it was deemed worth making the transition now, while the number of 64-bit users is still relatively small. (In 5 or 10 years, we may all be on 64-bit machines, and the transition would be more painful then.)\nThis change most strongly affects authors of C extension modules. Python strings and container types such as lists and tuples now use Py_ssize_t to store their size. Functions such as PyList_Size() now return Py_ssize_t. Code in extension modules may therefore need to have some variables changed to Py_ssize_t.\nThe PyArg_ParseTuple() and Py_BuildValue() functions have a new conversion code, n, for Py_ssize_t. PyArg_ParseTuple() \u2018s s# and t# still output int by default, but you can define the macro PY_SSIZE_T_CLEAN before including Python.h to make them return Py_ssize_t.\nPEP 353 has a section on conversion guidelines that extension authors should read to learn about supporting 64-bit platforms.\nSee also\nPEP 353 - Using ssize_t as the index type\nPEP written and implemented by Martin von L\u00f6wis.\nPEP 357: The \u2018__index__\u2019 method \u00b6\nThe NumPy developers had a problem that could only be solved by adding a new special method, __index__() . When using slice notation, as in [start:stop:step], the values of the start, stop, and step indexes must all be either integers or long integers. NumPy defines a variety of specialized integer types corresponding to unsigned and signed integers of 8, 16, 32, and 64 bits, but there was no way to signal that these types could be used as slice indexes.\nSlicing can\u2019t just use the existing __int__() method because that method is also used to implement coercion to integers. If slicing used __int__() , floating-point numbers would also become legal slice indexes and that\u2019s clearly an undesirable behaviour.\nInstead, a new special method called __index__() was added. It takes no arguments and returns an integer giving the slice index to use. For example:\nclass C: def __index__ (self): return self.value\nThe return value must be either a Python integer or long integer. The interpreter will check that the type returned is correct, and raises a TypeError if this requirement isn\u2019t met.\nA corresponding nb_index slot was added to the C-level PyNumberMethods structure to let C extensions implement this protocol. PyNumber_Index(obj)() can be used in extension code to call the __index__() function and retrieve its result.\nSee also\nPEP 357 - Allowing Any Object to be Used for Slicing\nPEP written and implemented by Travis Oliphant.\nOther Language Changes \u00b6\nHere are all of the changes that Python 2.5 makes to the core Python language.\nThe dict type has a new hook for letting subclasses provide a default value when a key isn\u2019t contained in the dictionary. When a key isn\u2019t found, the dictionary\u2019s __missing__(key)() method will be called. This hook is used to implement the new defaultdict class in the collections module. The following example defines a dictionary that returns zero for any missing key:\nclass zerodict (dict): def __missing__ (self, key): return 0 d = zerodict({1:1, 2:2}) print d[1], d[2] # Prints 1, 2 print d[3], d[4] # Prints 0, 0\nBoth 8-bit and Unicode strings have new partition(sep)() and rpartition(sep)() methods that simplify a common use case.\nThe find(S)() method is often used to get an index which is then used to slice the string and obtain the pieces that are before and after the separator. partition(sep)() condenses this pattern into a single method call that returns a 3-tuple containing the substring before the separator, the separator itself, and the substring after the separator. If the separator isn\u2019t found, the first element of the tuple is the entire string and the other two elements are empty. rpartition(sep)() also returns a 3-tuple but starts searching from the end of the string; the r stands for \u2018reverse\u2019.\nSome examples:\n>>> ('http://www.python.org').partition('://') ('http', '://', 'www.python.org') >>> ('file:/usr/share/doc/index.html').partition('://') ('file:/usr/share/doc/index.html', '', '') >>> (u'Subject: a quick question').partition(':') (u'Subject', u':', u' a quick question') >>> 'www.python.org'.rpartition('.') ('www.python', '.', 'org') >>> 'www.python.org'.rpartition(':') ('', '', 'www.python.org')\n(Implemented by Fredrik Lundh following a suggestion by Raymond Hettinger.)\nThe startswith() and endswith() methods of string types now accept tuples of strings to check for.\ndef is_image_file (filename): return filename.endswith(('.gif', '.jpg', '.tiff'))\n(Implemented by Georg Brandl following a suggestion by Tom Lynn.)\nThe min() and max() built-in functions gained a key keyword parameter analogous to the key argument for sort(). This parameter supplies a function that takes a single argument and is called for every value in the list; min() / max() will return the element with the smallest/largest return value from this function. For example, to find the longest string in a list, you can do:\nL = ['medium', 'longest', 'short'] # Prints 'longest' print max(L, key=len) # Prints 'short', because lexicographically 'short' has the largest value print max(L)\n(Contributed by Steven Bethard and Raymond Hettinger.)\nTwo new built-in functions, any() and all() , evaluate whether an iterator contains any true or false values. any() returns True if any value returned by the iterator is true; otherwise it will return False . all() returns True only if all of the values returned by the iterator evaluate as true. (Suggested by Guido van Rossum, and implemented by Raymond Hettinger.)\nThe result of a class\u2019s __hash__() method can now be either a long integer or a regular integer. If a long integer is returned, the hash of that value is taken. In earlier versions the hash value was required to be a regular integer, but in 2.5 the id() built-in was changed to always return non-negative numbers, and users often seem to use id(self) in __hash__() methods (though this is discouraged).\nASCII is now the default encoding for modules. It\u2019s now a syntax error if a module contains string literals with 8-bit characters but doesn\u2019t have an encoding declaration. In Python 2.4 this triggered a warning, not a syntax error. See PEP 263 for how to declare a module\u2019s encoding; for example, you might add a line like this near the top of the source file:\n# -*- coding: latin1 -*-\nA new warning, UnicodeWarning , is triggered when you attempt to compare a Unicode string and an 8-bit string that can\u2019t be converted to Unicode using the default ASCII encoding. The result of the comparison is false:\n>>> chr(128) == unichr(128) # Can't convert chr(128) to Unicode __main__:1: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal False >>> chr(127) == unichr(127) # chr(127) can be converted True\nPreviously this would raise a UnicodeDecodeError exception, but in 2.5 this could result in puzzling problems when accessing a dictionary. If you looked up unichr(128) and chr(128) was being used as a key, you\u2019d get a UnicodeDecodeError exception. Other changes in 2.5 resulted in this exception being raised instead of suppressed by the code in dictobject.c that implements dictionaries.\nRaising an exception for such a comparison is strictly correct, but the change might have broken code, so instead UnicodeWarning was introduced.\n(Implemented by Marc-Andr\u00e9 Lemburg.)\nOne error that Python programmers sometimes make is forgetting to include an __init__.py module in a package directory. Debugging this mistake can be confusing, and usually requires running Python with the -v switch to log all the paths searched. In Python 2.5, a new ImportWarning warning is triggered when an import would have picked up a directory as a package but no __init__.py was found. This warning is silently ignored by default; provide the -Wd option when running the Python executable to display the warning message. (Implemented by Thomas Wouters.)\nThe list of base classes in a class definition can now be empty. As an example, this is now legal:\nclass C(): pass\n(Implemented by Brett Cannon.)\nInteractive Interpreter Changes \u00b6\nIn the interactive interpreter, quit and exit have long been strings so that new users get a somewhat helpful message when they try to quit:\n>>> quit 'Use Ctrl-D (i.e. EOF) to exit.'\nIn Python 2.5, quit and exit are now objects that still produce string representations of themselves, but are also callable. Newbies who try quit() or exit() will now exit the interpreter as they expect. (Implemented by Georg Brandl.)\nThe Python executable now accepts the standard long options --help and --version ; on Windows, it also accepts the /? option for displaying a help message. (Implemented by Georg Brandl.)\nOptimizations \u00b6\nSeveral of the optimizations were developed at the NeedForSpeed sprint, an event held in Reykjavik, Iceland, from May 21\u201328 2006. The sprint focused on speed enhancements to the CPython implementation and was funded by EWT LLC with local support from CCP Games. Those optimizations added at this sprint are specially marked in the following list.\nWhen they were introduced in Python 2.4, the built-in set and frozenset types were built on top of Python\u2019s dictionary type. In 2.5 the internal data structure has been customized for implementing sets, and as a result sets will use a third less memory and are somewhat faster. (Implemented by Raymond Hettinger.)\nThe speed of some Unicode operations, such as finding substrings, string splitting, and character map encoding and decoding, has been improved. (Substring search and splitting improvements were added by Fredrik Lundh and Andrew Dalke at the NeedForSpeed sprint. Character maps were improved by Walter D\u00f6rwald and Martin von L\u00f6wis.)\nThe long(str, base)() function is now faster on long digit strings because fewer intermediate results are calculated. The peak is for strings of around 800\u20131000 digits where the function is 6 times faster. (Contributed by Alan McIntyre and committed at the NeedForSpeed sprint.)\nIt\u2019s now illegal to mix iterating over a file with for line in file and calling the file object\u2019s read()/ readline() /readlines() methods. Iteration uses an internal buffer and the read*() methods don\u2019t use that buffer. Instead they would return the data following the buffer, causing the data to appear out of order. Mixing iteration and these methods will now trigger a ValueError from the read*() method. (Implemented by Thomas Wouters.)\nThe struct module now compiles structure format strings into an internal representation and caches this representation, yielding a 20% speedup. (Contributed by Bob Ippolito at the NeedForSpeed sprint.)\nThe re module got a 1 or 2% speedup by switching to Python\u2019s allocator functions instead of the system\u2019s malloc() and free(). (Contributed by Jack Diederich at the NeedForSpeed sprint.)\nThe code generator\u2019s peephole optimizer now performs simple constant folding in expressions. If you write something like a = 2+3, the code generator will do the arithmetic and produce code corresponding to a = 5. (Proposed and implemented by Raymond Hettinger.)\nFunction calls are now faster because code objects now keep the most recently finished frame (a \u201czombie frame\u201d) in an internal field of the code object, reusing it the next time the code object is invoked. (Original patch by Michael Hudson, modified by Armin Rigo and Richard Jones; committed at the NeedForSpeed sprint.) Frame objects are also slightly smaller, which may improve cache locality and reduce memory usage a bit. (Contributed by Neal Norwitz.)\nPython\u2019s built-in exceptions are now new-style classes, a change that speeds up instantiation considerably. Exception handling in Python 2.5 is therefore about 30% faster than in 2.4. (Contributed by Richard Jones, Georg Brandl and Sean Reifschneider at the NeedForSpeed sprint.)\nImporting now caches the paths tried, recording whether they exist or not so that the interpreter makes fewer open() and stat() calls on startup. (Contributed by Martin von L\u00f6wis and Georg Brandl.)\nNew, Improved, and Removed Modules \u00b6\nThe standard library received many enhancements and bug fixes in Python 2.5. Here\u2019s a partial list of the most notable changes, sorted alphabetically by module name. Consult the Misc/NEWS file in the source tree for a more complete list of changes, or look through the SVN logs for all the details.\nThe audioop module now supports the a-LAW encoding, and the code for u-LAW encoding has been improved. (Contributed by Lars Immisch.)\nThe codecs module gained support for incremental codecs. The codec.lookup() function now returns a CodecInfo instance instead of a tuple. CodecInfo instances behave like a 4-tuple to preserve backward compatibility but also have the attributes encode, decode, incrementalencoder, incrementaldecoder, streamwriter, and streamreader. Incremental codecs can receive input and produce output in multiple chunks; the output is the same as if the entire input was fed to the non-incremental codec. See the codecs module documentation for details. (Designed and implemented by Walter D\u00f6rwald.)\nThe collections module gained a new type, defaultdict, that subclasses the standard dict type. The new type mostly behaves like a dictionary but constructs a default value when a key isn\u2019t present, automatically adding it to the dictionary for the requested key value.\nThe first argument to defaultdict\u2018s constructor is a factory function that gets called whenever a key is requested but not found. This factory function receives no arguments, so you can use built-in type constructors such as list() or int() . For example, you can make an index of words based on their initial letter like this:\nwords = \"\"\"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura che la diritta via era smarrita\"\"\".lower().split() index = defaultdict(list) for w in words: init_letter = w[0] index[init_letter].append(w)\nPrinting index results in the following output:\ndefaultdict(<type 'list'>, {'c': ['cammin', 'che'], 'e': ['era'], 'd': ['del', 'di', 'diritta'], 'm': ['mezzo', 'mi'], 'l': ['la'], 'o': ['oscura'], 'n': ['nel', 'nostra'], 'p': ['per'], 's': ['selva', 'smarrita'], 'r': ['ritrovai'], 'u': ['una'], 'v': ['vita', 'via']}\n(Contributed by Guido van Rossum.)\nThe deque double-ended queue type supplied by the collections module now has a remove(value)() method that removes the first occurrence of value in the queue, raising ValueError if the value isn\u2019t found. (Contributed by Raymond Hettinger.)\nNew module: The contextlib module contains helper functions for use with the new \u2018 with \u2018 statement. See section The contextlib module for more about this module.\nNew module: The cProfile module is a C implementation of the existing profile module that has much lower overhead. The module\u2019s interface is the same as profile : you run cProfile.run('main()') to profile a function, can save profile data to a file, etc. It\u2019s not yet known if the Hotshot profiler, which is also written in C but doesn\u2019t match the profile module\u2019s interface, will continue to be maintained in future versions of Python. (Contributed by Armin Rigo.)\nAlso, the pstats module for analyzing the data measured by the profiler now supports directing the output to any file object by supplying a stream argument to the Stats constructor. (Contributed by Skip Montanaro.)\nThe csv module, which parses files in comma-separated value format, received several enhancements and a number of bugfixes. You can now set the maximum size in bytes of a field by calling the csv.field_size_limit(new_limit)() function; omitting the new_limit argument will return the currently-set limit. The reader class now has a line_num attribute that counts the number of physical lines read from the source; records can span multiple physical lines, so line_num is not the same as the number of records read.\nThe CSV parser is now stricter about multi-line quoted fields. Previously, if a line ended within a quoted field without a terminating newline character, a newline would be inserted into the returned field. This behavior caused problems when reading files that contained carriage return characters within fields, so the code was changed to return the field without inserting newlines. As a consequence, if newlines embedded within fields are important, the input should be split into lines in a manner that preserves the newline characters.\n(Contributed by Skip Montanaro and Andrew McNamara.)\nThe datetime class in the datetime module now has a strptime(string, format)() method for parsing date strings, contributed by Josh Spoerri. It uses the same format characters as time.strptime() and time.strftime() :\nfrom datetime import datetime ts = datetime.strptime('10:13:15 2006-03-07', '%H:%M:%S %Y-%m-%d')\nThe SequenceMatcher.get_matching_blocks() method in the difflib module now guarantees to return a minimal list of blocks describing matching subsequences. Previously, the algorithm would occasionally break a block of matching elements into two list entries. (Enhancement by Tim Peters.)\nThe doctest module gained a SKIP option that keeps an example from being executed at all. This is intended for code snippets that are usage examples intended for the reader and aren\u2019t actually test cases.\nAn encoding parameter was added to the testfile() function and the DocFileSuite class to specify the file\u2019s encoding. This makes it easier to use non-ASCII characters in tests contained within a docstring. (Contributed by Bjorn Tillenius.)\nThe email package has been updated to version 4.0. (Contributed by Barry Warsaw.)\nThe fileinput module was made more flexible. Unicode filenames are now supported, and a mode parameter that defaults to \"r\" was added to the input() function to allow opening files in binary or universal-newline mode. Another new parameter, openhook, lets you use a function other than open() to open the input files. Once you\u2019re iterating over the set of files, the FileInput object\u2019s new fileno() returns the file descriptor for the currently opened file. (Contributed by Georg Brandl.)\nIn the gc module, the new get_count() function returns a 3-tuple containing the current collection counts for the three GC generations. This is accounting information for the garbage collector; when these counts reach a specified threshold, a garbage collection sweep will be made. The existing gc.collect() function now takes an optional generation argument of 0, 1, or 2 to specify which generation to collect. (Contributed by Barry Warsaw.)\nThe nsmallest() and nlargest() functions in the heapq module now support a key keyword parameter similar to the one provided by the min() / max() functions and the sort() methods. For example:\n>>> import heapq >>> L = [\"short\", 'medium', 'longest', 'longer still'] >>> heapq.nsmallest(2, L) # Return two lowest elements, lexicographically ['longer still', 'longest'] >>> heapq.nsmallest(2, L, key=len) # Return two shortest elements ['short', 'medium']\n(Contributed by Raymond Hettinger.)\nThe itertools.islice() function now accepts None for the start and step arguments. This makes it more compatible with the attributes of slice objects, so that you can now write the following:\ns = slice(5) # Create slice object itertools.islice(iterable, s.start, s.stop, s.step)\n(Contributed by Raymond Hettinger.)\nThe format() function in the locale module has been modified and two new functions were added, format_string() and currency().\nThe format() function\u2019s val parameter could previously be a string as long as no more than one %char specifier appeared; now the parameter must be exactly one %char specifier with no surrounding text. An optional monetary parameter was also added which, if True, will use the locale\u2019s rules for formatting currency in placing a separator between groups of three digits.\nTo format strings with multiple %char specifiers, use the new format_string() function that works like format() but also supports mixing %char specifiers with arbitrary text.\nA new currency() function was also added that formats a number according to the current locale\u2019s settings.\n(Contributed by Georg Brandl.)\nThe mailbox module underwent a massive rewrite to add the capability to modify mailboxes in addition to reading them. A new set of classes that include mbox, MH, and Maildir are used to read mailboxes, and have an add(message)() method to add messages, remove(key)() to remove messages, and lock()/unlock() to lock/unlock the mailbox. The following example converts a maildir-format mailbox into an mbox-format one:\nimport mailbox # 'factory=None' uses email.Message.Message as the class representing # individual messages. src = mailbox.Maildir('maildir', factory=None) dest = mailbox.mbox('/tmp/mbox') for msg in src: dest.add(msg)\n(Contributed by Gregory K. Johnson. Funding was provided by Google\u2019s 2005 Summer of Code.)\nNew module: the msilib module allows creating Microsoft Installer .msi files and CAB files. Some support for reading the .msi database is also included. (Contributed by Martin von L\u00f6wis.)\nThe nis module now supports accessing domains other than the system default domain by supplying a domain argument to the nis.match() and nis.maps() functions. (Contributed by Ben Bell.)\nThe operator module\u2019s itemgetter() and attrgetter() functions now support multiple fields. A call such as operator.attrgetter('a', 'b') will return a function that retrieves the a and b attributes. Combining this new feature with the sort() method\u2019s key parameter lets you easily sort lists using multiple fields. (Contributed by Raymond Hettinger.)\nThe optparse module was updated to version 1.5.1 of the Optik library. The OptionParser class gained an epilog attribute, a string that will be printed after the help message, and a destroy() method to break reference cycles created by the object. (Contributed by Greg Ward.)\nThe os module underwent several changes. The stat_float_times variable now defaults to true, meaning that os.stat() will now return time values as floats. (This doesn\u2019t necessarily mean that os.stat() will return times that are precise to fractions of a second; not all systems support such precision.)\nConstants named os.SEEK_SET , os.SEEK_CUR , and os.SEEK_END have been added; these are the parameters to the os.lseek() function. Two new constants for locking are os.O_SHLOCK and os.O_EXLOCK .\nTwo new functions, wait3() and wait4(), were added. They\u2019re similar the waitpid() function which waits for a child process to exit and returns a tuple of the process ID and its exit status, but wait3() and wait4() return additional information. wait3() doesn\u2019t take a process ID as input, so it waits for any child process to exit and returns a 3-tuple of process-id, exit-status, resource-usage as returned from the resource.getrusage() function. wait4(pid)() does take a process ID. (Contributed by Chad J. Schroeder.)\nOn FreeBSD, the os.stat() function now returns times with nanosecond resolution, and the returned object now has st_gen and st_birthtime. The st_flags attribute is also available, if the platform supports it. (Contributed by Antti Louko and Diego Petten\u00f2.)\nThe Python debugger provided by the pdb module can now store lists of commands to execute when a breakpoint is reached and execution stops. Once breakpoint #1 has been created, enter commands 1 and enter a series of commands to be executed, finishing the list with end. The command list can include commands that resume execution, such as continue or next. (Contributed by Gr\u00e9goire Dooms.)\nThe pickle and cPickle modules no longer accept a return value of None from the __reduce__() method; the method must return a tuple of arguments instead. The ability to return None was deprecated in Python 2.4, so this completes the removal of the feature.\nThe pkgutil module, containing various utility functions for finding packages, was enhanced to support PEP 302\u2019s import hooks and now also works for packages stored in ZIP-format archives. (Contributed by Phillip J. Eby.)\nThe pybench benchmark suite by Marc-Andr\u00e9 Lemburg is now included in the Tools/pybench directory. The pybench suite is an improvement on the commonly used pystone.py program because pybench provides a more detailed measurement of the interpreter\u2019s speed. It times particular operations such as function calls, tuple slicing, method lookups, and numeric operations, instead of performing many different operations and reducing the result to a single number as pystone.py does.\nThe pyexpat module now uses version 2.0 of the Expat parser. (Contributed by Trent Mick.)\nThe Queue class provided by the Queue module gained two new methods. join() blocks until all items in the queue have been retrieved and all processing work on the items have been completed. Worker threads call the other new method, task_done(), to signal that processing for an item has been completed. (Contributed by Raymond Hettinger.)\nThe old regex and regsub modules, which have been deprecated ever since Python 2.0, have finally been deleted. Other deleted modules: statcache, tzparse, whrandom.\nAlso deleted: the lib-old directory, which includes ancient modules such as dircmp and ni, was removed. lib-old wasn\u2019t on the default sys.path, so unless your programs explicitly added the directory to sys.path, this removal shouldn\u2019t affect your code.\nThe rlcompleter module is no longer dependent on importing the readline module and therefore now works on non-Unix platforms. (Patch from Robert Kiendl.)\nThe SimpleXMLRPCServer and DocXMLRPCServer classes now have a rpc_paths attribute that constrains XML-RPC operations to a limited set of URL paths; the default is to allow only '/' and '/RPC2'. Setting rpc_paths to None or an empty tuple disables this path checking.\nThe socket module now supports AF_NETLINK sockets on Linux, thanks to a patch from Philippe Biondi. Netlink sockets are a Linux-specific mechanism for communications between a user-space process and kernel code; an introductory article about them is at http://www.linuxjournal.com/article/7356 . In Python code, netlink addresses are represented as a tuple of 2 integers, (pid, group_mask).\nTwo new methods on socket objects, recv_into(buffer)() and recvfrom_into(buffer)(), store the received data in an object that supports the buffer protocol instead of returning the data as a string. This means you can put the data directly into an array or a memory-mapped file.\nSocket objects also gained getfamily(), gettype(), and getproto() accessor methods to retrieve the family, type, and protocol values for the socket.\nNew module: the spwd module provides functions for accessing the shadow password database on systems that support shadow passwords.\nThe struct is now faster because it compiles format strings into Struct objects with pack() and unpack() methods. This is similar to how the re module lets you create compiled regular expression objects. You can still use the module-level pack() and unpack() functions; they\u2019ll create Struct objects and cache them. Or you can use Struct instances directly:\ns = struct.Struct('ih3s') data = s.pack(1972, 187, 'abc') year, number, name = s.unpack(data)\nYou can also pack and unpack data to and from buffer objects directly using the pack_into(buffer, offset, v1, v2, ...)() and unpack_from(buffer, offset)() methods. This lets you store data directly into an array or a memory- mapped file.\n(Struct objects were implemented by Bob Ippolito at the NeedForSpeed sprint. Support for buffer objects was added by Martin Blais, also at the NeedForSpeed sprint.)\nThe Python developers switched from CVS to Subversion during the 2.5 development process. Information about the exact build version is available as the sys.subversion variable, a 3-tuple of (interpreter-name, branch-name, revision-range). For example, at the time of writing my copy of 2.5 was reporting ('CPython', 'trunk', '45313:45315').\nThis information is also available to C extensions via the Py_GetBuildInfo() function that returns a string of build information like this: \"trunk:45355:45356M, Apr 13 2006, 07:42:19\". (Contributed by Barry Warsaw.)\nAnother new function, sys._current_frames() , returns the current stack frames for all running threads as a dictionary mapping thread identifiers to the topmost stack frame currently active in that thread at the time the function is called. (Contributed by Tim Peters.)\nThe TarFile class in the tarfile module now has an extractall() method that extracts all members from the archive into the current working directory. It\u2019s also possible to set a different directory as the extraction target, and to unpack only a subset of the archive\u2019s members.\nThe compression used for a tarfile opened in stream mode can now be autodetected using the mode 'r|*'. (Contributed by Lars Gust\u00e4bel.)\nThe threading module now lets you set the stack size used when new threads are created. The stack_size([*size*])() function returns the currently configured stack size, and supplying the optional size parameter sets a new value. Not all platforms support changing the stack size, but Windows, POSIX threading, and OS/2 all do. (Contributed by Andrew MacIntyre.)\nThe unicodedata module has been updated to use version 4.1.0 of the Unicode character database. Version 3.2.0 is required by some specifications, so it\u2019s still available as unicodedata.ucd_3_2_0 .\nNew module: the uuid module generates universally unique identifiers (UUIDs) according to RFC 4122 . The RFC defines several different UUID versions that are generated from a starting string, from system properties, or purely randomly. This module contains a UUID class and functions named uuid1(), uuid3(), uuid4(), and uuid5() to generate different versions of UUID. (Version 2 UUIDs are not specified in RFC 4122 and are not supported by this module.)\n>>> import uuid >>> # make a UUID based on the host ID and current time >>> uuid.uuid1() UUID('a8098c1a-f86e-11da-bd1a-00112444be1e') >>> # make a UUID using an MD5 hash of a namespace UUID and a name >>> uuid.uuid3(uuid.NAMESPACE_DNS, 'python.org') UUID('6fa459ea-ee8a-3ca4-894e-db77e160355e') >>> # make a random UUID >>> uuid.uuid4() UUID('16fd2706-8baf-433b-82eb-8c7fada847da') >>> # make a UUID using a SHA-1 hash of a namespace UUID and a name >>> uuid.uuid5(uuid.NAMESPACE_DNS, 'python.org') UUID('886313e1-3b8a-5372-9b90-0c9aee199e5d')\n(Contributed by Ka-Ping Yee.)\nThe weakref module\u2019s WeakKeyDictionary and WeakValueDictionary types gained new methods for iterating over the weak references contained in the dictionary. iterkeyrefs() and keyrefs() methods were added to WeakKeyDictionary, and itervaluerefs() and valuerefs() were added to WeakValueDictionary. (Contributed by Fred L. Drake, Jr.)\nThe webbrowser module received a number of enhancements. It\u2019s now usable as a script with python -m webbrowser, taking a URL as the argument; there are a number of switches to control the behaviour (-n for a new browser window, -t for a new tab). New module-level functions, open_new() and open_new_tab(), were added to support this. The module\u2019s open() function supports an additional feature, an autoraise parameter that signals whether to raise the open window when possible. A number of additional browsers were added to the supported list such as Firefox, Opera, Konqueror, and elinks. (Contributed by Oleg Broytmann and Georg Brandl.)\nThe xmlrpclib module now supports returning datetime objects for the XML-RPC date type. Supply use_datetime=True to the loads() function or the Unmarshaller class to enable this feature. (Contributed by Skip Montanaro.)\nThe zipfile module now supports the ZIP64 version of the format, meaning that a .zip archive can now be larger than 4 GiB and can contain individual files larger than 4 GiB. (Contributed by Ronald Oussoren.)\nThe zlib module\u2019s Compress and Decompress objects now support a copy() method that makes a copy of the object\u2019s internal state and returns a new Compress or Decompress object. (Contributed by Chris AtLee.)\nThe ctypes package \u00b6\nThe ctypes package, written by Thomas Heller, has been added to the standard library. ctypes lets you call arbitrary functions in shared libraries or DLLs. Long-time users may remember the dl module, which provides functions for loading shared libraries and calling functions in them. The ctypes package is much fancier.\nTo load a shared library or DLL, you must create an instance of the CDLL class and provide the name or path of the shared library or DLL. Once that\u2019s done, you can call arbitrary functions by accessing them as attributes of the CDLL object.\nimport ctypes libc = ctypes.CDLL('libc.so.6') result = libc.printf(\"Line of output\\n\")\nType constructors for the various C types are provided: c_int(), c_float(), c_double(), c_char_p() (equivalent to char *), and so forth. Unlike Python\u2019s types, the C versions are all mutable; you can assign to their value attribute to change the wrapped value. Python integers and strings will be automatically converted to the corresponding C types, but for other types you must call the correct type constructor. (And I mean must; getting it wrong will often result in the interpreter crashing with a segmentation fault.)\nYou shouldn\u2019t use c_char_p() with a Python string when the C function will be modifying the memory area, because Python strings are supposed to be immutable; breaking this rule will cause puzzling bugs. When you need a modifiable memory area, use create_string_buffer():\ns = \"this is a string\" buf = ctypes.create_string_buffer(s) libc.strfry(buf)\nC functions are assumed to return integers, but you can set the restype attribute of the function object to change this:\n>>> libc.atof('2.71828') -1783957616 >>> libc.atof.restype = ctypes.c_double >>> libc.atof('2.71828') 2.71828\nctypes also provides a wrapper for Python\u2019s C API as the ctypes.pythonapi object. This object does not release the global interpreter lock before calling a function, because the lock must be held when calling into the interpreter\u2019s code. There\u2019s a py_object() type constructor that will create a PyObject * pointer. A simple usage:\nimport ctypes d = {} ctypes.pythonapi.PyObject_SetItem(ctypes.py_object(d), ctypes.py_object(\"abc\"), ctypes.py_object(1)) # d is now {'abc', 1}.\nDon\u2019t forget to use py_object(); if it\u2019s omitted you end up with a segmentation fault.\nctypes has been around for a while, but people still write and distribution hand-coded extension modules because you can\u2019t rely on ctypes being present. Perhaps developers will begin to write Python wrappers atop a library accessed through ctypes instead of extension modules, now that ctypes is included with core Python.\nSee also\nThe ctypes web page, with a tutorial, reference, and FAQ.\nThe documentation for the ctypes module.\nThe ElementTree package \u00b6\nA subset of Fredrik Lundh\u2019s ElementTree library for processing XML has been added to the standard library as xml.etree. The available modules are ElementTree, ElementPath, and ElementInclude from ElementTree 1.2.6. The cElementTree accelerator module is also included.\nThe rest of this section will provide a brief overview of using ElementTree. Full documentation for ElementTree is available at http://effbot.org/zone/element-index.htm .\nElementTree represents an XML document as a tree of element nodes. The text content of the document is stored as the text and tail attributes of (This is one of the major differences between ElementTree and the Document Object Model; in the DOM there are many different types of node, including TextNode.)\nThe most commonly used parsing function is parse(), that takes either a string (assumed to contain a filename) or a file-like object and returns an ElementTree instance:\nfrom xml.etree import ElementTree as ET tree = ET.parse('ex-1.xml') feed = urllib.urlopen( 'http://planet.python.org/rss10.xml') tree = ET.parse(feed)\nOnce you have an ElementTree instance, you can call its getroot() method to get the root Element node.\nThere\u2019s also an XML() function that takes a string literal and returns an Element node (not an ElementTree). This function provides a tidy way to incorporate XML fragments, approaching the convenience of an XML literal:\nsvg = ET.XML(\"\"\"<svg width=\"10px\" version=\"1.0\"> </svg>\"\"\") svg.set('height', '320px') svg.append(elem1)\nEach XML element supports some dictionary-like and some list-like access methods. Dictionary-like operations are used to access attribute values, and list-like operations are used to access child nodes.\nOperation\n"}, {"score": 915.1693, "uuid": "67663b9c-3f82-5668-8ad0-04c732e4c9fa", "index": "cw12", "trec_id": "clueweb12-0011wb-70-00891", "target_hostname": "wingide.com", "target_uri": "http://wingide.com/psupport/python-manual/3.2/whatsnew/2.5.html", "page_rank": 1.2017385e-09, "spam_rank": 69, "title": "What\u2019s New in <em>Python</em> 2.5 \u2014 <em>Python</em> v3.2.1 documentation", "snippet": "The decision was made to not require parentheses in the <em>Python</em> language\u2019s grammar, but as a matter of style <em>I</em> think you <em>should</em> always use them.", "explanation": null, "document": "An explanation of coroutines from a Perl point of view, written by Dan Sugalski.\nPEP 343: The \u2018with\u2019 statement \u00b6\nThe \u2018 with \u2018 statement clarifies code that previously would use try...finally blocks to ensure that clean-up code is executed. In this section, I\u2019ll discuss the statement as it will commonly be used. In the next section, I\u2019ll examine the implementation details and show how to write objects for use with this statement.\nThe \u2018 with \u2018 statement is a new control-flow structure whose basic structure is:\nwith expression [as variable]: with-block\nThe expression is evaluated, and it should result in an object that supports the context management protocol (that is, has __enter__() and __exit__() methods.\nThe object\u2019s __enter__() is called before with-block is executed and therefore can run set-up code. It also may return a value that is bound to the name variable, if given. (Note carefully that variable is not assigned the result of expression.)\nAfter execution of the with-block is finished, the object\u2019s __exit__() method is called, even if the block raised an exception, and can therefore run clean-up code.\nTo enable the statement in Python 2.5, you need to add the following directive to your module:\nfrom __future__ import with_statement\nThe statement will always be enabled in Python 2.6.\nSome standard Python objects now support the context management protocol and can be used with the \u2018 with \u2018 statement. File objects are one example:\nwith open('/etc/passwd', 'r') as f: for line in f: print line ... more processing code ...\nAfter this statement has executed, the file object in f will have been automatically closed, even if the for loop raised an exception part- way through the block.\nNote\nIn this case, f is the same object created by open() , because file.__enter__() returns self.\nThe threading module\u2019s locks and condition variables also support the \u2018 with \u2018 statement:\nlock = threading.Lock() with lock: # Critical section of code ...\nThe lock is acquired before the block is executed and always released once the block is complete.\nThe new localcontext() function in the decimal module makes it easy to save and restore the current decimal context, which encapsulates the desired precision and rounding characteristics for computations:\nfrom decimal import Decimal, Context, localcontext # Displays with default precision of 28 digits v = Decimal('578') print v.sqrt() with localcontext(Context(prec=16)): # All code in this block uses a precision of 16 digits. # The original context is restored on exiting the block. print v.sqrt()\nWriting Context Managers \u00b6\nUnder the hood, the \u2018 with \u2018 statement is fairly complicated. Most people will only use \u2018 with \u2018 in company with existing objects and don\u2019t need to know these details, so you can skip the rest of this section if you like. Authors of new objects will need to understand the details of the underlying implementation and should keep reading.\nA high-level explanation of the context management protocol is:\nThe expression is evaluated and should result in an object called a \u201ccontext manager\u201d. The context manager must have __enter__() and __exit__() methods.\nThe context manager\u2019s __enter__() method is called. The value returned is assigned to VAR. If no 'as VAR' clause is present, the value is simply discarded.\nThe code in BLOCK is executed.\nIf BLOCK raises an exception, the __exit__(type, value, traceback)() is called with the exception details, the same values returned by sys.exc_info() . The method\u2019s return value controls whether the exception is re-raised: any false value re-raises the exception, and True will result in suppressing it. You\u2019ll only rarely want to suppress the exception, because if you do the author of the code containing the \u2018 with \u2018 statement will never realize anything went wrong.\nIf BLOCK didn\u2019t raise an exception, the __exit__() method is still called, but type, value, and traceback are all None.\nLet\u2019s think through an example. I won\u2019t present detailed code but will only sketch the methods necessary for a database that supports transactions.\n(For people unfamiliar with database terminology: a set of changes to the database are grouped into a transaction. Transactions can be either committed, meaning that all the changes are written into the database, or rolled back, meaning that the changes are all discarded and the database is unchanged. See any database textbook for more information.)\nLet\u2019s assume there\u2019s an object representing a database connection. Our goal will be to let the user write code like this:\ndb_connection = DatabaseConnection() with db_connection as cursor: cursor.execute('insert into ...') cursor.execute('delete from ...') # ... more operations ...\nThe transaction should be committed if the code in the block runs flawlessly or rolled back if there\u2019s an exception. Here\u2019s the basic interface for DatabaseConnection that I\u2019ll assume:\nclass DatabaseConnection: # Database interface def cursor (self): \"Returns a cursor object and starts a new transaction\" def commit (self): \"Commits current transaction\" def rollback (self): \"Rolls back current transaction\"\nThe __enter__() method is pretty easy, having only to start a new transaction. For this application the resulting cursor object would be a useful result, so the method will return it. The user can then add as cursor to their \u2018 with \u2018 statement to bind the cursor to a variable name.\nclass DatabaseConnection: ... def __enter__ (self): # Code to start a new transaction cursor = self.cursor() return cursor\nThe __exit__() method is the most complicated because it\u2019s where most of the work has to be done. The method has to check if an exception occurred. If there was no exception, the transaction is committed. The transaction is rolled back if there was an exception.\nIn the code below, execution will just fall off the end of the function, returning the default value of None. None is false, so the exception will be re-raised automatically. If you wished, you could be more explicit and add a return statement at the marked location.\nclass DatabaseConnection: ... def __exit__ (self, type, value, tb): if tb is None: # No exception, so commit self.commit() else: # Exception occurred, so rollback. self.rollback() # return False\nThe contextlib module \u00b6\nThe new contextlib module provides some functions and a decorator that are useful for writing objects for use with the \u2018 with \u2018 statement.\nThe decorator is called contextmanager(), and lets you write a single generator function instead of defining a new class. The generator should yield exactly one value. The code up to the yield will be executed as the __enter__() method, and the value yielded will be the method\u2019s return value that will get bound to the variable in the \u2018 with \u2018 statement\u2019s as clause, if any. The code after the yield will be executed in the __exit__() method. Any exception raised in the block will be raised by the yield statement.\nOur database example from the previous section could be written using this decorator as:\nfrom contextlib import contextmanager @contextmanager def db_transaction (connection): cursor = connection.cursor() try: yield cursor except: connection.rollback() raise else: connection.commit() db = DatabaseConnection() with db_transaction(db) as cursor: ...\nThe contextlib module also has a nested(mgr1, mgr2, ...)() function that combines a number of context managers so you don\u2019t need to write nested \u2018 with \u2018 statements. In this example, the single \u2018 with \u2018 statement both starts a database transaction and acquires a thread lock:\nlock = threading.Lock() with nested (db_transaction(db), lock) as (cursor, locked): ...\nFinally, the closing(object)() function returns object so that it can be bound to a variable, and calls object.close at the end of the block.\nimport urllib, sys from contextlib import closing with closing(urllib.urlopen('http://www.yahoo.com')) as f: for line in f: sys.stdout.write(line)\nSee also\nPEP 343 - The \u201cwith\u201d statement\nPEP written by Guido van Rossum and Nick Coghlan; implemented by Mike Bland, Guido van Rossum, and Neal Norwitz. The PEP shows the code generated for a \u2018 with \u2018 statement, which can be helpful in learning how the statement works.\nThe documentation for the contextlib module.\nPEP 352: Exceptions as New-Style Classes \u00b6\nException classes can now be new-style classes, not just classic classes, and the built-in Exception class and all the standard built-in exceptions ( NameError , ValueError , etc.) are now new-style classes.\nThe inheritance hierarchy for exceptions has been rearranged a bit. In 2.5, the inheritance relationships are:\nBaseException # New in Python 2.5 |- KeyboardInterrupt |- SystemExit |- Exception |- (all other current built-in exceptions)\nThis rearrangement was done because people often want to catch all exceptions that indicate program errors. KeyboardInterrupt and SystemExit aren\u2019t errors, though, and usually represent an explicit action such as the user hitting Control-C or code calling sys.exit() . A bare except: will catch all exceptions, so you commonly need to list KeyboardInterrupt and SystemExit in order to re-raise them. The usual pattern is:\ntry: ... except (KeyboardInterrupt, SystemExit): raise except: # Log error... # Continue running program...\nIn Python 2.5, you can now write except Exception to achieve the same result, catching all the exceptions that usually indicate errors but leaving KeyboardInterrupt and SystemExit alone. As in previous versions, a bare except: still catches all exceptions.\nThe goal for Python 3.0 is to require any class raised as an exception to derive from BaseException or some descendant of BaseException , and future releases in the Python 2.x series may begin to enforce this constraint. Therefore, I suggest you begin making all your exception classes derive from Exception now. It\u2019s been suggested that the bare except: form should be removed in Python 3.0, but Guido van Rossum hasn\u2019t decided whether to do this or not.\nRaising of strings as exceptions, as in the statement raise \"Error occurred\", is deprecated in Python 2.5 and will trigger a warning. The aim is to be able to remove the string-exception feature in a few releases.\nSee also\nPEP 352 - Required Superclass for Exceptions\nPEP written by Brett Cannon and Guido van Rossum; implemented by Brett Cannon.\nPEP 353: Using ssize_t as the index type \u00b6\nA wide-ranging change to Python\u2019s C API, using a new Py_ssize_t type definition instead of int, will permit the interpreter to handle more data on 64-bit platforms. This change doesn\u2019t affect Python\u2019s capacity on 32-bit platforms.\nVarious pieces of the Python interpreter used C\u2019s int type to store sizes or counts; for example, the number of items in a list or tuple were stored in an int. The C compilers for most 64-bit platforms still define int as a 32-bit type, so that meant that lists could only hold up to 2**31 - 1 = 2147483647 items. (There are actually a few different programming models that 64-bit C compilers can use \u2013 see http://www.unix.org/version2/whatsnew/lp64_wp.html for a discussion \u2013 but the most commonly available model leaves int as 32 bits.)\nA limit of 2147483647 items doesn\u2019t really matter on a 32-bit platform because you\u2019ll run out of memory before hitting the length limit. Each list item requires space for a pointer, which is 4 bytes, plus space for a PyObject representing the item. 2147483647*4 is already more bytes than a 32-bit address space can contain.\nIt\u2019s possible to address that much memory on a 64-bit platform, however. The pointers for a list that size would only require 16 GiB of space, so it\u2019s not unreasonable that Python programmers might construct lists that large. Therefore, the Python interpreter had to be changed to use some type other than int, and this will be a 64-bit type on 64-bit platforms. The change will cause incompatibilities on 64-bit machines, so it was deemed worth making the transition now, while the number of 64-bit users is still relatively small. (In 5 or 10 years, we may all be on 64-bit machines, and the transition would be more painful then.)\nThis change most strongly affects authors of C extension modules. Python strings and container types such as lists and tuples now use Py_ssize_t to store their size. Functions such as PyList_Size() now return Py_ssize_t. Code in extension modules may therefore need to have some variables changed to Py_ssize_t.\nThe PyArg_ParseTuple() and Py_BuildValue() functions have a new conversion code, n, for Py_ssize_t. PyArg_ParseTuple() \u2018s s# and t# still output int by default, but you can define the macro PY_SSIZE_T_CLEAN before including Python.h to make them return Py_ssize_t.\nPEP 353 has a section on conversion guidelines that extension authors should read to learn about supporting 64-bit platforms.\nSee also\nPEP 353 - Using ssize_t as the index type\nPEP written and implemented by Martin von L\u00f6wis.\nPEP 357: The \u2018__index__\u2019 method \u00b6\nThe NumPy developers had a problem that could only be solved by adding a new special method, __index__() . When using slice notation, as in [start:stop:step], the values of the start, stop, and step indexes must all be either integers or long integers. NumPy defines a variety of specialized integer types corresponding to unsigned and signed integers of 8, 16, 32, and 64 bits, but there was no way to signal that these types could be used as slice indexes.\nSlicing can\u2019t just use the existing __int__() method because that method is also used to implement coercion to integers. If slicing used __int__() , floating-point numbers would also become legal slice indexes and that\u2019s clearly an undesirable behaviour.\nInstead, a new special method called __index__() was added. It takes no arguments and returns an integer giving the slice index to use. For example:\nclass C: def __index__ (self): return self.value\nThe return value must be either a Python integer or long integer. The interpreter will check that the type returned is correct, and raises a TypeError if this requirement isn\u2019t met.\nA corresponding nb_index slot was added to the C-level PyNumberMethods structure to let C extensions implement this protocol. PyNumber_Index(obj)() can be used in extension code to call the __index__() function and retrieve its result.\nSee also\nPEP 357 - Allowing Any Object to be Used for Slicing\nPEP written and implemented by Travis Oliphant.\nOther Language Changes \u00b6\nHere are all of the changes that Python 2.5 makes to the core Python language.\nThe dict type has a new hook for letting subclasses provide a default value when a key isn\u2019t contained in the dictionary. When a key isn\u2019t found, the dictionary\u2019s __missing__(key)() method will be called. This hook is used to implement the new defaultdict class in the collections module. The following example defines a dictionary that returns zero for any missing key:\nclass zerodict (dict): def __missing__ (self, key): return 0 d = zerodict({1:1, 2:2}) print d[1], d[2] # Prints 1, 2 print d[3], d[4] # Prints 0, 0\nBoth 8-bit and Unicode strings have new partition(sep)() and rpartition(sep)() methods that simplify a common use case.\nThe find(S)() method is often used to get an index which is then used to slice the string and obtain the pieces that are before and after the separator. partition(sep)() condenses this pattern into a single method call that returns a 3-tuple containing the substring before the separator, the separator itself, and the substring after the separator. If the separator isn\u2019t found, the first element of the tuple is the entire string and the other two elements are empty. rpartition(sep)() also returns a 3-tuple but starts searching from the end of the string; the r stands for \u2018reverse\u2019.\nSome examples:\n>>> ('http://www.python.org').partition('://') ('http', '://', 'www.python.org') >>> ('file:/usr/share/doc/index.html').partition('://') ('file:/usr/share/doc/index.html', '', '') >>> (u'Subject: a quick question').partition(':') (u'Subject', u':', u' a quick question') >>> 'www.python.org'.rpartition('.') ('www.python', '.', 'org') >>> 'www.python.org'.rpartition(':') ('', '', 'www.python.org')\n(Implemented by Fredrik Lundh following a suggestion by Raymond Hettinger.)\nThe startswith() and endswith() methods of string types now accept tuples of strings to check for.\ndef is_image_file (filename): return filename.endswith(('.gif', '.jpg', '.tiff'))\n(Implemented by Georg Brandl following a suggestion by Tom Lynn.)\nThe min() and max() built-in functions gained a key keyword parameter analogous to the key argument for sort(). This parameter supplies a function that takes a single argument and is called for every value in the list; min() / max() will return the element with the smallest/largest return value from this function. For example, to find the longest string in a list, you can do:\nL = ['medium', 'longest', 'short'] # Prints 'longest' print max(L, key=len) # Prints 'short', because lexicographically 'short' has the largest value print max(L)\n(Contributed by Steven Bethard and Raymond Hettinger.)\nTwo new built-in functions, any() and all() , evaluate whether an iterator contains any true or false values. any() returns True if any value returned by the iterator is true; otherwise it will return False . all() returns True only if all of the values returned by the iterator evaluate as true. (Suggested by Guido van Rossum, and implemented by Raymond Hettinger.)\nThe result of a class\u2019s __hash__() method can now be either a long integer or a regular integer. If a long integer is returned, the hash of that value is taken. In earlier versions the hash value was required to be a regular integer, but in 2.5 the id() built-in was changed to always return non-negative numbers, and users often seem to use id(self) in __hash__() methods (though this is discouraged).\nASCII is now the default encoding for modules. It\u2019s now a syntax error if a module contains string literals with 8-bit characters but doesn\u2019t have an encoding declaration. In Python 2.4 this triggered a warning, not a syntax error. See PEP 263 for how to declare a module\u2019s encoding; for example, you might add a line like this near the top of the source file:\n# -*- coding: latin1 -*-\nA new warning, UnicodeWarning , is triggered when you attempt to compare a Unicode string and an 8-bit string that can\u2019t be converted to Unicode using the default ASCII encoding. The result of the comparison is false:\n>>> chr(128) == unichr(128) # Can't convert chr(128) to Unicode __main__:1: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal False >>> chr(127) == unichr(127) # chr(127) can be converted True\nPreviously this would raise a UnicodeDecodeError exception, but in 2.5 this could result in puzzling problems when accessing a dictionary. If you looked up unichr(128) and chr(128) was being used as a key, you\u2019d get a UnicodeDecodeError exception. Other changes in 2.5 resulted in this exception being raised instead of suppressed by the code in dictobject.c that implements dictionaries.\nRaising an exception for such a comparison is strictly correct, but the change might have broken code, so instead UnicodeWarning was introduced.\n(Implemented by Marc-Andr\u00e9 Lemburg.)\nOne error that Python programmers sometimes make is forgetting to include an __init__.py module in a package directory. Debugging this mistake can be confusing, and usually requires running Python with the -v switch to log all the paths searched. In Python 2.5, a new ImportWarning warning is triggered when an import would have picked up a directory as a package but no __init__.py was found. This warning is silently ignored by default; provide the -Wd option when running the Python executable to display the warning message. (Implemented by Thomas Wouters.)\nThe list of base classes in a class definition can now be empty. As an example, this is now legal:\nclass C(): pass\n(Implemented by Brett Cannon.)\nInteractive Interpreter Changes \u00b6\nIn the interactive interpreter, quit and exit have long been strings so that new users get a somewhat helpful message when they try to quit:\n>>> quit 'Use Ctrl-D (i.e. EOF) to exit.'\nIn Python 2.5, quit and exit are now objects that still produce string representations of themselves, but are also callable. Newbies who try quit() or exit() will now exit the interpreter as they expect. (Implemented by Georg Brandl.)\nThe Python executable now accepts the standard long options --help and --version ; on Windows, it also accepts the /? option for displaying a help message. (Implemented by Georg Brandl.)\nOptimizations \u00b6\nSeveral of the optimizations were developed at the NeedForSpeed sprint, an event held in Reykjavik, Iceland, from May 21\u201328 2006. The sprint focused on speed enhancements to the CPython implementation and was funded by EWT LLC with local support from CCP Games. Those optimizations added at this sprint are specially marked in the following list.\nWhen they were introduced in Python 2.4, the built-in set and frozenset types were built on top of Python\u2019s dictionary type. In 2.5 the internal data structure has been customized for implementing sets, and as a result sets will use a third less memory and are somewhat faster. (Implemented by Raymond Hettinger.)\nThe speed of some Unicode operations, such as finding substrings, string splitting, and character map encoding and decoding, has been improved. (Substring search and splitting improvements were added by Fredrik Lundh and Andrew Dalke at the NeedForSpeed sprint. Character maps were improved by Walter D\u00f6rwald and Martin von L\u00f6wis.)\nThe long(str, base)() function is now faster on long digit strings because fewer intermediate results are calculated. The peak is for strings of around 800\u20131000 digits where the function is 6 times faster. (Contributed by Alan McIntyre and committed at the NeedForSpeed sprint.)\nIt\u2019s now illegal to mix iterating over a file with for line in file and calling the file object\u2019s read()/ readline() /readlines() methods. Iteration uses an internal buffer and the read*() methods don\u2019t use that buffer. Instead they would return the data following the buffer, causing the data to appear out of order. Mixing iteration and these methods will now trigger a ValueError from the read*() method. (Implemented by Thomas Wouters.)\nThe struct module now compiles structure format strings into an internal representation and caches this representation, yielding a 20% speedup. (Contributed by Bob Ippolito at the NeedForSpeed sprint.)\nThe re module got a 1 or 2% speedup by switching to Python\u2019s allocator functions instead of the system\u2019s malloc() and free(). (Contributed by Jack Diederich at the NeedForSpeed sprint.)\nThe code generator\u2019s peephole optimizer now performs simple constant folding in expressions. If you write something like a = 2+3, the code generator will do the arithmetic and produce code corresponding to a = 5. (Proposed and implemented by Raymond Hettinger.)\nFunction calls are now faster because code objects now keep the most recently finished frame (a \u201czombie frame\u201d) in an internal field of the code object, reusing it the next time the code object is invoked. (Original patch by Michael Hudson, modified by Armin Rigo and Richard Jones; committed at the NeedForSpeed sprint.) Frame objects are also slightly smaller, which may improve cache locality and reduce memory usage a bit. (Contributed by Neal Norwitz.)\nPython\u2019s built-in exceptions are now new-style classes, a change that speeds up instantiation considerably. Exception handling in Python 2.5 is therefore about 30% faster than in 2.4. (Contributed by Richard Jones, Georg Brandl and Sean Reifschneider at the NeedForSpeed sprint.)\nImporting now caches the paths tried, recording whether they exist or not so that the interpreter makes fewer open() and stat() calls on startup. (Contributed by Martin von L\u00f6wis and Georg Brandl.)\nNew, Improved, and Removed Modules \u00b6\nThe standard library received many enhancements and bug fixes in Python 2.5. Here\u2019s a partial list of the most notable changes, sorted alphabetically by module name. Consult the Misc/NEWS file in the source tree for a more complete list of changes, or look through the SVN logs for all the details.\nThe audioop module now supports the a-LAW encoding, and the code for u-LAW encoding has been improved. (Contributed by Lars Immisch.)\nThe codecs module gained support for incremental codecs. The codec.lookup() function now returns a CodecInfo instance instead of a tuple. CodecInfo instances behave like a 4-tuple to preserve backward compatibility but also have the attributes encode, decode, incrementalencoder, incrementaldecoder, streamwriter, and streamreader. Incremental codecs can receive input and produce output in multiple chunks; the output is the same as if the entire input was fed to the non-incremental codec. See the codecs module documentation for details. (Designed and implemented by Walter D\u00f6rwald.)\nThe collections module gained a new type, defaultdict, that subclasses the standard dict type. The new type mostly behaves like a dictionary but constructs a default value when a key isn\u2019t present, automatically adding it to the dictionary for the requested key value.\nThe first argument to defaultdict\u2018s constructor is a factory function that gets called whenever a key is requested but not found. This factory function receives no arguments, so you can use built-in type constructors such as list() or int() . For example, you can make an index of words based on their initial letter like this:\nwords = \"\"\"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura che la diritta via era smarrita\"\"\".lower().split() index = defaultdict(list) for w in words: init_letter = w[0] index[init_letter].append(w)\nPrinting index results in the following output:\ndefaultdict(<type 'list'>, {'c': ['cammin', 'che'], 'e': ['era'], 'd': ['del', 'di', 'diritta'], 'm': ['mezzo', 'mi'], 'l': ['la'], 'o': ['oscura'], 'n': ['nel', 'nostra'], 'p': ['per'], 's': ['selva', 'smarrita'], 'r': ['ritrovai'], 'u': ['una'], 'v': ['vita', 'via']}\n(Contributed by Guido van Rossum.)\nThe deque double-ended queue type supplied by the collections module now has a remove(value)() method that removes the first occurrence of value in the queue, raising ValueError if the value isn\u2019t found. (Contributed by Raymond Hettinger.)\nNew module: The contextlib module contains helper functions for use with the new \u2018 with \u2018 statement. See section The contextlib module for more about this module.\nNew module: The cProfile module is a C implementation of the existing profile module that has much lower overhead. The module\u2019s interface is the same as profile : you run cProfile.run('main()') to profile a function, can save profile data to a file, etc. It\u2019s not yet known if the Hotshot profiler, which is also written in C but doesn\u2019t match the profile module\u2019s interface, will continue to be maintained in future versions of Python. (Contributed by Armin Rigo.)\nAlso, the pstats module for analyzing the data measured by the profiler now supports directing the output to any file object by supplying a stream argument to the Stats constructor. (Contributed by Skip Montanaro.)\nThe csv module, which parses files in comma-separated value format, received several enhancements and a number of bugfixes. You can now set the maximum size in bytes of a field by calling the csv.field_size_limit(new_limit)() function; omitting the new_limit argument will return the currently-set limit. The reader class now has a line_num attribute that counts the number of physical lines read from the source; records can span multiple physical lines, so line_num is not the same as the number of records read.\nThe CSV parser is now stricter about multi-line quoted fields. Previously, if a line ended within a quoted field without a terminating newline character, a newline would be inserted into the returned field. This behavior caused problems when reading files that contained carriage return characters within fields, so the code was changed to return the field without inserting newlines. As a consequence, if newlines embedded within fields are important, the input should be split into lines in a manner that preserves the newline characters.\n(Contributed by Skip Montanaro and Andrew McNamara.)\nThe datetime class in the datetime module now has a strptime(string, format)() method for parsing date strings, contributed by Josh Spoerri. It uses the same format characters as time.strptime() and time.strftime() :\nfrom datetime import datetime ts = datetime.strptime('10:13:15 2006-03-07', '%H:%M:%S %Y-%m-%d')\nThe SequenceMatcher.get_matching_blocks() method in the difflib module now guarantees to return a minimal list of blocks describing matching subsequences. Previously, the algorithm would occasionally break a block of matching elements into two list entries. (Enhancement by Tim Peters.)\nThe doctest module gained a SKIP option that keeps an example from being executed at all. This is intended for code snippets that are usage examples intended for the reader and aren\u2019t actually test cases.\nAn encoding parameter was added to the testfile() function and the DocFileSuite class to specify the file\u2019s encoding. This makes it easier to use non-ASCII characters in tests contained within a docstring. (Contributed by Bjorn Tillenius.)\nThe email package has been updated to version 4.0. (Contributed by Barry Warsaw.)\nThe fileinput module was made more flexible. Unicode filenames are now supported, and a mode parameter that defaults to \"r\" was added to the input() function to allow opening files in binary or universal-newline mode. Another new parameter, openhook, lets you use a function other than open() to open the input files. Once you\u2019re iterating over the set of files, the FileInput object\u2019s new fileno() returns the file descriptor for the currently opened file. (Contributed by Georg Brandl.)\nIn the gc module, the new get_count() function returns a 3-tuple containing the current collection counts for the three GC generations. This is accounting information for the garbage collector; when these counts reach a specified threshold, a garbage collection sweep will be made. The existing gc.collect() function now takes an optional generation argument of 0, 1, or 2 to specify which generation to collect. (Contributed by Barry Warsaw.)\nThe nsmallest() and nlargest() functions in the heapq module now support a key keyword parameter similar to the one provided by the min() / max() functions and the sort() methods. For example:\n>>> import heapq >>> L = [\"short\", 'medium', 'longest', 'longer still'] >>> heapq.nsmallest(2, L) # Return two lowest elements, lexicographically ['longer still', 'longest'] >>> heapq.nsmallest(2, L, key=len) # Return two shortest elements ['short', 'medium']\n(Contributed by Raymond Hettinger.)\nThe itertools.islice() function now accepts None for the start and step arguments. This makes it more compatible with the attributes of slice objects, so that you can now write the following:\ns = slice(5) # Create slice object itertools.islice(iterable, s.start, s.stop, s.step)\n(Contributed by Raymond Hettinger.)\nThe format() function in the locale module has been modified and two new functions were added, format_string() and currency().\nThe format() function\u2019s val parameter could previously be a string as long as no more than one %char specifier appeared; now the parameter must be exactly one %char specifier with no surrounding text. An optional monetary parameter was also added which, if True, will use the locale\u2019s rules for formatting currency in placing a separator between groups of three digits.\nTo format strings with multiple %char specifiers, use the new format_string() function that works like format() but also supports mixing %char specifiers with arbitrary text.\nA new currency() function was also added that formats a number according to the current locale\u2019s settings.\n(Contributed by Georg Brandl.)\nThe mailbox module underwent a massive rewrite to add the capability to modify mailboxes in addition to reading them. A new set of classes that include mbox, MH, and Maildir are used to read mailboxes, and have an add(message)() method to add messages, remove(key)() to remove messages, and lock()/unlock() to lock/unlock the mailbox. The following example converts a maildir-format mailbox into an mbox-format one:\nimport mailbox # 'factory=None' uses email.Message.Message as the class representing # individual messages. src = mailbox.Maildir('maildir', factory=None) dest = mailbox.mbox('/tmp/mbox') for msg in src: dest.add(msg)\n(Contributed by Gregory K. Johnson. Funding was provided by Google\u2019s 2005 Summer of Code.)\nNew module: the msilib module allows creating Microsoft Installer .msi files and CAB files. Some support for reading the .msi database is also included. (Contributed by Martin von L\u00f6wis.)\nThe nis module now supports accessing domains other than the system default domain by supplying a domain argument to the nis.match() and nis.maps() functions. (Contributed by Ben Bell.)\nThe operator module\u2019s itemgetter() and attrgetter() functions now support multiple fields. A call such as operator.attrgetter('a', 'b') will return a function that retrieves the a and b attributes. Combining this new feature with the sort() method\u2019s key parameter lets you easily sort lists using multiple fields. (Contributed by Raymond Hettinger.)\nThe optparse module was updated to version 1.5.1 of the Optik library. The OptionParser class gained an epilog attribute, a string that will be printed after the help message, and a destroy() method to break reference cycles created by the object. (Contributed by Greg Ward.)\nThe os module underwent several changes. The stat_float_times variable now defaults to true, meaning that os.stat() will now return time values as floats. (This doesn\u2019t necessarily mean that os.stat() will return times that are precise to fractions of a second; not all systems support such precision.)\nConstants named os.SEEK_SET , os.SEEK_CUR , and os.SEEK_END have been added; these are the parameters to the os.lseek() function. Two new constants for locking are os.O_SHLOCK and os.O_EXLOCK .\nTwo new functions, wait3() and wait4(), were added. They\u2019re similar the waitpid() function which waits for a child process to exit and returns a tuple of the process ID and its exit status, but wait3() and wait4() return additional information. wait3() doesn\u2019t take a process ID as input, so it waits for any child process to exit and returns a 3-tuple of process-id, exit-status, resource-usage as returned from the resource.getrusage() function. wait4(pid)() does take a process ID. (Contributed by Chad J. Schroeder.)\nOn FreeBSD, the os.stat() function now returns times with nanosecond resolution, and the returned object now has st_gen and st_birthtime. The st_flags attribute is also available, if the platform supports it. (Contributed by Antti Louko and Diego Petten\u00f2.)\nThe Python debugger provided by the pdb module can now store lists of commands to execute when a breakpoint is reached and execution stops. Once breakpoint #1 has been created, enter commands 1 and enter a series of commands to be executed, finishing the list with end. The command list can include commands that resume execution, such as continue or next. (Contributed by Gr\u00e9goire Dooms.)\nThe pickle and cPickle modules no longer accept a return value of None from the __reduce__() method; the method must return a tuple of arguments instead. The ability to return None was deprecated in Python 2.4, so this completes the removal of the feature.\nThe pkgutil module, containing various utility functions for finding packages, was enhanced to support PEP 302\u2019s import hooks and now also works for packages stored in ZIP-format archives. (Contributed by Phillip J. Eby.)\nThe pybench benchmark suite by Marc-Andr\u00e9 Lemburg is now included in the Tools/pybench directory. The pybench suite is an improvement on the commonly used pystone.py program because pybench provides a more detailed measurement of the interpreter\u2019s speed. It times particular operations such as function calls, tuple slicing, method lookups, and numeric operations, instead of performing many different operations and reducing the result to a single number as pystone.py does.\nThe pyexpat module now uses version 2.0 of the Expat parser. (Contributed by Trent Mick.)\nThe Queue class provided by the Queue module gained two new methods. join() blocks until all items in the queue have been retrieved and all processing work on the items have been completed. Worker threads call the other new method, task_done(), to signal that processing for an item has been completed. (Contributed by Raymond Hettinger.)\nThe old regex and regsub modules, which have been deprecated ever since Python 2.0, have finally been deleted. Other deleted modules: statcache, tzparse, whrandom.\nAlso deleted: the lib-old directory, which includes ancient modules such as dircmp and ni, was removed. lib-old wasn\u2019t on the default sys.path, so unless your programs explicitly added the directory to sys.path, this removal shouldn\u2019t affect your code.\nThe rlcompleter module is no longer dependent on importing the readline module and therefore now works on non-Unix platforms. (Patch from Robert Kiendl.)\nThe SimpleXMLRPCServer and DocXMLRPCServer classes now have a rpc_paths attribute that constrains XML-RPC operations to a limited set of URL paths; the default is to allow only '/' and '/RPC2'. Setting rpc_paths to None or an empty tuple disables this path checking.\nThe socket module now supports AF_NETLINK sockets on Linux, thanks to a patch from Philippe Biondi. Netlink sockets are a Linux-specific mechanism for communications between a user-space process and kernel code; an introductory article about them is at http://www.linuxjournal.com/article/7356 . In Python code, netlink addresses are represented as a tuple of 2 integers, (pid, group_mask).\nTwo new methods on socket objects, recv_into(buffer)() and recvfrom_into(buffer)(), store the received data in an object that supports the buffer protocol instead of returning the data as a string. This means you can put the data directly into an array or a memory-mapped file.\nSocket objects also gained getfamily(), gettype(), and getproto() accessor methods to retrieve the family, type, and protocol values for the socket.\nNew module: the spwd module provides functions for accessing the shadow password database on systems that support shadow passwords.\nThe struct is now faster because it compiles format strings into Struct objects with pack() and unpack() methods. This is similar to how the re module lets you create compiled regular expression objects. You can still use the module-level pack() and unpack() functions; they\u2019ll create Struct objects and cache them. Or you can use Struct instances directly:\ns = struct.Struct('ih3s') data = s.pack(1972, 187, 'abc') year, number, name = s.unpack(data)\nYou can also pack and unpack data to and from buffer objects directly using the pack_into(buffer, offset, v1, v2, ...)() and unpack_from(buffer, offset)() methods. This lets you store data directly into an array or a memory- mapped file.\n(Struct objects were implemented by Bob Ippolito at the NeedForSpeed sprint. Support for buffer objects was added by Martin Blais, also at the NeedForSpeed sprint.)\nThe Python developers switched from CVS to Subversion during the 2.5 development process. Information about the exact build version is available as the sys.subversion variable, a 3-tuple of (interpreter-name, branch-name, revision-range). For example, at the time of writing my copy of 2.5 was reporting ('CPython', 'trunk', '45313:45315').\nThis information is also available to C extensions via the Py_GetBuildInfo() function that returns a string of build information like this: \"trunk:45355:45356M, Apr 13 2006, 07:42:19\". (Contributed by Barry Warsaw.)\nAnother new function, sys._current_frames() , returns the current stack frames for all running threads as a dictionary mapping thread identifiers to the topmost stack frame currently active in that thread at the time the function is called. (Contributed by Tim Peters.)\nThe TarFile class in the tarfile module now has an extractall() method that extracts all members from the archive into the current working directory. It\u2019s also possible to set a different directory as the extraction target, and to unpack only a subset of the archive\u2019s members.\nThe compression used for a tarfile opened in stream mode can now be autodetected using the mode 'r|*'. (Contributed by Lars Gust\u00e4bel.)\nThe threading module now lets you set the stack size used when new threads are created. The stack_size([*size*])() function returns the currently configured stack size, and supplying the optional size parameter sets a new value. Not all platforms support changing the stack size, but Windows, POSIX threading, and OS/2 all do. (Contributed by Andrew MacIntyre.)\nThe unicodedata module has been updated to use version 4.1.0 of the Unicode character database. Version 3.2.0 is required by some specifications, so it\u2019s still available as unicodedata.ucd_3_2_0 .\nNew module: the uuid module generates universally unique identifiers (UUIDs) according to RFC 4122 . The RFC defines several different UUID versions that are generated from a starting string, from system properties, or purely randomly. This module contains a UUID class and functions named uuid1(), uuid3(), uuid4(), and uuid5() to generate different versions of UUID. (Version 2 UUIDs are not specified in RFC 4122 and are not supported by this module.)\n>>> import uuid >>> # make a UUID based on the host ID and current time >>> uuid.uuid1() UUID('a8098c1a-f86e-11da-bd1a-00112444be1e') >>> # make a UUID using an MD5 hash of a namespace UUID and a name >>> uuid.uuid3(uuid.NAMESPACE_DNS, 'python.org') UUID('6fa459ea-ee8a-3ca4-894e-db77e160355e') >>> # make a random UUID >>> uuid.uuid4() UUID('16fd2706-8baf-433b-82eb-8c7fada847da') >>> # make a UUID using a SHA-1 hash of a namespace UUID and a name >>> uuid.uuid5(uuid.NAMESPACE_DNS, 'python.org') UUID('886313e1-3b8a-5372-9b90-0c9aee199e5d')\n(Contributed by Ka-Ping Yee.)\nThe weakref module\u2019s WeakKeyDictionary and WeakValueDictionary types gained new methods for iterating over the weak references contained in the dictionary. iterkeyrefs() and keyrefs() methods were added to WeakKeyDictionary, and itervaluerefs() and valuerefs() were added to WeakValueDictionary. (Contributed by Fred L. Drake, Jr.)\nThe webbrowser module received a number of enhancements. It\u2019s now usable as a script with python -m webbrowser, taking a URL as the argument; there are a number of switches to control the behaviour (-n for a new browser window, -t for a new tab). New module-level functions, open_new() and open_new_tab(), were added to support this. The module\u2019s open() function supports an additional feature, an autoraise parameter that signals whether to raise the open window when possible. A number of additional browsers were added to the supported list such as Firefox, Opera, Konqueror, and elinks. (Contributed by Oleg Broytmann and Georg Brandl.)\nThe xmlrpclib module now supports returning datetime objects for the XML-RPC date type. Supply use_datetime=True to the loads() function or the Unmarshaller class to enable this feature. (Contributed by Skip Montanaro.)\nThe zipfile module now supports the ZIP64 version of the format, meaning that a .zip archive can now be larger than 4 GiB and can contain individual files larger than 4 GiB. (Contributed by Ronald Oussoren.)\nThe zlib module\u2019s Compress and Decompress objects now support a copy() method that makes a copy of the object\u2019s internal state and returns a new Compress or Decompress object. (Contributed by Chris AtLee.)\nThe ctypes package \u00b6\nThe ctypes package, written by Thomas Heller, has been added to the standard library. ctypes lets you call arbitrary functions in shared libraries or DLLs. Long-time users may remember the dl module, which provides functions for loading shared libraries and calling functions in them. The ctypes package is much fancier.\nTo load a shared library or DLL, you must create an instance of the CDLL class and provide the name or path of the shared library or DLL. Once that\u2019s done, you can call arbitrary functions by accessing them as attributes of the CDLL object.\nimport ctypes libc = ctypes.CDLL('libc.so.6') result = libc.printf(\"Line of output\\n\")\nType constructors for the various C types are provided: c_int(), c_float(), c_double(), c_char_p() (equivalent to char *), and so forth. Unlike Python\u2019s types, the C versions are all mutable; you can assign to their value attribute to change the wrapped value. Python integers and strings will be automatically converted to the corresponding C types, but for other types you must call the correct type constructor. (And I mean must; getting it wrong will often result in the interpreter crashing with a segmentation fault.)\nYou shouldn\u2019t use c_char_p() with a Python string when the C function will be modifying the memory area, because Python strings are supposed to be immutable; breaking this rule will cause puzzling bugs. When you need a modifiable memory area, use create_string_buffer():\ns = \"this is a string\" buf = ctypes.create_string_buffer(s) libc.strfry(buf)\nC functions are assumed to return integers, but you can set the restype attribute of the function object to change this:\n>>> libc.atof('2.71828') -1783957616 >>> libc.atof.restype = ctypes.c_double >>> libc.atof('2.71828') 2.71828\nctypes also provides a wrapper for Python\u2019s C API as the ctypes.pythonapi object. This object does not release the global interpreter lock before calling a function, because the lock must be held when calling into the interpreter\u2019s code. There\u2019s a py_object() type constructor that will create a PyObject * pointer. A simple usage:\nimport ctypes d = {} ctypes.pythonapi.PyObject_SetItem(ctypes.py_object(d), ctypes.py_object(\"abc\"), ctypes.py_object(1)) # d is now {'abc', 1}.\nDon\u2019t forget to use py_object(); if it\u2019s omitted you end up with a segmentation fault.\nctypes has been around for a while, but people still write and distribution hand-coded extension modules because you can\u2019t rely on ctypes being present. Perhaps developers will begin to write Python wrappers atop a library accessed through ctypes instead of extension modules, now that ctypes is included with core Python.\nSee also\nThe ctypes web page, with a tutorial, reference, and FAQ.\nThe documentation for the ctypes module.\nThe ElementTree package \u00b6\nA subset of Fredrik Lundh\u2019s ElementTree library for processing XML has been added to the standard library as xml.etree. The available modules are ElementTree, ElementPath, and ElementInclude from ElementTree 1.2.6. The cElementTree accelerator module is also included.\nThe rest of this section will provide a brief overview of using ElementTree. Full documentation for ElementTree is available at http://effbot.org/zone/element-index.htm .\nElementTree represents an XML document as a tree of element nodes. The text content of the document is stored as the text and tail attributes of (This is one of the major differences between ElementTree and the Document Object Model; in the DOM there are many different types of node, including TextNode.)\nThe most commonly used parsing function is parse(), that takes either a string (assumed to contain a filename) or a file-like object and returns an ElementTree instance:\nfrom xml.etree import ElementTree as ET tree = ET.parse('ex-1.xml') feed = urllib.urlopen( 'http://planet.python.org/rss10.xml') tree = ET.parse(feed)\nOnce you have an ElementTree instance, you can call its getroot() method to get the root Element node.\nThere\u2019s also an XML() function that takes a string literal and returns an Element node (not an ElementTree). This function provides a tidy way to incorporate XML fragments, approaching the convenience of an XML literal:\nsvg = ET.XML(\"\"\"<svg width=\"10px\" version=\"1.0\"> </svg>\"\"\") svg.set('height', '320px') svg.append(elem1)\nEach XML element supports some dictionary-like and some list-like access methods. Dictionary-like operations are used to access attribute values, and list-like operations are used to access child nodes.\nOperation\n"}], [{"score": 914.3866, "uuid": "3926cde3-7bb7-5705-98f7-a1ca062c09fe", "index": "cw12", "trec_id": "clueweb12-0009wb-10-29076", "target_hostname": "docs.python.org", "target_uri": "http://docs.python.org/py3k/whatsnew/2.5.html", "page_rank": 1.2021922e-09, "spam_rank": 70, "title": "What\u2019s New in <em>Python</em> 2.5 \u2014 <em>Python</em> v3.2.2 documentation", "snippet": "The decision was made to not require parentheses in the <em>Python</em> language\u2019s grammar, but as a matter of style <em>I</em> think you <em>should</em> always use them.", "explanation": null, "document": "An explanation of coroutines from a Perl point of view, written by Dan Sugalski.\nPEP 343: The \u2018with\u2019 statement \u00b6\nThe \u2018 with \u2018 statement clarifies code that previously would use try...finally blocks to ensure that clean-up code is executed. In this section, I\u2019ll discuss the statement as it will commonly be used. In the next section, I\u2019ll examine the implementation details and show how to write objects for use with this statement.\nThe \u2018 with \u2018 statement is a new control-flow structure whose basic structure is:\nwith expression [as variable]: with-block\nThe expression is evaluated, and it should result in an object that supports the context management protocol (that is, has __enter__() and __exit__() methods.\nThe object\u2019s __enter__() is called before with-block is executed and therefore can run set-up code. It also may return a value that is bound to the name variable, if given. (Note carefully that variable is not assigned the result of expression.)\nAfter execution of the with-block is finished, the object\u2019s __exit__() method is called, even if the block raised an exception, and can therefore run clean-up code.\nTo enable the statement in Python 2.5, you need to add the following directive to your module:\nfrom __future__ import with_statement\nThe statement will always be enabled in Python 2.6.\nSome standard Python objects now support the context management protocol and can be used with the \u2018 with \u2018 statement. File objects are one example:\nwith open('/etc/passwd', 'r') as f: for line in f: print line ... more processing code ...\nAfter this statement has executed, the file object in f will have been automatically closed, even if the for loop raised an exception part- way through the block.\nNote\nIn this case, f is the same object created by open() , because file.__enter__() returns self.\nThe threading module\u2019s locks and condition variables also support the \u2018 with \u2018 statement:\nlock = threading.Lock() with lock: # Critical section of code ...\nThe lock is acquired before the block is executed and always released once the block is complete.\nThe new localcontext() function in the decimal module makes it easy to save and restore the current decimal context, which encapsulates the desired precision and rounding characteristics for computations:\nfrom decimal import Decimal, Context, localcontext # Displays with default precision of 28 digits v = Decimal('578') print v.sqrt() with localcontext(Context(prec=16)): # All code in this block uses a precision of 16 digits. # The original context is restored on exiting the block. print v.sqrt()\nWriting Context Managers \u00b6\nUnder the hood, the \u2018 with \u2018 statement is fairly complicated. Most people will only use \u2018 with \u2018 in company with existing objects and don\u2019t need to know these details, so you can skip the rest of this section if you like. Authors of new objects will need to understand the details of the underlying implementation and should keep reading.\nA high-level explanation of the context management protocol is:\nThe expression is evaluated and should result in an object called a \u201ccontext manager\u201d. The context manager must have __enter__() and __exit__() methods.\nThe context manager\u2019s __enter__() method is called. The value returned is assigned to VAR. If no 'as VAR' clause is present, the value is simply discarded.\nThe code in BLOCK is executed.\nIf BLOCK raises an exception, the __exit__(type, value, traceback)() is called with the exception details, the same values returned by sys.exc_info() . The method\u2019s return value controls whether the exception is re-raised: any false value re-raises the exception, and True will result in suppressing it. You\u2019ll only rarely want to suppress the exception, because if you do the author of the code containing the \u2018 with \u2018 statement will never realize anything went wrong.\nIf BLOCK didn\u2019t raise an exception, the __exit__() method is still called, but type, value, and traceback are all None.\nLet\u2019s think through an example. I won\u2019t present detailed code but will only sketch the methods necessary for a database that supports transactions.\n(For people unfamiliar with database terminology: a set of changes to the database are grouped into a transaction. Transactions can be either committed, meaning that all the changes are written into the database, or rolled back, meaning that the changes are all discarded and the database is unchanged. See any database textbook for more information.)\nLet\u2019s assume there\u2019s an object representing a database connection. Our goal will be to let the user write code like this:\ndb_connection = DatabaseConnection() with db_connection as cursor: cursor.execute('insert into ...') cursor.execute('delete from ...') # ... more operations ...\nThe transaction should be committed if the code in the block runs flawlessly or rolled back if there\u2019s an exception. Here\u2019s the basic interface for DatabaseConnection that I\u2019ll assume:\nclass DatabaseConnection: # Database interface def cursor (self): \"Returns a cursor object and starts a new transaction\" def commit (self): \"Commits current transaction\" def rollback (self): \"Rolls back current transaction\"\nThe __enter__() method is pretty easy, having only to start a new transaction. For this application the resulting cursor object would be a useful result, so the method will return it. The user can then add as cursor to their \u2018 with \u2018 statement to bind the cursor to a variable name.\nclass DatabaseConnection: ... def __enter__ (self): # Code to start a new transaction cursor = self.cursor() return cursor\nThe __exit__() method is the most complicated because it\u2019s where most of the work has to be done. The method has to check if an exception occurred. If there was no exception, the transaction is committed. The transaction is rolled back if there was an exception.\nIn the code below, execution will just fall off the end of the function, returning the default value of None. None is false, so the exception will be re-raised automatically. If you wished, you could be more explicit and add a return statement at the marked location.\nclass DatabaseConnection: ... def __exit__ (self, type, value, tb): if tb is None: # No exception, so commit self.commit() else: # Exception occurred, so rollback. self.rollback() # return False\nThe contextlib module \u00b6\nThe new contextlib module provides some functions and a decorator that are useful for writing objects for use with the \u2018 with \u2018 statement.\nThe decorator is called contextmanager(), and lets you write a single generator function instead of defining a new class. The generator should yield exactly one value. The code up to the yield will be executed as the __enter__() method, and the value yielded will be the method\u2019s return value that will get bound to the variable in the \u2018 with \u2018 statement\u2019s as clause, if any. The code after the yield will be executed in the __exit__() method. Any exception raised in the block will be raised by the yield statement.\nOur database example from the previous section could be written using this decorator as:\nfrom contextlib import contextmanager @contextmanager def db_transaction (connection): cursor = connection.cursor() try: yield cursor except: connection.rollback() raise else: connection.commit() db = DatabaseConnection() with db_transaction(db) as cursor: ...\nThe contextlib module also has a nested(mgr1, mgr2, ...)() function that combines a number of context managers so you don\u2019t need to write nested \u2018 with \u2018 statements. In this example, the single \u2018 with \u2018 statement both starts a database transaction and acquires a thread lock:\nlock = threading.Lock() with nested (db_transaction(db), lock) as (cursor, locked): ...\nFinally, the closing(object)() function returns object so that it can be bound to a variable, and calls object.close at the end of the block.\nimport urllib, sys from contextlib import closing with closing(urllib.urlopen('http://www.yahoo.com')) as f: for line in f: sys.stdout.write(line)\nSee also\nPEP 343 - The \u201cwith\u201d statement\nPEP written by Guido van Rossum and Nick Coghlan; implemented by Mike Bland, Guido van Rossum, and Neal Norwitz. The PEP shows the code generated for a \u2018 with \u2018 statement, which can be helpful in learning how the statement works.\nThe documentation for the contextlib module.\nPEP 352: Exceptions as New-Style Classes \u00b6\nException classes can now be new-style classes, not just classic classes, and the built-in Exception class and all the standard built-in exceptions ( NameError , ValueError , etc.) are now new-style classes.\nThe inheritance hierarchy for exceptions has been rearranged a bit. In 2.5, the inheritance relationships are:\nBaseException # New in Python 2.5 |- KeyboardInterrupt |- SystemExit |- Exception |- (all other current built-in exceptions)\nThis rearrangement was done because people often want to catch all exceptions that indicate program errors. KeyboardInterrupt and SystemExit aren\u2019t errors, though, and usually represent an explicit action such as the user hitting Control-C or code calling sys.exit() . A bare except: will catch all exceptions, so you commonly need to list KeyboardInterrupt and SystemExit in order to re-raise them. The usual pattern is:\ntry: ... except (KeyboardInterrupt, SystemExit): raise except: # Log error... # Continue running program...\nIn Python 2.5, you can now write except Exception to achieve the same result, catching all the exceptions that usually indicate errors but leaving KeyboardInterrupt and SystemExit alone. As in previous versions, a bare except: still catches all exceptions.\nThe goal for Python 3.0 is to require any class raised as an exception to derive from BaseException or some descendant of BaseException , and future releases in the Python 2.x series may begin to enforce this constraint. Therefore, I suggest you begin making all your exception classes derive from Exception now. It\u2019s been suggested that the bare except: form should be removed in Python 3.0, but Guido van Rossum hasn\u2019t decided whether to do this or not.\nRaising of strings as exceptions, as in the statement raise \"Error occurred\", is deprecated in Python 2.5 and will trigger a warning. The aim is to be able to remove the string-exception feature in a few releases.\nSee also\nPEP 352 - Required Superclass for Exceptions\nPEP written by Brett Cannon and Guido van Rossum; implemented by Brett Cannon.\nPEP 353: Using ssize_t as the index type \u00b6\nA wide-ranging change to Python\u2019s C API, using a new Py_ssize_t type definition instead of int, will permit the interpreter to handle more data on 64-bit platforms. This change doesn\u2019t affect Python\u2019s capacity on 32-bit platforms.\nVarious pieces of the Python interpreter used C\u2019s int type to store sizes or counts; for example, the number of items in a list or tuple were stored in an int. The C compilers for most 64-bit platforms still define int as a 32-bit type, so that meant that lists could only hold up to 2**31 - 1 = 2147483647 items. (There are actually a few different programming models that 64-bit C compilers can use \u2013 see http://www.unix.org/version2/whatsnew/lp64_wp.html for a discussion \u2013 but the most commonly available model leaves int as 32 bits.)\nA limit of 2147483647 items doesn\u2019t really matter on a 32-bit platform because you\u2019ll run out of memory before hitting the length limit. Each list item requires space for a pointer, which is 4 bytes, plus space for a PyObject representing the item. 2147483647*4 is already more bytes than a 32-bit address space can contain.\nIt\u2019s possible to address that much memory on a 64-bit platform, however. The pointers for a list that size would only require 16 GiB of space, so it\u2019s not unreasonable that Python programmers might construct lists that large. Therefore, the Python interpreter had to be changed to use some type other than int, and this will be a 64-bit type on 64-bit platforms. The change will cause incompatibilities on 64-bit machines, so it was deemed worth making the transition now, while the number of 64-bit users is still relatively small. (In 5 or 10 years, we may all be on 64-bit machines, and the transition would be more painful then.)\nThis change most strongly affects authors of C extension modules. Python strings and container types such as lists and tuples now use Py_ssize_t to store their size. Functions such as PyList_Size() now return Py_ssize_t. Code in extension modules may therefore need to have some variables changed to Py_ssize_t.\nThe PyArg_ParseTuple() and Py_BuildValue() functions have a new conversion code, n, for Py_ssize_t. PyArg_ParseTuple() \u2018s s# and t# still output int by default, but you can define the macro PY_SSIZE_T_CLEAN before including Python.h to make them return Py_ssize_t.\nPEP 353 has a section on conversion guidelines that extension authors should read to learn about supporting 64-bit platforms.\nSee also\nPEP 353 - Using ssize_t as the index type\nPEP written and implemented by Martin von L\u00f6wis.\nPEP 357: The \u2018__index__\u2019 method \u00b6\nThe NumPy developers had a problem that could only be solved by adding a new special method, __index__() . When using slice notation, as in [start:stop:step], the values of the start, stop, and step indexes must all be either integers or long integers. NumPy defines a variety of specialized integer types corresponding to unsigned and signed integers of 8, 16, 32, and 64 bits, but there was no way to signal that these types could be used as slice indexes.\nSlicing can\u2019t just use the existing __int__() method because that method is also used to implement coercion to integers. If slicing used __int__() , floating-point numbers would also become legal slice indexes and that\u2019s clearly an undesirable behaviour.\nInstead, a new special method called __index__() was added. It takes no arguments and returns an integer giving the slice index to use. For example:\nclass C: def __index__ (self): return self.value\nThe return value must be either a Python integer or long integer. The interpreter will check that the type returned is correct, and raises a TypeError if this requirement isn\u2019t met.\nA corresponding nb_index slot was added to the C-level PyNumberMethods structure to let C extensions implement this protocol. PyNumber_Index(obj)() can be used in extension code to call the __index__() function and retrieve its result.\nSee also\nPEP 357 - Allowing Any Object to be Used for Slicing\nPEP written and implemented by Travis Oliphant.\nOther Language Changes \u00b6\nHere are all of the changes that Python 2.5 makes to the core Python language.\nThe dict type has a new hook for letting subclasses provide a default value when a key isn\u2019t contained in the dictionary. When a key isn\u2019t found, the dictionary\u2019s __missing__(key)() method will be called. This hook is used to implement the new defaultdict class in the collections module. The following example defines a dictionary that returns zero for any missing key:\nclass zerodict (dict): def __missing__ (self, key): return 0 d = zerodict({1:1, 2:2}) print d[1], d[2] # Prints 1, 2 print d[3], d[4] # Prints 0, 0\nBoth 8-bit and Unicode strings have new partition(sep)() and rpartition(sep)() methods that simplify a common use case.\nThe find(S)() method is often used to get an index which is then used to slice the string and obtain the pieces that are before and after the separator. partition(sep)() condenses this pattern into a single method call that returns a 3-tuple containing the substring before the separator, the separator itself, and the substring after the separator. If the separator isn\u2019t found, the first element of the tuple is the entire string and the other two elements are empty. rpartition(sep)() also returns a 3-tuple but starts searching from the end of the string; the r stands for \u2018reverse\u2019.\nSome examples:\n>>> ('http://www.python.org').partition('://') ('http', '://', 'www.python.org') >>> ('file:/usr/share/doc/index.html').partition('://') ('file:/usr/share/doc/index.html', '', '') >>> (u'Subject: a quick question').partition(':') (u'Subject', u':', u' a quick question') >>> 'www.python.org'.rpartition('.') ('www.python', '.', 'org') >>> 'www.python.org'.rpartition(':') ('', '', 'www.python.org')\n(Implemented by Fredrik Lundh following a suggestion by Raymond Hettinger.)\nThe startswith() and endswith() methods of string types now accept tuples of strings to check for.\ndef is_image_file (filename): return filename.endswith(('.gif', '.jpg', '.tiff'))\n(Implemented by Georg Brandl following a suggestion by Tom Lynn.)\nThe min() and max() built-in functions gained a key keyword parameter analogous to the key argument for sort(). This parameter supplies a function that takes a single argument and is called for every value in the list; min() / max() will return the element with the smallest/largest return value from this function. For example, to find the longest string in a list, you can do:\nL = ['medium', 'longest', 'short'] # Prints 'longest' print max(L, key=len) # Prints 'short', because lexicographically 'short' has the largest value print max(L)\n(Contributed by Steven Bethard and Raymond Hettinger.)\nTwo new built-in functions, any() and all() , evaluate whether an iterator contains any true or false values. any() returns True if any value returned by the iterator is true; otherwise it will return False . all() returns True only if all of the values returned by the iterator evaluate as true. (Suggested by Guido van Rossum, and implemented by Raymond Hettinger.)\nThe result of a class\u2019s __hash__() method can now be either a long integer or a regular integer. If a long integer is returned, the hash of that value is taken. In earlier versions the hash value was required to be a regular integer, but in 2.5 the id() built-in was changed to always return non-negative numbers, and users often seem to use id(self) in __hash__() methods (though this is discouraged).\nASCII is now the default encoding for modules. It\u2019s now a syntax error if a module contains string literals with 8-bit characters but doesn\u2019t have an encoding declaration. In Python 2.4 this triggered a warning, not a syntax error. See PEP 263 for how to declare a module\u2019s encoding; for example, you might add a line like this near the top of the source file:\n# -*- coding: latin1 -*-\nA new warning, UnicodeWarning , is triggered when you attempt to compare a Unicode string and an 8-bit string that can\u2019t be converted to Unicode using the default ASCII encoding. The result of the comparison is false:\n>>> chr(128) == unichr(128) # Can't convert chr(128) to Unicode __main__:1: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal False >>> chr(127) == unichr(127) # chr(127) can be converted True\nPreviously this would raise a UnicodeDecodeError exception, but in 2.5 this could result in puzzling problems when accessing a dictionary. If you looked up unichr(128) and chr(128) was being used as a key, you\u2019d get a UnicodeDecodeError exception. Other changes in 2.5 resulted in this exception being raised instead of suppressed by the code in dictobject.c that implements dictionaries.\nRaising an exception for such a comparison is strictly correct, but the change might have broken code, so instead UnicodeWarning was introduced.\n(Implemented by Marc-Andr\u00e9 Lemburg.)\nOne error that Python programmers sometimes make is forgetting to include an __init__.py module in a package directory. Debugging this mistake can be confusing, and usually requires running Python with the -v switch to log all the paths searched. In Python 2.5, a new ImportWarning warning is triggered when an import would have picked up a directory as a package but no __init__.py was found. This warning is silently ignored by default; provide the -Wd option when running the Python executable to display the warning message. (Implemented by Thomas Wouters.)\nThe list of base classes in a class definition can now be empty. As an example, this is now legal:\nclass C(): pass\n(Implemented by Brett Cannon.)\nInteractive Interpreter Changes \u00b6\nIn the interactive interpreter, quit and exit have long been strings so that new users get a somewhat helpful message when they try to quit:\n>>> quit 'Use Ctrl-D (i.e. EOF) to exit.'\nIn Python 2.5, quit and exit are now objects that still produce string representations of themselves, but are also callable. Newbies who try quit() or exit() will now exit the interpreter as they expect. (Implemented by Georg Brandl.)\nThe Python executable now accepts the standard long options --help and --version ; on Windows, it also accepts the /? option for displaying a help message. (Implemented by Georg Brandl.)\nOptimizations \u00b6\nSeveral of the optimizations were developed at the NeedForSpeed sprint, an event held in Reykjavik, Iceland, from May 21\u201328 2006. The sprint focused on speed enhancements to the CPython implementation and was funded by EWT LLC with local support from CCP Games. Those optimizations added at this sprint are specially marked in the following list.\nWhen they were introduced in Python 2.4, the built-in set and frozenset types were built on top of Python\u2019s dictionary type. In 2.5 the internal data structure has been customized for implementing sets, and as a result sets will use a third less memory and are somewhat faster. (Implemented by Raymond Hettinger.)\nThe speed of some Unicode operations, such as finding substrings, string splitting, and character map encoding and decoding, has been improved. (Substring search and splitting improvements were added by Fredrik Lundh and Andrew Dalke at the NeedForSpeed sprint. Character maps were improved by Walter D\u00f6rwald and Martin von L\u00f6wis.)\nThe long(str, base)() function is now faster on long digit strings because fewer intermediate results are calculated. The peak is for strings of around 800\u20131000 digits where the function is 6 times faster. (Contributed by Alan McIntyre and committed at the NeedForSpeed sprint.)\nIt\u2019s now illegal to mix iterating over a file with for line in file and calling the file object\u2019s read()/ readline() /readlines() methods. Iteration uses an internal buffer and the read*() methods don\u2019t use that buffer. Instead they would return the data following the buffer, causing the data to appear out of order. Mixing iteration and these methods will now trigger a ValueError from the read*() method. (Implemented by Thomas Wouters.)\nThe struct module now compiles structure format strings into an internal representation and caches this representation, yielding a 20% speedup. (Contributed by Bob Ippolito at the NeedForSpeed sprint.)\nThe re module got a 1 or 2% speedup by switching to Python\u2019s allocator functions instead of the system\u2019s malloc() and free(). (Contributed by Jack Diederich at the NeedForSpeed sprint.)\nThe code generator\u2019s peephole optimizer now performs simple constant folding in expressions. If you write something like a = 2+3, the code generator will do the arithmetic and produce code corresponding to a = 5. (Proposed and implemented by Raymond Hettinger.)\nFunction calls are now faster because code objects now keep the most recently finished frame (a \u201czombie frame\u201d) in an internal field of the code object, reusing it the next time the code object is invoked. (Original patch by Michael Hudson, modified by Armin Rigo and Richard Jones; committed at the NeedForSpeed sprint.) Frame objects are also slightly smaller, which may improve cache locality and reduce memory usage a bit. (Contributed by Neal Norwitz.)\nPython\u2019s built-in exceptions are now new-style classes, a change that speeds up instantiation considerably. Exception handling in Python 2.5 is therefore about 30% faster than in 2.4. (Contributed by Richard Jones, Georg Brandl and Sean Reifschneider at the NeedForSpeed sprint.)\nImporting now caches the paths tried, recording whether they exist or not so that the interpreter makes fewer open() and stat() calls on startup. (Contributed by Martin von L\u00f6wis and Georg Brandl.)\nNew, Improved, and Removed Modules \u00b6\nThe standard library received many enhancements and bug fixes in Python 2.5. Here\u2019s a partial list of the most notable changes, sorted alphabetically by module name. Consult the Misc/NEWS file in the source tree for a more complete list of changes, or look through the SVN logs for all the details.\nThe audioop module now supports the a-LAW encoding, and the code for u-LAW encoding has been improved. (Contributed by Lars Immisch.)\nThe codecs module gained support for incremental codecs. The codec.lookup() function now returns a CodecInfo instance instead of a tuple. CodecInfo instances behave like a 4-tuple to preserve backward compatibility but also have the attributes encode, decode, incrementalencoder, incrementaldecoder, streamwriter, and streamreader. Incremental codecs can receive input and produce output in multiple chunks; the output is the same as if the entire input was fed to the non-incremental codec. See the codecs module documentation for details. (Designed and implemented by Walter D\u00f6rwald.)\nThe collections module gained a new type, defaultdict, that subclasses the standard dict type. The new type mostly behaves like a dictionary but constructs a default value when a key isn\u2019t present, automatically adding it to the dictionary for the requested key value.\nThe first argument to defaultdict\u2018s constructor is a factory function that gets called whenever a key is requested but not found. This factory function receives no arguments, so you can use built-in type constructors such as list() or int() . For example, you can make an index of words based on their initial letter like this:\nwords = \"\"\"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura che la diritta via era smarrita\"\"\".lower().split() index = defaultdict(list) for w in words: init_letter = w[0] index[init_letter].append(w)\nPrinting index results in the following output:\ndefaultdict(<type 'list'>, {'c': ['cammin', 'che'], 'e': ['era'], 'd': ['del', 'di', 'diritta'], 'm': ['mezzo', 'mi'], 'l': ['la'], 'o': ['oscura'], 'n': ['nel', 'nostra'], 'p': ['per'], 's': ['selva', 'smarrita'], 'r': ['ritrovai'], 'u': ['una'], 'v': ['vita', 'via']}\n(Contributed by Guido van Rossum.)\nThe deque double-ended queue type supplied by the collections module now has a remove(value)() method that removes the first occurrence of value in the queue, raising ValueError if the value isn\u2019t found. (Contributed by Raymond Hettinger.)\nNew module: The contextlib module contains helper functions for use with the new \u2018 with \u2018 statement. See section The contextlib module for more about this module.\nNew module: The cProfile module is a C implementation of the existing profile module that has much lower overhead. The module\u2019s interface is the same as profile : you run cProfile.run('main()') to profile a function, can save profile data to a file, etc. It\u2019s not yet known if the Hotshot profiler, which is also written in C but doesn\u2019t match the profile module\u2019s interface, will continue to be maintained in future versions of Python. (Contributed by Armin Rigo.)\nAlso, the pstats module for analyzing the data measured by the profiler now supports directing the output to any file object by supplying a stream argument to the Stats constructor. (Contributed by Skip Montanaro.)\nThe csv module, which parses files in comma-separated value format, received several enhancements and a number of bugfixes. You can now set the maximum size in bytes of a field by calling the csv.field_size_limit(new_limit)() function; omitting the new_limit argument will return the currently-set limit. The reader class now has a line_num attribute that counts the number of physical lines read from the source; records can span multiple physical lines, so line_num is not the same as the number of records read.\nThe CSV parser is now stricter about multi-line quoted fields. Previously, if a line ended within a quoted field without a terminating newline character, a newline would be inserted into the returned field. This behavior caused problems when reading files that contained carriage return characters within fields, so the code was changed to return the field without inserting newlines. As a consequence, if newlines embedded within fields are important, the input should be split into lines in a manner that preserves the newline characters.\n(Contributed by Skip Montanaro and Andrew McNamara.)\nThe datetime class in the datetime module now has a strptime(string, format)() method for parsing date strings, contributed by Josh Spoerri. It uses the same format characters as time.strptime() and time.strftime() :\nfrom datetime import datetime ts = datetime.strptime('10:13:15 2006-03-07', '%H:%M:%S %Y-%m-%d')\nThe SequenceMatcher.get_matching_blocks() method in the difflib module now guarantees to return a minimal list of blocks describing matching subsequences. Previously, the algorithm would occasionally break a block of matching elements into two list entries. (Enhancement by Tim Peters.)\nThe doctest module gained a SKIP option that keeps an example from being executed at all. This is intended for code snippets that are usage examples intended for the reader and aren\u2019t actually test cases.\nAn encoding parameter was added to the testfile() function and the DocFileSuite class to specify the file\u2019s encoding. This makes it easier to use non-ASCII characters in tests contained within a docstring. (Contributed by Bjorn Tillenius.)\nThe email package has been updated to version 4.0. (Contributed by Barry Warsaw.)\nThe fileinput module was made more flexible. Unicode filenames are now supported, and a mode parameter that defaults to \"r\" was added to the input() function to allow opening files in binary or universal-newline mode. Another new parameter, openhook, lets you use a function other than open() to open the input files. Once you\u2019re iterating over the set of files, the FileInput object\u2019s new fileno() returns the file descriptor for the currently opened file. (Contributed by Georg Brandl.)\nIn the gc module, the new get_count() function returns a 3-tuple containing the current collection counts for the three GC generations. This is accounting information for the garbage collector; when these counts reach a specified threshold, a garbage collection sweep will be made. The existing gc.collect() function now takes an optional generation argument of 0, 1, or 2 to specify which generation to collect. (Contributed by Barry Warsaw.)\nThe nsmallest() and nlargest() functions in the heapq module now support a key keyword parameter similar to the one provided by the min() / max() functions and the sort() methods. For example:\n>>> import heapq >>> L = [\"short\", 'medium', 'longest', 'longer still'] >>> heapq.nsmallest(2, L) # Return two lowest elements, lexicographically ['longer still', 'longest'] >>> heapq.nsmallest(2, L, key=len) # Return two shortest elements ['short', 'medium']\n(Contributed by Raymond Hettinger.)\nThe itertools.islice() function now accepts None for the start and step arguments. This makes it more compatible with the attributes of slice objects, so that you can now write the following:\ns = slice(5) # Create slice object itertools.islice(iterable, s.start, s.stop, s.step)\n(Contributed by Raymond Hettinger.)\nThe format() function in the locale module has been modified and two new functions were added, format_string() and currency().\nThe format() function\u2019s val parameter could previously be a string as long as no more than one %char specifier appeared; now the parameter must be exactly one %char specifier with no surrounding text. An optional monetary parameter was also added which, if True, will use the locale\u2019s rules for formatting currency in placing a separator between groups of three digits.\nTo format strings with multiple %char specifiers, use the new format_string() function that works like format() but also supports mixing %char specifiers with arbitrary text.\nA new currency() function was also added that formats a number according to the current locale\u2019s settings.\n(Contributed by Georg Brandl.)\nThe mailbox module underwent a massive rewrite to add the capability to modify mailboxes in addition to reading them. A new set of classes that include mbox, MH, and Maildir are used to read mailboxes, and have an add(message)() method to add messages, remove(key)() to remove messages, and lock()/unlock() to lock/unlock the mailbox. The following example converts a maildir-format mailbox into an mbox-format one:\nimport mailbox # 'factory=None' uses email.Message.Message as the class representing # individual messages. src = mailbox.Maildir('maildir', factory=None) dest = mailbox.mbox('/tmp/mbox') for msg in src: dest.add(msg)\n(Contributed by Gregory K. Johnson. Funding was provided by Google\u2019s 2005 Summer of Code.)\nNew module: the msilib module allows creating Microsoft Installer .msi files and CAB files. Some support for reading the .msi database is also included. (Contributed by Martin von L\u00f6wis.)\nThe nis module now supports accessing domains other than the system default domain by supplying a domain argument to the nis.match() and nis.maps() functions. (Contributed by Ben Bell.)\nThe operator module\u2019s itemgetter() and attrgetter() functions now support multiple fields. A call such as operator.attrgetter('a', 'b') will return a function that retrieves the a and b attributes. Combining this new feature with the sort() method\u2019s key parameter lets you easily sort lists using multiple fields. (Contributed by Raymond Hettinger.)\nThe optparse module was updated to version 1.5.1 of the Optik library. The OptionParser class gained an epilog attribute, a string that will be printed after the help message, and a destroy() method to break reference cycles created by the object. (Contributed by Greg Ward.)\nThe os module underwent several changes. The stat_float_times variable now defaults to true, meaning that os.stat() will now return time values as floats. (This doesn\u2019t necessarily mean that os.stat() will return times that are precise to fractions of a second; not all systems support such precision.)\nConstants named os.SEEK_SET , os.SEEK_CUR , and os.SEEK_END have been added; these are the parameters to the os.lseek() function. Two new constants for locking are os.O_SHLOCK and os.O_EXLOCK .\nTwo new functions, wait3() and wait4(), were added. They\u2019re similar the waitpid() function which waits for a child process to exit and returns a tuple of the process ID and its exit status, but wait3() and wait4() return additional information. wait3() doesn\u2019t take a process ID as input, so it waits for any child process to exit and returns a 3-tuple of process-id, exit-status, resource-usage as returned from the resource.getrusage() function. wait4(pid)() does take a process ID. (Contributed by Chad J. Schroeder.)\nOn FreeBSD, the os.stat() function now returns times with nanosecond resolution, and the returned object now has st_gen and st_birthtime. The st_flags attribute is also available, if the platform supports it. (Contributed by Antti Louko and Diego Petten\u00f2.)\nThe Python debugger provided by the pdb module can now store lists of commands to execute when a breakpoint is reached and execution stops. Once breakpoint #1 has been created, enter commands 1 and enter a series of commands to be executed, finishing the list with end. The command list can include commands that resume execution, such as continue or next. (Contributed by Gr\u00e9goire Dooms.)\nThe pickle and cPickle modules no longer accept a return value of None from the __reduce__() method; the method must return a tuple of arguments instead. The ability to return None was deprecated in Python 2.4, so this completes the removal of the feature.\nThe pkgutil module, containing various utility functions for finding packages, was enhanced to support PEP 302\u2019s import hooks and now also works for packages stored in ZIP-format archives. (Contributed by Phillip J. Eby.)\nThe pybench benchmark suite by Marc-Andr\u00e9 Lemburg is now included in the Tools/pybench directory. The pybench suite is an improvement on the commonly used pystone.py program because pybench provides a more detailed measurement of the interpreter\u2019s speed. It times particular operations such as function calls, tuple slicing, method lookups, and numeric operations, instead of performing many different operations and reducing the result to a single number as pystone.py does.\nThe pyexpat module now uses version 2.0 of the Expat parser. (Contributed by Trent Mick.)\nThe Queue class provided by the Queue module gained two new methods. join() blocks until all items in the queue have been retrieved and all processing work on the items have been completed. Worker threads call the other new method, task_done(), to signal that processing for an item has been completed. (Contributed by Raymond Hettinger.)\nThe old regex and regsub modules, which have been deprecated ever since Python 2.0, have finally been deleted. Other deleted modules: statcache, tzparse, whrandom.\nAlso deleted: the lib-old directory, which includes ancient modules such as dircmp and ni, was removed. lib-old wasn\u2019t on the default sys.path, so unless your programs explicitly added the directory to sys.path, this removal shouldn\u2019t affect your code.\nThe rlcompleter module is no longer dependent on importing the readline module and therefore now works on non-Unix platforms. (Patch from Robert Kiendl.)\nThe SimpleXMLRPCServer and DocXMLRPCServer classes now have a rpc_paths attribute that constrains XML-RPC operations to a limited set of URL paths; the default is to allow only '/' and '/RPC2'. Setting rpc_paths to None or an empty tuple disables this path checking.\nThe socket module now supports AF_NETLINK sockets on Linux, thanks to a patch from Philippe Biondi. Netlink sockets are a Linux-specific mechanism for communications between a user-space process and kernel code; an introductory article about them is at http://www.linuxjournal.com/article/7356 . In Python code, netlink addresses are represented as a tuple of 2 integers, (pid, group_mask).\nTwo new methods on socket objects, recv_into(buffer)() and recvfrom_into(buffer)(), store the received data in an object that supports the buffer protocol instead of returning the data as a string. This means you can put the data directly into an array or a memory-mapped file.\nSocket objects also gained getfamily(), gettype(), and getproto() accessor methods to retrieve the family, type, and protocol values for the socket.\nNew module: the spwd module provides functions for accessing the shadow password database on systems that support shadow passwords.\nThe struct is now faster because it compiles format strings into Struct objects with pack() and unpack() methods. This is similar to how the re module lets you create compiled regular expression objects. You can still use the module-level pack() and unpack() functions; they\u2019ll create Struct objects and cache them. Or you can use Struct instances directly:\ns = struct.Struct('ih3s') data = s.pack(1972, 187, 'abc') year, number, name = s.unpack(data)\nYou can also pack and unpack data to and from buffer objects directly using the pack_into(buffer, offset, v1, v2, ...)() and unpack_from(buffer, offset)() methods. This lets you store data directly into an array or a memory- mapped file.\n(Struct objects were implemented by Bob Ippolito at the NeedForSpeed sprint. Support for buffer objects was added by Martin Blais, also at the NeedForSpeed sprint.)\nThe Python developers switched from CVS to Subversion during the 2.5 development process. Information about the exact build version is available as the sys.subversion variable, a 3-tuple of (interpreter-name, branch-name, revision-range). For example, at the time of writing my copy of 2.5 was reporting ('CPython', 'trunk', '45313:45315').\nThis information is also available to C extensions via the Py_GetBuildInfo() function that returns a string of build information like this: \"trunk:45355:45356M, Apr 13 2006, 07:42:19\". (Contributed by Barry Warsaw.)\nAnother new function, sys._current_frames() , returns the current stack frames for all running threads as a dictionary mapping thread identifiers to the topmost stack frame currently active in that thread at the time the function is called. (Contributed by Tim Peters.)\nThe TarFile class in the tarfile module now has an extractall() method that extracts all members from the archive into the current working directory. It\u2019s also possible to set a different directory as the extraction target, and to unpack only a subset of the archive\u2019s members.\nThe compression used for a tarfile opened in stream mode can now be autodetected using the mode 'r|*'. (Contributed by Lars Gust\u00e4bel.)\nThe threading module now lets you set the stack size used when new threads are created. The stack_size([*size*])() function returns the currently configured stack size, and supplying the optional size parameter sets a new value. Not all platforms support changing the stack size, but Windows, POSIX threading, and OS/2 all do. (Contributed by Andrew MacIntyre.)\nThe unicodedata module has been updated to use version 4.1.0 of the Unicode character database. Version 3.2.0 is required by some specifications, so it\u2019s still available as unicodedata.ucd_3_2_0 .\nNew module: the uuid module generates universally unique identifiers (UUIDs) according to RFC 4122 . The RFC defines several different UUID versions that are generated from a starting string, from system properties, or purely randomly. This module contains a UUID class and functions named uuid1(), uuid3(), uuid4(), and uuid5() to generate different versions of UUID. (Version 2 UUIDs are not specified in RFC 4122 and are not supported by this module.)\n>>> import uuid >>> # make a UUID based on the host ID and current time >>> uuid.uuid1() UUID('a8098c1a-f86e-11da-bd1a-00112444be1e') >>> # make a UUID using an MD5 hash of a namespace UUID and a name >>> uuid.uuid3(uuid.NAMESPACE_DNS, 'python.org') UUID('6fa459ea-ee8a-3ca4-894e-db77e160355e') >>> # make a random UUID >>> uuid.uuid4() UUID('16fd2706-8baf-433b-82eb-8c7fada847da') >>> # make a UUID using a SHA-1 hash of a namespace UUID and a name >>> uuid.uuid5(uuid.NAMESPACE_DNS, 'python.org') UUID('886313e1-3b8a-5372-9b90-0c9aee199e5d')\n(Contributed by Ka-Ping Yee.)\nThe weakref module\u2019s WeakKeyDictionary and WeakValueDictionary types gained new methods for iterating over the weak references contained in the dictionary. iterkeyrefs() and keyrefs() methods were added to WeakKeyDictionary, and itervaluerefs() and valuerefs() were added to WeakValueDictionary. (Contributed by Fred L. Drake, Jr.)\nThe webbrowser module received a number of enhancements. It\u2019s now usable as a script with python -m webbrowser, taking a URL as the argument; there are a number of switches to control the behaviour (-n for a new browser window, -t for a new tab). New module-level functions, open_new() and open_new_tab(), were added to support this. The module\u2019s open() function supports an additional feature, an autoraise parameter that signals whether to raise the open window when possible. A number of additional browsers were added to the supported list such as Firefox, Opera, Konqueror, and elinks. (Contributed by Oleg Broytmann and Georg Brandl.)\nThe xmlrpclib module now supports returning datetime objects for the XML-RPC date type. Supply use_datetime=True to the loads() function or the Unmarshaller class to enable this feature. (Contributed by Skip Montanaro.)\nThe zipfile module now supports the ZIP64 version of the format, meaning that a .zip archive can now be larger than 4 GiB and can contain individual files larger than 4 GiB. (Contributed by Ronald Oussoren.)\nThe zlib module\u2019s Compress and Decompress objects now support a copy() method that makes a copy of the object\u2019s internal state and returns a new Compress or Decompress object. (Contributed by Chris AtLee.)\nThe ctypes package \u00b6\nThe ctypes package, written by Thomas Heller, has been added to the standard library. ctypes lets you call arbitrary functions in shared libraries or DLLs. Long-time users may remember the dl module, which provides functions for loading shared libraries and calling functions in them. The ctypes package is much fancier.\nTo load a shared library or DLL, you must create an instance of the CDLL class and provide the name or path of the shared library or DLL. Once that\u2019s done, you can call arbitrary functions by accessing them as attributes of the CDLL object.\nimport ctypes libc = ctypes.CDLL('libc.so.6') result = libc.printf(\"Line of output\\n\")\nType constructors for the various C types are provided: c_int(), c_float(), c_double(), c_char_p() (equivalent to char *), and so forth. Unlike Python\u2019s types, the C versions are all mutable; you can assign to their value attribute to change the wrapped value. Python integers and strings will be automatically converted to the corresponding C types, but for other types you must call the correct type constructor. (And I mean must; getting it wrong will often result in the interpreter crashing with a segmentation fault.)\nYou shouldn\u2019t use c_char_p() with a Python string when the C function will be modifying the memory area, because Python strings are supposed to be immutable; breaking this rule will cause puzzling bugs. When you need a modifiable memory area, use create_string_buffer():\ns = \"this is a string\" buf = ctypes.create_string_buffer(s) libc.strfry(buf)\nC functions are assumed to return integers, but you can set the restype attribute of the function object to change this:\n>>> libc.atof('2.71828') -1783957616 >>> libc.atof.restype = ctypes.c_double >>> libc.atof('2.71828') 2.71828\nctypes also provides a wrapper for Python\u2019s C API as the ctypes.pythonapi object. This object does not release the global interpreter lock before calling a function, because the lock must be held when calling into the interpreter\u2019s code. There\u2019s a py_object() type constructor that will create a PyObject * pointer. A simple usage:\nimport ctypes d = {} ctypes.pythonapi.PyObject_SetItem(ctypes.py_object(d), ctypes.py_object(\"abc\"), ctypes.py_object(1)) # d is now {'abc', 1}.\nDon\u2019t forget to use py_object(); if it\u2019s omitted you end up with a segmentation fault.\nctypes has been around for a while, but people still write and distribution hand-coded extension modules because you can\u2019t rely on ctypes being present. Perhaps developers will begin to write Python wrappers atop a library accessed through ctypes instead of extension modules, now that ctypes is included with core Python.\nSee also\nThe ctypes web page, with a tutorial, reference, and FAQ.\nThe documentation for the ctypes module.\nThe ElementTree package \u00b6\nA subset of Fredrik Lundh\u2019s ElementTree library for processing XML has been added to the standard library as xml.etree. The available modules are ElementTree, ElementPath, and ElementInclude from ElementTree 1.2.6. The cElementTree accelerator module is also included.\nThe rest of this section will provide a brief overview of using ElementTree. Full documentation for ElementTree is available at http://effbot.org/zone/element-index.htm .\nElementTree represents an XML document as a tree of element nodes. The text content of the document is stored as the text and tail attributes of (This is one of the major differences between ElementTree and the Document Object Model; in the DOM there are many different types of node, including TextNode.)\nThe most commonly used parsing function is parse(), that takes either a string (assumed to contain a filename) or a file-like object and returns an ElementTree instance:\nfrom xml.etree import ElementTree as ET tree = ET.parse('ex-1.xml') feed = urllib.urlopen( 'http://planet.python.org/rss10.xml') tree = ET.parse(feed)\nOnce you have an ElementTree instance, you can call its getroot() method to get the root Element node.\nThere\u2019s also an XML() function that takes a string literal and returns an Element node (not an ElementTree). This function provides a tidy way to incorporate XML fragments, approaching the convenience of an XML literal:\nsvg = ET.XML(\"\"\"<svg width=\"10px\" version=\"1.0\"> </svg>\"\"\") svg.set('height', '320px') svg.append(elem1)\nEach XML element supports some dictionary-like and some list-like access methods. Dictionary-like operations are used to access attribute values, and list-like operations are used to access child nodes.\nOperation\n"}, {"score": 913.29596, "uuid": "11a0a97f-15e1-5a69-b1be-5197a63cd2d5", "index": "cw12", "trec_id": "clueweb12-0007wb-30-32538", "target_hostname": "docs.python.org", "target_uri": "http://docs.python.org/whatsnew/2.5.html", "page_rank": 1.2603624e-09, "spam_rank": 73, "title": "What\u2019s New in <em>Python</em> 2.5 \u2014 <em>Python</em> v2.7.2 documentation", "snippet": "The decision was made to not require parentheses in the <em>Python</em> language\u2019s grammar, but as a matter of style <em>I</em> think you <em>should</em> always use them.", "explanation": null, "document": "An explanation of coroutines from a Perl point of view, written by Dan Sugalski.\nPEP 343: The \u2018with\u2019 statement \u00b6\nThe \u2018 with \u2018 statement clarifies code that previously would use try...finally blocks to ensure that clean-up code is executed. In this section, I\u2019ll discuss the statement as it will commonly be used. In the next section, I\u2019ll examine the implementation details and show how to write objects for use with this statement.\nThe \u2018 with \u2018 statement is a new control-flow structure whose basic structure is:\nwith expression [as variable]:\n    with-block\nThe expression is evaluated, and it should result in an object that supports the context management protocol (that is, has __enter__() and __exit__() methods.\nThe object\u2019s __enter__() is called before with-block is executed and therefore can run set-up code. It also may return a value that is bound to the name variable, if given. (Note carefully that variable is not assigned the result of expression.)\nAfter execution of the with-block is finished, the object\u2019s __exit__() method is called, even if the block raised an exception, and can therefore run clean-up code.\nTo enable the statement in Python 2.5, you need to add the following directive to your module:\nfrom __future__ import with_statement\nThe statement will always be enabled in Python 2.6.\nSome standard Python objects now support the context management protocol and can be used with the \u2018 with \u2018 statement. File objects are one example:\nwith open('/etc/passwd', 'r') as f: for line in f: print line ... more processing code ...\nAfter this statement has executed, the file object in f will have been automatically closed, even if the for loop raised an exception part- way through the block.\nNote\nIn this case, f is the same object created by open() , because file.__enter__() returns self.\nThe threading module\u2019s locks and condition variables also support the \u2018 with \u2018 statement:\nlock = threading.Lock() with lock: # Critical section of code ...\nThe lock is acquired before the block is executed and always released once the block is complete.\nThe new localcontext() function in the decimal module makes it easy to save and restore the current decimal context, which encapsulates the desired precision and rounding characteristics for computations:\nfrom decimal import Decimal, Context, localcontext # Displays with default precision of 28 digits v = Decimal('578') print v.sqrt() with localcontext(Context(prec=16)): # All code in this block uses a precision of 16 digits. # The original context is restored on exiting the block. print v.sqrt()\nWriting Context Managers \u00b6\nUnder the hood, the \u2018 with \u2018 statement is fairly complicated. Most people will only use \u2018 with \u2018 in company with existing objects and don\u2019t need to know these details, so you can skip the rest of this section if you like. Authors of new objects will need to understand the details of the underlying implementation and should keep reading.\nA high-level explanation of the context management protocol is:\nThe expression is evaluated and should result in an object called a \u201ccontext manager\u201d. The context manager must have __enter__() and __exit__() methods.\nThe context manager\u2019s __enter__() method is called. The value returned is assigned to VAR. If no 'as VAR' clause is present, the value is simply discarded.\nThe code in BLOCK is executed.\nIf BLOCK raises an exception, the __exit__(type, value, traceback)() is called with the exception details, the same values returned by sys.exc_info() . The method\u2019s return value controls whether the exception is re-raised: any false value re-raises the exception, and True will result in suppressing it. You\u2019ll only rarely want to suppress the exception, because if you do the author of the code containing the \u2018 with \u2018 statement will never realize anything went wrong.\nIf BLOCK didn\u2019t raise an exception, the __exit__() method is still called, but type, value, and traceback are all None.\nLet\u2019s think through an example. I won\u2019t present detailed code but will only sketch the methods necessary for a database that supports transactions.\n(For people unfamiliar with database terminology: a set of changes to the database are grouped into a transaction. Transactions can be either committed, meaning that all the changes are written into the database, or rolled back, meaning that the changes are all discarded and the database is unchanged. See any database textbook for more information.)\nLet\u2019s assume there\u2019s an object representing a database connection. Our goal will be to let the user write code like this:\ndb_connection = DatabaseConnection() with db_connection as cursor: cursor.execute('insert into ...') cursor.execute('delete from ...') # ... more operations ...\nThe transaction should be committed if the code in the block runs flawlessly or rolled back if there\u2019s an exception. Here\u2019s the basic interface for DatabaseConnection that I\u2019ll assume:\nclass DatabaseConnection: # Database interface def cursor (self): \"Returns a cursor object and starts a new transaction\" def commit (self): \"Commits current transaction\" def rollback (self): \"Rolls back current transaction\"\nThe __enter__() method is pretty easy, having only to start a new transaction. For this application the resulting cursor object would be a useful result, so the method will return it. The user can then add as cursor to their \u2018 with \u2018 statement to bind the cursor to a variable name.\nclass DatabaseConnection: ... def __enter__ (self): # Code to start a new transaction cursor = self.cursor() return cursor\nThe __exit__() method is the most complicated because it\u2019s where most of the work has to be done. The method has to check if an exception occurred. If there was no exception, the transaction is committed. The transaction is rolled back if there was an exception.\nIn the code below, execution will just fall off the end of the function, returning the default value of None. None is false, so the exception will be re-raised automatically. If you wished, you could be more explicit and add a return statement at the marked location.\nclass DatabaseConnection: ... def __exit__ (self, type, value, tb): if tb is None: # No exception, so commit self.commit() else: # Exception occurred, so rollback. self.rollback() # return False\nThe contextlib module \u00b6\nThe new contextlib module provides some functions and a decorator that are useful for writing objects for use with the \u2018 with \u2018 statement.\nThe decorator is called contextmanager(), and lets you write a single generator function instead of defining a new class. The generator should yield exactly one value. The code up to the yield will be executed as the __enter__() method, and the value yielded will be the method\u2019s return value that will get bound to the variable in the \u2018 with \u2018 statement\u2019s as clause, if any. The code after the yield will be executed in the __exit__() method. Any exception raised in the block will be raised by the yield statement.\nOur database example from the previous section could be written using this decorator as:\nfrom contextlib import contextmanager @contextmanager def db_transaction (connection): cursor = connection.cursor() try: yield cursor except: connection.rollback() raise else: connection.commit() db = DatabaseConnection() with db_transaction(db) as cursor: ...\nThe contextlib module also has a nested(mgr1, mgr2, ...)() function that combines a number of context managers so you don\u2019t need to write nested \u2018 with \u2018 statements. In this example, the single \u2018 with \u2018 statement both starts a database transaction and acquires a thread lock:\nlock = threading.Lock() with nested (db_transaction(db), lock) as (cursor, locked): ...\nFinally, the closing(object)() function returns object so that it can be bound to a variable, and calls object.close at the end of the block.\nimport urllib, sys from contextlib import closing with closing(urllib.urlopen('http://www.yahoo.com')) as f: for line in f: sys.stdout.write(line)\nSee also\nPEP 343 - The \u201cwith\u201d statement\nPEP written by Guido van Rossum and Nick Coghlan; implemented by Mike Bland, Guido van Rossum, and Neal Norwitz. The PEP shows the code generated for a \u2018 with \u2018 statement, which can be helpful in learning how the statement works.\nThe documentation for the contextlib module.\nPEP 352: Exceptions as New-Style Classes \u00b6\nException classes can now be new-style classes, not just classic classes, and the built-in Exception class and all the standard built-in exceptions ( NameError , ValueError , etc.) are now new-style classes.\nThe inheritance hierarchy for exceptions has been rearranged a bit. In 2.5, the inheritance relationships are:\nBaseException       # New in Python 2.5\n|- KeyboardInterrupt\n|- SystemExit\n|- Exception\n   |- (all other current built-in exceptions)\nThis rearrangement was done because people often want to catch all exceptions that indicate program errors. KeyboardInterrupt and SystemExit aren\u2019t errors, though, and usually represent an explicit action such as the user hitting Control-C or code calling sys.exit() . A bare except: will catch all exceptions, so you commonly need to list KeyboardInterrupt and SystemExit in order to re-raise them. The usual pattern is:\ntry:\n    ...\nexcept (KeyboardInterrupt, SystemExit):\n    raise\nexcept:\n    # Log error...\n    # Continue running program...\nIn Python 2.5, you can now write except Exception to achieve the same result, catching all the exceptions that usually indicate errors but leaving KeyboardInterrupt and SystemExit alone. As in previous versions, a bare except: still catches all exceptions.\nThe goal for Python 3.0 is to require any class raised as an exception to derive from BaseException or some descendant of BaseException , and future releases in the Python 2.x series may begin to enforce this constraint. Therefore, I suggest you begin making all your exception classes derive from Exception now. It\u2019s been suggested that the bare except: form should be removed in Python 3.0, but Guido van Rossum hasn\u2019t decided whether to do this or not.\nRaising of strings as exceptions, as in the statement raise \"Error occurred\", is deprecated in Python 2.5 and will trigger a warning. The aim is to be able to remove the string-exception feature in a few releases.\nSee also\nPEP 352 - Required Superclass for Exceptions\nPEP written by Brett Cannon and Guido van Rossum; implemented by Brett Cannon.\nPEP 353: Using ssize_t as the index type \u00b6\nA wide-ranging change to Python\u2019s C API, using a new Py_ssize_t type definition instead of int, will permit the interpreter to handle more data on 64-bit platforms. This change doesn\u2019t affect Python\u2019s capacity on 32-bit platforms.\nVarious pieces of the Python interpreter used C\u2019s int type to store sizes or counts; for example, the number of items in a list or tuple were stored in an int. The C compilers for most 64-bit platforms still define int as a 32-bit type, so that meant that lists could only hold up to 2**31 - 1 = 2147483647 items. (There are actually a few different programming models that 64-bit C compilers can use \u2013 see http://www.unix.org/version2/whatsnew/lp64_wp.html for a discussion \u2013 but the most commonly available model leaves int as 32 bits.)\nA limit of 2147483647 items doesn\u2019t really matter on a 32-bit platform because you\u2019ll run out of memory before hitting the length limit. Each list item requires space for a pointer, which is 4 bytes, plus space for a PyObject representing the item. 2147483647*4 is already more bytes than a 32-bit address space can contain.\nIt\u2019s possible to address that much memory on a 64-bit platform, however. The pointers for a list that size would only require 16 GiB of space, so it\u2019s not unreasonable that Python programmers might construct lists that large. Therefore, the Python interpreter had to be changed to use some type other than int, and this will be a 64-bit type on 64-bit platforms. The change will cause incompatibilities on 64-bit machines, so it was deemed worth making the transition now, while the number of 64-bit users is still relatively small. (In 5 or 10 years, we may all be on 64-bit machines, and the transition would be more painful then.)\nThis change most strongly affects authors of C extension modules. Python strings and container types such as lists and tuples now use Py_ssize_t to store their size. Functions such as PyList_Size() now return Py_ssize_t. Code in extension modules may therefore need to have some variables changed to Py_ssize_t.\nThe PyArg_ParseTuple() and Py_BuildValue() functions have a new conversion code, n, for Py_ssize_t. PyArg_ParseTuple() \u2018s s# and t# still output int by default, but you can define the macro PY_SSIZE_T_CLEAN before including Python.h to make them return Py_ssize_t.\nPEP 353 has a section on conversion guidelines that extension authors should read to learn about supporting 64-bit platforms.\nSee also\nPEP 353 - Using ssize_t as the index type\nPEP written and implemented by Martin von L\u00f6wis.\nPEP 357: The \u2018__index__\u2019 method \u00b6\nThe NumPy developers had a problem that could only be solved by adding a new special method, __index__() . When using slice notation, as in [start:stop:step], the values of the start, stop, and step indexes must all be either integers or long integers. NumPy defines a variety of specialized integer types corresponding to unsigned and signed integers of 8, 16, 32, and 64 bits, but there was no way to signal that these types could be used as slice indexes.\nSlicing can\u2019t just use the existing __int__() method because that method is also used to implement coercion to integers. If slicing used __int__() , floating-point numbers would also become legal slice indexes and that\u2019s clearly an undesirable behaviour.\nInstead, a new special method called __index__() was added. It takes no arguments and returns an integer giving the slice index to use. For example:\nclass C: def __index__ (self): return self.value\nThe return value must be either a Python integer or long integer. The interpreter will check that the type returned is correct, and raises a TypeError if this requirement isn\u2019t met.\nA corresponding nb_index slot was added to the C-level PyNumberMethods structure to let C extensions implement this protocol. PyNumber_Index(obj)() can be used in extension code to call the __index__() function and retrieve its result.\nSee also\nPEP 357 - Allowing Any Object to be Used for Slicing\nPEP written and implemented by Travis Oliphant.\nOther Language Changes \u00b6\nHere are all of the changes that Python 2.5 makes to the core Python language.\nThe dict type has a new hook for letting subclasses provide a default value when a key isn\u2019t contained in the dictionary. When a key isn\u2019t found, the dictionary\u2019s __missing__(key)() method will be called. This hook is used to implement the new defaultdict class in the collections module. The following example defines a dictionary that returns zero for any missing key:\nclass zerodict (dict): def __missing__ (self, key): return 0 d = zerodict({1:1, 2:2}) print d[1], d[2] # Prints 1, 2 print d[3], d[4] # Prints 0, 0\nBoth 8-bit and Unicode strings have new partition(sep)() and rpartition(sep)() methods that simplify a common use case.\nThe find(S)() method is often used to get an index which is then used to slice the string and obtain the pieces that are before and after the separator. partition(sep)() condenses this pattern into a single method call that returns a 3-tuple containing the substring before the separator, the separator itself, and the substring after the separator. If the separator isn\u2019t found, the first element of the tuple is the entire string and the other two elements are empty. rpartition(sep)() also returns a 3-tuple but starts searching from the end of the string; the r stands for \u2018reverse\u2019.\nSome examples:\n>>> ('http://www.python.org').partition('://') ('http', '://', 'www.python.org') >>> ('file:/usr/share/doc/index.html').partition('://') ('file:/usr/share/doc/index.html', '', '') >>> (u'Subject: a quick question').partition(':') (u'Subject', u':', u' a quick question') >>> 'www.python.org'.rpartition('.') ('www.python', '.', 'org') >>> 'www.python.org'.rpartition(':') ('', '', 'www.python.org')\n(Implemented by Fredrik Lundh following a suggestion by Raymond Hettinger.)\nThe startswith() and endswith() methods of string types now accept tuples of strings to check for.\ndef is_image_file (filename): return filename.endswith(('.gif', '.jpg', '.tiff'))\n(Implemented by Georg Brandl following a suggestion by Tom Lynn.)\nThe min() and max() built-in functions gained a key keyword parameter analogous to the key argument for sort(). This parameter supplies a function that takes a single argument and is called for every value in the list; min() / max() will return the element with the smallest/largest return value from this function. For example, to find the longest string in a list, you can do:\nL = ['medium', 'longest', 'short'] # Prints 'longest' print max(L, key=len) # Prints 'short', because lexicographically 'short' has the largest value print max(L)\n(Contributed by Steven Bethard and Raymond Hettinger.)\nTwo new built-in functions, any() and all() , evaluate whether an iterator contains any true or false values. any() returns True if any value returned by the iterator is true; otherwise it will return False . all() returns True only if all of the values returned by the iterator evaluate as true. (Suggested by Guido van Rossum, and implemented by Raymond Hettinger.)\nThe result of a class\u2019s __hash__() method can now be either a long integer or a regular integer. If a long integer is returned, the hash of that value is taken. In earlier versions the hash value was required to be a regular integer, but in 2.5 the id() built-in was changed to always return non-negative numbers, and users often seem to use id(self) in __hash__() methods (though this is discouraged).\nASCII is now the default encoding for modules. It\u2019s now a syntax error if a module contains string literals with 8-bit characters but doesn\u2019t have an encoding declaration. In Python 2.4 this triggered a warning, not a syntax error. See PEP 263 for how to declare a module\u2019s encoding; for example, you might add a line like this near the top of the source file:\n# -*- coding: latin1 -*-\nA new warning, UnicodeWarning, is triggered when you attempt to compare a Unicode string and an 8-bit string that can\u2019t be converted to Unicode using the default ASCII encoding. The result of the comparison is false:\n>>> chr(128) == unichr(128) # Can't convert chr(128) to Unicode __main__:1: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal False >>> chr(127) == unichr(127) # chr(127) can be converted True\nPreviously this would raise a UnicodeDecodeError exception, but in 2.5 this could result in puzzling problems when accessing a dictionary. If you looked up unichr(128) and chr(128) was being used as a key, you\u2019d get a UnicodeDecodeError exception. Other changes in 2.5 resulted in this exception being raised instead of suppressed by the code in dictobject.c that implements dictionaries.\nRaising an exception for such a comparison is strictly correct, but the change might have broken code, so instead UnicodeWarning was introduced.\n(Implemented by Marc-Andr\u00e9 Lemburg.)\nOne error that Python programmers sometimes make is forgetting to include an __init__.py module in a package directory. Debugging this mistake can be confusing, and usually requires running Python with the -v switch to log all the paths searched. In Python 2.5, a new ImportWarning warning is triggered when an import would have picked up a directory as a package but no __init__.py was found. This warning is silently ignored by default; provide the -Wd option when running the Python executable to display the warning message. (Implemented by Thomas Wouters.)\nThe list of base classes in a class definition can now be empty. As an example, this is now legal:\nclass C(): pass\n(Implemented by Brett Cannon.)\nInteractive Interpreter Changes \u00b6\nIn the interactive interpreter, quit and exit have long been strings so that new users get a somewhat helpful message when they try to quit:\n>>> quit 'Use Ctrl-D (i.e. EOF) to exit.'\nIn Python 2.5, quit and exit are now objects that still produce string representations of themselves, but are also callable. Newbies who try quit() or exit() will now exit the interpreter as they expect. (Implemented by Georg Brandl.)\nThe Python executable now accepts the standard long options --help and --version ; on Windows, it also accepts the /? option for displaying a help message. (Implemented by Georg Brandl.)\nOptimizations \u00b6\nSeveral of the optimizations were developed at the NeedForSpeed sprint, an event held in Reykjavik, Iceland, from May 21\u201328 2006. The sprint focused on speed enhancements to the CPython implementation and was funded by EWT LLC with local support from CCP Games. Those optimizations added at this sprint are specially marked in the following list.\nWhen they were introduced in Python 2.4, the built-in set and frozenset types were built on top of Python\u2019s dictionary type. In 2.5 the internal data structure has been customized for implementing sets, and as a result sets will use a third less memory and are somewhat faster. (Implemented by Raymond Hettinger.)\nThe speed of some Unicode operations, such as finding substrings, string splitting, and character map encoding and decoding, has been improved. (Substring search and splitting improvements were added by Fredrik Lundh and Andrew Dalke at the NeedForSpeed sprint. Character maps were improved by Walter D\u00f6rwald and Martin von L\u00f6wis.)\nThe long(str, base)() function is now faster on long digit strings because fewer intermediate results are calculated. The peak is for strings of around 800\u20131000 digits where the function is 6 times faster. (Contributed by Alan McIntyre and committed at the NeedForSpeed sprint.)\nIt\u2019s now illegal to mix iterating over a file with for line in file and calling the file object\u2019s read()/ readline() /readlines() methods. Iteration uses an internal buffer and the read*() methods don\u2019t use that buffer. Instead they would return the data following the buffer, causing the data to appear out of order. Mixing iteration and these methods will now trigger a ValueError from the read*() method. (Implemented by Thomas Wouters.)\nThe struct module now compiles structure format strings into an internal representation and caches this representation, yielding a 20% speedup. (Contributed by Bob Ippolito at the NeedForSpeed sprint.)\nThe re module got a 1 or 2% speedup by switching to Python\u2019s allocator functions instead of the system\u2019s malloc() and free(). (Contributed by Jack Diederich at the NeedForSpeed sprint.)\nThe code generator\u2019s peephole optimizer now performs simple constant folding in expressions. If you write something like a = 2+3, the code generator will do the arithmetic and produce code corresponding to a = 5. (Proposed and implemented by Raymond Hettinger.)\nFunction calls are now faster because code objects now keep the most recently finished frame (a \u201czombie frame\u201d) in an internal field of the code object, reusing it the next time the code object is invoked. (Original patch by Michael Hudson, modified by Armin Rigo and Richard Jones; committed at the NeedForSpeed sprint.) Frame objects are also slightly smaller, which may improve cache locality and reduce memory usage a bit. (Contributed by Neal Norwitz.)\nPython\u2019s built-in exceptions are now new-style classes, a change that speeds up instantiation considerably. Exception handling in Python 2.5 is therefore about 30% faster than in 2.4. (Contributed by Richard Jones, Georg Brandl and Sean Reifschneider at the NeedForSpeed sprint.)\nImporting now caches the paths tried, recording whether they exist or not so that the interpreter makes fewer open() and stat() calls on startup. (Contributed by Martin von L\u00f6wis and Georg Brandl.)\nNew, Improved, and Removed Modules \u00b6\nThe standard library received many enhancements and bug fixes in Python 2.5. Here\u2019s a partial list of the most notable changes, sorted alphabetically by module name. Consult the Misc/NEWS file in the source tree for a more complete list of changes, or look through the SVN logs for all the details.\nThe audioop module now supports the a-LAW encoding, and the code for u-LAW encoding has been improved. (Contributed by Lars Immisch.)\nThe codecs module gained support for incremental codecs. The codec.lookup() function now returns a CodecInfo instance instead of a tuple. CodecInfo instances behave like a 4-tuple to preserve backward compatibility but also have the attributes encode, decode, incrementalencoder, incrementaldecoder, streamwriter, and streamreader. Incremental codecs can receive input and produce output in multiple chunks; the output is the same as if the entire input was fed to the non-incremental codec. See the codecs module documentation for details. (Designed and implemented by Walter D\u00f6rwald.)\nThe collections module gained a new type, defaultdict, that subclasses the standard dict type. The new type mostly behaves like a dictionary but constructs a default value when a key isn\u2019t present, automatically adding it to the dictionary for the requested key value.\nThe first argument to defaultdict\u2018s constructor is a factory function that gets called whenever a key is requested but not found. This factory function receives no arguments, so you can use built-in type constructors such as list() or int() . For example, you can make an index of words based on their initial letter like this:\nwords = \"\"\"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura che la diritta via era smarrita\"\"\".lower().split() index = defaultdict(list) for w in words: init_letter = w[0] index[init_letter].append(w)\nPrinting index results in the following output:\ndefaultdict(<type 'list'>, {'c': ['cammin', 'che'], 'e': ['era'],\n        'd': ['del', 'di', 'diritta'], 'm': ['mezzo', 'mi'],\n        'l': ['la'], 'o': ['oscura'], 'n': ['nel', 'nostra'],\n        'p': ['per'], 's': ['selva', 'smarrita'],\n        'r': ['ritrovai'], 'u': ['una'], 'v': ['vita', 'via']}\n(Contributed by Guido van Rossum.)\nThe deque double-ended queue type supplied by the collections module now has a remove(value)() method that removes the first occurrence of value in the queue, raising ValueError if the value isn\u2019t found. (Contributed by Raymond Hettinger.)\nNew module: The contextlib module contains helper functions for use with the new \u2018 with \u2018 statement. See section The contextlib module for more about this module.\nNew module: The cProfile module is a C implementation of the existing profile module that has much lower overhead. The module\u2019s interface is the same as profile : you run cProfile.run('main()') to profile a function, can save profile data to a file, etc. It\u2019s not yet known if the Hotshot profiler, which is also written in C but doesn\u2019t match the profile module\u2019s interface, will continue to be maintained in future versions of Python. (Contributed by Armin Rigo.)\nAlso, the pstats module for analyzing the data measured by the profiler now supports directing the output to any file object by supplying a stream argument to the Stats constructor. (Contributed by Skip Montanaro.)\nThe csv module, which parses files in comma-separated value format, received several enhancements and a number of bugfixes. You can now set the maximum size in bytes of a field by calling the csv.field_size_limit(new_limit)() function; omitting the new_limit argument will return the currently-set limit. The reader class now has a line_num attribute that counts the number of physical lines read from the source; records can span multiple physical lines, so line_num is not the same as the number of records read.\nThe CSV parser is now stricter about multi-line quoted fields. Previously, if a line ended within a quoted field without a terminating newline character, a newline would be inserted into the returned field. This behavior caused problems when reading files that contained carriage return characters within fields, so the code was changed to return the field without inserting newlines. As a consequence, if newlines embedded within fields are important, the input should be split into lines in a manner that preserves the newline characters.\n(Contributed by Skip Montanaro and Andrew McNamara.)\nThe datetime class in the datetime module now has a strptime(string, format)() method for parsing date strings, contributed by Josh Spoerri. It uses the same format characters as time.strptime() and time.strftime() :\nfrom datetime import datetime ts = datetime.strptime('10:13:15 2006-03-07', '%H:%M:%S %Y-%m-%d')\nThe SequenceMatcher.get_matching_blocks() method in the difflib module now guarantees to return a minimal list of blocks describing matching subsequences. Previously, the algorithm would occasionally break a block of matching elements into two list entries. (Enhancement by Tim Peters.)\nThe doctest module gained a SKIP option that keeps an example from being executed at all. This is intended for code snippets that are usage examples intended for the reader and aren\u2019t actually test cases.\nAn encoding parameter was added to the testfile() function and the DocFileSuite class to specify the file\u2019s encoding. This makes it easier to use non-ASCII characters in tests contained within a docstring. (Contributed by Bjorn Tillenius.)\nThe email package has been updated to version 4.0. (Contributed by Barry Warsaw.)\nThe fileinput module was made more flexible. Unicode filenames are now supported, and a mode parameter that defaults to \"r\" was added to the input() function to allow opening files in binary or universal-newline mode. Another new parameter, openhook, lets you use a function other than open() to open the input files. Once you\u2019re iterating over the set of files, the FileInput object\u2019s new fileno() returns the file descriptor for the currently opened file. (Contributed by Georg Brandl.)\nIn the gc module, the new get_count() function returns a 3-tuple containing the current collection counts for the three GC generations. This is accounting information for the garbage collector; when these counts reach a specified threshold, a garbage collection sweep will be made. The existing gc.collect() function now takes an optional generation argument of 0, 1, or 2 to specify which generation to collect. (Contributed by Barry Warsaw.)\nThe nsmallest() and nlargest() functions in the heapq module now support a key keyword parameter similar to the one provided by the min() / max() functions and the sort() methods. For example:\n>>> import heapq >>> L = [\"short\", 'medium', 'longest', 'longer still'] >>> heapq.nsmallest(2, L) # Return two lowest elements, lexicographically ['longer still', 'longest'] >>> heapq.nsmallest(2, L, key=len) # Return two shortest elements ['short', 'medium']\n(Contributed by Raymond Hettinger.)\nThe itertools.islice() function now accepts None for the start and step arguments. This makes it more compatible with the attributes of slice objects, so that you can now write the following:\ns = slice(5) # Create slice object itertools.islice(iterable, s.start, s.stop, s.step)\n(Contributed by Raymond Hettinger.)\nThe format() function in the locale module has been modified and two new functions were added, format_string() and currency().\nThe format() function\u2019s val parameter could previously be a string as long as no more than one %char specifier appeared; now the parameter must be exactly one %char specifier with no surrounding text. An optional monetary parameter was also added which, if True, will use the locale\u2019s rules for formatting currency in placing a separator between groups of three digits.\nTo format strings with multiple %char specifiers, use the new format_string() function that works like format() but also supports mixing %char specifiers with arbitrary text.\nA new currency() function was also added that formats a number according to the current locale\u2019s settings.\n(Contributed by Georg Brandl.)\nThe mailbox module underwent a massive rewrite to add the capability to modify mailboxes in addition to reading them. A new set of classes that include mbox, MH, and Maildir are used to read mailboxes, and have an add(message)() method to add messages, remove(key)() to remove messages, and lock()/unlock() to lock/unlock the mailbox. The following example converts a maildir-format mailbox into an mbox-format one:\nimport mailbox # 'factory=None' uses email.Message.Message as the class representing # individual messages. src = mailbox.Maildir('maildir', factory=None) dest = mailbox.mbox('/tmp/mbox') for msg in src: dest.add(msg)\n(Contributed by Gregory K. Johnson. Funding was provided by Google\u2019s 2005 Summer of Code.)\nNew module: the msilib module allows creating Microsoft Installer .msi files and CAB files. Some support for reading the .msi database is also included. (Contributed by Martin von L\u00f6wis.)\nThe nis module now supports accessing domains other than the system default domain by supplying a domain argument to the nis.match() and nis.maps() functions. (Contributed by Ben Bell.)\nThe operator module\u2019s itemgetter() and attrgetter() functions now support multiple fields. A call such as operator.attrgetter('a', 'b') will return a function that retrieves the a and b attributes. Combining this new feature with the sort() method\u2019s key parameter lets you easily sort lists using multiple fields. (Contributed by Raymond Hettinger.)\nThe optparse module was updated to version 1.5.1 of the Optik library. The OptionParser class gained an epilog attribute, a string that will be printed after the help message, and a destroy() method to break reference cycles created by the object. (Contributed by Greg Ward.)\nThe os module underwent several changes. The stat_float_times variable now defaults to true, meaning that os.stat() will now return time values as floats. (This doesn\u2019t necessarily mean that os.stat() will return times that are precise to fractions of a second; not all systems support such precision.)\nConstants named os.SEEK_SET , os.SEEK_CUR , and os.SEEK_END have been added; these are the parameters to the os.lseek() function. Two new constants for locking are os.O_SHLOCK and os.O_EXLOCK .\nTwo new functions, wait3() and wait4(), were added. They\u2019re similar the waitpid() function which waits for a child process to exit and returns a tuple of the process ID and its exit status, but wait3() and wait4() return additional information. wait3() doesn\u2019t take a process ID as input, so it waits for any child process to exit and returns a 3-tuple of process-id, exit-status, resource-usage as returned from the resource.getrusage() function. wait4(pid)() does take a process ID. (Contributed by Chad J. Schroeder.)\nOn FreeBSD, the os.stat() function now returns times with nanosecond resolution, and the returned object now has st_gen and st_birthtime. The st_flags attribute is also available, if the platform supports it. (Contributed by Antti Louko and Diego Petten\u00f2.)\nThe Python debugger provided by the pdb module can now store lists of commands to execute when a breakpoint is reached and execution stops. Once breakpoint #1 has been created, enter commands 1 and enter a series of commands to be executed, finishing the list with end. The command list can include commands that resume execution, such as continue or next. (Contributed by Gr\u00e9goire Dooms.)\nThe pickle and cPickle modules no longer accept a return value of None from the __reduce__() method; the method must return a tuple of arguments instead. The ability to return None was deprecated in Python 2.4, so this completes the removal of the feature.\nThe pkgutil module, containing various utility functions for finding packages, was enhanced to support PEP 302\u2019s import hooks and now also works for packages stored in ZIP-format archives. (Contributed by Phillip J. Eby.)\nThe pybench benchmark suite by Marc-Andr\u00e9 Lemburg is now included in the Tools/pybench directory. The pybench suite is an improvement on the commonly used pystone.py program because pybench provides a more detailed measurement of the interpreter\u2019s speed. It times particular operations such as function calls, tuple slicing, method lookups, and numeric operations, instead of performing many different operations and reducing the result to a single number as pystone.py does.\nThe pyexpat module now uses version 2.0 of the Expat parser. (Contributed by Trent Mick.)\nThe Queue class provided by the Queue module gained two new methods. join() blocks until all items in the queue have been retrieved and all processing work on the items have been completed. Worker threads call the other new method, task_done(), to signal that processing for an item has been completed. (Contributed by Raymond Hettinger.)\nThe old regex and regsub modules, which have been deprecated ever since Python 2.0, have finally been deleted. Other deleted modules: statcache, tzparse, whrandom.\nAlso deleted: the lib-old directory, which includes ancient modules such as dircmp and ni, was removed. lib-old wasn\u2019t on the default sys.path, so unless your programs explicitly added the directory to sys.path, this removal shouldn\u2019t affect your code.\nThe rlcompleter module is no longer dependent on importing the readline module and therefore now works on non-Unix platforms. (Patch from Robert Kiendl.)\nThe SimpleXMLRPCServer and DocXMLRPCServer classes now have a rpc_paths attribute that constrains XML-RPC operations to a limited set of URL paths; the default is to allow only '/' and '/RPC2'. Setting rpc_paths to None or an empty tuple disables this path checking.\nThe socket module now supports AF_NETLINK sockets on Linux, thanks to a patch from Philippe Biondi. Netlink sockets are a Linux-specific mechanism for communications between a user-space process and kernel code; an introductory article about them is at http://www.linuxjournal.com/article/7356 . In Python code, netlink addresses are represented as a tuple of 2 integers, (pid, group_mask).\nTwo new methods on socket objects, recv_into(buffer)() and recvfrom_into(buffer)(), store the received data in an object that supports the buffer protocol instead of returning the data as a string. This means you can put the data directly into an array or a memory-mapped file.\nSocket objects also gained getfamily(), gettype(), and getproto() accessor methods to retrieve the family, type, and protocol values for the socket.\nNew module: the spwd module provides functions for accessing the shadow password database on systems that support shadow passwords.\nThe struct is now faster because it compiles format strings into Struct objects with pack() and unpack() methods. This is similar to how the re module lets you create compiled regular expression objects. You can still use the module-level pack() and unpack() functions; they\u2019ll create Struct objects and cache them. Or you can use Struct instances directly:\ns = struct.Struct('ih3s') data = s.pack(1972, 187, 'abc') year, number, name = s.unpack(data)\nYou can also pack and unpack data to and from buffer objects directly using the pack_into(buffer, offset, v1, v2, ...)() and unpack_from(buffer, offset)() methods. This lets you store data directly into an array or a memory- mapped file.\n(Struct objects were implemented by Bob Ippolito at the NeedForSpeed sprint. Support for buffer objects was added by Martin Blais, also at the NeedForSpeed sprint.)\nThe Python developers switched from CVS to Subversion during the 2.5 development process. Information about the exact build version is available as the sys.subversion variable, a 3-tuple of (interpreter-name, branch-name, revision-range). For example, at the time of writing my copy of 2.5 was reporting ('CPython', 'trunk', '45313:45315').\nThis information is also available to C extensions via the Py_GetBuildInfo() function that returns a string of build information like this: \"trunk:45355:45356M, Apr 13 2006, 07:42:19\". (Contributed by Barry Warsaw.)\nAnother new function, sys._current_frames() , returns the current stack frames for all running threads as a dictionary mapping thread identifiers to the topmost stack frame currently active in that thread at the time the function is called. (Contributed by Tim Peters.)\nThe TarFile class in the tarfile module now has an extractall() method that extracts all members from the archive into the current working directory. It\u2019s also possible to set a different directory as the extraction target, and to unpack only a subset of the archive\u2019s members.\nThe compression used for a tarfile opened in stream mode can now be autodetected using the mode 'r|*'. (Contributed by Lars Gust\u00e4bel.)\nThe threading module now lets you set the stack size used when new threads are created. The stack_size([*size*])() function returns the currently configured stack size, and supplying the optional size parameter sets a new value. Not all platforms support changing the stack size, but Windows, POSIX threading, and OS/2 all do. (Contributed by Andrew MacIntyre.)\nThe unicodedata module has been updated to use version 4.1.0 of the Unicode character database. Version 3.2.0 is required by some specifications, so it\u2019s still available as unicodedata.ucd_3_2_0 .\nNew module: the uuid module generates universally unique identifiers (UUIDs) according to RFC 4122 . The RFC defines several different UUID versions that are generated from a starting string, from system properties, or purely randomly. This module contains a UUID class and functions named uuid1(), uuid3(), uuid4(), and uuid5() to generate different versions of UUID. (Version 2 UUIDs are not specified in RFC 4122 and are not supported by this module.)\n>>> import uuid >>> # make a UUID based on the host ID and current time >>> uuid.uuid1() UUID('a8098c1a-f86e-11da-bd1a-00112444be1e') >>> # make a UUID using an MD5 hash of a namespace UUID and a name >>> uuid.uuid3(uuid.NAMESPACE_DNS, 'python.org') UUID('6fa459ea-ee8a-3ca4-894e-db77e160355e') >>> # make a random UUID >>> uuid.uuid4() UUID('16fd2706-8baf-433b-82eb-8c7fada847da') >>> # make a UUID using a SHA-1 hash of a namespace UUID and a name >>> uuid.uuid5(uuid.NAMESPACE_DNS, 'python.org') UUID('886313e1-3b8a-5372-9b90-0c9aee199e5d')\n(Contributed by Ka-Ping Yee.)\nThe weakref module\u2019s WeakKeyDictionary and WeakValueDictionary types gained new methods for iterating over the weak references contained in the dictionary. iterkeyrefs() and keyrefs() methods were added to WeakKeyDictionary, and itervaluerefs() and valuerefs() were added to WeakValueDictionary. (Contributed by Fred L. Drake, Jr.)\nThe webbrowser module received a number of enhancements. It\u2019s now usable as a script with python -m webbrowser, taking a URL as the argument; there are a number of switches to control the behaviour (-n for a new browser window, -t for a new tab). New module-level functions, open_new() and open_new_tab(), were added to support this. The module\u2019s open() function supports an additional feature, an autoraise parameter that signals whether to raise the open window when possible. A number of additional browsers were added to the supported list such as Firefox, Opera, Konqueror, and elinks. (Contributed by Oleg Broytmann and Georg Brandl.)\nThe xmlrpclib module now supports returning datetime objects for the XML-RPC date type. Supply use_datetime=True to the loads() function or the Unmarshaller class to enable this feature. (Contributed by Skip Montanaro.)\nThe zipfile module now supports the ZIP64 version of the format, meaning that a .zip archive can now be larger than 4 GiB and can contain individual files larger than 4 GiB. (Contributed by Ronald Oussoren.)\nThe zlib module\u2019s Compress and Decompress objects now support a copy() method that makes a copy of the object\u2019s internal state and returns a new Compress or Decompress object. (Contributed by Chris AtLee.)\nThe ctypes package \u00b6\nThe ctypes package, written by Thomas Heller, has been added to the standard library. ctypes lets you call arbitrary functions in shared libraries or DLLs. Long-time users may remember the dl module, which provides functions for loading shared libraries and calling functions in them. The ctypes package is much fancier.\nTo load a shared library or DLL, you must create an instance of the CDLL class and provide the name or path of the shared library or DLL. Once that\u2019s done, you can call arbitrary functions by accessing them as attributes of the CDLL object.\nimport ctypes libc = ctypes.CDLL('libc.so.6') result = libc.printf(\"Line of output\\n\")\nType constructors for the various C types are provided: c_int(), c_float(), c_double(), c_char_p() (equivalent to char *), and so forth. Unlike Python\u2019s types, the C versions are all mutable; you can assign to their value attribute to change the wrapped value. Python integers and strings will be automatically converted to the corresponding C types, but for other types you must call the correct type constructor. (And I mean must; getting it wrong will often result in the interpreter crashing with a segmentation fault.)\nYou shouldn\u2019t use c_char_p() with a Python string when the C function will be modifying the memory area, because Python strings are supposed to be immutable; breaking this rule will cause puzzling bugs. When you need a modifiable memory area, use create_string_buffer():\ns = \"this is a string\" buf = ctypes.create_string_buffer(s) libc.strfry(buf)\nC functions are assumed to return integers, but you can set the restype attribute of the function object to change this:\n>>> libc.atof('2.71828') -1783957616 >>> libc.atof.restype = ctypes.c_double >>> libc.atof('2.71828') 2.71828\nctypes also provides a wrapper for Python\u2019s C API as the ctypes.pythonapi object. This object does not release the global interpreter lock before calling a function, because the lock must be held when calling into the interpreter\u2019s code. There\u2019s a py_object() type constructor that will create a PyObject * pointer. A simple usage:\nimport ctypes d = {} ctypes.pythonapi.PyObject_SetItem(ctypes.py_object(d), ctypes.py_object(\"abc\"), ctypes.py_object(1)) # d is now {'abc', 1}.\nDon\u2019t forget to use py_object(); if it\u2019s omitted you end up with a segmentation fault.\nctypes has been around for a while, but people still write and distribution hand-coded extension modules because you can\u2019t rely on ctypes being present. Perhaps developers will begin to write Python wrappers atop a library accessed through ctypes instead of extension modules, now that ctypes is included with core Python.\nSee also\nThe ctypes web page, with a tutorial, reference, and FAQ.\nThe documentation for the ctypes module.\nThe ElementTree package \u00b6\nA subset of Fredrik Lundh\u2019s ElementTree library for processing XML has been added to the standard library as xml.etree. The available modules are ElementTree, ElementPath, and ElementInclude from ElementTree 1.2.6. The cElementTree accelerator module is also included.\nThe rest of this section will provide a brief overview of using ElementTree. Full documentation for ElementTree is available at http://effbot.org/zone/element-index.htm .\nElementTree represents an XML document as a tree of element nodes. The text content of the document is stored as the text and tail attributes of (This is one of the major differences between ElementTree and the Document Object Model; in the DOM there are many different types of node, including TextNode.)\nThe most commonly used parsing function is parse(), that takes either a string (assumed to contain a filename) or a file-like object and returns an ElementTree instance:\nfrom xml.etree import ElementTree as ET tree = ET.parse('ex-1.xml') feed = urllib.urlopen( 'http://planet.python.org/rss10.xml') tree = ET.parse(feed)\nOnce you have an ElementTree instance, you can call its getroot() method to get the root Element node.\nThere\u2019s also an XML() function that takes a string literal and returns an Element node (not an ElementTree). This function provides a tidy way to incorporate XML fragments, approaching the convenience of an XML literal:\nsvg = ET.XML(\"\"\"<svg width=\"10px\" version=\"1.0\"> </svg>\"\"\") svg.set('height', '320px') svg.append(elem1)\nEach XML element supports some dictionary-like and some list-like access methods. Dictionary-like operations are used to access attribute values, and list-like operations are used to access child nodes.\nOperation\n"}, {"score": 912.69745, "uuid": "1e6a0c7e-28ac-5e0b-a7f1-53e02a8fca5e", "index": "cw12", "trec_id": "clueweb12-0307wb-53-25569", "target_hostname": "docs.python.org", "target_uri": "http://docs.python.org/dev/whatsnew/2.5.html", "page_rank": 1.2058023e-09, "spam_rank": 70, "title": "What\u2019s New in <em>Python</em> 2.5 \u2014 <em>Python</em> v3.3a0 documentation", "snippet": "The decision was made to not require parentheses in the <em>Python</em> language\u2019s grammar, but as a matter of style <em>I</em> think you <em>should</em> always use them.", "explanation": null, "document": "An explanation of coroutines from a Perl point of view, written by Dan Sugalski.\nPEP 343: The \u2018with\u2019 statement \u00b6\nThe \u2018 with \u2018 statement clarifies code that previously would use try...finally blocks to ensure that clean-up code is executed. In this section, I\u2019ll discuss the statement as it will commonly be used. In the next section, I\u2019ll examine the implementation details and show how to write objects for use with this statement.\nThe \u2018 with \u2018 statement is a new control-flow structure whose basic structure is:\nwith expression [as variable]: with-block\nThe expression is evaluated, and it should result in an object that supports the context management protocol (that is, has __enter__() and __exit__() methods.\nThe object\u2019s __enter__() is called before with-block is executed and therefore can run set-up code. It also may return a value that is bound to the name variable, if given. (Note carefully that variable is not assigned the result of expression.)\nAfter execution of the with-block is finished, the object\u2019s __exit__() method is called, even if the block raised an exception, and can therefore run clean-up code.\nTo enable the statement in Python 2.5, you need to add the following directive to your module:\nfrom __future__ import with_statement\nThe statement will always be enabled in Python 2.6.\nSome standard Python objects now support the context management protocol and can be used with the \u2018 with \u2018 statement. File objects are one example:\nwith open('/etc/passwd', 'r') as f: for line in f: print line ... more processing code ...\nAfter this statement has executed, the file object in f will have been automatically closed, even if the for loop raised an exception part- way through the block.\nNote\nIn this case, f is the same object created by open() , because file.__enter__() returns self.\nThe threading module\u2019s locks and condition variables also support the \u2018 with \u2018 statement:\nlock = threading.Lock() with lock: # Critical section of code ...\nThe lock is acquired before the block is executed and always released once the block is complete.\nThe new localcontext() function in the decimal module makes it easy to save and restore the current decimal context, which encapsulates the desired precision and rounding characteristics for computations:\nfrom decimal import Decimal, Context, localcontext # Displays with default precision of 28 digits v = Decimal('578') print v.sqrt() with localcontext(Context(prec=16)): # All code in this block uses a precision of 16 digits. # The original context is restored on exiting the block. print v.sqrt()\nWriting Context Managers \u00b6\nUnder the hood, the \u2018 with \u2018 statement is fairly complicated. Most people will only use \u2018 with \u2018 in company with existing objects and don\u2019t need to know these details, so you can skip the rest of this section if you like. Authors of new objects will need to understand the details of the underlying implementation and should keep reading.\nA high-level explanation of the context management protocol is:\nThe expression is evaluated and should result in an object called a \u201ccontext manager\u201d. The context manager must have __enter__() and __exit__() methods.\nThe context manager\u2019s __enter__() method is called. The value returned is assigned to VAR. If no 'as VAR' clause is present, the value is simply discarded.\nThe code in BLOCK is executed.\nIf BLOCK raises an exception, the __exit__(type, value, traceback)() is called with the exception details, the same values returned by sys.exc_info() . The method\u2019s return value controls whether the exception is re-raised: any false value re-raises the exception, and True will result in suppressing it. You\u2019ll only rarely want to suppress the exception, because if you do the author of the code containing the \u2018 with \u2018 statement will never realize anything went wrong.\nIf BLOCK didn\u2019t raise an exception, the __exit__() method is still called, but type, value, and traceback are all None.\nLet\u2019s think through an example. I won\u2019t present detailed code but will only sketch the methods necessary for a database that supports transactions.\n(For people unfamiliar with database terminology: a set of changes to the database are grouped into a transaction. Transactions can be either committed, meaning that all the changes are written into the database, or rolled back, meaning that the changes are all discarded and the database is unchanged. See any database textbook for more information.)\nLet\u2019s assume there\u2019s an object representing a database connection. Our goal will be to let the user write code like this:\ndb_connection = DatabaseConnection() with db_connection as cursor: cursor.execute('insert into ...') cursor.execute('delete from ...') # ... more operations ...\nThe transaction should be committed if the code in the block runs flawlessly or rolled back if there\u2019s an exception. Here\u2019s the basic interface for DatabaseConnection that I\u2019ll assume:\nclass DatabaseConnection: # Database interface def cursor (self): \"Returns a cursor object and starts a new transaction\" def commit (self): \"Commits current transaction\" def rollback (self): \"Rolls back current transaction\"\nThe __enter__() method is pretty easy, having only to start a new transaction. For this application the resulting cursor object would be a useful result, so the method will return it. The user can then add as cursor to their \u2018 with \u2018 statement to bind the cursor to a variable name.\nclass DatabaseConnection: ... def __enter__ (self): # Code to start a new transaction cursor = self.cursor() return cursor\nThe __exit__() method is the most complicated because it\u2019s where most of the work has to be done. The method has to check if an exception occurred. If there was no exception, the transaction is committed. The transaction is rolled back if there was an exception.\nIn the code below, execution will just fall off the end of the function, returning the default value of None. None is false, so the exception will be re-raised automatically. If you wished, you could be more explicit and add a return statement at the marked location.\nclass DatabaseConnection: ... def __exit__ (self, type, value, tb): if tb is None: # No exception, so commit self.commit() else: # Exception occurred, so rollback. self.rollback() # return False\nThe contextlib module \u00b6\nThe new contextlib module provides some functions and a decorator that are useful for writing objects for use with the \u2018 with \u2018 statement.\nThe decorator is called contextmanager(), and lets you write a single generator function instead of defining a new class. The generator should yield exactly one value. The code up to the yield will be executed as the __enter__() method, and the value yielded will be the method\u2019s return value that will get bound to the variable in the \u2018 with \u2018 statement\u2019s as clause, if any. The code after the yield will be executed in the __exit__() method. Any exception raised in the block will be raised by the yield statement.\nOur database example from the previous section could be written using this decorator as:\nfrom contextlib import contextmanager @contextmanager def db_transaction (connection): cursor = connection.cursor() try: yield cursor except: connection.rollback() raise else: connection.commit() db = DatabaseConnection() with db_transaction(db) as cursor: ...\nThe contextlib module also has a nested(mgr1, mgr2, ...)() function that combines a number of context managers so you don\u2019t need to write nested \u2018 with \u2018 statements. In this example, the single \u2018 with \u2018 statement both starts a database transaction and acquires a thread lock:\nlock = threading.Lock() with nested (db_transaction(db), lock) as (cursor, locked): ...\nFinally, the closing(object)() function returns object so that it can be bound to a variable, and calls object.close at the end of the block.\nimport urllib, sys from contextlib import closing with closing(urllib.urlopen('http://www.yahoo.com')) as f: for line in f: sys.stdout.write(line)\nSee also\nPEP 343 - The \u201cwith\u201d statement\nPEP written by Guido van Rossum and Nick Coghlan; implemented by Mike Bland, Guido van Rossum, and Neal Norwitz. The PEP shows the code generated for a \u2018 with \u2018 statement, which can be helpful in learning how the statement works.\nThe documentation for the contextlib module.\nPEP 352: Exceptions as New-Style Classes \u00b6\nException classes can now be new-style classes, not just classic classes, and the built-in Exception class and all the standard built-in exceptions ( NameError , ValueError , etc.) are now new-style classes.\nThe inheritance hierarchy for exceptions has been rearranged a bit. In 2.5, the inheritance relationships are:\nBaseException # New in Python 2.5 |- KeyboardInterrupt |- SystemExit |- Exception |- (all other current built-in exceptions)\nThis rearrangement was done because people often want to catch all exceptions that indicate program errors. KeyboardInterrupt and SystemExit aren\u2019t errors, though, and usually represent an explicit action such as the user hitting Control-C or code calling sys.exit() . A bare except: will catch all exceptions, so you commonly need to list KeyboardInterrupt and SystemExit in order to re-raise them. The usual pattern is:\ntry: ... except (KeyboardInterrupt, SystemExit): raise except: # Log error... # Continue running program...\nIn Python 2.5, you can now write except Exception to achieve the same result, catching all the exceptions that usually indicate errors but leaving KeyboardInterrupt and SystemExit alone. As in previous versions, a bare except: still catches all exceptions.\nThe goal for Python 3.0 is to require any class raised as an exception to derive from BaseException or some descendant of BaseException , and future releases in the Python 2.x series may begin to enforce this constraint. Therefore, I suggest you begin making all your exception classes derive from Exception now. It\u2019s been suggested that the bare except: form should be removed in Python 3.0, but Guido van Rossum hasn\u2019t decided whether to do this or not.\nRaising of strings as exceptions, as in the statement raise \"Error occurred\", is deprecated in Python 2.5 and will trigger a warning. The aim is to be able to remove the string-exception feature in a few releases.\nSee also\nPEP 352 - Required Superclass for Exceptions\nPEP written by Brett Cannon and Guido van Rossum; implemented by Brett Cannon.\nPEP 353: Using ssize_t as the index type \u00b6\nA wide-ranging change to Python\u2019s C API, using a new Py_ssize_t type definition instead of int, will permit the interpreter to handle more data on 64-bit platforms. This change doesn\u2019t affect Python\u2019s capacity on 32-bit platforms.\nVarious pieces of the Python interpreter used C\u2019s int type to store sizes or counts; for example, the number of items in a list or tuple were stored in an int. The C compilers for most 64-bit platforms still define int as a 32-bit type, so that meant that lists could only hold up to 2**31 - 1 = 2147483647 items. (There are actually a few different programming models that 64-bit C compilers can use \u2013 see http://www.unix.org/version2/whatsnew/lp64_wp.html for a discussion \u2013 but the most commonly available model leaves int as 32 bits.)\nA limit of 2147483647 items doesn\u2019t really matter on a 32-bit platform because you\u2019ll run out of memory before hitting the length limit. Each list item requires space for a pointer, which is 4 bytes, plus space for a PyObject representing the item. 2147483647*4 is already more bytes than a 32-bit address space can contain.\nIt\u2019s possible to address that much memory on a 64-bit platform, however. The pointers for a list that size would only require 16 GiB of space, so it\u2019s not unreasonable that Python programmers might construct lists that large. Therefore, the Python interpreter had to be changed to use some type other than int, and this will be a 64-bit type on 64-bit platforms. The change will cause incompatibilities on 64-bit machines, so it was deemed worth making the transition now, while the number of 64-bit users is still relatively small. (In 5 or 10 years, we may all be on 64-bit machines, and the transition would be more painful then.)\nThis change most strongly affects authors of C extension modules. Python strings and container types such as lists and tuples now use Py_ssize_t to store their size. Functions such as PyList_Size() now return Py_ssize_t. Code in extension modules may therefore need to have some variables changed to Py_ssize_t.\nThe PyArg_ParseTuple() and Py_BuildValue() functions have a new conversion code, n, for Py_ssize_t. PyArg_ParseTuple() \u2018s s# and t# still output int by default, but you can define the macro PY_SSIZE_T_CLEAN before including Python.h to make them return Py_ssize_t.\nPEP 353 has a section on conversion guidelines that extension authors should read to learn about supporting 64-bit platforms.\nSee also\nPEP 353 - Using ssize_t as the index type\nPEP written and implemented by Martin von L\u00f6wis.\nPEP 357: The \u2018__index__\u2019 method \u00b6\nThe NumPy developers had a problem that could only be solved by adding a new special method, __index__() . When using slice notation, as in [start:stop:step], the values of the start, stop, and step indexes must all be either integers or long integers. NumPy defines a variety of specialized integer types corresponding to unsigned and signed integers of 8, 16, 32, and 64 bits, but there was no way to signal that these types could be used as slice indexes.\nSlicing can\u2019t just use the existing __int__() method because that method is also used to implement coercion to integers. If slicing used __int__() , floating-point numbers would also become legal slice indexes and that\u2019s clearly an undesirable behaviour.\nInstead, a new special method called __index__() was added. It takes no arguments and returns an integer giving the slice index to use. For example:\nclass C: def __index__ (self): return self.value\nThe return value must be either a Python integer or long integer. The interpreter will check that the type returned is correct, and raises a TypeError if this requirement isn\u2019t met.\nA corresponding nb_index slot was added to the C-level PyNumberMethods structure to let C extensions implement this protocol. PyNumber_Index(obj)() can be used in extension code to call the __index__() function and retrieve its result.\nSee also\nPEP 357 - Allowing Any Object to be Used for Slicing\nPEP written and implemented by Travis Oliphant.\nOther Language Changes \u00b6\nHere are all of the changes that Python 2.5 makes to the core Python language.\nThe dict type has a new hook for letting subclasses provide a default value when a key isn\u2019t contained in the dictionary. When a key isn\u2019t found, the dictionary\u2019s __missing__(key)() method will be called. This hook is used to implement the new defaultdict class in the collections module. The following example defines a dictionary that returns zero for any missing key:\nclass zerodict (dict): def __missing__ (self, key): return 0 d = zerodict({1:1, 2:2}) print d[1], d[2] # Prints 1, 2 print d[3], d[4] # Prints 0, 0\nBoth 8-bit and Unicode strings have new partition(sep)() and rpartition(sep)() methods that simplify a common use case.\nThe find(S)() method is often used to get an index which is then used to slice the string and obtain the pieces that are before and after the separator. partition(sep)() condenses this pattern into a single method call that returns a 3-tuple containing the substring before the separator, the separator itself, and the substring after the separator. If the separator isn\u2019t found, the first element of the tuple is the entire string and the other two elements are empty. rpartition(sep)() also returns a 3-tuple but starts searching from the end of the string; the r stands for \u2018reverse\u2019.\nSome examples:\n>>> ('http://www.python.org').partition('://') ('http', '://', 'www.python.org') >>> ('file:/usr/share/doc/index.html').partition('://') ('file:/usr/share/doc/index.html', '', '') >>> (u'Subject: a quick question').partition(':') (u'Subject', u':', u' a quick question') >>> 'www.python.org'.rpartition('.') ('www.python', '.', 'org') >>> 'www.python.org'.rpartition(':') ('', '', 'www.python.org')\n(Implemented by Fredrik Lundh following a suggestion by Raymond Hettinger.)\nThe startswith() and endswith() methods of string types now accept tuples of strings to check for.\ndef is_image_file (filename): return filename.endswith(('.gif', '.jpg', '.tiff'))\n(Implemented by Georg Brandl following a suggestion by Tom Lynn.)\nThe min() and max() built-in functions gained a key keyword parameter analogous to the key argument for sort(). This parameter supplies a function that takes a single argument and is called for every value in the list; min() / max() will return the element with the smallest/largest return value from this function. For example, to find the longest string in a list, you can do:\nL = ['medium', 'longest', 'short'] # Prints 'longest' print max(L, key=len) # Prints 'short', because lexicographically 'short' has the largest value print max(L)\n(Contributed by Steven Bethard and Raymond Hettinger.)\nTwo new built-in functions, any() and all() , evaluate whether an iterator contains any true or false values. any() returns True if any value returned by the iterator is true; otherwise it will return False . all() returns True only if all of the values returned by the iterator evaluate as true. (Suggested by Guido van Rossum, and implemented by Raymond Hettinger.)\nThe result of a class\u2019s __hash__() method can now be either a long integer or a regular integer. If a long integer is returned, the hash of that value is taken. In earlier versions the hash value was required to be a regular integer, but in 2.5 the id() built-in was changed to always return non-negative numbers, and users often seem to use id(self) in __hash__() methods (though this is discouraged).\nASCII is now the default encoding for modules. It\u2019s now a syntax error if a module contains string literals with 8-bit characters but doesn\u2019t have an encoding declaration. In Python 2.4 this triggered a warning, not a syntax error. See PEP 263 for how to declare a module\u2019s encoding; for example, you might add a line like this near the top of the source file:\n# -*- coding: latin1 -*-\nA new warning, UnicodeWarning , is triggered when you attempt to compare a Unicode string and an 8-bit string that can\u2019t be converted to Unicode using the default ASCII encoding. The result of the comparison is false:\n>>> chr(128) == unichr(128) # Can't convert chr(128) to Unicode __main__:1: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal False >>> chr(127) == unichr(127) # chr(127) can be converted True\nPreviously this would raise a UnicodeDecodeError exception, but in 2.5 this could result in puzzling problems when accessing a dictionary. If you looked up unichr(128) and chr(128) was being used as a key, you\u2019d get a UnicodeDecodeError exception. Other changes in 2.5 resulted in this exception being raised instead of suppressed by the code in dictobject.c that implements dictionaries.\nRaising an exception for such a comparison is strictly correct, but the change might have broken code, so instead UnicodeWarning was introduced.\n(Implemented by Marc-Andr\u00e9 Lemburg.)\nOne error that Python programmers sometimes make is forgetting to include an __init__.py module in a package directory. Debugging this mistake can be confusing, and usually requires running Python with the -v switch to log all the paths searched. In Python 2.5, a new ImportWarning warning is triggered when an import would have picked up a directory as a package but no __init__.py was found. This warning is silently ignored by default; provide the -Wd option when running the Python executable to display the warning message. (Implemented by Thomas Wouters.)\nThe list of base classes in a class definition can now be empty. As an example, this is now legal:\nclass C(): pass\n(Implemented by Brett Cannon.)\nInteractive Interpreter Changes \u00b6\nIn the interactive interpreter, quit and exit have long been strings so that new users get a somewhat helpful message when they try to quit:\n>>> quit 'Use Ctrl-D (i.e. EOF) to exit.'\nIn Python 2.5, quit and exit are now objects that still produce string representations of themselves, but are also callable. Newbies who try quit() or exit() will now exit the interpreter as they expect. (Implemented by Georg Brandl.)\nThe Python executable now accepts the standard long options --help and --version ; on Windows, it also accepts the /? option for displaying a help message. (Implemented by Georg Brandl.)\nOptimizations \u00b6\nSeveral of the optimizations were developed at the NeedForSpeed sprint, an event held in Reykjavik, Iceland, from May 21\u201328 2006. The sprint focused on speed enhancements to the CPython implementation and was funded by EWT LLC with local support from CCP Games. Those optimizations added at this sprint are specially marked in the following list.\nWhen they were introduced in Python 2.4, the built-in set and frozenset types were built on top of Python\u2019s dictionary type. In 2.5 the internal data structure has been customized for implementing sets, and as a result sets will use a third less memory and are somewhat faster. (Implemented by Raymond Hettinger.)\nThe speed of some Unicode operations, such as finding substrings, string splitting, and character map encoding and decoding, has been improved. (Substring search and splitting improvements were added by Fredrik Lundh and Andrew Dalke at the NeedForSpeed sprint. Character maps were improved by Walter D\u00f6rwald and Martin von L\u00f6wis.)\nThe long(str, base)() function is now faster on long digit strings because fewer intermediate results are calculated. The peak is for strings of around 800\u20131000 digits where the function is 6 times faster. (Contributed by Alan McIntyre and committed at the NeedForSpeed sprint.)\nIt\u2019s now illegal to mix iterating over a file with for line in file and calling the file object\u2019s read()/ readline() /readlines() methods. Iteration uses an internal buffer and the read*() methods don\u2019t use that buffer. Instead they would return the data following the buffer, causing the data to appear out of order. Mixing iteration and these methods will now trigger a ValueError from the read*() method. (Implemented by Thomas Wouters.)\nThe struct module now compiles structure format strings into an internal representation and caches this representation, yielding a 20% speedup. (Contributed by Bob Ippolito at the NeedForSpeed sprint.)\nThe re module got a 1 or 2% speedup by switching to Python\u2019s allocator functions instead of the system\u2019s malloc() and free(). (Contributed by Jack Diederich at the NeedForSpeed sprint.)\nThe code generator\u2019s peephole optimizer now performs simple constant folding in expressions. If you write something like a = 2+3, the code generator will do the arithmetic and produce code corresponding to a = 5. (Proposed and implemented by Raymond Hettinger.)\nFunction calls are now faster because code objects now keep the most recently finished frame (a \u201czombie frame\u201d) in an internal field of the code object, reusing it the next time the code object is invoked. (Original patch by Michael Hudson, modified by Armin Rigo and Richard Jones; committed at the NeedForSpeed sprint.) Frame objects are also slightly smaller, which may improve cache locality and reduce memory usage a bit. (Contributed by Neal Norwitz.)\nPython\u2019s built-in exceptions are now new-style classes, a change that speeds up instantiation considerably. Exception handling in Python 2.5 is therefore about 30% faster than in 2.4. (Contributed by Richard Jones, Georg Brandl and Sean Reifschneider at the NeedForSpeed sprint.)\nImporting now caches the paths tried, recording whether they exist or not so that the interpreter makes fewer open() and stat() calls on startup. (Contributed by Martin von L\u00f6wis and Georg Brandl.)\nNew, Improved, and Removed Modules \u00b6\nThe standard library received many enhancements and bug fixes in Python 2.5. Here\u2019s a partial list of the most notable changes, sorted alphabetically by module name. Consult the Misc/NEWS file in the source tree for a more complete list of changes, or look through the SVN logs for all the details.\nThe audioop module now supports the a-LAW encoding, and the code for u-LAW encoding has been improved. (Contributed by Lars Immisch.)\nThe codecs module gained support for incremental codecs. The codec.lookup() function now returns a CodecInfo instance instead of a tuple. CodecInfo instances behave like a 4-tuple to preserve backward compatibility but also have the attributes encode, decode, incrementalencoder, incrementaldecoder, streamwriter, and streamreader. Incremental codecs can receive input and produce output in multiple chunks; the output is the same as if the entire input was fed to the non-incremental codec. See the codecs module documentation for details. (Designed and implemented by Walter D\u00f6rwald.)\nThe collections module gained a new type, defaultdict, that subclasses the standard dict type. The new type mostly behaves like a dictionary but constructs a default value when a key isn\u2019t present, automatically adding it to the dictionary for the requested key value.\nThe first argument to defaultdict\u2018s constructor is a factory function that gets called whenever a key is requested but not found. This factory function receives no arguments, so you can use built-in type constructors such as list() or int() . For example, you can make an index of words based on their initial letter like this:\nwords = \"\"\"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura che la diritta via era smarrita\"\"\".lower().split() index = defaultdict(list) for w in words: init_letter = w[0] index[init_letter].append(w)\nPrinting index results in the following output:\ndefaultdict(<type 'list'>, {'c': ['cammin', 'che'], 'e': ['era'], 'd': ['del', 'di', 'diritta'], 'm': ['mezzo', 'mi'], 'l': ['la'], 'o': ['oscura'], 'n': ['nel', 'nostra'], 'p': ['per'], 's': ['selva', 'smarrita'], 'r': ['ritrovai'], 'u': ['una'], 'v': ['vita', 'via']}\n(Contributed by Guido van Rossum.)\nThe deque double-ended queue type supplied by the collections module now has a remove(value)() method that removes the first occurrence of value in the queue, raising ValueError if the value isn\u2019t found. (Contributed by Raymond Hettinger.)\nNew module: The contextlib module contains helper functions for use with the new \u2018 with \u2018 statement. See section The contextlib module for more about this module.\nNew module: The cProfile module is a C implementation of the existing profile module that has much lower overhead. The module\u2019s interface is the same as profile : you run cProfile.run('main()') to profile a function, can save profile data to a file, etc. It\u2019s not yet known if the Hotshot profiler, which is also written in C but doesn\u2019t match the profile module\u2019s interface, will continue to be maintained in future versions of Python. (Contributed by Armin Rigo.)\nAlso, the pstats module for analyzing the data measured by the profiler now supports directing the output to any file object by supplying a stream argument to the Stats constructor. (Contributed by Skip Montanaro.)\nThe csv module, which parses files in comma-separated value format, received several enhancements and a number of bugfixes. You can now set the maximum size in bytes of a field by calling the csv.field_size_limit(new_limit)() function; omitting the new_limit argument will return the currently-set limit. The reader class now has a line_num attribute that counts the number of physical lines read from the source; records can span multiple physical lines, so line_num is not the same as the number of records read.\nThe CSV parser is now stricter about multi-line quoted fields. Previously, if a line ended within a quoted field without a terminating newline character, a newline would be inserted into the returned field. This behavior caused problems when reading files that contained carriage return characters within fields, so the code was changed to return the field without inserting newlines. As a consequence, if newlines embedded within fields are important, the input should be split into lines in a manner that preserves the newline characters.\n(Contributed by Skip Montanaro and Andrew McNamara.)\nThe datetime class in the datetime module now has a strptime(string, format)() method for parsing date strings, contributed by Josh Spoerri. It uses the same format characters as time.strptime() and time.strftime() :\nfrom datetime import datetime ts = datetime.strptime('10:13:15 2006-03-07', '%H:%M:%S %Y-%m-%d')\nThe SequenceMatcher.get_matching_blocks() method in the difflib module now guarantees to return a minimal list of blocks describing matching subsequences. Previously, the algorithm would occasionally break a block of matching elements into two list entries. (Enhancement by Tim Peters.)\nThe doctest module gained a SKIP option that keeps an example from being executed at all. This is intended for code snippets that are usage examples intended for the reader and aren\u2019t actually test cases.\nAn encoding parameter was added to the testfile() function and the DocFileSuite class to specify the file\u2019s encoding. This makes it easier to use non-ASCII characters in tests contained within a docstring. (Contributed by Bjorn Tillenius.)\nThe email package has been updated to version 4.0. (Contributed by Barry Warsaw.)\nThe fileinput module was made more flexible. Unicode filenames are now supported, and a mode parameter that defaults to \"r\" was added to the input() function to allow opening files in binary or universal-newline mode. Another new parameter, openhook, lets you use a function other than open() to open the input files. Once you\u2019re iterating over the set of files, the FileInput object\u2019s new fileno() returns the file descriptor for the currently opened file. (Contributed by Georg Brandl.)\nIn the gc module, the new get_count() function returns a 3-tuple containing the current collection counts for the three GC generations. This is accounting information for the garbage collector; when these counts reach a specified threshold, a garbage collection sweep will be made. The existing gc.collect() function now takes an optional generation argument of 0, 1, or 2 to specify which generation to collect. (Contributed by Barry Warsaw.)\nThe nsmallest() and nlargest() functions in the heapq module now support a key keyword parameter similar to the one provided by the min() / max() functions and the sort() methods. For example:\n>>> import heapq >>> L = [\"short\", 'medium', 'longest', 'longer still'] >>> heapq.nsmallest(2, L) # Return two lowest elements, lexicographically ['longer still', 'longest'] >>> heapq.nsmallest(2, L, key=len) # Return two shortest elements ['short', 'medium']\n(Contributed by Raymond Hettinger.)\nThe itertools.islice() function now accepts None for the start and step arguments. This makes it more compatible with the attributes of slice objects, so that you can now write the following:\ns = slice(5) # Create slice object itertools.islice(iterable, s.start, s.stop, s.step)\n(Contributed by Raymond Hettinger.)\nThe format() function in the locale module has been modified and two new functions were added, format_string() and currency().\nThe format() function\u2019s val parameter could previously be a string as long as no more than one %char specifier appeared; now the parameter must be exactly one %char specifier with no surrounding text. An optional monetary parameter was also added which, if True, will use the locale\u2019s rules for formatting currency in placing a separator between groups of three digits.\nTo format strings with multiple %char specifiers, use the new format_string() function that works like format() but also supports mixing %char specifiers with arbitrary text.\nA new currency() function was also added that formats a number according to the current locale\u2019s settings.\n(Contributed by Georg Brandl.)\nThe mailbox module underwent a massive rewrite to add the capability to modify mailboxes in addition to reading them. A new set of classes that include mbox, MH, and Maildir are used to read mailboxes, and have an add(message)() method to add messages, remove(key)() to remove messages, and lock()/unlock() to lock/unlock the mailbox. The following example converts a maildir-format mailbox into an mbox-format one:\nimport mailbox # 'factory=None' uses email.Message.Message as the class representing # individual messages. src = mailbox.Maildir('maildir', factory=None) dest = mailbox.mbox('/tmp/mbox') for msg in src: dest.add(msg)\n(Contributed by Gregory K. Johnson. Funding was provided by Google\u2019s 2005 Summer of Code.)\nNew module: the msilib module allows creating Microsoft Installer .msi files and CAB files. Some support for reading the .msi database is also included. (Contributed by Martin von L\u00f6wis.)\nThe nis module now supports accessing domains other than the system default domain by supplying a domain argument to the nis.match() and nis.maps() functions. (Contributed by Ben Bell.)\nThe operator module\u2019s itemgetter() and attrgetter() functions now support multiple fields. A call such as operator.attrgetter('a', 'b') will return a function that retrieves the a and b attributes. Combining this new feature with the sort() method\u2019s key parameter lets you easily sort lists using multiple fields. (Contributed by Raymond Hettinger.)\nThe optparse module was updated to version 1.5.1 of the Optik library. The OptionParser class gained an epilog attribute, a string that will be printed after the help message, and a destroy() method to break reference cycles created by the object. (Contributed by Greg Ward.)\nThe os module underwent several changes. The stat_float_times variable now defaults to true, meaning that os.stat() will now return time values as floats. (This doesn\u2019t necessarily mean that os.stat() will return times that are precise to fractions of a second; not all systems support such precision.)\nConstants named os.SEEK_SET , os.SEEK_CUR , and os.SEEK_END have been added; these are the parameters to the os.lseek() function. Two new constants for locking are os.O_SHLOCK and os.O_EXLOCK .\nTwo new functions, wait3() and wait4(), were added. They\u2019re similar the waitpid() function which waits for a child process to exit and returns a tuple of the process ID and its exit status, but wait3() and wait4() return additional information. wait3() doesn\u2019t take a process ID as input, so it waits for any child process to exit and returns a 3-tuple of process-id, exit-status, resource-usage as returned from the resource.getrusage() function. wait4(pid)() does take a process ID. (Contributed by Chad J. Schroeder.)\nOn FreeBSD, the os.stat() function now returns times with nanosecond resolution, and the returned object now has st_gen and st_birthtime. The st_flags attribute is also available, if the platform supports it. (Contributed by Antti Louko and Diego Petten\u00f2.)\nThe Python debugger provided by the pdb module can now store lists of commands to execute when a breakpoint is reached and execution stops. Once breakpoint #1 has been created, enter commands 1 and enter a series of commands to be executed, finishing the list with end. The command list can include commands that resume execution, such as continue or next. (Contributed by Gr\u00e9goire Dooms.)\nThe pickle and cPickle modules no longer accept a return value of None from the __reduce__() method; the method must return a tuple of arguments instead. The ability to return None was deprecated in Python 2.4, so this completes the removal of the feature.\nThe pkgutil module, containing various utility functions for finding packages, was enhanced to support PEP 302\u2019s import hooks and now also works for packages stored in ZIP-format archives. (Contributed by Phillip J. Eby.)\nThe pybench benchmark suite by Marc-Andr\u00e9 Lemburg is now included in the Tools/pybench directory. The pybench suite is an improvement on the commonly used pystone.py program because pybench provides a more detailed measurement of the interpreter\u2019s speed. It times particular operations such as function calls, tuple slicing, method lookups, and numeric operations, instead of performing many different operations and reducing the result to a single number as pystone.py does.\nThe pyexpat module now uses version 2.0 of the Expat parser. (Contributed by Trent Mick.)\nThe Queue class provided by the Queue module gained two new methods. join() blocks until all items in the queue have been retrieved and all processing work on the items have been completed. Worker threads call the other new method, task_done(), to signal that processing for an item has been completed. (Contributed by Raymond Hettinger.)\nThe old regex and regsub modules, which have been deprecated ever since Python 2.0, have finally been deleted. Other deleted modules: statcache, tzparse, whrandom.\nAlso deleted: the lib-old directory, which includes ancient modules such as dircmp and ni, was removed. lib-old wasn\u2019t on the default sys.path, so unless your programs explicitly added the directory to sys.path, this removal shouldn\u2019t affect your code.\nThe rlcompleter module is no longer dependent on importing the readline module and therefore now works on non-Unix platforms. (Patch from Robert Kiendl.)\nThe SimpleXMLRPCServer and DocXMLRPCServer classes now have a rpc_paths attribute that constrains XML-RPC operations to a limited set of URL paths; the default is to allow only '/' and '/RPC2'. Setting rpc_paths to None or an empty tuple disables this path checking.\nThe socket module now supports AF_NETLINK sockets on Linux, thanks to a patch from Philippe Biondi. Netlink sockets are a Linux-specific mechanism for communications between a user-space process and kernel code; an introductory article about them is at http://www.linuxjournal.com/article/7356 . In Python code, netlink addresses are represented as a tuple of 2 integers, (pid, group_mask).\nTwo new methods on socket objects, recv_into(buffer)() and recvfrom_into(buffer)(), store the received data in an object that supports the buffer protocol instead of returning the data as a string. This means you can put the data directly into an array or a memory-mapped file.\nSocket objects also gained getfamily(), gettype(), and getproto() accessor methods to retrieve the family, type, and protocol values for the socket.\nNew module: the spwd module provides functions for accessing the shadow password database on systems that support shadow passwords.\nThe struct is now faster because it compiles format strings into Struct objects with pack() and unpack() methods. This is similar to how the re module lets you create compiled regular expression objects. You can still use the module-level pack() and unpack() functions; they\u2019ll create Struct objects and cache them. Or you can use Struct instances directly:\ns = struct.Struct('ih3s') data = s.pack(1972, 187, 'abc') year, number, name = s.unpack(data)\nYou can also pack and unpack data to and from buffer objects directly using the pack_into(buffer, offset, v1, v2, ...)() and unpack_from(buffer, offset)() methods. This lets you store data directly into an array or a memory- mapped file.\n(Struct objects were implemented by Bob Ippolito at the NeedForSpeed sprint. Support for buffer objects was added by Martin Blais, also at the NeedForSpeed sprint.)\nThe Python developers switched from CVS to Subversion during the 2.5 development process. Information about the exact build version is available as the sys.subversion variable, a 3-tuple of (interpreter-name, branch-name, revision-range). For example, at the time of writing my copy of 2.5 was reporting ('CPython', 'trunk', '45313:45315').\nThis information is also available to C extensions via the Py_GetBuildInfo() function that returns a string of build information like this: \"trunk:45355:45356M, Apr 13 2006, 07:42:19\". (Contributed by Barry Warsaw.)\nAnother new function, sys._current_frames() , returns the current stack frames for all running threads as a dictionary mapping thread identifiers to the topmost stack frame currently active in that thread at the time the function is called. (Contributed by Tim Peters.)\nThe TarFile class in the tarfile module now has an extractall() method that extracts all members from the archive into the current working directory. It\u2019s also possible to set a different directory as the extraction target, and to unpack only a subset of the archive\u2019s members.\nThe compression used for a tarfile opened in stream mode can now be autodetected using the mode 'r|*'. (Contributed by Lars Gust\u00e4bel.)\nThe threading module now lets you set the stack size used when new threads are created. The stack_size([*size*])() function returns the currently configured stack size, and supplying the optional size parameter sets a new value. Not all platforms support changing the stack size, but Windows, POSIX threading, and OS/2 all do. (Contributed by Andrew MacIntyre.)\nThe unicodedata module has been updated to use version 4.1.0 of the Unicode character database. Version 3.2.0 is required by some specifications, so it\u2019s still available as unicodedata.ucd_3_2_0 .\nNew module: the uuid module generates universally unique identifiers (UUIDs) according to RFC 4122 . The RFC defines several different UUID versions that are generated from a starting string, from system properties, or purely randomly. This module contains a UUID class and functions named uuid1(), uuid3(), uuid4(), and uuid5() to generate different versions of UUID. (Version 2 UUIDs are not specified in RFC 4122 and are not supported by this module.)\n>>> import uuid >>> # make a UUID based on the host ID and current time >>> uuid.uuid1() UUID('a8098c1a-f86e-11da-bd1a-00112444be1e') >>> # make a UUID using an MD5 hash of a namespace UUID and a name >>> uuid.uuid3(uuid.NAMESPACE_DNS, 'python.org') UUID('6fa459ea-ee8a-3ca4-894e-db77e160355e') >>> # make a random UUID >>> uuid.uuid4() UUID('16fd2706-8baf-433b-82eb-8c7fada847da') >>> # make a UUID using a SHA-1 hash of a namespace UUID and a name >>> uuid.uuid5(uuid.NAMESPACE_DNS, 'python.org') UUID('886313e1-3b8a-5372-9b90-0c9aee199e5d')\n(Contributed by Ka-Ping Yee.)\nThe weakref module\u2019s WeakKeyDictionary and WeakValueDictionary types gained new methods for iterating over the weak references contained in the dictionary. iterkeyrefs() and keyrefs() methods were added to WeakKeyDictionary, and itervaluerefs() and valuerefs() were added to WeakValueDictionary. (Contributed by Fred L. Drake, Jr.)\nThe webbrowser module received a number of enhancements. It\u2019s now usable as a script with python -m webbrowser, taking a URL as the argument; there are a number of switches to control the behaviour (-n for a new browser window, -t for a new tab). New module-level functions, open_new() and open_new_tab(), were added to support this. The module\u2019s open() function supports an additional feature, an autoraise parameter that signals whether to raise the open window when possible. A number of additional browsers were added to the supported list such as Firefox, Opera, Konqueror, and elinks. (Contributed by Oleg Broytmann and Georg Brandl.)\nThe xmlrpclib module now supports returning datetime objects for the XML-RPC date type. Supply use_datetime=True to the loads() function or the Unmarshaller class to enable this feature. (Contributed by Skip Montanaro.)\nThe zipfile module now supports the ZIP64 version of the format, meaning that a .zip archive can now be larger than 4 GiB and can contain individual files larger than 4 GiB. (Contributed by Ronald Oussoren.)\nThe zlib module\u2019s Compress and Decompress objects now support a copy() method that makes a copy of the object\u2019s internal state and returns a new Compress or Decompress object. (Contributed by Chris AtLee.)\nThe ctypes package \u00b6\nThe ctypes package, written by Thomas Heller, has been added to the standard library. ctypes lets you call arbitrary functions in shared libraries or DLLs. Long-time users may remember the dl module, which provides functions for loading shared libraries and calling functions in them. The ctypes package is much fancier.\nTo load a shared library or DLL, you must create an instance of the CDLL class and provide the name or path of the shared library or DLL. Once that\u2019s done, you can call arbitrary functions by accessing them as attributes of the CDLL object.\nimport ctypes libc = ctypes.CDLL('libc.so.6') result = libc.printf(\"Line of output\\n\")\nType constructors for the various C types are provided: c_int(), c_float(), c_double(), c_char_p() (equivalent to char *), and so forth. Unlike Python\u2019s types, the C versions are all mutable; you can assign to their value attribute to change the wrapped value. Python integers and strings will be automatically converted to the corresponding C types, but for other types you must call the correct type constructor. (And I mean must; getting it wrong will often result in the interpreter crashing with a segmentation fault.)\nYou shouldn\u2019t use c_char_p() with a Python string when the C function will be modifying the memory area, because Python strings are supposed to be immutable; breaking this rule will cause puzzling bugs. When you need a modifiable memory area, use create_string_buffer():\ns = \"this is a string\" buf = ctypes.create_string_buffer(s) libc.strfry(buf)\nC functions are assumed to return integers, but you can set the restype attribute of the function object to change this:\n>>> libc.atof('2.71828') -1783957616 >>> libc.atof.restype = ctypes.c_double >>> libc.atof('2.71828') 2.71828\nctypes also provides a wrapper for Python\u2019s C API as the ctypes.pythonapi object. This object does not release the global interpreter lock before calling a function, because the lock must be held when calling into the interpreter\u2019s code. There\u2019s a py_object() type constructor that will create a PyObject * pointer. A simple usage:\nimport ctypes d = {} ctypes.pythonapi.PyObject_SetItem(ctypes.py_object(d), ctypes.py_object(\"abc\"), ctypes.py_object(1)) # d is now {'abc', 1}.\nDon\u2019t forget to use py_object(); if it\u2019s omitted you end up with a segmentation fault.\nctypes has been around for a while, but people still write and distribution hand-coded extension modules because you can\u2019t rely on ctypes being present. Perhaps developers will begin to write Python wrappers atop a library accessed through ctypes instead of extension modules, now that ctypes is included with core Python.\nSee also\nThe ctypes web page, with a tutorial, reference, and FAQ.\nThe documentation for the ctypes module.\nThe ElementTree package \u00b6\nA subset of Fredrik Lundh\u2019s ElementTree library for processing XML has been added to the standard library as xml.etree. The available modules are ElementTree, ElementPath, and ElementInclude from ElementTree 1.2.6. The cElementTree accelerator module is also included.\nThe rest of this section will provide a brief overview of using ElementTree. Full documentation for ElementTree is available at http://effbot.org/zone/element-index.htm .\nElementTree represents an XML document as a tree of element nodes. The text content of the document is stored as the text and tail attributes of (This is one of the major differences between ElementTree and the Document Object Model; in the DOM there are many different types of node, including TextNode.)\nThe most commonly used parsing function is parse(), that takes either a string (assumed to contain a filename) or a file-like object and returns an ElementTree instance:\nfrom xml.etree import ElementTree as ET tree = ET.parse('ex-1.xml') feed = urllib.urlopen( 'http://planet.python.org/rss10.xml') tree = ET.parse(feed)\nOnce you have an ElementTree instance, you can call its getroot() method to get the root Element node.\nThere\u2019s also an XML() function that takes a string literal and returns an Element node (not an ElementTree). This function provides a tidy way to incorporate XML fragments, approaching the convenience of an XML literal:\nsvg = ET.XML(\"\"\"<svg width=\"10px\" version=\"1.0\"> </svg>\"\"\") svg.set('height', '320px') svg.append(elem1)\nEach XML element supports some dictionary-like and some list-like access methods. Dictionary-like operations are used to access attribute values, and list-like operations are used to access child nodes.\nOperation\n"}, {"score": 914.3738, "uuid": "c7eabb0c-f8f7-502d-8d5f-5a34316872a3", "index": "cw12", "trec_id": "clueweb12-0005wb-07-22692", "target_hostname": "archaeopteryx.com", "target_uri": "http://archaeopteryx.com/psupport/python-manual/3.2/whatsnew/2.5.html", "page_rank": 1.20234e-09, "spam_rank": 70, "title": "What\u2019s New in <em>Python</em> 2.5 \u2014 <em>Python</em> v3.2.1 documentation", "snippet": "The decision was made to not require parentheses in the <em>Python</em> language\u2019s grammar, but as a matter of style <em>I</em> think you <em>should</em> always use them.", "explanation": null, "document": "An explanation of coroutines from a Perl point of view, written by Dan Sugalski.\nPEP 343: The \u2018with\u2019 statement \u00b6\nThe \u2018 with \u2018 statement clarifies code that previously would use try...finally blocks to ensure that clean-up code is executed. In this section, I\u2019ll discuss the statement as it will commonly be used. In the next section, I\u2019ll examine the implementation details and show how to write objects for use with this statement.\nThe \u2018 with \u2018 statement is a new control-flow structure whose basic structure is:\nwith expression [as variable]: with-block\nThe expression is evaluated, and it should result in an object that supports the context management protocol (that is, has __enter__() and __exit__() methods.\nThe object\u2019s __enter__() is called before with-block is executed and therefore can run set-up code. It also may return a value that is bound to the name variable, if given. (Note carefully that variable is not assigned the result of expression.)\nAfter execution of the with-block is finished, the object\u2019s __exit__() method is called, even if the block raised an exception, and can therefore run clean-up code.\nTo enable the statement in Python 2.5, you need to add the following directive to your module:\nfrom __future__ import with_statement\nThe statement will always be enabled in Python 2.6.\nSome standard Python objects now support the context management protocol and can be used with the \u2018 with \u2018 statement. File objects are one example:\nwith open('/etc/passwd', 'r') as f: for line in f: print line ... more processing code ...\nAfter this statement has executed, the file object in f will have been automatically closed, even if the for loop raised an exception part- way through the block.\nNote\nIn this case, f is the same object created by open() , because file.__enter__() returns self.\nThe threading module\u2019s locks and condition variables also support the \u2018 with \u2018 statement:\nlock = threading.Lock() with lock: # Critical section of code ...\nThe lock is acquired before the block is executed and always released once the block is complete.\nThe new localcontext() function in the decimal module makes it easy to save and restore the current decimal context, which encapsulates the desired precision and rounding characteristics for computations:\nfrom decimal import Decimal, Context, localcontext # Displays with default precision of 28 digits v = Decimal('578') print v.sqrt() with localcontext(Context(prec=16)): # All code in this block uses a precision of 16 digits. # The original context is restored on exiting the block. print v.sqrt()\nWriting Context Managers \u00b6\nUnder the hood, the \u2018 with \u2018 statement is fairly complicated. Most people will only use \u2018 with \u2018 in company with existing objects and don\u2019t need to know these details, so you can skip the rest of this section if you like. Authors of new objects will need to understand the details of the underlying implementation and should keep reading.\nA high-level explanation of the context management protocol is:\nThe expression is evaluated and should result in an object called a \u201ccontext manager\u201d. The context manager must have __enter__() and __exit__() methods.\nThe context manager\u2019s __enter__() method is called. The value returned is assigned to VAR. If no 'as VAR' clause is present, the value is simply discarded.\nThe code in BLOCK is executed.\nIf BLOCK raises an exception, the __exit__(type, value, traceback)() is called with the exception details, the same values returned by sys.exc_info() . The method\u2019s return value controls whether the exception is re-raised: any false value re-raises the exception, and True will result in suppressing it. You\u2019ll only rarely want to suppress the exception, because if you do the author of the code containing the \u2018 with \u2018 statement will never realize anything went wrong.\nIf BLOCK didn\u2019t raise an exception, the __exit__() method is still called, but type, value, and traceback are all None.\nLet\u2019s think through an example. I won\u2019t present detailed code but will only sketch the methods necessary for a database that supports transactions.\n(For people unfamiliar with database terminology: a set of changes to the database are grouped into a transaction. Transactions can be either committed, meaning that all the changes are written into the database, or rolled back, meaning that the changes are all discarded and the database is unchanged. See any database textbook for more information.)\nLet\u2019s assume there\u2019s an object representing a database connection. Our goal will be to let the user write code like this:\ndb_connection = DatabaseConnection() with db_connection as cursor: cursor.execute('insert into ...') cursor.execute('delete from ...') # ... more operations ...\nThe transaction should be committed if the code in the block runs flawlessly or rolled back if there\u2019s an exception. Here\u2019s the basic interface for DatabaseConnection that I\u2019ll assume:\nclass DatabaseConnection: # Database interface def cursor (self): \"Returns a cursor object and starts a new transaction\" def commit (self): \"Commits current transaction\" def rollback (self): \"Rolls back current transaction\"\nThe __enter__() method is pretty easy, having only to start a new transaction. For this application the resulting cursor object would be a useful result, so the method will return it. The user can then add as cursor to their \u2018 with \u2018 statement to bind the cursor to a variable name.\nclass DatabaseConnection: ... def __enter__ (self): # Code to start a new transaction cursor = self.cursor() return cursor\nThe __exit__() method is the most complicated because it\u2019s where most of the work has to be done. The method has to check if an exception occurred. If there was no exception, the transaction is committed. The transaction is rolled back if there was an exception.\nIn the code below, execution will just fall off the end of the function, returning the default value of None. None is false, so the exception will be re-raised automatically. If you wished, you could be more explicit and add a return statement at the marked location.\nclass DatabaseConnection: ... def __exit__ (self, type, value, tb): if tb is None: # No exception, so commit self.commit() else: # Exception occurred, so rollback. self.rollback() # return False\nThe contextlib module \u00b6\nThe new contextlib module provides some functions and a decorator that are useful for writing objects for use with the \u2018 with \u2018 statement.\nThe decorator is called contextmanager(), and lets you write a single generator function instead of defining a new class. The generator should yield exactly one value. The code up to the yield will be executed as the __enter__() method, and the value yielded will be the method\u2019s return value that will get bound to the variable in the \u2018 with \u2018 statement\u2019s as clause, if any. The code after the yield will be executed in the __exit__() method. Any exception raised in the block will be raised by the yield statement.\nOur database example from the previous section could be written using this decorator as:\nfrom contextlib import contextmanager @contextmanager def db_transaction (connection): cursor = connection.cursor() try: yield cursor except: connection.rollback() raise else: connection.commit() db = DatabaseConnection() with db_transaction(db) as cursor: ...\nThe contextlib module also has a nested(mgr1, mgr2, ...)() function that combines a number of context managers so you don\u2019t need to write nested \u2018 with \u2018 statements. In this example, the single \u2018 with \u2018 statement both starts a database transaction and acquires a thread lock:\nlock = threading.Lock() with nested (db_transaction(db), lock) as (cursor, locked): ...\nFinally, the closing(object)() function returns object so that it can be bound to a variable, and calls object.close at the end of the block.\nimport urllib, sys from contextlib import closing with closing(urllib.urlopen('http://www.yahoo.com')) as f: for line in f: sys.stdout.write(line)\nSee also\nPEP 343 - The \u201cwith\u201d statement\nPEP written by Guido van Rossum and Nick Coghlan; implemented by Mike Bland, Guido van Rossum, and Neal Norwitz. The PEP shows the code generated for a \u2018 with \u2018 statement, which can be helpful in learning how the statement works.\nThe documentation for the contextlib module.\nPEP 352: Exceptions as New-Style Classes \u00b6\nException classes can now be new-style classes, not just classic classes, and the built-in Exception class and all the standard built-in exceptions ( NameError , ValueError , etc.) are now new-style classes.\nThe inheritance hierarchy for exceptions has been rearranged a bit. In 2.5, the inheritance relationships are:\nBaseException # New in Python 2.5 |- KeyboardInterrupt |- SystemExit |- Exception |- (all other current built-in exceptions)\nThis rearrangement was done because people often want to catch all exceptions that indicate program errors. KeyboardInterrupt and SystemExit aren\u2019t errors, though, and usually represent an explicit action such as the user hitting Control-C or code calling sys.exit() . A bare except: will catch all exceptions, so you commonly need to list KeyboardInterrupt and SystemExit in order to re-raise them. The usual pattern is:\ntry: ... except (KeyboardInterrupt, SystemExit): raise except: # Log error... # Continue running program...\nIn Python 2.5, you can now write except Exception to achieve the same result, catching all the exceptions that usually indicate errors but leaving KeyboardInterrupt and SystemExit alone. As in previous versions, a bare except: still catches all exceptions.\nThe goal for Python 3.0 is to require any class raised as an exception to derive from BaseException or some descendant of BaseException , and future releases in the Python 2.x series may begin to enforce this constraint. Therefore, I suggest you begin making all your exception classes derive from Exception now. It\u2019s been suggested that the bare except: form should be removed in Python 3.0, but Guido van Rossum hasn\u2019t decided whether to do this or not.\nRaising of strings as exceptions, as in the statement raise \"Error occurred\", is deprecated in Python 2.5 and will trigger a warning. The aim is to be able to remove the string-exception feature in a few releases.\nSee also\nPEP 352 - Required Superclass for Exceptions\nPEP written by Brett Cannon and Guido van Rossum; implemented by Brett Cannon.\nPEP 353: Using ssize_t as the index type \u00b6\nA wide-ranging change to Python\u2019s C API, using a new Py_ssize_t type definition instead of int, will permit the interpreter to handle more data on 64-bit platforms. This change doesn\u2019t affect Python\u2019s capacity on 32-bit platforms.\nVarious pieces of the Python interpreter used C\u2019s int type to store sizes or counts; for example, the number of items in a list or tuple were stored in an int. The C compilers for most 64-bit platforms still define int as a 32-bit type, so that meant that lists could only hold up to 2**31 - 1 = 2147483647 items. (There are actually a few different programming models that 64-bit C compilers can use \u2013 see http://www.unix.org/version2/whatsnew/lp64_wp.html for a discussion \u2013 but the most commonly available model leaves int as 32 bits.)\nA limit of 2147483647 items doesn\u2019t really matter on a 32-bit platform because you\u2019ll run out of memory before hitting the length limit. Each list item requires space for a pointer, which is 4 bytes, plus space for a PyObject representing the item. 2147483647*4 is already more bytes than a 32-bit address space can contain.\nIt\u2019s possible to address that much memory on a 64-bit platform, however. The pointers for a list that size would only require 16 GiB of space, so it\u2019s not unreasonable that Python programmers might construct lists that large. Therefore, the Python interpreter had to be changed to use some type other than int, and this will be a 64-bit type on 64-bit platforms. The change will cause incompatibilities on 64-bit machines, so it was deemed worth making the transition now, while the number of 64-bit users is still relatively small. (In 5 or 10 years, we may all be on 64-bit machines, and the transition would be more painful then.)\nThis change most strongly affects authors of C extension modules. Python strings and container types such as lists and tuples now use Py_ssize_t to store their size. Functions such as PyList_Size() now return Py_ssize_t. Code in extension modules may therefore need to have some variables changed to Py_ssize_t.\nThe PyArg_ParseTuple() and Py_BuildValue() functions have a new conversion code, n, for Py_ssize_t. PyArg_ParseTuple() \u2018s s# and t# still output int by default, but you can define the macro PY_SSIZE_T_CLEAN before including Python.h to make them return Py_ssize_t.\nPEP 353 has a section on conversion guidelines that extension authors should read to learn about supporting 64-bit platforms.\nSee also\nPEP 353 - Using ssize_t as the index type\nPEP written and implemented by Martin von L\u00f6wis.\nPEP 357: The \u2018__index__\u2019 method \u00b6\nThe NumPy developers had a problem that could only be solved by adding a new special method, __index__() . When using slice notation, as in [start:stop:step], the values of the start, stop, and step indexes must all be either integers or long integers. NumPy defines a variety of specialized integer types corresponding to unsigned and signed integers of 8, 16, 32, and 64 bits, but there was no way to signal that these types could be used as slice indexes.\nSlicing can\u2019t just use the existing __int__() method because that method is also used to implement coercion to integers. If slicing used __int__() , floating-point numbers would also become legal slice indexes and that\u2019s clearly an undesirable behaviour.\nInstead, a new special method called __index__() was added. It takes no arguments and returns an integer giving the slice index to use. For example:\nclass C: def __index__ (self): return self.value\nThe return value must be either a Python integer or long integer. The interpreter will check that the type returned is correct, and raises a TypeError if this requirement isn\u2019t met.\nA corresponding nb_index slot was added to the C-level PyNumberMethods structure to let C extensions implement this protocol. PyNumber_Index(obj)() can be used in extension code to call the __index__() function and retrieve its result.\nSee also\nPEP 357 - Allowing Any Object to be Used for Slicing\nPEP written and implemented by Travis Oliphant.\nOther Language Changes \u00b6\nHere are all of the changes that Python 2.5 makes to the core Python language.\nThe dict type has a new hook for letting subclasses provide a default value when a key isn\u2019t contained in the dictionary. When a key isn\u2019t found, the dictionary\u2019s __missing__(key)() method will be called. This hook is used to implement the new defaultdict class in the collections module. The following example defines a dictionary that returns zero for any missing key:\nclass zerodict (dict): def __missing__ (self, key): return 0 d = zerodict({1:1, 2:2}) print d[1], d[2] # Prints 1, 2 print d[3], d[4] # Prints 0, 0\nBoth 8-bit and Unicode strings have new partition(sep)() and rpartition(sep)() methods that simplify a common use case.\nThe find(S)() method is often used to get an index which is then used to slice the string and obtain the pieces that are before and after the separator. partition(sep)() condenses this pattern into a single method call that returns a 3-tuple containing the substring before the separator, the separator itself, and the substring after the separator. If the separator isn\u2019t found, the first element of the tuple is the entire string and the other two elements are empty. rpartition(sep)() also returns a 3-tuple but starts searching from the end of the string; the r stands for \u2018reverse\u2019.\nSome examples:\n>>> ('http://www.python.org').partition('://') ('http', '://', 'www.python.org') >>> ('file:/usr/share/doc/index.html').partition('://') ('file:/usr/share/doc/index.html', '', '') >>> (u'Subject: a quick question').partition(':') (u'Subject', u':', u' a quick question') >>> 'www.python.org'.rpartition('.') ('www.python', '.', 'org') >>> 'www.python.org'.rpartition(':') ('', '', 'www.python.org')\n(Implemented by Fredrik Lundh following a suggestion by Raymond Hettinger.)\nThe startswith() and endswith() methods of string types now accept tuples of strings to check for.\ndef is_image_file (filename): return filename.endswith(('.gif', '.jpg', '.tiff'))\n(Implemented by Georg Brandl following a suggestion by Tom Lynn.)\nThe min() and max() built-in functions gained a key keyword parameter analogous to the key argument for sort(). This parameter supplies a function that takes a single argument and is called for every value in the list; min() / max() will return the element with the smallest/largest return value from this function. For example, to find the longest string in a list, you can do:\nL = ['medium', 'longest', 'short'] # Prints 'longest' print max(L, key=len) # Prints 'short', because lexicographically 'short' has the largest value print max(L)\n(Contributed by Steven Bethard and Raymond Hettinger.)\nTwo new built-in functions, any() and all() , evaluate whether an iterator contains any true or false values. any() returns True if any value returned by the iterator is true; otherwise it will return False . all() returns True only if all of the values returned by the iterator evaluate as true. (Suggested by Guido van Rossum, and implemented by Raymond Hettinger.)\nThe result of a class\u2019s __hash__() method can now be either a long integer or a regular integer. If a long integer is returned, the hash of that value is taken. In earlier versions the hash value was required to be a regular integer, but in 2.5 the id() built-in was changed to always return non-negative numbers, and users often seem to use id(self) in __hash__() methods (though this is discouraged).\nASCII is now the default encoding for modules. It\u2019s now a syntax error if a module contains string literals with 8-bit characters but doesn\u2019t have an encoding declaration. In Python 2.4 this triggered a warning, not a syntax error. See PEP 263 for how to declare a module\u2019s encoding; for example, you might add a line like this near the top of the source file:\n# -*- coding: latin1 -*-\nA new warning, UnicodeWarning , is triggered when you attempt to compare a Unicode string and an 8-bit string that can\u2019t be converted to Unicode using the default ASCII encoding. The result of the comparison is false:\n>>> chr(128) == unichr(128) # Can't convert chr(128) to Unicode __main__:1: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal False >>> chr(127) == unichr(127) # chr(127) can be converted True\nPreviously this would raise a UnicodeDecodeError exception, but in 2.5 this could result in puzzling problems when accessing a dictionary. If you looked up unichr(128) and chr(128) was being used as a key, you\u2019d get a UnicodeDecodeError exception. Other changes in 2.5 resulted in this exception being raised instead of suppressed by the code in dictobject.c that implements dictionaries.\nRaising an exception for such a comparison is strictly correct, but the change might have broken code, so instead UnicodeWarning was introduced.\n(Implemented by Marc-Andr\u00e9 Lemburg.)\nOne error that Python programmers sometimes make is forgetting to include an __init__.py module in a package directory. Debugging this mistake can be confusing, and usually requires running Python with the -v switch to log all the paths searched. In Python 2.5, a new ImportWarning warning is triggered when an import would have picked up a directory as a package but no __init__.py was found. This warning is silently ignored by default; provide the -Wd option when running the Python executable to display the warning message. (Implemented by Thomas Wouters.)\nThe list of base classes in a class definition can now be empty. As an example, this is now legal:\nclass C(): pass\n(Implemented by Brett Cannon.)\nInteractive Interpreter Changes \u00b6\nIn the interactive interpreter, quit and exit have long been strings so that new users get a somewhat helpful message when they try to quit:\n>>> quit 'Use Ctrl-D (i.e. EOF) to exit.'\nIn Python 2.5, quit and exit are now objects that still produce string representations of themselves, but are also callable. Newbies who try quit() or exit() will now exit the interpreter as they expect. (Implemented by Georg Brandl.)\nThe Python executable now accepts the standard long options --help and --version ; on Windows, it also accepts the /? option for displaying a help message. (Implemented by Georg Brandl.)\nOptimizations \u00b6\nSeveral of the optimizations were developed at the NeedForSpeed sprint, an event held in Reykjavik, Iceland, from May 21\u201328 2006. The sprint focused on speed enhancements to the CPython implementation and was funded by EWT LLC with local support from CCP Games. Those optimizations added at this sprint are specially marked in the following list.\nWhen they were introduced in Python 2.4, the built-in set and frozenset types were built on top of Python\u2019s dictionary type. In 2.5 the internal data structure has been customized for implementing sets, and as a result sets will use a third less memory and are somewhat faster. (Implemented by Raymond Hettinger.)\nThe speed of some Unicode operations, such as finding substrings, string splitting, and character map encoding and decoding, has been improved. (Substring search and splitting improvements were added by Fredrik Lundh and Andrew Dalke at the NeedForSpeed sprint. Character maps were improved by Walter D\u00f6rwald and Martin von L\u00f6wis.)\nThe long(str, base)() function is now faster on long digit strings because fewer intermediate results are calculated. The peak is for strings of around 800\u20131000 digits where the function is 6 times faster. (Contributed by Alan McIntyre and committed at the NeedForSpeed sprint.)\nIt\u2019s now illegal to mix iterating over a file with for line in file and calling the file object\u2019s read()/ readline() /readlines() methods. Iteration uses an internal buffer and the read*() methods don\u2019t use that buffer. Instead they would return the data following the buffer, causing the data to appear out of order. Mixing iteration and these methods will now trigger a ValueError from the read*() method. (Implemented by Thomas Wouters.)\nThe struct module now compiles structure format strings into an internal representation and caches this representation, yielding a 20% speedup. (Contributed by Bob Ippolito at the NeedForSpeed sprint.)\nThe re module got a 1 or 2% speedup by switching to Python\u2019s allocator functions instead of the system\u2019s malloc() and free(). (Contributed by Jack Diederich at the NeedForSpeed sprint.)\nThe code generator\u2019s peephole optimizer now performs simple constant folding in expressions. If you write something like a = 2+3, the code generator will do the arithmetic and produce code corresponding to a = 5. (Proposed and implemented by Raymond Hettinger.)\nFunction calls are now faster because code objects now keep the most recently finished frame (a \u201czombie frame\u201d) in an internal field of the code object, reusing it the next time the code object is invoked. (Original patch by Michael Hudson, modified by Armin Rigo and Richard Jones; committed at the NeedForSpeed sprint.) Frame objects are also slightly smaller, which may improve cache locality and reduce memory usage a bit. (Contributed by Neal Norwitz.)\nPython\u2019s built-in exceptions are now new-style classes, a change that speeds up instantiation considerably. Exception handling in Python 2.5 is therefore about 30% faster than in 2.4. (Contributed by Richard Jones, Georg Brandl and Sean Reifschneider at the NeedForSpeed sprint.)\nImporting now caches the paths tried, recording whether they exist or not so that the interpreter makes fewer open() and stat() calls on startup. (Contributed by Martin von L\u00f6wis and Georg Brandl.)\nNew, Improved, and Removed Modules \u00b6\nThe standard library received many enhancements and bug fixes in Python 2.5. Here\u2019s a partial list of the most notable changes, sorted alphabetically by module name. Consult the Misc/NEWS file in the source tree for a more complete list of changes, or look through the SVN logs for all the details.\nThe audioop module now supports the a-LAW encoding, and the code for u-LAW encoding has been improved. (Contributed by Lars Immisch.)\nThe codecs module gained support for incremental codecs. The codec.lookup() function now returns a CodecInfo instance instead of a tuple. CodecInfo instances behave like a 4-tuple to preserve backward compatibility but also have the attributes encode, decode, incrementalencoder, incrementaldecoder, streamwriter, and streamreader. Incremental codecs can receive input and produce output in multiple chunks; the output is the same as if the entire input was fed to the non-incremental codec. See the codecs module documentation for details. (Designed and implemented by Walter D\u00f6rwald.)\nThe collections module gained a new type, defaultdict, that subclasses the standard dict type. The new type mostly behaves like a dictionary but constructs a default value when a key isn\u2019t present, automatically adding it to the dictionary for the requested key value.\nThe first argument to defaultdict\u2018s constructor is a factory function that gets called whenever a key is requested but not found. This factory function receives no arguments, so you can use built-in type constructors such as list() or int() . For example, you can make an index of words based on their initial letter like this:\nwords = \"\"\"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura che la diritta via era smarrita\"\"\".lower().split() index = defaultdict(list) for w in words: init_letter = w[0] index[init_letter].append(w)\nPrinting index results in the following output:\ndefaultdict(<type 'list'>, {'c': ['cammin', 'che'], 'e': ['era'], 'd': ['del', 'di', 'diritta'], 'm': ['mezzo', 'mi'], 'l': ['la'], 'o': ['oscura'], 'n': ['nel', 'nostra'], 'p': ['per'], 's': ['selva', 'smarrita'], 'r': ['ritrovai'], 'u': ['una'], 'v': ['vita', 'via']}\n(Contributed by Guido van Rossum.)\nThe deque double-ended queue type supplied by the collections module now has a remove(value)() method that removes the first occurrence of value in the queue, raising ValueError if the value isn\u2019t found. (Contributed by Raymond Hettinger.)\nNew module: The contextlib module contains helper functions for use with the new \u2018 with \u2018 statement. See section The contextlib module for more about this module.\nNew module: The cProfile module is a C implementation of the existing profile module that has much lower overhead. The module\u2019s interface is the same as profile : you run cProfile.run('main()') to profile a function, can save profile data to a file, etc. It\u2019s not yet known if the Hotshot profiler, which is also written in C but doesn\u2019t match the profile module\u2019s interface, will continue to be maintained in future versions of Python. (Contributed by Armin Rigo.)\nAlso, the pstats module for analyzing the data measured by the profiler now supports directing the output to any file object by supplying a stream argument to the Stats constructor. (Contributed by Skip Montanaro.)\nThe csv module, which parses files in comma-separated value format, received several enhancements and a number of bugfixes. You can now set the maximum size in bytes of a field by calling the csv.field_size_limit(new_limit)() function; omitting the new_limit argument will return the currently-set limit. The reader class now has a line_num attribute that counts the number of physical lines read from the source; records can span multiple physical lines, so line_num is not the same as the number of records read.\nThe CSV parser is now stricter about multi-line quoted fields. Previously, if a line ended within a quoted field without a terminating newline character, a newline would be inserted into the returned field. This behavior caused problems when reading files that contained carriage return characters within fields, so the code was changed to return the field without inserting newlines. As a consequence, if newlines embedded within fields are important, the input should be split into lines in a manner that preserves the newline characters.\n(Contributed by Skip Montanaro and Andrew McNamara.)\nThe datetime class in the datetime module now has a strptime(string, format)() method for parsing date strings, contributed by Josh Spoerri. It uses the same format characters as time.strptime() and time.strftime() :\nfrom datetime import datetime ts = datetime.strptime('10:13:15 2006-03-07', '%H:%M:%S %Y-%m-%d')\nThe SequenceMatcher.get_matching_blocks() method in the difflib module now guarantees to return a minimal list of blocks describing matching subsequences. Previously, the algorithm would occasionally break a block of matching elements into two list entries. (Enhancement by Tim Peters.)\nThe doctest module gained a SKIP option that keeps an example from being executed at all. This is intended for code snippets that are usage examples intended for the reader and aren\u2019t actually test cases.\nAn encoding parameter was added to the testfile() function and the DocFileSuite class to specify the file\u2019s encoding. This makes it easier to use non-ASCII characters in tests contained within a docstring. (Contributed by Bjorn Tillenius.)\nThe email package has been updated to version 4.0. (Contributed by Barry Warsaw.)\nThe fileinput module was made more flexible. Unicode filenames are now supported, and a mode parameter that defaults to \"r\" was added to the input() function to allow opening files in binary or universal-newline mode. Another new parameter, openhook, lets you use a function other than open() to open the input files. Once you\u2019re iterating over the set of files, the FileInput object\u2019s new fileno() returns the file descriptor for the currently opened file. (Contributed by Georg Brandl.)\nIn the gc module, the new get_count() function returns a 3-tuple containing the current collection counts for the three GC generations. This is accounting information for the garbage collector; when these counts reach a specified threshold, a garbage collection sweep will be made. The existing gc.collect() function now takes an optional generation argument of 0, 1, or 2 to specify which generation to collect. (Contributed by Barry Warsaw.)\nThe nsmallest() and nlargest() functions in the heapq module now support a key keyword parameter similar to the one provided by the min() / max() functions and the sort() methods. For example:\n>>> import heapq >>> L = [\"short\", 'medium', 'longest', 'longer still'] >>> heapq.nsmallest(2, L) # Return two lowest elements, lexicographically ['longer still', 'longest'] >>> heapq.nsmallest(2, L, key=len) # Return two shortest elements ['short', 'medium']\n(Contributed by Raymond Hettinger.)\nThe itertools.islice() function now accepts None for the start and step arguments. This makes it more compatible with the attributes of slice objects, so that you can now write the following:\ns = slice(5) # Create slice object itertools.islice(iterable, s.start, s.stop, s.step)\n(Contributed by Raymond Hettinger.)\nThe format() function in the locale module has been modified and two new functions were added, format_string() and currency().\nThe format() function\u2019s val parameter could previously be a string as long as no more than one %char specifier appeared; now the parameter must be exactly one %char specifier with no surrounding text. An optional monetary parameter was also added which, if True, will use the locale\u2019s rules for formatting currency in placing a separator between groups of three digits.\nTo format strings with multiple %char specifiers, use the new format_string() function that works like format() but also supports mixing %char specifiers with arbitrary text.\nA new currency() function was also added that formats a number according to the current locale\u2019s settings.\n(Contributed by Georg Brandl.)\nThe mailbox module underwent a massive rewrite to add the capability to modify mailboxes in addition to reading them. A new set of classes that include mbox, MH, and Maildir are used to read mailboxes, and have an add(message)() method to add messages, remove(key)() to remove messages, and lock()/unlock() to lock/unlock the mailbox. The following example converts a maildir-format mailbox into an mbox-format one:\nimport mailbox # 'factory=None' uses email.Message.Message as the class representing # individual messages. src = mailbox.Maildir('maildir', factory=None) dest = mailbox.mbox('/tmp/mbox') for msg in src: dest.add(msg)\n(Contributed by Gregory K. Johnson. Funding was provided by Google\u2019s 2005 Summer of Code.)\nNew module: the msilib module allows creating Microsoft Installer .msi files and CAB files. Some support for reading the .msi database is also included. (Contributed by Martin von L\u00f6wis.)\nThe nis module now supports accessing domains other than the system default domain by supplying a domain argument to the nis.match() and nis.maps() functions. (Contributed by Ben Bell.)\nThe operator module\u2019s itemgetter() and attrgetter() functions now support multiple fields. A call such as operator.attrgetter('a', 'b') will return a function that retrieves the a and b attributes. Combining this new feature with the sort() method\u2019s key parameter lets you easily sort lists using multiple fields. (Contributed by Raymond Hettinger.)\nThe optparse module was updated to version 1.5.1 of the Optik library. The OptionParser class gained an epilog attribute, a string that will be printed after the help message, and a destroy() method to break reference cycles created by the object. (Contributed by Greg Ward.)\nThe os module underwent several changes. The stat_float_times variable now defaults to true, meaning that os.stat() will now return time values as floats. (This doesn\u2019t necessarily mean that os.stat() will return times that are precise to fractions of a second; not all systems support such precision.)\nConstants named os.SEEK_SET , os.SEEK_CUR , and os.SEEK_END have been added; these are the parameters to the os.lseek() function. Two new constants for locking are os.O_SHLOCK and os.O_EXLOCK .\nTwo new functions, wait3() and wait4(), were added. They\u2019re similar the waitpid() function which waits for a child process to exit and returns a tuple of the process ID and its exit status, but wait3() and wait4() return additional information. wait3() doesn\u2019t take a process ID as input, so it waits for any child process to exit and returns a 3-tuple of process-id, exit-status, resource-usage as returned from the resource.getrusage() function. wait4(pid)() does take a process ID. (Contributed by Chad J. Schroeder.)\nOn FreeBSD, the os.stat() function now returns times with nanosecond resolution, and the returned object now has st_gen and st_birthtime. The st_flags attribute is also available, if the platform supports it. (Contributed by Antti Louko and Diego Petten\u00f2.)\nThe Python debugger provided by the pdb module can now store lists of commands to execute when a breakpoint is reached and execution stops. Once breakpoint #1 has been created, enter commands 1 and enter a series of commands to be executed, finishing the list with end. The command list can include commands that resume execution, such as continue or next. (Contributed by Gr\u00e9goire Dooms.)\nThe pickle and cPickle modules no longer accept a return value of None from the __reduce__() method; the method must return a tuple of arguments instead. The ability to return None was deprecated in Python 2.4, so this completes the removal of the feature.\nThe pkgutil module, containing various utility functions for finding packages, was enhanced to support PEP 302\u2019s import hooks and now also works for packages stored in ZIP-format archives. (Contributed by Phillip J. Eby.)\nThe pybench benchmark suite by Marc-Andr\u00e9 Lemburg is now included in the Tools/pybench directory. The pybench suite is an improvement on the commonly used pystone.py program because pybench provides a more detailed measurement of the interpreter\u2019s speed. It times particular operations such as function calls, tuple slicing, method lookups, and numeric operations, instead of performing many different operations and reducing the result to a single number as pystone.py does.\nThe pyexpat module now uses version 2.0 of the Expat parser. (Contributed by Trent Mick.)\nThe Queue class provided by the Queue module gained two new methods. join() blocks until all items in the queue have been retrieved and all processing work on the items have been completed. Worker threads call the other new method, task_done(), to signal that processing for an item has been completed. (Contributed by Raymond Hettinger.)\nThe old regex and regsub modules, which have been deprecated ever since Python 2.0, have finally been deleted. Other deleted modules: statcache, tzparse, whrandom.\nAlso deleted: the lib-old directory, which includes ancient modules such as dircmp and ni, was removed. lib-old wasn\u2019t on the default sys.path, so unless your programs explicitly added the directory to sys.path, this removal shouldn\u2019t affect your code.\nThe rlcompleter module is no longer dependent on importing the readline module and therefore now works on non-Unix platforms. (Patch from Robert Kiendl.)\nThe SimpleXMLRPCServer and DocXMLRPCServer classes now have a rpc_paths attribute that constrains XML-RPC operations to a limited set of URL paths; the default is to allow only '/' and '/RPC2'. Setting rpc_paths to None or an empty tuple disables this path checking.\nThe socket module now supports AF_NETLINK sockets on Linux, thanks to a patch from Philippe Biondi. Netlink sockets are a Linux-specific mechanism for communications between a user-space process and kernel code; an introductory article about them is at http://www.linuxjournal.com/article/7356 . In Python code, netlink addresses are represented as a tuple of 2 integers, (pid, group_mask).\nTwo new methods on socket objects, recv_into(buffer)() and recvfrom_into(buffer)(), store the received data in an object that supports the buffer protocol instead of returning the data as a string. This means you can put the data directly into an array or a memory-mapped file.\nSocket objects also gained getfamily(), gettype(), and getproto() accessor methods to retrieve the family, type, and protocol values for the socket.\nNew module: the spwd module provides functions for accessing the shadow password database on systems that support shadow passwords.\nThe struct is now faster because it compiles format strings into Struct objects with pack() and unpack() methods. This is similar to how the re module lets you create compiled regular expression objects. You can still use the module-level pack() and unpack() functions; they\u2019ll create Struct objects and cache them. Or you can use Struct instances directly:\ns = struct.Struct('ih3s') data = s.pack(1972, 187, 'abc') year, number, name = s.unpack(data)\nYou can also pack and unpack data to and from buffer objects directly using the pack_into(buffer, offset, v1, v2, ...)() and unpack_from(buffer, offset)() methods. This lets you store data directly into an array or a memory- mapped file.\n(Struct objects were implemented by Bob Ippolito at the NeedForSpeed sprint. Support for buffer objects was added by Martin Blais, also at the NeedForSpeed sprint.)\nThe Python developers switched from CVS to Subversion during the 2.5 development process. Information about the exact build version is available as the sys.subversion variable, a 3-tuple of (interpreter-name, branch-name, revision-range). For example, at the time of writing my copy of 2.5 was reporting ('CPython', 'trunk', '45313:45315').\nThis information is also available to C extensions via the Py_GetBuildInfo() function that returns a string of build information like this: \"trunk:45355:45356M, Apr 13 2006, 07:42:19\". (Contributed by Barry Warsaw.)\nAnother new function, sys._current_frames() , returns the current stack frames for all running threads as a dictionary mapping thread identifiers to the topmost stack frame currently active in that thread at the time the function is called. (Contributed by Tim Peters.)\nThe TarFile class in the tarfile module now has an extractall() method that extracts all members from the archive into the current working directory. It\u2019s also possible to set a different directory as the extraction target, and to unpack only a subset of the archive\u2019s members.\nThe compression used for a tarfile opened in stream mode can now be autodetected using the mode 'r|*'. (Contributed by Lars Gust\u00e4bel.)\nThe threading module now lets you set the stack size used when new threads are created. The stack_size([*size*])() function returns the currently configured stack size, and supplying the optional size parameter sets a new value. Not all platforms support changing the stack size, but Windows, POSIX threading, and OS/2 all do. (Contributed by Andrew MacIntyre.)\nThe unicodedata module has been updated to use version 4.1.0 of the Unicode character database. Version 3.2.0 is required by some specifications, so it\u2019s still available as unicodedata.ucd_3_2_0 .\nNew module: the uuid module generates universally unique identifiers (UUIDs) according to RFC 4122 . The RFC defines several different UUID versions that are generated from a starting string, from system properties, or purely randomly. This module contains a UUID class and functions named uuid1(), uuid3(), uuid4(), and uuid5() to generate different versions of UUID. (Version 2 UUIDs are not specified in RFC 4122 and are not supported by this module.)\n>>> import uuid >>> # make a UUID based on the host ID and current time >>> uuid.uuid1() UUID('a8098c1a-f86e-11da-bd1a-00112444be1e') >>> # make a UUID using an MD5 hash of a namespace UUID and a name >>> uuid.uuid3(uuid.NAMESPACE_DNS, 'python.org') UUID('6fa459ea-ee8a-3ca4-894e-db77e160355e') >>> # make a random UUID >>> uuid.uuid4() UUID('16fd2706-8baf-433b-82eb-8c7fada847da') >>> # make a UUID using a SHA-1 hash of a namespace UUID and a name >>> uuid.uuid5(uuid.NAMESPACE_DNS, 'python.org') UUID('886313e1-3b8a-5372-9b90-0c9aee199e5d')\n(Contributed by Ka-Ping Yee.)\nThe weakref module\u2019s WeakKeyDictionary and WeakValueDictionary types gained new methods for iterating over the weak references contained in the dictionary. iterkeyrefs() and keyrefs() methods were added to WeakKeyDictionary, and itervaluerefs() and valuerefs() were added to WeakValueDictionary. (Contributed by Fred L. Drake, Jr.)\nThe webbrowser module received a number of enhancements. It\u2019s now usable as a script with python -m webbrowser, taking a URL as the argument; there are a number of switches to control the behaviour (-n for a new browser window, -t for a new tab). New module-level functions, open_new() and open_new_tab(), were added to support this. The module\u2019s open() function supports an additional feature, an autoraise parameter that signals whether to raise the open window when possible. A number of additional browsers were added to the supported list such as Firefox, Opera, Konqueror, and elinks. (Contributed by Oleg Broytmann and Georg Brandl.)\nThe xmlrpclib module now supports returning datetime objects for the XML-RPC date type. Supply use_datetime=True to the loads() function or the Unmarshaller class to enable this feature. (Contributed by Skip Montanaro.)\nThe zipfile module now supports the ZIP64 version of the format, meaning that a .zip archive can now be larger than 4 GiB and can contain individual files larger than 4 GiB. (Contributed by Ronald Oussoren.)\nThe zlib module\u2019s Compress and Decompress objects now support a copy() method that makes a copy of the object\u2019s internal state and returns a new Compress or Decompress object. (Contributed by Chris AtLee.)\nThe ctypes package \u00b6\nThe ctypes package, written by Thomas Heller, has been added to the standard library. ctypes lets you call arbitrary functions in shared libraries or DLLs. Long-time users may remember the dl module, which provides functions for loading shared libraries and calling functions in them. The ctypes package is much fancier.\nTo load a shared library or DLL, you must create an instance of the CDLL class and provide the name or path of the shared library or DLL. Once that\u2019s done, you can call arbitrary functions by accessing them as attributes of the CDLL object.\nimport ctypes libc = ctypes.CDLL('libc.so.6') result = libc.printf(\"Line of output\\n\")\nType constructors for the various C types are provided: c_int(), c_float(), c_double(), c_char_p() (equivalent to char *), and so forth. Unlike Python\u2019s types, the C versions are all mutable; you can assign to their value attribute to change the wrapped value. Python integers and strings will be automatically converted to the corresponding C types, but for other types you must call the correct type constructor. (And I mean must; getting it wrong will often result in the interpreter crashing with a segmentation fault.)\nYou shouldn\u2019t use c_char_p() with a Python string when the C function will be modifying the memory area, because Python strings are supposed to be immutable; breaking this rule will cause puzzling bugs. When you need a modifiable memory area, use create_string_buffer():\ns = \"this is a string\" buf = ctypes.create_string_buffer(s) libc.strfry(buf)\nC functions are assumed to return integers, but you can set the restype attribute of the function object to change this:\n>>> libc.atof('2.71828') -1783957616 >>> libc.atof.restype = ctypes.c_double >>> libc.atof('2.71828') 2.71828\nctypes also provides a wrapper for Python\u2019s C API as the ctypes.pythonapi object. This object does not release the global interpreter lock before calling a function, because the lock must be held when calling into the interpreter\u2019s code. There\u2019s a py_object() type constructor that will create a PyObject * pointer. A simple usage:\nimport ctypes d = {} ctypes.pythonapi.PyObject_SetItem(ctypes.py_object(d), ctypes.py_object(\"abc\"), ctypes.py_object(1)) # d is now {'abc', 1}.\nDon\u2019t forget to use py_object(); if it\u2019s omitted you end up with a segmentation fault.\nctypes has been around for a while, but people still write and distribution hand-coded extension modules because you can\u2019t rely on ctypes being present. Perhaps developers will begin to write Python wrappers atop a library accessed through ctypes instead of extension modules, now that ctypes is included with core Python.\nSee also\nThe ctypes web page, with a tutorial, reference, and FAQ.\nThe documentation for the ctypes module.\nThe ElementTree package \u00b6\nA subset of Fredrik Lundh\u2019s ElementTree library for processing XML has been added to the standard library as xml.etree. The available modules are ElementTree, ElementPath, and ElementInclude from ElementTree 1.2.6. The cElementTree accelerator module is also included.\nThe rest of this section will provide a brief overview of using ElementTree. Full documentation for ElementTree is available at http://effbot.org/zone/element-index.htm .\nElementTree represents an XML document as a tree of element nodes. The text content of the document is stored as the text and tail attributes of (This is one of the major differences between ElementTree and the Document Object Model; in the DOM there are many different types of node, including TextNode.)\nThe most commonly used parsing function is parse(), that takes either a string (assumed to contain a filename) or a file-like object and returns an ElementTree instance:\nfrom xml.etree import ElementTree as ET tree = ET.parse('ex-1.xml') feed = urllib.urlopen( 'http://planet.python.org/rss10.xml') tree = ET.parse(feed)\nOnce you have an ElementTree instance, you can call its getroot() method to get the root Element node.\nThere\u2019s also an XML() function that takes a string literal and returns an Element node (not an ElementTree). This function provides a tidy way to incorporate XML fragments, approaching the convenience of an XML literal:\nsvg = ET.XML(\"\"\"<svg width=\"10px\" version=\"1.0\"> </svg>\"\"\") svg.set('height', '320px') svg.append(elem1)\nEach XML element supports some dictionary-like and some list-like access methods. Dictionary-like operations are used to access attribute values, and list-like operations are used to access child nodes.\nOperation\n"}, {"score": 912.7353, "uuid": "da2be52c-07d2-5cd4-926a-1abc3e16360d", "index": "cw12", "trec_id": "clueweb12-0012wb-15-01269", "target_hostname": "wiki.cacr.caltech.edu", "target_uri": "http://wiki.cacr.caltech.edu/danse/index.php/2/27/06_%28monday%29:_Design_for_the_SANS_Data_Analysis_Application", "page_rank": 1.1712918e-09, "spam_rank": 89, "title": "2&#x2F;27&#x2F;06 (monday): Design <em>for</em> the SANS <em>Data</em> <em>Analysis</em> Application - DANSE", "snippet": "BF&gt; So that&#x27;s a separate task <em>or</em> does that fit into this type of user interface? JZ&gt; <em>I</em> think that one will come with a model of independent <em>analysis</em>. Once you get the reduced experimental <em>data</em> you can get direct it at the P(<em>R</em>) from the <em>I</em>", "explanation": null, "document": "2/27/06 (monday): Design for the SANS Data Analysis Application\nFrom DANSE\nRelated Materials: mp3 (http://cartman.gps.caltech.edu/people/mcq/danse/devmtg-20060227.mp3), presentation , design document .\nattendees: Mike McKerns, Brent Fultz\n(by phone):John McCorquodale, Jiao Lin, Jing Zhou, Paul Butler, Dennis Mikkelson, Paul Kienzle, Pavol Juhas, Chris Farrow, Jiwu Liu, Seung-Yub Lee, Ersan Ustundag, Michael Aivazis, Tom Worlton, Jim Hammonds\nBF> The NSF has gone forward with the budget into their legal dept. It is going to be a Cooperative agreement. This changes a number of things for the project, and we have to determine how we get reviewed, how we have oversight committies, and to define what is the role of the NSF.\nMaybe we will have a yearly review, and maybe we will request a review committee much like the Site Visit review committee... a group that would stay with us for the term of the project.\nThere will be lots of paperwork, and keeping track of things like publications, outreach activities, and the like is a fair administrative burden that comes with these cooperative agreements. We will have to do all of this before the awards are granted, and so I may be contacting some of you shortly for input.\nWe have a presentation today from the SANS group. Jing, do you want to proceed?\nJZ> Today I'll talk about the design work we've been doing for SANS data analysis.\nJING PRESENTATION BEGIN\nJING PRESENTATION END\nJZ> I've been writing the design document for this application, where I've followed the template for Mike's design document. I have put in a list of acceptance criteria, the user interface idea, and a use case diagram.\nThe description for each componenet is included. Also the class diagram for the components we will implement are included.\nSo for the coding, I will start to implement from here. I've started to implement the classes 'geoshape' and 'sphere.' I have also generated the doxygen document for geoshape and sphere. It shows the files, the inheritance, class members, member functions, variables and some descriptions.\nThat's what I've done so far for design work for SANS data analysis.\nBF> Thank you very much, that's very good. In getting various types of I(Q) and your FFTs. I wonder if the MSU group has any tools that would be useful here.\nPJ> For FT we have a program implemented here in IDL. I am not sure how easy it would be to use it as part a library.\nBF> This looks like a good example of a common alg that we can use early.\nPK> Is there anything special about the FFT?\nJZ> It's just FFT.\nPK> Which Python already has a number of...\nBF> We ought to make a decision on something like that. If you do the doxygen generation, it is good to try it out. We probably should adhere to some standards for the amount of text and maybe a purpose at the beginning of the class just so it's clear to someone reading through the doxygen pages what this class is actually for.\nJZ> OK.\nBF> But it's good you've got it started. Now that you've seen how it works you can decide what you want to do for inline documentation for example.\nJZ> Uh-huh.\nMM> I'll go over that with Jing.\nBF> We probably should come up with some good examples that people can follow and refer people to it.\nMM> Another thing I wanted to cover with Jing is the interface and the types of variables of the inputs - qmax & numI. The information you gave was what their type and shape is. Saying that it's required is good instead of being optional, but you may also want to have a description of what they are. Qmax is a good descriptive name but you may want to put it in words. If you do it now, then it's easy to do when you're writing your manual. Otherwise I saw this as an excellent example of what everyone should be doing.\nJZ> OK, basically I will have the arguments of the variables have a meaningful name.\nMM> Not only have a meaningful name; you saw when you read through your example; this is qmax and it is a scaler and basically it is required and then you explained what it was. You should have the definition in there as well.\nBF> Jing, I thought of something else. I didn't notice, maybe I missed it, but do you have an extraction from measured I(Q)'s of radial distribution functions directly or is that not part of this scope?\nJZ> Which one, the exceptional what?\nBF> If you have a measured I(Q) just take the forward calculation and get some sort of a P(R) out of it, that's not included in this is it?\nJZ> It's not, it's the backward way. I(Q) gets the P(R) from the point then from P(R) to I(Q).\nBF> So that's a separate task or does that fit into this type of user interface?\nJZ> I think that one will come with a model of independent analysis. Once you get the reduced experimental data you can get direct it at the P(R) from the I(Q).\nBF> I am just wondering if you see that as a separate application or a eventual capability.\nJZ> I see that as a separate function in another application.\nBF> OK.\nPB> I think the way Jing came up with this application is really focusing on the simulation part. So you have some model and you simulate an I(Q). I think she mentioned in the beginning that we are not talking about fitting it to actual real data, but using it independently of real data. Whereas, what you are talking about would require actually having the data.\nBF> Or it could require a simulation like this.\nPB> Well it could require a simulated dataset. But it requires a dataset.\nBF> Or you could do it that way. It might be curious to see how spheres look when they are convoluted together and stuff like that.\nMM> A real educational tool.\nBF> That's right. That's great. Thank you very much, that was very clear. I learned a lot from this one. Anybody else have comments or design issues?\nDM> I just have one comment regarding the interval from min to max with the number of steps. We've encountered situations where people want the evaluation points to be distributed according to some geometric progression rather than uniformly. Were you thinking about including the possibility of increasing the step size by 5% each time or something?\nJZ> That's what we do for the analytical form factor in IGOR, right Paul?\nPB> Yes, i think it's actually a logorithmic scale rather than linear.\nDM> But it won't always be that way then?\nPB> That's currently how the NIST IGOR software does it. I think it is where Jing is starting off in patterning. Your point of giving a user the flexibility of different ways of spacing them adds quite a level of complexity, but it's perhaps something we should think a little bit about. I'm not sure entirely worth it at this level.\nDM> Well as as long as case of the logorithmic scale is taken care of by this. So that when you specify the start/stop and number of points that it is assumed to be distributed logorithmically.\nPB> Right.\nDM> I think you're OK then.\nPB> That's essentially the plan, yes.\nBF> Thank you very much Jing and Paul. Next week Paul Kienzle will do the technical presentation. Is there something else?\nMM> If this was actually our review, I think we are in fairly good shape. I am glad to see you are actually coding right now Jing, because you seem to be in the state where you are ready to do that. Is this how you are going to proceed next?\nJZ> Yes, in the next step I am trying to implement each class 1-by-1.\nJL> I was just wondering if you have looked at the geometric modeling stuff in pyre? I wonder if Jing has looked at Michael Aivazis' pyre solid codes?\nJZ> I have never seen that geometric, I don't know that part. There may be something in common that I can use. But I don't know which one you are talking about actually.\nMM> The geometric modeleler in Pyre, is that right Jiao?\nJL> I think it's under Pyre solids.\nMM> It is part of the geo-framework stuff.\nJZ> Geo-framework?\nMM> We'll send you a link to it.\nJZ> OK.\nBF> We'll take a look at it and talk about it. All right, any other ideas? Maybe we should see if there are any breakout sessions or focus groups that have occurred over the last week. I know mostly people have been interacting with Mike McKerns on getting some of these design issues nailed down.\nMM> There weren't any breakout sessions per se, we've have had a lot of discussions with the project leaders about sequencing of tasks and getting some tasks changed. So it's mostly at a higher level. John McCorquodale and I had a small run-through of some of those procedures; things we've been working on. We are actually going to have a breakout session today.\nBF> Great. Well, that's an important one. There's a couple of other items that have come up in our discussions that the group should be alert to I think. There is a possibility that we might want to switch from the CVS system to Subversion which is a little bit more modern and it is nicely integrated into a package called TRAC for generating feature requests and bug tracking, for example. It also has associated with it an HTML generator to do things like what you just saw here on the screen right now. We haven't made a decision on this, but there are a couple of things pushing us towards this. One of them is that the SNS has been using it and they like it very much and it might be a little easier to integrate with them. It seems that it's mature enough for us to use. It shouldn't too much more complicated to use than CVS but it would be different; people would have to get used to it. The decision hasn't been made but it's worth discussing. Maybe right now would be appropriate if anyone has opinions on Subversion to speak up, and we can have a very brief discussion. It might lead to a focus group.\nMM> Personally I think Subversion is slightly a bit better. The bonus is that it comes with TRAC project management and project tracking and bug tracking software. It is basically almost exactly what we proposed at the site visit. It's to me worth it hands down to switch to Subversion right away. What I mean is \"right away,\" as in \"before the project starts\".\nBF> So we can migrate all of the CVS repository more or less intact into Subversion there's tools to do this but it probably wouldn't be an instant thing.\nJM> Yeah, I think there is more of a restructuring involved than just copying because the critical difference between CVS and Subversion that in Subversion a version number applies to the entire repository it's the state of everything in your repository at the instant, rather than a version of each file. So that is the critical advantage. That means that the way you use it to store your stuff is different than CVS. You want to take your code-base and put it up into units of independent development and put each of those in its own repository so that there are versions independently. So that if one thing breaks the rest of the world can continue to advance. You can use an equal older working version before it broke.\nJH> I believe there's conversion utilities to do that though. That can work between the CVS and Subversion.\nMM> Yeah, I believe so.\nJM> Keeping your history data is what the conversion tools let you to do. That's clearly critical.\nBF> OK, this is interesting. We would see if there are any issues and have a plan of how we are going to do it here to see who is going to be in charge of this thing. We have to make some thinking on that. Are there any issues that would impact differently the different sub groups who are doing development? Would the subgroups basically be seeing this in the same way they see CVS now right, there wouldn't be that much change?\nJM> The complexity issue of having used both in close proximity over the last three months. Subversion, once you learn the commands, the four of them that you use on a daily basis, just feels quicker and simpler than CVS, so I think it should be a positive sort of emotional reaction.\nBF> All right, OK. Anyone had experience with Subversion and CVS that want to comment on it?\nMM> Tom Swain I don't think is here. The people at Oak Ridge have found it pretty positive. It's too bad we don't have access to their repository.\nBF> We probably ought to figure out internally at Caltech how we are going to make this transition. Where we oversee it. We can talk about that in the next few days. We want to do this quickly. I think you're right Mike, that now is the time at the beginning of the project before we are too far along if we're going to make this change.\nMM> I don't know if we have time to try it out, the ARCS beta release; it's probably too short of a time.\nBF> It could be too much I think.\nMM> I think it might be a little much.\nJM> We could certainly try a little piece of something, maybe put Pyre in subversion.\nBF> Yes, that's a point. Michael what do you think? Do you want to transition all of Pyre to Subversion.\n<no response from Michael Aivazis>\nJM> That's the silence of yes.\nBF> All right, we'll actually talk to Michael when he gets back. Another thing that has come up in discussion is a user interface toolkit. Do we want make a transition for example, to wxGlade? Do we want to take that path? That's a bigger topic and it doesn't need to be decided quite as instantly I think. We might want to have a group start to look at that soon.\nMM> I think a lot of people would be happy with a wxWindows toolkit, but I don't want to limit us by using wxWindows. I think it's better to have the toolkit and make sure it is designed so that there is toolkit independence. What happens for this the GUI toolkit infrastructure if wxGlade is modified then scooped out then you could write it with Tkinter with a minimal amount of rewrite and that's good. We need to at restructuring that task into something that has to be done. A lot of people want that apparently and we need to take a good look at that. Paul Kienzle have been talking about that already. We'll probably have a breakout session on this.\nBF> That might be a good one to have. The MSU group has already put a lot of investment in this of course, and if Jing is going to start to work on user interface maybe that's a sensible thing to do. Are there other issues that need to be discussed Mike?\nMM> I know Tom Swain's not here, I wish he was. He and I talked a little bit about starting to go through using Protoseq on an item in DANSE and he and I came to an agreement that I would go through gnuplot. We have bindings with gnuplot that I worked on for maybe a week. I want to be able to start with those because they are fairly young in the development and push them up to something that's going to work in Pyre and build the API on top of those. I want to get those done early for the ARCS beta release but also get it done so Tom and I can really get Protoseq working in DANSE and get a better feel of how it's done.\nBF> That's probably sensible and probably some discussion on that after you've tried it would be helpful.\nMM> Yes, I will write something up.\nBF> I don't think we have to have everyone do this at first.\nMM> My plan is I'll go through it with them and write something up with Tom and have some instructions.\nBF> Maybe I should ask the group who's on the phone if they anyone wants to participate in a discussion of a transition to Subversion. That's something we might have to do fairly early. What we're going to be doing here at Caltech is figuring out how we're going to do it, that's the first thing. What's involved in it. Others may want to participate on this decision to use it at this point, which has not yet been made. Anyone interested in following up on this?\n<no responses were made>\nBF> I'll take that as a No. Then we'll get back to you on what's involved in making this transition. Presumably it won't make your life any more difficult. We'll try to look out for you as we would for the transition issues here.\nJM> And presumably we'll have a Monday morning demo...\nBF> I guess so. It's probably a good thing to do.\nMM> Might be your slot, Brent... no, you're Inelastic... I'll be Michael.\nBF> OK good, we hope Michael will like it. Any other issues that we should raise here?\nMM> Yes, I just want to thank all of the project leaders for their input on the dependency draft that we have. It's been helpful in allowing me to see things that we may be able to push further out in the project or need to push out earlier. I see that this opitmization algorithm needs to come earlier, that's a no-brainer. And things like VASP may actually be able to slide out another year into the project. There will be some rearrangement and we will try to satisfy all the dependencies and needs as early as possible. I still would like to get input from Simon. Simon and I need to go back and forth on this a little bit. He's the only one I have a blank column for.\nBF> Any other items of business? In that case, thank you all for coming. And Jing thank you very much. That was really quite clear.\nJZ> Oh, thank you.\n"}, {"score": 912.69745, "uuid": "d7061178-b4ca-54d8-a46d-d6ebe54bc036", "index": "cw12", "trec_id": "clueweb12-0410wb-56-32507", "target_hostname": "wingide.com", "target_uri": "http://wingide.com/psupport/python-manual/3.1/whatsnew/2.5.html", "page_rank": 1.2029513e-09, "spam_rank": 75, "title": "What\u2019s New in <em>Python</em> 2.5 \u2014 <em>Python</em> v3.1.4 documentation", "snippet": "The decision was made to not require parentheses in the <em>Python</em> language\u2019s grammar, but as a matter of style <em>I</em> think you <em>should</em> always use them.", "explanation": null, "document": "An explanation of coroutines from a Perl point of view, written by Dan Sugalski.\nPEP 343: The \u2018with\u2019 statement \u00b6\nThe \u2018 with \u2018 statement clarifies code that previously would use try...finally blocks to ensure that clean-up code is executed. In this section, I\u2019ll discuss the statement as it will commonly be used. In the next section, I\u2019ll examine the implementation details and show how to write objects for use with this statement.\nThe \u2018 with \u2018 statement is a new control-flow structure whose basic structure is:\nwith expression [as variable]: with-block\nThe expression is evaluated, and it should result in an object that supports the context management protocol (that is, has __enter__() and __exit__() methods.\nThe object\u2019s __enter__() is called before with-block is executed and therefore can run set-up code. It also may return a value that is bound to the name variable, if given. (Note carefully that variable is not assigned the result of expression.)\nAfter execution of the with-block is finished, the object\u2019s __exit__() method is called, even if the block raised an exception, and can therefore run clean-up code.\nTo enable the statement in Python 2.5, you need to add the following directive to your module:\nfrom __future__ import with_statement\nThe statement will always be enabled in Python 2.6.\nSome standard Python objects now support the context management protocol and can be used with the \u2018 with \u2018 statement. File objects are one example:\nwith open('/etc/passwd', 'r') as f: for line in f: print line ... more processing code ...\nAfter this statement has executed, the file object in f will have been automatically closed, even if the for loop raised an exception part- way through the block.\nNote\nIn this case, f is the same object created by open() , because file.__enter__() returns self.\nThe threading module\u2019s locks and condition variables also support the \u2018 with \u2018 statement:\nlock = threading.Lock() with lock: # Critical section of code ...\nThe lock is acquired before the block is executed and always released once the block is complete.\nThe new localcontext() function in the decimal module makes it easy to save and restore the current decimal context, which encapsulates the desired precision and rounding characteristics for computations:\nfrom decimal import Decimal, Context, localcontext # Displays with default precision of 28 digits v = Decimal('578') print v.sqrt() with localcontext(Context(prec=16)): # All code in this block uses a precision of 16 digits. # The original context is restored on exiting the block. print v.sqrt()\nWriting Context Managers \u00b6\nUnder the hood, the \u2018 with \u2018 statement is fairly complicated. Most people will only use \u2018 with \u2018 in company with existing objects and don\u2019t need to know these details, so you can skip the rest of this section if you like. Authors of new objects will need to understand the details of the underlying implementation and should keep reading.\nA high-level explanation of the context management protocol is:\nThe expression is evaluated and should result in an object called a \u201ccontext manager\u201d. The context manager must have __enter__() and __exit__() methods.\nThe context manager\u2019s __enter__() method is called. The value returned is assigned to VAR. If no 'as VAR' clause is present, the value is simply discarded.\nThe code in BLOCK is executed.\nIf BLOCK raises an exception, the __exit__(type, value, traceback)() is called with the exception details, the same values returned by sys.exc_info() . The method\u2019s return value controls whether the exception is re-raised: any false value re-raises the exception, and True will result in suppressing it. You\u2019ll only rarely want to suppress the exception, because if you do the author of the code containing the \u2018 with \u2018 statement will never realize anything went wrong.\nIf BLOCK didn\u2019t raise an exception, the __exit__() method is still called, but type, value, and traceback are all None.\nLet\u2019s think through an example. I won\u2019t present detailed code but will only sketch the methods necessary for a database that supports transactions.\n(For people unfamiliar with database terminology: a set of changes to the database are grouped into a transaction. Transactions can be either committed, meaning that all the changes are written into the database, or rolled back, meaning that the changes are all discarded and the database is unchanged. See any database textbook for more information.)\nLet\u2019s assume there\u2019s an object representing a database connection. Our goal will be to let the user write code like this:\ndb_connection = DatabaseConnection() with db_connection as cursor: cursor.execute('insert into ...') cursor.execute('delete from ...') # ... more operations ...\nThe transaction should be committed if the code in the block runs flawlessly or rolled back if there\u2019s an exception. Here\u2019s the basic interface for DatabaseConnection that I\u2019ll assume:\nclass DatabaseConnection: # Database interface def cursor (self): \"Returns a cursor object and starts a new transaction\" def commit (self): \"Commits current transaction\" def rollback (self): \"Rolls back current transaction\"\nThe __enter__() method is pretty easy, having only to start a new transaction. For this application the resulting cursor object would be a useful result, so the method will return it. The user can then add as cursor to their \u2018 with \u2018 statement to bind the cursor to a variable name.\nclass DatabaseConnection: ... def __enter__ (self): # Code to start a new transaction cursor = self.cursor() return cursor\nThe __exit__() method is the most complicated because it\u2019s where most of the work has to be done. The method has to check if an exception occurred. If there was no exception, the transaction is committed. The transaction is rolled back if there was an exception.\nIn the code below, execution will just fall off the end of the function, returning the default value of None. None is false, so the exception will be re-raised automatically. If you wished, you could be more explicit and add a return statement at the marked location.\nclass DatabaseConnection: ... def __exit__ (self, type, value, tb): if tb is None: # No exception, so commit self.commit() else: # Exception occurred, so rollback. self.rollback() # return False\nThe contextlib module \u00b6\nThe new contextlib module provides some functions and a decorator that are useful for writing objects for use with the \u2018 with \u2018 statement.\nThe decorator is called contextmanager(), and lets you write a single generator function instead of defining a new class. The generator should yield exactly one value. The code up to the yield will be executed as the __enter__() method, and the value yielded will be the method\u2019s return value that will get bound to the variable in the \u2018 with \u2018 statement\u2019s as clause, if any. The code after the yield will be executed in the __exit__() method. Any exception raised in the block will be raised by the yield statement.\nOur database example from the previous section could be written using this decorator as:\nfrom contextlib import contextmanager @contextmanager def db_transaction (connection): cursor = connection.cursor() try: yield cursor except: connection.rollback() raise else: connection.commit() db = DatabaseConnection() with db_transaction(db) as cursor: ...\nThe contextlib module also has a nested(mgr1, mgr2, ...)() function that combines a number of context managers so you don\u2019t need to write nested \u2018 with \u2018 statements. In this example, the single \u2018 with \u2018 statement both starts a database transaction and acquires a thread lock:\nlock = threading.Lock() with nested (db_transaction(db), lock) as (cursor, locked): ...\nFinally, the closing(object)() function returns object so that it can be bound to a variable, and calls object.close at the end of the block.\nimport urllib, sys from contextlib import closing with closing(urllib.urlopen('http://www.yahoo.com')) as f: for line in f: sys.stdout.write(line)\nSee also\nPEP 343 - The \u201cwith\u201d statement\nPEP written by Guido van Rossum and Nick Coghlan; implemented by Mike Bland, Guido van Rossum, and Neal Norwitz. The PEP shows the code generated for a \u2018 with \u2018 statement, which can be helpful in learning how the statement works.\nThe documentation for the contextlib module.\nPEP 352: Exceptions as New-Style Classes \u00b6\nException classes can now be new-style classes, not just classic classes, and the built-in Exception class and all the standard built-in exceptions ( NameError , ValueError , etc.) are now new-style classes.\nThe inheritance hierarchy for exceptions has been rearranged a bit. In 2.5, the inheritance relationships are:\nBaseException # New in Python 2.5 |- KeyboardInterrupt |- SystemExit |- Exception |- (all other current built-in exceptions)\nThis rearrangement was done because people often want to catch all exceptions that indicate program errors. KeyboardInterrupt and SystemExit aren\u2019t errors, though, and usually represent an explicit action such as the user hitting Control-C or code calling sys.exit() . A bare except: will catch all exceptions, so you commonly need to list KeyboardInterrupt and SystemExit in order to re-raise them. The usual pattern is:\ntry: ... except (KeyboardInterrupt, SystemExit): raise except: # Log error... # Continue running program...\nIn Python 2.5, you can now write except Exception to achieve the same result, catching all the exceptions that usually indicate errors but leaving KeyboardInterrupt and SystemExit alone. As in previous versions, a bare except: still catches all exceptions.\nThe goal for Python 3.0 is to require any class raised as an exception to derive from BaseException or some descendant of BaseException , and future releases in the Python 2.x series may begin to enforce this constraint. Therefore, I suggest you begin making all your exception classes derive from Exception now. It\u2019s been suggested that the bare except: form should be removed in Python 3.0, but Guido van Rossum hasn\u2019t decided whether to do this or not.\nRaising of strings as exceptions, as in the statement raise \"Error occurred\", is deprecated in Python 2.5 and will trigger a warning. The aim is to be able to remove the string-exception feature in a few releases.\nSee also\nPEP 352 - Required Superclass for Exceptions\nPEP written by Brett Cannon and Guido van Rossum; implemented by Brett Cannon.\nPEP 353: Using ssize_t as the index type \u00b6\nA wide-ranging change to Python\u2019s C API, using a new Py_ssize_t type definition instead of int, will permit the interpreter to handle more data on 64-bit platforms. This change doesn\u2019t affect Python\u2019s capacity on 32-bit platforms.\nVarious pieces of the Python interpreter used C\u2019s int type to store sizes or counts; for example, the number of items in a list or tuple were stored in an int. The C compilers for most 64-bit platforms still define int as a 32-bit type, so that meant that lists could only hold up to 2**31 - 1 = 2147483647 items. (There are actually a few different programming models that 64-bit C compilers can use \u2013 see http://www.unix.org/version2/whatsnew/lp64_wp.html for a discussion \u2013 but the most commonly available model leaves int as 32 bits.)\nA limit of 2147483647 items doesn\u2019t really matter on a 32-bit platform because you\u2019ll run out of memory before hitting the length limit. Each list item requires space for a pointer, which is 4 bytes, plus space for a PyObject representing the item. 2147483647*4 is already more bytes than a 32-bit address space can contain.\nIt\u2019s possible to address that much memory on a 64-bit platform, however. The pointers for a list that size would only require 16 GiB of space, so it\u2019s not unreasonable that Python programmers might construct lists that large. Therefore, the Python interpreter had to be changed to use some type other than int, and this will be a 64-bit type on 64-bit platforms. The change will cause incompatibilities on 64-bit machines, so it was deemed worth making the transition now, while the number of 64-bit users is still relatively small. (In 5 or 10 years, we may all be on 64-bit machines, and the transition would be more painful then.)\nThis change most strongly affects authors of C extension modules. Python strings and container types such as lists and tuples now use Py_ssize_t to store their size. Functions such as PyList_Size() now return Py_ssize_t. Code in extension modules may therefore need to have some variables changed to Py_ssize_t.\nThe PyArg_ParseTuple() and Py_BuildValue() functions have a new conversion code, n, for Py_ssize_t. PyArg_ParseTuple() \u2018s s# and t# still output int by default, but you can define the macro PY_SSIZE_T_CLEAN before including Python.h to make them return Py_ssize_t.\nPEP 353 has a section on conversion guidelines that extension authors should read to learn about supporting 64-bit platforms.\nSee also\nPEP 353 - Using ssize_t as the index type\nPEP written and implemented by Martin von L\u00f6wis.\nPEP 357: The \u2018__index__\u2019 method \u00b6\nThe NumPy developers had a problem that could only be solved by adding a new special method, __index__() . When using slice notation, as in [start:stop:step], the values of the start, stop, and step indexes must all be either integers or long integers. NumPy defines a variety of specialized integer types corresponding to unsigned and signed integers of 8, 16, 32, and 64 bits, but there was no way to signal that these types could be used as slice indexes.\nSlicing can\u2019t just use the existing __int__() method because that method is also used to implement coercion to integers. If slicing used __int__() , floating-point numbers would also become legal slice indexes and that\u2019s clearly an undesirable behaviour.\nInstead, a new special method called __index__() was added. It takes no arguments and returns an integer giving the slice index to use. For example:\nclass C: def __index__ (self): return self.value\nThe return value must be either a Python integer or long integer. The interpreter will check that the type returned is correct, and raises a TypeError if this requirement isn\u2019t met.\nA corresponding nb_index slot was added to the C-level PyNumberMethods structure to let C extensions implement this protocol. PyNumber_Index(obj)() can be used in extension code to call the __index__() function and retrieve its result.\nSee also\nPEP 357 - Allowing Any Object to be Used for Slicing\nPEP written and implemented by Travis Oliphant.\nOther Language Changes \u00b6\nHere are all of the changes that Python 2.5 makes to the core Python language.\nThe dict type has a new hook for letting subclasses provide a default value when a key isn\u2019t contained in the dictionary. When a key isn\u2019t found, the dictionary\u2019s __missing__(key)() method will be called. This hook is used to implement the new defaultdict class in the collections module. The following example defines a dictionary that returns zero for any missing key:\nclass zerodict (dict): def __missing__ (self, key): return 0 d = zerodict({1:1, 2:2}) print d[1], d[2] # Prints 1, 2 print d[3], d[4] # Prints 0, 0\nBoth 8-bit and Unicode strings have new partition(sep)() and rpartition(sep)() methods that simplify a common use case.\nThe find(S)() method is often used to get an index which is then used to slice the string and obtain the pieces that are before and after the separator. partition(sep)() condenses this pattern into a single method call that returns a 3-tuple containing the substring before the separator, the separator itself, and the substring after the separator. If the separator isn\u2019t found, the first element of the tuple is the entire string and the other two elements are empty. rpartition(sep)() also returns a 3-tuple but starts searching from the end of the string; the r stands for \u2018reverse\u2019.\nSome examples:\n>>> ('http://www.python.org').partition('://') ('http', '://', 'www.python.org') >>> ('file:/usr/share/doc/index.html').partition('://') ('file:/usr/share/doc/index.html', '', '') >>> (u'Subject: a quick question').partition(':') (u'Subject', u':', u' a quick question') >>> 'www.python.org'.rpartition('.') ('www.python', '.', 'org') >>> 'www.python.org'.rpartition(':') ('', '', 'www.python.org')\n(Implemented by Fredrik Lundh following a suggestion by Raymond Hettinger.)\nThe startswith() and endswith() methods of string types now accept tuples of strings to check for.\ndef is_image_file (filename): return filename.endswith(('.gif', '.jpg', '.tiff'))\n(Implemented by Georg Brandl following a suggestion by Tom Lynn.)\nThe min() and max() built-in functions gained a key keyword parameter analogous to the key argument for sort(). This parameter supplies a function that takes a single argument and is called for every value in the list; min() / max() will return the element with the smallest/largest return value from this function. For example, to find the longest string in a list, you can do:\nL = ['medium', 'longest', 'short'] # Prints 'longest' print max(L, key=len) # Prints 'short', because lexicographically 'short' has the largest value print max(L)\n(Contributed by Steven Bethard and Raymond Hettinger.)\nTwo new built-in functions, any() and all() , evaluate whether an iterator contains any true or false values. any() returns True if any value returned by the iterator is true; otherwise it will return False . all() returns True only if all of the values returned by the iterator evaluate as true. (Suggested by Guido van Rossum, and implemented by Raymond Hettinger.)\nThe result of a class\u2019s __hash__() method can now be either a long integer or a regular integer. If a long integer is returned, the hash of that value is taken. In earlier versions the hash value was required to be a regular integer, but in 2.5 the id() built-in was changed to always return non-negative numbers, and users often seem to use id(self) in __hash__() methods (though this is discouraged).\nASCII is now the default encoding for modules. It\u2019s now a syntax error if a module contains string literals with 8-bit characters but doesn\u2019t have an encoding declaration. In Python 2.4 this triggered a warning, not a syntax error. See PEP 263 for how to declare a module\u2019s encoding; for example, you might add a line like this near the top of the source file:\n# -*- coding: latin1 -*-\nA new warning, UnicodeWarning , is triggered when you attempt to compare a Unicode string and an 8-bit string that can\u2019t be converted to Unicode using the default ASCII encoding. The result of the comparison is false:\n>>> chr(128) == unichr(128) # Can't convert chr(128) to Unicode __main__:1: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal False >>> chr(127) == unichr(127) # chr(127) can be converted True\nPreviously this would raise a UnicodeDecodeError exception, but in 2.5 this could result in puzzling problems when accessing a dictionary. If you looked up unichr(128) and chr(128) was being used as a key, you\u2019d get a UnicodeDecodeError exception. Other changes in 2.5 resulted in this exception being raised instead of suppressed by the code in dictobject.c that implements dictionaries.\nRaising an exception for such a comparison is strictly correct, but the change might have broken code, so instead UnicodeWarning was introduced.\n(Implemented by Marc-Andr\u00e9 Lemburg.)\nOne error that Python programmers sometimes make is forgetting to include an __init__.py module in a package directory. Debugging this mistake can be confusing, and usually requires running Python with the -v switch to log all the paths searched. In Python 2.5, a new ImportWarning warning is triggered when an import would have picked up a directory as a package but no __init__.py was found. This warning is silently ignored by default; provide the -Wd option when running the Python executable to display the warning message. (Implemented by Thomas Wouters.)\nThe list of base classes in a class definition can now be empty. As an example, this is now legal:\nclass C(): pass\n(Implemented by Brett Cannon.)\nInteractive Interpreter Changes \u00b6\nIn the interactive interpreter, quit and exit have long been strings so that new users get a somewhat helpful message when they try to quit:\n>>> quit 'Use Ctrl-D (i.e. EOF) to exit.'\nIn Python 2.5, quit and exit are now objects that still produce string representations of themselves, but are also callable. Newbies who try quit() or exit() will now exit the interpreter as they expect. (Implemented by Georg Brandl.)\nThe Python executable now accepts the standard long options --help and --version ; on Windows, it also accepts the /? option for displaying a help message. (Implemented by Georg Brandl.)\nOptimizations \u00b6\nSeveral of the optimizations were developed at the NeedForSpeed sprint, an event held in Reykjavik, Iceland, from May 21\u201328 2006. The sprint focused on speed enhancements to the CPython implementation and was funded by EWT LLC with local support from CCP Games. Those optimizations added at this sprint are specially marked in the following list.\nWhen they were introduced in Python 2.4, the built-in set and frozenset types were built on top of Python\u2019s dictionary type. In 2.5 the internal data structure has been customized for implementing sets, and as a result sets will use a third less memory and are somewhat faster. (Implemented by Raymond Hettinger.)\nThe speed of some Unicode operations, such as finding substrings, string splitting, and character map encoding and decoding, has been improved. (Substring search and splitting improvements were added by Fredrik Lundh and Andrew Dalke at the NeedForSpeed sprint. Character maps were improved by Walter D\u00f6rwald and Martin von L\u00f6wis.)\nThe long(str, base)() function is now faster on long digit strings because fewer intermediate results are calculated. The peak is for strings of around 800\u20131000 digits where the function is 6 times faster. (Contributed by Alan McIntyre and committed at the NeedForSpeed sprint.)\nIt\u2019s now illegal to mix iterating over a file with for line in file and calling the file object\u2019s read()/readline()/readlines() methods. Iteration uses an internal buffer and the read*() methods don\u2019t use that buffer. Instead they would return the data following the buffer, causing the data to appear out of order. Mixing iteration and these methods will now trigger a ValueError from the read*() method. (Implemented by Thomas Wouters.)\nThe struct module now compiles structure format strings into an internal representation and caches this representation, yielding a 20% speedup. (Contributed by Bob Ippolito at the NeedForSpeed sprint.)\nThe re module got a 1 or 2% speedup by switching to Python\u2019s allocator functions instead of the system\u2019s malloc() and free(). (Contributed by Jack Diederich at the NeedForSpeed sprint.)\nThe code generator\u2019s peephole optimizer now performs simple constant folding in expressions. If you write something like a = 2+3, the code generator will do the arithmetic and produce code corresponding to a = 5. (Proposed and implemented by Raymond Hettinger.)\nFunction calls are now faster because code objects now keep the most recently finished frame (a \u201czombie frame\u201d) in an internal field of the code object, reusing it the next time the code object is invoked. (Original patch by Michael Hudson, modified by Armin Rigo and Richard Jones; committed at the NeedForSpeed sprint.) Frame objects are also slightly smaller, which may improve cache locality and reduce memory usage a bit. (Contributed by Neal Norwitz.)\nPython\u2019s built-in exceptions are now new-style classes, a change that speeds up instantiation considerably. Exception handling in Python 2.5 is therefore about 30% faster than in 2.4. (Contributed by Richard Jones, Georg Brandl and Sean Reifschneider at the NeedForSpeed sprint.)\nImporting now caches the paths tried, recording whether they exist or not so that the interpreter makes fewer open() and stat() calls on startup. (Contributed by Martin von L\u00f6wis and Georg Brandl.)\nNew, Improved, and Removed Modules \u00b6\nThe standard library received many enhancements and bug fixes in Python 2.5. Here\u2019s a partial list of the most notable changes, sorted alphabetically by module name. Consult the Misc/NEWS file in the source tree for a more complete list of changes, or look through the SVN logs for all the details.\nThe audioop module now supports the a-LAW encoding, and the code for u-LAW encoding has been improved. (Contributed by Lars Immisch.)\nThe codecs module gained support for incremental codecs. The codec.lookup() function now returns a CodecInfo instance instead of a tuple. CodecInfo instances behave like a 4-tuple to preserve backward compatibility but also have the attributes encode, decode, incrementalencoder, incrementaldecoder, streamwriter, and streamreader. Incremental codecs can receive input and produce output in multiple chunks; the output is the same as if the entire input was fed to the non-incremental codec. See the codecs module documentation for details. (Designed and implemented by Walter D\u00f6rwald.)\nThe collections module gained a new type, defaultdict, that subclasses the standard dict type. The new type mostly behaves like a dictionary but constructs a default value when a key isn\u2019t present, automatically adding it to the dictionary for the requested key value.\nThe first argument to defaultdict\u2018s constructor is a factory function that gets called whenever a key is requested but not found. This factory function receives no arguments, so you can use built-in type constructors such as list() or int() . For example, you can make an index of words based on their initial letter like this:\nwords = \"\"\"Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura che la diritta via era smarrita\"\"\".lower().split() index = defaultdict(list) for w in words: init_letter = w[0] index[init_letter].append(w)\nPrinting index results in the following output:\ndefaultdict(<type 'list'>, {'c': ['cammin', 'che'], 'e': ['era'], 'd': ['del', 'di', 'diritta'], 'm': ['mezzo', 'mi'], 'l': ['la'], 'o': ['oscura'], 'n': ['nel', 'nostra'], 'p': ['per'], 's': ['selva', 'smarrita'], 'r': ['ritrovai'], 'u': ['una'], 'v': ['vita', 'via']}\n(Contributed by Guido van Rossum.)\nThe deque double-ended queue type supplied by the collections module now has a remove(value)() method that removes the first occurrence of value in the queue, raising ValueError if the value isn\u2019t found. (Contributed by Raymond Hettinger.)\nNew module: The contextlib module contains helper functions for use with the new \u2018 with \u2018 statement. See section The contextlib module for more about this module.\nNew module: The cProfile module is a C implementation of the existing profile module that has much lower overhead. The module\u2019s interface is the same as profile : you run cProfile.run('main()') to profile a function, can save profile data to a file, etc. It\u2019s not yet known if the Hotshot profiler, which is also written in C but doesn\u2019t match the profile module\u2019s interface, will continue to be maintained in future versions of Python. (Contributed by Armin Rigo.)\nAlso, the pstats module for analyzing the data measured by the profiler now supports directing the output to any file object by supplying a stream argument to the Stats constructor. (Contributed by Skip Montanaro.)\nThe csv module, which parses files in comma-separated value format, received several enhancements and a number of bugfixes. You can now set the maximum size in bytes of a field by calling the csv.field_size_limit(new_limit)() function; omitting the new_limit argument will return the currently-set limit. The reader class now has a line_num attribute that counts the number of physical lines read from the source; records can span multiple physical lines, so line_num is not the same as the number of records read.\nThe CSV parser is now stricter about multi-line quoted fields. Previously, if a line ended within a quoted field without a terminating newline character, a newline would be inserted into the returned field. This behavior caused problems when reading files that contained carriage return characters within fields, so the code was changed to return the field without inserting newlines. As a consequence, if newlines embedded within fields are important, the input should be split into lines in a manner that preserves the newline characters.\n(Contributed by Skip Montanaro and Andrew McNamara.)\nThe datetime class in the datetime module now has a strptime(string, format)() method for parsing date strings, contributed by Josh Spoerri. It uses the same format characters as time.strptime() and time.strftime() :\nfrom datetime import datetime ts = datetime.strptime('10:13:15 2006-03-07', '%H:%M:%S %Y-%m-%d')\nThe SequenceMatcher.get_matching_blocks() method in the difflib module now guarantees to return a minimal list of blocks describing matching subsequences. Previously, the algorithm would occasionally break a block of matching elements into two list entries. (Enhancement by Tim Peters.)\nThe doctest module gained a SKIP option that keeps an example from being executed at all. This is intended for code snippets that are usage examples intended for the reader and aren\u2019t actually test cases.\nAn encoding parameter was added to the testfile() function and the DocFileSuite class to specify the file\u2019s encoding. This makes it easier to use non-ASCII characters in tests contained within a docstring. (Contributed by Bjorn Tillenius.)\nThe email package has been updated to version 4.0. (Contributed by Barry Warsaw.)\nThe fileinput module was made more flexible. Unicode filenames are now supported, and a mode parameter that defaults to \"r\" was added to the input() function to allow opening files in binary or universal-newline mode. Another new parameter, openhook, lets you use a function other than open() to open the input files. Once you\u2019re iterating over the set of files, the FileInput object\u2019s new fileno() returns the file descriptor for the currently opened file. (Contributed by Georg Brandl.)\nIn the gc module, the new get_count() function returns a 3-tuple containing the current collection counts for the three GC generations. This is accounting information for the garbage collector; when these counts reach a specified threshold, a garbage collection sweep will be made. The existing gc.collect() function now takes an optional generation argument of 0, 1, or 2 to specify which generation to collect. (Contributed by Barry Warsaw.)\nThe nsmallest() and nlargest() functions in the heapq module now support a key keyword parameter similar to the one provided by the min() / max() functions and the sort() methods. For example:\n>>> import heapq >>> L = [\"short\", 'medium', 'longest', 'longer still'] >>> heapq.nsmallest(2, L) # Return two lowest elements, lexicographically ['longer still', 'longest'] >>> heapq.nsmallest(2, L, key=len) # Return two shortest elements ['short', 'medium']\n(Contributed by Raymond Hettinger.)\nThe itertools.islice() function now accepts None for the start and step arguments. This makes it more compatible with the attributes of slice objects, so that you can now write the following:\ns = slice(5) # Create slice object itertools.islice(iterable, s.start, s.stop, s.step)\n(Contributed by Raymond Hettinger.)\nThe format() function in the locale module has been modified and two new functions were added, format_string() and currency().\nThe format() function\u2019s val parameter could previously be a string as long as no more than one %char specifier appeared; now the parameter must be exactly one %char specifier with no surrounding text. An optional monetary parameter was also added which, if True, will use the locale\u2019s rules for formatting currency in placing a separator between groups of three digits.\nTo format strings with multiple %char specifiers, use the new format_string() function that works like format() but also supports mixing %char specifiers with arbitrary text.\nA new currency() function was also added that formats a number according to the current locale\u2019s settings.\n(Contributed by Georg Brandl.)\nThe mailbox module underwent a massive rewrite to add the capability to modify mailboxes in addition to reading them. A new set of classes that include mbox, MH, and Maildir are used to read mailboxes, and have an add(message)() method to add messages, remove(key)() to remove messages, and lock()/unlock() to lock/unlock the mailbox. The following example converts a maildir-format mailbox into an mbox-format one:\nimport mailbox # 'factory=None' uses email.Message.Message as the class representing # individual messages. src = mailbox.Maildir('maildir', factory=None) dest = mailbox.mbox('/tmp/mbox') for msg in src: dest.add(msg)\n(Contributed by Gregory K. Johnson. Funding was provided by Google\u2019s 2005 Summer of Code.)\nNew module: the msilib module allows creating Microsoft Installer .msi files and CAB files. Some support for reading the .msi database is also included. (Contributed by Martin von L\u00f6wis.)\nThe nis module now supports accessing domains other than the system default domain by supplying a domain argument to the nis.match() and nis.maps() functions. (Contributed by Ben Bell.)\nThe operator module\u2019s itemgetter() and attrgetter() functions now support multiple fields. A call such as operator.attrgetter('a', 'b') will return a function that retrieves the a and b attributes. Combining this new feature with the sort() method\u2019s key parameter lets you easily sort lists using multiple fields. (Contributed by Raymond Hettinger.)\nThe optparse module was updated to version 1.5.1 of the Optik library. The OptionParser class gained an epilog attribute, a string that will be printed after the help message, and a destroy() method to break reference cycles created by the object. (Contributed by Greg Ward.)\nThe os module underwent several changes. The stat_float_times variable now defaults to true, meaning that os.stat() will now return time values as floats. (This doesn\u2019t necessarily mean that os.stat() will return times that are precise to fractions of a second; not all systems support such precision.)\nConstants named os.SEEK_SET , os.SEEK_CUR , and os.SEEK_END have been added; these are the parameters to the os.lseek() function. Two new constants for locking are os.O_SHLOCK and os.O_EXLOCK .\nTwo new functions, wait3() and wait4(), were added. They\u2019re similar the waitpid() function which waits for a child process to exit and returns a tuple of the process ID and its exit status, but wait3() and wait4() return additional information. wait3() doesn\u2019t take a process ID as input, so it waits for any child process to exit and returns a 3-tuple of process-id, exit-status, resource-usage as returned from the resource.getrusage() function. wait4(pid)() does take a process ID. (Contributed by Chad J. Schroeder.)\nOn FreeBSD, the os.stat() function now returns times with nanosecond resolution, and the returned object now has st_gen and st_birthtime. The st_flags member is also available, if the platform supports it. (Contributed by Antti Louko and Diego Petten\u00f2.)\nThe Python debugger provided by the pdb module can now store lists of commands to execute when a breakpoint is reached and execution stops. Once breakpoint #1 has been created, enter commands 1 and enter a series of commands to be executed, finishing the list with end. The command list can include commands that resume execution, such as continue or next. (Contributed by Gr\u00e9goire Dooms.)\nThe pickle and cPickle modules no longer accept a return value of None from the __reduce__() method; the method must return a tuple of arguments instead. The ability to return None was deprecated in Python 2.4, so this completes the removal of the feature.\nThe pkgutil module, containing various utility functions for finding packages, was enhanced to support PEP 302\u2019s import hooks and now also works for packages stored in ZIP-format archives. (Contributed by Phillip J. Eby.)\nThe pybench benchmark suite by Marc-Andr\u00e9 Lemburg is now included in the Tools/pybench directory. The pybench suite is an improvement on the commonly used pystone.py program because pybench provides a more detailed measurement of the interpreter\u2019s speed. It times particular operations such as function calls, tuple slicing, method lookups, and numeric operations, instead of performing many different operations and reducing the result to a single number as pystone.py does.\nThe pyexpat module now uses version 2.0 of the Expat parser. (Contributed by Trent Mick.)\nThe Queue class provided by the Queue module gained two new methods. join() blocks until all items in the queue have been retrieved and all processing work on the items have been completed. Worker threads call the other new method, task_done(), to signal that processing for an item has been completed. (Contributed by Raymond Hettinger.)\nThe old regex and regsub modules, which have been deprecated ever since Python 2.0, have finally been deleted. Other deleted modules: statcache, tzparse, whrandom.\nAlso deleted: the lib-old directory, which includes ancient modules such as dircmp and ni, was removed. lib-old wasn\u2019t on the default sys.path, so unless your programs explicitly added the directory to sys.path, this removal shouldn\u2019t affect your code.\nThe rlcompleter module is no longer dependent on importing the readline module and therefore now works on non-Unix platforms. (Patch from Robert Kiendl.)\nThe SimpleXMLRPCServer and DocXMLRPCServer classes now have a rpc_paths attribute that constrains XML-RPC operations to a limited set of URL paths; the default is to allow only '/' and '/RPC2'. Setting rpc_paths to None or an empty tuple disables this path checking.\nThe socket module now supports AF_NETLINK sockets on Linux, thanks to a patch from Philippe Biondi. Netlink sockets are a Linux-specific mechanism for communications between a user-space process and kernel code; an introductory article about them is at http://www.linuxjournal.com/article/7356 . In Python code, netlink addresses are represented as a tuple of 2 integers, (pid, group_mask).\nTwo new methods on socket objects, recv_into(buffer)() and recvfrom_into(buffer)(), store the received data in an object that supports the buffer protocol instead of returning the data as a string. This means you can put the data directly into an array or a memory-mapped file.\nSocket objects also gained getfamily(), gettype(), and getproto() accessor methods to retrieve the family, type, and protocol values for the socket.\nNew module: the spwd module provides functions for accessing the shadow password database on systems that support shadow passwords.\nThe struct is now faster because it compiles format strings into Struct objects with pack() and unpack() methods. This is similar to how the re module lets you create compiled regular expression objects. You can still use the module-level pack() and unpack() functions; they\u2019ll create Struct objects and cache them. Or you can use Struct instances directly:\ns = struct.Struct('ih3s') data = s.pack(1972, 187, 'abc') year, number, name = s.unpack(data)\nYou can also pack and unpack data to and from buffer objects directly using the pack_into(buffer, offset, v1, v2, ...)() and unpack_from(buffer, offset)() methods. This lets you store data directly into an array or a memory- mapped file.\n(Struct objects were implemented by Bob Ippolito at the NeedForSpeed sprint. Support for buffer objects was added by Martin Blais, also at the NeedForSpeed sprint.)\nThe Python developers switched from CVS to Subversion during the 2.5 development process. Information about the exact build version is available as the sys.subversion variable, a 3-tuple of (interpreter-name, branch-name, revision-range). For example, at the time of writing my copy of 2.5 was reporting ('CPython', 'trunk', '45313:45315').\nThis information is also available to C extensions via the Py_GetBuildInfo() function that returns a string of build information like this: \"trunk:45355:45356M, Apr 13 2006, 07:42:19\". (Contributed by Barry Warsaw.)\nAnother new function, sys._current_frames() , returns the current stack frames for all running threads as a dictionary mapping thread identifiers to the topmost stack frame currently active in that thread at the time the function is called. (Contributed by Tim Peters.)\nThe TarFile class in the tarfile module now has an extractall() method that extracts all members from the archive into the current working directory. It\u2019s also possible to set a different directory as the extraction target, and to unpack only a subset of the archive\u2019s members.\nThe compression used for a tarfile opened in stream mode can now be autodetected using the mode 'r|*'. (Contributed by Lars Gust\u00e4bel.)\nThe threading module now lets you set the stack size used when new threads are created. The stack_size([*size*])() function returns the currently configured stack size, and supplying the optional size parameter sets a new value. Not all platforms support changing the stack size, but Windows, POSIX threading, and OS/2 all do. (Contributed by Andrew MacIntyre.)\nThe unicodedata module has been updated to use version 4.1.0 of the Unicode character database. Version 3.2.0 is required by some specifications, so it\u2019s still available as unicodedata.ucd_3_2_0 .\nNew module: the uuid module generates universally unique identifiers (UUIDs) according to RFC 4122 . The RFC defines several different UUID versions that are generated from a starting string, from system properties, or purely randomly. This module contains a UUID class and functions named uuid1(), uuid3(), uuid4(), and uuid5() to generate different versions of UUID. (Version 2 UUIDs are not specified in RFC 4122 and are not supported by this module.)\n>>> import uuid >>> # make a UUID based on the host ID and current time >>> uuid.uuid1() UUID('a8098c1a-f86e-11da-bd1a-00112444be1e') >>> # make a UUID using an MD5 hash of a namespace UUID and a name >>> uuid.uuid3(uuid.NAMESPACE_DNS, 'python.org') UUID('6fa459ea-ee8a-3ca4-894e-db77e160355e') >>> # make a random UUID >>> uuid.uuid4() UUID('16fd2706-8baf-433b-82eb-8c7fada847da') >>> # make a UUID using a SHA-1 hash of a namespace UUID and a name >>> uuid.uuid5(uuid.NAMESPACE_DNS, 'python.org') UUID('886313e1-3b8a-5372-9b90-0c9aee199e5d')\n(Contributed by Ka-Ping Yee.)\nThe weakref module\u2019s WeakKeyDictionary and WeakValueDictionary types gained new methods for iterating over the weak references contained in the dictionary. iterkeyrefs() and keyrefs() methods were added to WeakKeyDictionary, and itervaluerefs() and valuerefs() were added to WeakValueDictionary. (Contributed by Fred L. Drake, Jr.)\nThe webbrowser module received a number of enhancements. It\u2019s now usable as a script with python -m webbrowser, taking a URL as the argument; there are a number of switches to control the behaviour (-n for a new browser window, -t for a new tab). New module-level functions, open_new() and open_new_tab(), were added to support this. The module\u2019s open() function supports an additional feature, an autoraise parameter that signals whether to raise the open window when possible. A number of additional browsers were added to the supported list such as Firefox, Opera, Konqueror, and elinks. (Contributed by Oleg Broytmann and Georg Brandl.)\nThe xmlrpclib module now supports returning datetime objects for the XML-RPC date type. Supply use_datetime=True to the loads() function or the Unmarshaller class to enable this feature. (Contributed by Skip Montanaro.)\nThe zipfile module now supports the ZIP64 version of the format, meaning that a .zip archive can now be larger than 4 GiB and can contain individual files larger than 4 GiB. (Contributed by Ronald Oussoren.)\nThe zlib module\u2019s Compress and Decompress objects now support a copy() method that makes a copy of the object\u2019s internal state and returns a new Compress or Decompress object. (Contributed by Chris AtLee.)\nThe ctypes package \u00b6\nThe ctypes package, written by Thomas Heller, has been added to the standard library. ctypes lets you call arbitrary functions in shared libraries or DLLs. Long-time users may remember the dl module, which provides functions for loading shared libraries and calling functions in them. The ctypes package is much fancier.\nTo load a shared library or DLL, you must create an instance of the CDLL class and provide the name or path of the shared library or DLL. Once that\u2019s done, you can call arbitrary functions by accessing them as attributes of the CDLL object.\nimport ctypes libc = ctypes.CDLL('libc.so.6') result = libc.printf(\"Line of output\\n\")\nType constructors for the various C types are provided: c_int(), c_float(), c_double(), c_char_p() (equivalent to char *), and so forth. Unlike Python\u2019s types, the C versions are all mutable; you can assign to their value attribute to change the wrapped value. Python integers and strings will be automatically converted to the corresponding C types, but for other types you must call the correct type constructor. (And I mean must; getting it wrong will often result in the interpreter crashing with a segmentation fault.)\nYou shouldn\u2019t use c_char_p() with a Python string when the C function will be modifying the memory area, because Python strings are supposed to be immutable; breaking this rule will cause puzzling bugs. When you need a modifiable memory area, use create_string_buffer():\ns = \"this is a string\" buf = ctypes.create_string_buffer(s) libc.strfry(buf)\nC functions are assumed to return integers, but you can set the restype attribute of the function object to change this:\n>>> libc.atof('2.71828') -1783957616 >>> libc.atof.restype = ctypes.c_double >>> libc.atof('2.71828') 2.71828\nctypes also provides a wrapper for Python\u2019s C API as the ctypes.pythonapi object. This object does not release the global interpreter lock before calling a function, because the lock must be held when calling into the interpreter\u2019s code. There\u2019s a py_object() type constructor that will create a PyObject * pointer. A simple usage:\nimport ctypes d = {} ctypes.pythonapi.PyObject_SetItem(ctypes.py_object(d), ctypes.py_object(\"abc\"), ctypes.py_object(1)) # d is now {'abc', 1}.\nDon\u2019t forget to use py_object(); if it\u2019s omitted you end up with a segmentation fault.\nctypes has been around for a while, but people still write and distribution hand-coded extension modules because you can\u2019t rely on ctypes being present. Perhaps developers will begin to write Python wrappers atop a library accessed through ctypes instead of extension modules, now that ctypes is included with core Python.\nSee also\nThe ctypes web page, with a tutorial, reference, and FAQ.\nThe documentation for the ctypes module.\nThe ElementTree package \u00b6\nA subset of Fredrik Lundh\u2019s ElementTree library for processing XML has been added to the standard library as xml.etree. The available modules are ElementTree, ElementPath, and ElementInclude from ElementTree 1.2.6. The cElementTree accelerator module is also included.\nThe rest of this section will provide a brief overview of using ElementTree. Full documentation for ElementTree is available at http://effbot.org/zone/element-index.htm .\nElementTree represents an XML document as a tree of element nodes. The text content of the document is stored as the text and tail attributes of (This is one of the major differences between ElementTree and the Document Object Model; in the DOM there are many different types of node, including TextNode.)\nThe most commonly used parsing function is parse(), that takes either a string (assumed to contain a filename) or a file-like object and returns an ElementTree instance:\nfrom xml.etree import ElementTree as ET tree = ET.parse('ex-1.xml') feed = urllib.urlopen( 'http://planet.python.org/rss10.xml') tree = ET.parse(feed)\nOnce you have an ElementTree instance, you can call its getroot() method to get the root Element node.\nThere\u2019s also an XML() function that takes a string literal and returns an Element node (not an ElementTree). This function provides a tidy way to incorporate XML fragments, approaching the convenience of an XML literal:\nsvg = ET.XML(\"\"\"<svg width=\"10px\" version=\"1.0\"> </svg>\"\"\") svg.set('height', '320px') svg.append(elem1)\nEach XML element supports some dictionary-like and some list-like access methods. Dictionary-like operations are used to access attribute values, and list-like operations are used to access child nodes.\nOperation\n"}, {"score": 909.8386, "uuid": "f1c592de-c526-5459-8e61-20ddf2a1a2f2", "index": "cw12", "trec_id": "clueweb12-0700tw-40-17745", "target_hostname": "pythonconquerstheuniverse.wordpress.com", "target_uri": "http://pythonconquerstheuniverse.wordpress.com/", "page_rank": 4.1670263e-09, "spam_rank": 80, "title": "<em>Python</em> Conquers The Universe", "snippet": "So <em>I</em> <em>should</em> probably say something about my choice of technical terminology. <em>For</em> the purposes of this <em>analysis</em>, <em>I</em> prefer not to use the traditional Unix vocabulary of options, <em>for</em> a number of reason. ", "explanation": null, "document": "Filed under: Miscellaneous \u2014 Steve Ferg @ 10:17 pm\nJust in case someone might find this useful \u2026\nI recently had something bad happen to me. I use Thunderbird (on Windows Vista) as my email client. I asked Thunderbird to compact my email files, and it wiped out a bunch of my email messages. (I think that one of my email files must have been corrupt, and when I compacted it, the compaction process wiped out messages that should not have been wiped out.)\nYou can recover deleted email messages \u2026 but not after the email file has been compacted. So the messages were not recoverable. Bummer.\nThe upside is that this nasty incident led me to learn some things.\nOne thing that I learned was that the disk backup utility that I was using at the time did NOT backup my email files. The email files were stored in a directory called AppData, and the AppData directory is a \u201chidden\u201d directory. So the backup utility didn\u2019t see the AppData directory, and didn\u2019t back it up. So I had no backup of the deleted messages.\nLearning that led me to investigate ways to backup my email files, and I found this: Five ways to keep your emails backed up\nFor backing up Thunderbird files, it recommends MozBackup as being fast, free and easy to use. So I tried MozBackup, and those claims seem to be true.\nNow I\u2019m evaluating different disk backup options.\nThe take-away here is that you need to pay special attention to backing up your email files. So if you\u2019re not backing up your email files, take a look at Five ways to keep your emails backed up (and read the comments, which are useful) or google something like \u201cemail backup\u201d .\n[Note that this applies only if you are using an email client such as Thunderbird, Outlook, Outlook Express, etc. If you don't use an email client, and do all of your email work through a Web interface to your Internet Service Provider, then this is not an issue.]\n"}, {"score": 906.0557, "uuid": "7027ea85-bd49-5e1a-b68b-42d4bcfa0884", "index": "cw12", "trec_id": "clueweb12-1603wb-33-26182", "target_hostname": "dwwp.decontextualize.com", "target_uri": "http://dwwp.decontextualize.com/", "page_rank": 1.304467e-09, "spam_rank": 86, "title": "Digital Writing with <em>Python</em>: ITP Summer 2009", "snippet": "<em>Python</em> <em>I</em>: Making decisions about strings. <em>Python</em> II: Simple models of text. <em>Python</em> V: Classes and objects. <em>Python</em> VI: Getting <em>data</em> from the web. Nate and Si present.", "explanation": null, "document": "Digital Writing with Python: ITP Summer 2009\nMondays and Wednesdays, 3:15pm to 6:10pm (Summer Session II)\n721 Broadway, New York, NY\n4th Floor, Room 447\nDescription\nFrom the official course description:\nThis course introduces the Python programming language as a tool for writing digital text. This course is specifically geared to serve as a general-purpose introduction to programming in Python, but will be of special interest to students interested in poetics, language, creative writing and text analysis. Weekly programming exercises work toward a midterm project and culminate in a final project. Python topics covered include: functions; object-oriented programming; functional programming (list comprehensions, recursion); getting data from the web; displaying data on the web; parsing data formats (e.g., markup languages); visualization and interactivity with Python. Poetics topics covered include: character encodings (and other technical issues); cut-up and re-mixed texts; the algorithmic nature of poetic form (proposing poetic forms, generating text that conforms to poetic forms); transcoding/transcription (from/to text); generative algorithms: n-gram analysis, context-free grammars; performing digital writing. Prerequisites: Introduction to Computational Media or equivalent programming experience.\nThat's both an ambitious and ambiguous description! Here's what it boils down to:\nThis is a creative writing course.\nInstead of writing texts ourselves,\nwe'll be writing programs\nWe'll learn how to use the Python programming language\nto make programs\n(Why Python? Because it's easy to learn,\nit's elegant,\nand it makes text processing easy.)\nThe goal of the course is to have fun messing with language and literature while exploring the aesthetic, technical and expressive possibilities of computer-generated (and -mangled) text. You'll become literate in a fantastic programming language as part of the bargain.\nWarning: this course is oriented toward performance. Text, after all, affords both visual and vocal readings\u2014so don't expect the output of your programs to stay on your computer screen. The final project will take the form of a public reading (you must read a text/poem/piece generated by a program that you wrote). You may be asked, when presenting your completed homework assignments, to read the output of your program out loud.\nGrading Policy\n10%\nReading\nReading material will be assigned most weeks, and will be made available either as links to documents on the web or as handouts. (There is no official textbook or reader.) Generally, the first twenty to thirty minutes of each class will be devoted to a discussion of the reading.\nThe assigned reading material may include the following:\nCharles Hartman and Travesty\nBlog\nYou are expected to maintain a blog for this class. You'll use this blog for posting documentation of your homework assignments and projects. If you use an existing blog, please make sure that entries relating to this class are specifically marked as such (by, e.g., tags, categories, etc.). As soon as you have this blog up and running, please send me a link.\nHomework Expectations\nThere are a total of three homework assignments, which in aggregate are worth nearly one third (30%) of your grade. In addition to complying with the parameters of the assignment as outlined in class, you are expected to post (to your blog) documentation of your assignment. This documentation should include:\na description of what the program does;\nwhat kind of input the program expects; and\nwhat the output of the program looks like.\nStudents may be called upon (and are encouraged to volunteer) to present their homework assignments in class.\nHomework assignments will not be accepted after their respective due dates.\nProject Expectations\nThere are two projects in this class. (Further details will be made available)\nThe midterm project must be an object-oriented program that generates poems that conform to a new poetic form of your own design;\nThe final project will take the form of a public reading: you must read aloud one (or more) of the texts generated by a Python program.\nYou will be asked to present your projects in-class. You must also document your projects on your blog, and send links to your documentation to the instructor.\nWriting Analysis and Presentation\nYou will choose a piece of writing or other work that somehow incorporates text, writing, or language, and then present this work to your classmates. Your presentation should answer the following questions: what's interesting about the work, and how does it do whatever it is that makes it interesting? Relate your subject to either the technical or conceptual content of the course.\nWe'll have one student presentation per class session, starting after the third or fourth session. A sign-up sheet will be made available in class.\nAttendance Policy and In-Class Behavior Expectations\nAttendance\nBecause of the condensed nature of summer courses, consistent attendance is vital. You are expected to attend all class sessions. Absences due to non-emergency situations will only be cleared if you let me know a week (or more) in advance, and even then only for compelling personal or professional reasons (e.g., attending an important conference, going to a wedding). If you're unable to attend class due to contagious or incapacitation illness, please let me know (by phone or e-mail) before class begins.\nEach unexcused absence will deduct 5% from your final grade. If you have five or more unexcused absences, you risk failing the course.\nLateness\nBe on time to class. If you're more than fifteen minutes late, or if you leave early (without my clearance), it will count as an unexcused absence.\nIn-class behavior\nLaptops must be closed while your fellow students are presenting work. You're otherwise welcome to use laptops in class, but only to follow along with the in-class tutorials and to take notes.\nsandbox.decontextualize.com policies\nAn Ubuntu Linux server is available at sandbox.decontextualize.com for the duration of the course. (Details on how to log in will be given in class; talk to the instructor for more details.)\nUnless you've obtained prior permission, please do not use this server for hosting (uploading or downloading!) large files. (The instructor is paying for disk space and bandwidth.)\nNote: After the final day of class, the server will be taken offline (forever!). Please make sure to have your files off of the server by then. Always keep local copies of your work!\nSchedule\n"}, {"score": 904.7103, "uuid": "e7081ad2-1d7e-5a38-a4c7-338a0ab43610", "index": "cw12", "trec_id": "clueweb12-1704wb-49-01947", "target_hostname": "www.dumpanalysis.org", "target_uri": "http://www.dumpanalysis.org/blog/index.php/category/malware-analysis/", "page_rank": 1.3270804e-09, "spam_rank": 83, "title": "Crash Dump <em>Analysis</em> \u00bb Malware <em>Analysis</em>", "snippet": "After browsing Amazon <em>for</em> some time <em>I</em> chose this book <em>for</em> study: About 5 <em>or</em> 6 years ago <em>I</em> was thinking about using a Prolog interpreter to write an expert system <em>for</em> crash dump <em>analysis</em>.", "explanation": null, "document": "Metadefect Template Library (Part 0)\nSunday, May 1st, 2011\nTo model software behavior at application and system levels, test generative debugging scenarious\u00a0and construct software defects using metaprogramming I started working on MdTL (Metadefect Template Library). Its consists of C++ templates for structural and behavioral patterns.\u00a0The simplest examples include\u00a0Threads<Spike> and Spike<Thread>. The template classes can be more complex, of course, utilizing the full power of C++, STL, and existing libraries like Boost. The unique and novel feature of this library is\u00a0the inclusion of dual classes of behaviour such as Leak<>, Deadlock<>, Overflow<>, Residue<>, Exception<>, Contention<>, etc. parameterized by various structural\u00a0memory classes like Process<>, Thread<>, Heap<>, Stack<>, Region<>, Buffer<>, etc.\u00a0MdTL also includes classes to model software tracing and this will be\u00a0used for simultaneous software trace and memory dump analysis case study\u00a0in the forthcoming presentation . I also plan to extend this metaprogramming approach in the future to model malware analysis patterns by introducing Metamalware Template Library (MmTL).\nForthcoming parts will introduce\u00a0template classes and examples.\n"}, {"score": 903.92395, "uuid": "ea865000-710d-546e-a891-21df102f3013", "index": "cw12", "trec_id": "clueweb12-1703wb-44-19315", "target_hostname": "www.altsci.com", "target_uri": "http://www.altsci.com/concepts/page.php?s=spam&p=2", "page_rank": 1.3772689e-09, "spam_rank": 67, "title": "AltSci Concepts Computer Journal - Japanese Spam <em>Analysis</em> (<em>or</em> Artificially", "snippet": "<em>I</em> am very pleased with the results of this research. Using a very unuseful set of <em>data</em>, <em>I</em> was able to provide statistical <em>analysis</em> and retrieve interesting and quite useful <em>data</em> from it.", "explanation": null, "document": "\u304a: 1284\n\u308a: 1217\n\u4f1a: 1080 The first actual kanji is more common than quite a few syllables most notably \u3042 (a). This kanji means meeting and/or understanding. Sadly, it is not actual the most popular kanji in the Japanese language, so this means our data set is definitely skewed toward words used in e-mail spam. It is also possible that it could be skewed toward Japanese advertising e-mail or friendly e-mail, but more likely, this kanji is simply a popular word. A use I saw was \u51fa\u4f1a (shukkai translates to \"encounter\"), you can guess the context.\n\u308c: 961\n\u6027: 944 The second most popular kanji is rather telling, gender (sei) is often used for gender and sexual topics. The word for sexual intercourse (\u6027\u4ea4) accounts for probably nearly half of these mentions. Note that not far down the list, the second kanji in that word (\u4ea4) is mentioned 509 times, over half as many times as sei. Non-sexual uses of this kanji are possible and common, since it's a very common idea, for example \u6027\u884c means \"character and conduct\", \u6027\u8cea means \"nature\", \"property\", or \"disposition\", \u9670\u6027 means negative, and many other examples exist. Foreigners learning useful words for paperwork on a trip to Japan should memorize this kanji as well as the various words made with this kanji. Scientists, especially physicists and engineers need to know this kanji because it is used quite often in conjunction to describe properties such as: \u611f\u53d7\u6027 (sensitivity), \u63ee\u767a\u6027 (volatile), \u507d\u6027 (pseudo), \u73fe\u5b9f\u6027 (feasible), \u5408\u6cd5\u6027 (lawfulness), \u6c34\u6eb6\u6027 (water-soluble), \u6f5c\u4f0f\u6027 (latency), and so forth. This word is very versatile and cannot be added to bayesian spam filters without connecting it with the specific trailers that cause its meaning to be sexual. A common use of this in spam is \"\u5973\u6027\u304c\u7537\u6027\u3078\u8b1d\u793c\u3092\u652f\u6255\u3046\u30fb\u30fb\u30fb\" (onnasei ga otokosei he sharei wo shiharai translates to \"A woman pays reward to a man\").\n\uff01: 899 The exclamation is used quite often in spam. No other explanation neccesary.\n\u3042: 892\n\u5973: 860 The third most popular kanji (onna) gives away the source to any Japanese student, woman. No source of information anywhere in the entire body of works in human history has ever discussed women with this frequency. Only an author of post-modern poetry who pastes the character a hundred times in a row comes close to using this kanji at this frequency. It accounts for 860/606463 = 0.14% of all e-mail content. Comparatively, whitespace (including the ideographic space and tab) accounts for 63351/606463 = 10%. That means for every 74 whitespace, there is one mention of a woman. I think this proves more about spam than it does the human race though.\n\u65b9: 631 The fourth most popular kanji is a bit of a surprise, (kata) means \"person\". It has many uses other than person of course, such as (hou) \"side\". Combinations often use the hou form or one of the many other forms. A very popular word, \u5f7c\u65b9 (achira translates to \"there\") uses another form of the kanji.\n\u51fa: 569 The fifth most popular kanji is de meaning \"outflow/coming (going) out\". Although the literal is similar to a popular English spam word, the most popular use of \u51fa is as part of the very common verb \u51fa\u6765\u308b (\u3067\u304d\u308b dekiru means \"to be able\"). A use in a spam subject is: \u3054\u767b\u9332\u6e96\u5099\u304c\u51fa\u6765\u307e\u3057\u305f (\u3054\u3068\u3046\u308d\u304f\u3058\u3085\u3093\u3073\u304c\u3067\u304d\u307e\u3057\u305f gotourokujyunbigadekimashita means \"Entry ready able\").\n\u4ea4: 509 The sixth most popular kanji is maji meaning \"trade or mix\". A use in spam is \u5168\u56fd47\u90fd\u9053\u5e9c\u770c\u30fb\u4e2d\u9ad8\u5e74\u8089\u4f53\u4ea4\u6d41\u306e\u4f1a which translates to \"Meeting of nationwide 47 metropolis and districts middle to elderly-aged physical interchange\"\n\u7121: 483 The seventh most popular kanji nai meaning \"no/nothing/without\" is quite useful because it's quite common in Japanese signs and advertising.\n\u4eba: 435 The eighth most popular kanji nin means \"person\". It is used quite often in Japanese signs and communication.\n\u756a: 416 The ninth most popular kanji ban is used as a counter for position. For example \u4e00\u756a (ichiban meaning \"best/first\") is used quite often in anime and film. A use in spam is \u756a\u7d44\u30b9\u30bf\u30c3\u30d5\u304c\u660e\u304b\u3059\uff01 (\u3070\u3093\u3050\u307f\u30b9\u30bf\u30c3\u30d5\u304c\u3042\u304b\u3059\uff01 bangumi staff ga akasu! which means \"TV program staff is revealed!\") In this case ban is used in the noun bangumi which means TV program.\nConclusion\nI am very pleased with the results of this research. Using a very unuseful set of data, I was able to provide statistical analysis and retrieve interesting and quite useful data from it. The data set is very large and continues to grow daily, so analysis can actually continue as a permanent project.\nI am very interested in continuing this research for projects such as my kanji handwriting analysis project and for amateur manga translation projects. Projects such as Kanjilish and the Unicode Fuzzer are also very interesting projects that would definitely benefit from the data in this project.\nBibliography\n[5] Japanese For Busy People I, Kana Version, page 66.\nIf you are interested in analyzing Japanese Spam or other languages using my research methods, feel free to contact me .\n"}], [{"score": 903.30206, "uuid": "9239ef42-8b70-5822-8fe9-e55c448bfc2e", "index": "cw12", "trec_id": "clueweb12-1407wb-87-03353", "target_hostname": "www.buccaneerscholar.com", "target_uri": "http://www.buccaneerscholar.com/blog/archives/40", "page_rank": 1.1898285e-09, "spam_rank": 66, "title": "How <em>I</em> <em>Learn</em> Stuff \u00bb Try a Puzzle", "snippet": "(Tried this <em>for</em> Set 2) Conjecture: There <em>should</em> be sufficient info as presented to solve the puzzle. Look <em>for</em> patterns. Hypotheses must apply to all known evidence.", "explanation": null, "document": "Filed under: Uncategorized \u2014 james @ 1:01 am\nA lot of my learning is motivated by puzzles. I particularly like math puzzles. Here\u2019s one:\n1 2 3 4 5 = 1\n5 4 3 2 1 = 2\n1 1 1 1 1 = 5\n2 2 2 2 2 = 1\n3 3 3 3 3 = 6\n4 4 4 4 4 = 2\n5 5 5 5 5 = 7\n6 6 6 6 6 = 3\n1 1 2 2 2 = 6\n3 3 4 4 4 = 7\n1 1 1 1 2 = 1\n1 1 1 1 3 = 6\n1 1 1 1 4 = 2\n1 1 1 1 5 = 7\n2 2 2 2 6 = 3\n2 2 2 2 5 = 7\n2 2 2 2 4 = 2\n2 2 2 2 3 = 6\n2 2 2 2 2 = 1\n6 1 1 1 1 = 2\n5 1 1 1 1 = 8\n4 1 1 1 1 = 5\n3 1 1 1 1 = 2\n2 1 1 1 1 = 8\n3 1 4 1 5 = ?\nThis puzzle came to me from Trey Klein. I found it difficult to solve. Took me a couple of hours.\nMost buccaneers, I will hazard to say, enjoy the challenge of a good puzzle. If you, like me, can\u2019t resist puzzles, then of course you\u2019ll have to try this one. But, I have a request. Whether or not you solve it, I\u2019d like you to write a comment telling us how you tried to solve it. What steps did you take? What ideas did you have? What techniques or tools did you apply?\nTo puzzle is to learn.\nPuzzles are fabulously educational even if you fail to solve them. So, don\u2019t get too hung up on the solution itself. I could tell you the solution to the puzzle above, right now, and you would gain something. Sure. But if you try very hard to find the answer, fail, and THEN ask me for the answer, you will gain a lot more. The answer will have much more power and meaning for you, because of how you suffered for it. You will learn something critical about your strategy of problem-solving.\nI talk about this in my book. I call it the \u201cSail Power Principle.\u201d If you don\u2019t struggle with a problem at all, that\u2019s like sails with no wind in them. If you struggle too much, then you\u2019ll get discouraged, and that\u2019s like sails with too much wind\u2013 blowing them to shreds. You want to find that sweet spot. Put some effort into it, and when you begin to despair, say to yourself \u201cI\u2019m about to learn something important!\u201d then you can LOOK AT THE ANSWER .\nOf course, you may solve the puzzle, yourself. That\u2019s cool, too.\n"}, {"score": 899.57294, "uuid": "f02a96e5-01b7-5b62-9d98-dba34a3e55ab", "index": "cw12", "trec_id": "clueweb12-1512wb-96-10332", "target_hostname": "svitsrv25.epfl.ch", "target_uri": "http://svitsrv25.epfl.ch/R-doc/doc/manual/R-FAQ.html", "page_rank": 1.3393752e-09, "spam_rank": 94, "title": "<em>R</em> FAQ", "snippet": "Finally, Rwui is a web application to to create user-friendly web interfaces <em>for</em> <em>R</em> scripts. All code <em>for</em> the web interface is created automatically. There is no need <em>for</em> the user to do any extra scripting <em>or</em> <em>learn</em> any new scripting techniques.", "explanation": null, "document": "Next: Introduction , Previous: (dir) , Up: (dir)\nR FAQ\nFrequently Asked Questions on R\nVersion 2.7.2008-09-17\n1.1 Legalese\nThis document is copyright \u00a9 1998\u20132008 by Kurt Hornik.\nThis document is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2, or (at your option) any later version.\nThis document is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\nA copy of the GNU General Public License is available via WWW at\nFrom there, you can obtain versions converted to plain ASCII text , DVI , GNU info , HTML , PDF , PostScript as well as the Texinfo source used for creating all these formats using the GNU Texinfo system .\nYou can also obtain the R FAQ from the\ndoc/FAQ\nNext: Notation , Previous: Obtaining this document , Up: Introduction\n1.3 Citing this document\nIn publications, please refer to this FAQ as Hornik (2008), \u201cThe R FAQ \u201d, and give the above, official URL and the ISBN 3-900051-08-9:\n@Misc{,\n       author        = {Kurt Hornik},\n       title         = {The {R} {FAQ}},\n       year          = {2008},\n       note          = {{ISBN} 3-900051-08-9},\n       url           = {http://CRAN.R-project.org/doc/FAQ/R-FAQ.html}\n     }\nEverything should be pretty standard. `\nR>\n' is used for the R prompt, and a `\n$\n1.5 Feedback\nFeedback via email to Kurt.Hornik@R-project.org is of course most welcome.\nIn particular, note that I do not have access to Windows or Macintosh systems. Features specific to the Windows and Mac OS X ports of R are described in the \u201cR for Windows FAQ \u201d and the \u201cR for Mac OS X FAQ . If you have information on Macintosh or Windows systems that you think should be added to this document, please let me know.\nNext: What machines does R run on? , Previous: R Basics , Up: R Basics\n2.1 What is R?\nR is a system for statistical computation and graphics. It consists of a language plus a run-time environment with graphics, a debugger, access to certain system functions, and the ability to run programs stored in script files.\nThe design of R has been heavily influenced by two existing languages: Becker, Chambers & Wilks' S (see What is S? ) and Sussman's Scheme . Whereas the resulting language is very similar in appearance to S, the underlying implementation and semantics are derived from Scheme. See What are the differences between R and S? , for further details.\nThe core of R is an interpreted computer language which allows branching and looping as well as modular programming using functions. Most of the user-visible functions in R are written in R. It is possible for the user to interface to procedures written in the C, C++, or FORTRAN languages for efficiency. The R distribution contains functionality for a large number of statistical procedures. Among these are: linear and generalized linear models, nonlinear regression models, time series analysis, classical parametric and nonparametric tests, clustering and smoothing. There is also a large set of functions which provide a flexible graphical environment for creating various kinds of data presentations. Additional modules (\u201cadd-on packages\u201d) are available for a variety of specific purposes (see R Add-On Packages ).\nR was initially written by Ross Ihaka and Robert Gentleman at the Department of Statistics of the University of Auckland in Auckland, New Zealand. In addition, a large group of individuals has contributed to R by sending code and bug reports.\nSince mid-1997 there has been a core group (the \u201cR Core Team\u201d) who can modify the R source code archive. The group currently consists of Doug Bates, John Chambers, Peter Dalgaard, Robert Gentleman, Kurt Hornik, Stefano Iacus, Ross Ihaka, Friedrich Leisch, Thomas Lumley, Martin Maechler, Duncan Murdoch, Paul Murrell, Martyn Plummer, Brian Ripley, Duncan Temple Lang, Luke Tierney, and Simon Urbanek.\nR has a home page at http://www.R-project.org/ . It is free software distributed under a GNU -style copyleft , and an official part of the GNU project (\u201c GNU S\u201d).\nNext: What is the current version of R? , Previous: What is R? , Up: R Basics\n2.2 What machines does R run on?\nR is being developed for the Unix, Windows and Mac families of operating systems. Support for Mac OS Classic ended with R 1.7.1.\nThe current version of R will configure and build under a number of common Unix platforms including cpu-linux-gnu for the i386, alpha, arm, hppa, ia64, m68k, mips/mipsel, powerpc, s390, sparc (e.g., http://buildd.debian.org/build.php?&pkg=r-base ), and x86_64 CPUs, powerpc-apple-darwin, mips-sgi-irix, rs6000-ibm-aix, and sparc-sun-solaris.\nIf you know about other platforms, please drop us a note.\nNext: How can R be obtained? , Previous: What machines does R run on? , Up: R Basics\n2.3 What is the current version of R?\nThe current released version is 2.7.2. Based on this `major.minor.patchlevel' numbering scheme, there are two development versions of R, a patched version of the current release (`r-patched') and one working towards the next minor or eventually major (`r-devel') releases of R, respectively. Version r-patched is for bug fixes mostly. New features are typically introduced in r-devel.\nNext: How can R be installed? , Previous: What is the current version of R? , Up: R Basics\n2.4 How can R be obtained?\nSources, binaries and documentation for R can be obtained via CRAN , the \u201cComprehensive R Archive Network\u201d (see What is CRAN? ).\nSources are also available via https://svn.R-project.org/R/ , the R Subversion repository, but currently not via anonymous rsync (nor CVS).\nTarballs with daily snapshots of the r-devel and r-patched development versions of R can be found at ftp://ftp.stat.math.ethz.ch/Software/R .\n2.5.1 How can R be installed (Unix)\nIf R is already installed, it can be started by typing\nR\nat the shell prompt (of course, provided that the executable is in your path).\nIf binaries are available for your platform (see Are there Unix binaries for R? ), you can use these, following the instructions that come with them.\nOtherwise, you can compile and install R yourself, which can be done very easily under a number of common Unix platforms (see What machines does R run on? ). The file\nINSTALL\nthat comes with the R distribution contains a brief introduction, and the \u201cR Installation and Administration\u201d guide (see What documentation exists for R? ) has full details.\nNote that you need a FORTRAN compiler or perhaps\nf2c\nin addition to a C compiler to build R. Also, you need Perl version 5 to build the R object documentations. (If this is not available on your system, you can obtain a PDF version of the object reference manual via CRAN .)\nIn the simplest case, untar the R source code, change to the directory thus created, and issue the following commands (at the shell prompt):\n$ ./configure\n     $ make\nIf these commands execute successfully, the R binary and a shell script front-end called\nR\nare created and copied to the\nbin\ndirectory. You can copy the script to a place where users can invoke it, for example to\n/usr/local/bin\n. In addition, plain text help pages as well as HTML and LaTeX versions of the documentation are built.\nUse\nto create DVI versions of the R manuals, such as\nrefman.dvi\n(an R object reference index) and\nR-exts.dvi\n, the \u201cR Extension Writers Guide\u201d, in the\ndoc/manual\nsubdirectory. These files can be previewed and printed using standard programs such as\nxdvi\n. You can also use\nmake pdf\nto build PDF (Portable Document Format) version of the manuals, and view these using e.g. Acrobat. Manuals written in the GNU Texinfo system can also be converted to info files suitable for reading online with Emacs or stand-alone GNU Info; use\nmake info\nto create these versions (note that this requires Makeinfo version 4.5).\nFinally, use\nto find out whether your R system works correctly.\nYou can also perform a \u201csystem-wide\u201d installation using\nmake install\n. By default, this will install to the following directories:\n${prefix}/bin\nall the rest (libraries, on-line help system,\n...\n). This is the \u201cR Home Directory\u201d (\nR_HOME\nIn the above, prefix is determined during configuration (typically\n/usr/local\n) and can be set by running\nconfigure\n(E.g., the R executable will then be installed into\n/where/you/want/R/to/go/bin\nTo install DVI, info and PDF versions of the manuals, use\nmake install-dvi\nmake install-pdf\n, respectively.\n2.5.2 How can R be installed (Windows)\nThe\nbin/windows\ndirectory of a CRAN site contains binaries for a base distribution and a large number of add-on packages from CRAN to run on Windows 2000 and later (including 64-bit versions of Windows) on ix86 and x86_64 chips. The Windows version of R was created by Robert Gentleman and Guido Masarotto, and is now being developed and maintained by Duncan Murdoch and Brian D. Ripley .\nFor most installations the Windows installer program will be the easiest tool to use.\n2.5.3 How can R be installed (Macintosh)\nThe\nbin/macosx\ndirectory of a CRAN site contains a standard Apple installer package inside a disk image named\nR.dmg\n. Once downloaded and executed, the installer will install the current non-developer release of R. RAqua is a native Mac OS X Darwin version of R with a R.app Mac OS X GUI . Inside\nbin/macosx/powerpc/contrib/x.y\nthere are prebuilt binary packages (for powerpc version of Mac OS X) to be used with RAqua corresponding to the \u201cx.y\u201d release of R. The installation of these packages is available through the \u201cPackage\u201d menu of the R.app GUI . This port of R for Mac OS X is maintained by Stefano Iacus . The \u201cR for Mac OS X FAQ has more details.\nThe\ndirectory of a CRAN site contains bin-hexed (\nhqx\n) and stuffit (\nsit\n) archives for a base distribution and a large number of add-on packages of R 1.7.1 to run under Mac OS 8.6 to Mac OS 9.2.2. This port of R for Macintosh is no longer supported.\n2.6 Are there Unix binaries for R?\nThe\ndirectory of a CRAN site contains the following packages.\nCPU\ndapper/feisty/gutsy/hardy\nMichael Rutter\nDebian packages, maintained by Dirk Eddelbuettel and Doug Bates, have long been part of the Debian distribution, and can be accessed through APT, the Debian package maintenance tool. Use e.g. apt-get install r-base r-recommended to install the R environment and recommended packages. If you also want to build R packages from source, also run apt-get install r-base-dev to obtain the additional tools required for this. So-called \u201cbackports\u201d of the current R packages for the\nstable\ndistribution of Debian are provided by Johannes Ranke, and available from CRAN. Simply add the line\ndeb http://CRAN.R-project.org/bin/linux/debian stable/\n(feel free to use a CRAN mirror instead of the master) to the file\n/etc/apt/sources.list\n, and install as usual. More details on installing and administering R on Debian Linux can be found at http://CRAN.R-project.org/bin/linux/debian/README . These backports should also be suitable for other Debian derivatives. Native backports for Ubuntu are provided by Vincent Goulet and Michael Rutter.\nOn SUSE, you can set up an installation source for R within Yast by setting (e.g.)\nProtocol: HTTP\n     Server name: software.openSUSE.org\n     Directory: /download/home:/dsteuer/openSUSE_10.2/\nWith this setting, online updates will check for new versions of R.\nThe\nbin/solaris\ndirectory of a CRAN site contains binary packages for Solaris on the SPARC and x64 platforms, provided by Mithun Sridharan.\nNo other binary distributions are currently publically available via CRAN .\nA \u201clive\u201d Linux distribution with a particular focus on R is\nQuantian\n, which provides a directly bootable and self-configuring \u201cLive DVD\u201d containing numerous applications of interests to scientists and researchers, including several hundred CRAN and Bioconductor packages, the \u201cESS\u201d extensions for Emacs, the \u201cJGR\u201d Java GUI for R, the Ggobi visualization tool as well as several other R interfaces. The\nQuantian\nNext: Citing R , Previous: Are there Unix binaries for R? , Up: R Basics\n2.7 What documentation exists for R?\nOnline documentation for most of the functions and variables in R exists, and can be printed on-screen by typing\nhelp(\n(or\n?\nname) at the R prompt, where name is the name of the topic help is sought for. (In the case of unary and binary operators and control-flow special forms, the name may need to be be quoted.)\nThis documentation can also be made available as one reference manual for on-line reading in HTML and PDF formats, and as hardcopy via LaTeX, see How can R be installed? . An up-to-date HTML version is always available for web browsing at http://stat.ethz.ch/R-manual/ .\nPrinted copies of the R reference manual for some version(s) are available from Network Theory Ltd, at http://www.network-theory.co.uk/R/base/ . For each set of manuals sold, the publisher donates USD 10 to the R Foundation (see What is the R Foundation? ).\nThe R distribution also comes with the following manuals.\n\u201cAn Introduction to R\u201d (\nR-intro\n) includes information on data types, programming elements, statistical modeling and graphics. This document is based on the \u201cNotes on S-Plus\u201d by Bill Venables and David Smith.\n\u201cWriting R Extensions\u201d (\nR-exts\n) currently describes the process of creating R add-on packages, writing R documentation, R's system and foreign language interfaces, and the R API .\n\u201cR Data Import/Export\u201d (\n) is a guide to importing and exporting data to and from R.\n\u201cThe R Language Definition\u201d (\nR-lang\n), a first version of the \u201cKernighan & Ritchie of R\u201d, explains evaluation, parsing, object oriented programming, computing on the language, and so forth.\n\u201cR Installation and Administration\u201d (\n) is a guide to R's internal structures. (Added in R 2.4.0.)\nBooks on R include\nP. Dalgaard (2002), \u201cIntroductory Statistics with R\u201d, Springer: New York, ISBN 0-387-95475-9, http://www.biostat.ku.dk/~pd/ISwR.html .\nJ. Fox (2002), \u201cAn R and S-Plus Companion to Applied Regression\u201d, Sage Publications, ISBN 0-761-92280-6 (softcover) or 0-761-92279-2 (hardcover), http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/ .\nJ. Maindonald and J. Braun (2003), \u201cData Analysis and Graphics Using R: An Example-Based Approach\u201d, Cambridge University Press, ISBN 0-521-81336-0, http://wwwmaths.anu.edu.au/~johnm/ .\nS. M. Iacus and G. Masarotto (2002), \u201cLaboratorio di statistica con R\u201d, McGraw-Hill, ISBN 88-386-6084-0 (in Italian),\nP. Murrell (2005), \u201cR Graphics\u201d, Chapman & Hall/CRC, ISBN: 1-584-88486-X, http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html .\nThe book\nW. N. Venables and B. D. Ripley (2002), \u201cModern Applied Statistics with S\u201d (4th edition). Springer, ISBN 0-387-95457-0\nhas a home page at http://www.stats.ox.ac.uk/pub/MASS4/ providing additional material. Its companion is\nW. N. Venables and B. D. Ripley (2000), \u201cS Programming\u201d. Springer, ISBN 0-387-98966-8\nand provides an in-depth guide to writing software in the S language which forms the basis of both the commercial S-Plus and the Open Source R data analysis software systems. See http://www.stats.ox.ac.uk/pub/MASS3/Sprog/ for more information.\nIn addition to material written specifically or explicitly for R, documentation for S/S-Plus (see R and S ) can be used in combination with this FAQ (see What are the differences between R and S? ). Introductory books include\nP. Spector (1994), \u201cAn introduction to S and S-Plus\u201d, Duxbury Press.\nA. Krause and M. Olsen (2005), \u201cThe Basics of S-Plus\u201d (4th edition). Springer, ISBN 0-387-26109-5.\nThe book\nJ. C. Pinheiro and D. M. Bates (2000), \u201cMixed-Effects Models in S and S-Plus\u201d, Springer, ISBN 0-387-98957-0\nprovides a comprehensive guide to the use of the nlme package for linear and nonlinear mixed-effects models.\nAs an example of how R can be used in teaching an advanced introductory statistics course, see\nD. Nolan and T. Speed (2000), \u201cStat Labs: Mathematical Statistics Through Applications\u201d, Springer Texts in Statistics, ISBN 0-387-98974-9\nThis integrates theory of statistics with the practice of statistics through a collection of case studies (\u201clabs\u201d), and uses R to analyze the data. More information can be found at http://www.stat.Berkeley.EDU/users/statlabs/ .\nLast, but not least, Ross' and Robert's experience in designing and implementing R is described in Ihaka & Gentleman (1996), \u201cR: A Language for Data Analysis and Graphics\u201d, Journal of Computational and Graphical Statistics , 5, 299\u2013314.\nAn annotated bibliography (BibTeX format) of R-related publications which includes most of the above references can be found at\n2.8 Citing R\nTo cite R in publications, use\n@Manual{,\n       title        = {R: A Language and Environment for Statistical\n                       Computing},\n       author       = {{R Development Core Team}},\n       organization = {R Foundation for Statistical Computing},\n       address      = {Vienna, Austria},\n       year         = 2008,\n       note         = {{ISBN} 3-900051-07-0},\n       url          = {http://www.R-project.org}\n     }\nCitation strings (or BibTeX entries) for R and R packages can also be obtained by citation().\n2.9 What mailing lists exist for R?\nThanks to Martin Maechler , there are four mailing lists devoted to R.\nR-announce\nA moderated list for major announcements about the development of R and the availability of new code.\nR-packages\nA moderated list for announcements on the availability of new or enhanced contributed packages.\nR-help\nThe `main' R mailing list, for discussion about problems and solutions using R, announcements (not covered by `R-announce' and `R-packages') about the development of R and the availability of new code.\nR-devel\nThis list is for questions and discussion about code development in R.\nPlease read the posting guide before sending anything to any mailing list.\nNote in particular that R-help is intended to be comprehensible to people who want to use R to solve problems but who are not necessarily interested in or knowledgeable about programming. Questions likely to prompt discussion unintelligible to non-programmers (e.g., questions involving C or C++) should go to R-devel.\nConvenient access to information on these lists, subscription, and archives is provided by the web interface at http://stat.ethz.ch/mailman/listinfo/ . One can also subscribe (or unsubscribe) via email, e.g. to R-help by sending `\nsubscribe\nunsubscribe\n') in the body of the message (not in the subject!) to R-help-request@lists.R-project.org .\nSend email to R-help@lists.R-project.org to send a message to everyone on the R-help mailing list. Subscription and posting to the other lists is done analogously, with `\nR-help\n', and `\nR-devel\n', respectively. Note that the R-announce and R-packages lists are gatewayed into R-help. Hence, you should subscribe to either of them only in case you are not subscribed to R-help.\nIt is recommended that you send mail to R-help rather than only to the R Core developers (who are also subscribed to the list, of course). This may save them precious time they can use for constantly improving R, and will typically also result in much quicker feedback for yourself.\nOf course, in the case of bug reports it would be very helpful to have code which reliably reproduces the problem. Also, make sure that you include information on the system and version of R being used. See R Bugs for more details.\nSee http://www.R-project.org/mail.html for more information on the R mailing lists.\nThe R Core Team can be reached at R-core@lists.R-project.org for comments and reports.\nMany of the R project's mailing lists are also available via Gmane , from which they can be read with a web browser, using an NNTP news reader, or via RSS feeds. See http://dir.gmane.org/index.php?prefix=gmane.comp.lang.r. for the available mailing lists, and http://www.gmane.org/rss.php for details on RSS feeds.\nNext: Can I use R for commercial purposes? , Previous: What mailing lists exist for R? , Up: R Basics\n2.10 What is CRAN ?\nThe \u201cComprehensive R Archive Network\u201d ( CRAN ) is a collection of sites which carry identical material, consisting of the R distribution(s), the contributed extensions, documentation for R, and binaries.\nThe CRAN master site at Wirtschaftsuniversit\u00e4t Wien, Austria, can be found at the URL\nhttp://cran.za.R-project.org/\n(Rhodes U, South Africa)\nSee http://CRAN.R-project.org/mirrors.html for a complete list of mirrors. Please use the CRAN site closest to you to reduce network load.\nFrom CRAN , you can obtain the latest official release of R, daily snapshots of R (copies of the current source trees), as gzipped and bzipped tar files, a wealth of additional contributed code, as well as prebuilt binaries for various operating systems (Linux, Mac OS Classic, Mac OS X, and MS Windows). CRAN also provides access to documentation on R, existing mailing lists and the R Bug Tracking system.\nTo \u201csubmit\u201d to CRAN , simply upload to ftp://CRAN.R-project.org/incoming/ and send an email to CRAN@R-project.org . Note that CRAN generally does not accept submissions of precompiled binaries due to security reasons. In particular, binary packages for Windows and Mac OS X are provided by the respective binary package maintainers.\nNote: It is very important that you indicate the copyright (license) information ( GPL-2 , GPL-3 , BSD , Artistic,\n...\nNext: Why is R named R? , Previous: What is CRAN? , Up: R Basics\n2.11 Can I use R for commercial purposes?\nR is released under the GNU General Public License (GPL) . If you have any questions regarding the legality of using R in any particular situation you should bring it up with your legal counsel. We are in no position to offer legal advice.\nIt is the opinion of the R Core Team that one can use R for commercial purposes (e.g., in business or in consulting). The GPL, like all Open Source licenses, permits all and any use of the package. It only restricts distribution of R or of other programs containing code from R. This is made clear in clause 6 (\u201cNo Discrimination Against Fields of Endeavor\u201d) of the Open Source Definition :\nThe license must not restrict anyone from making use of the program in a specific field of endeavor. For example, it may not restrict the program from being used in a business, or from being used for genetic research.\nIt is also explicitly stated in clause 0 of the GPL, which says in part\nActivities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program.\nMost add-on packages, including all recommended ones, also explicitly allow commercial use in this way. A few packages are restricted to \u201cnon-commercial use\u201d; you should contact the author to clarify whether these may be used or seek the advice of your legal counsel.\nNone of the discussion in this section constitutes legal advice. The R Core Team does not provide legal advice under any circumstances.\nNext: What is the R Foundation? , Previous: Can I use R for commercial purposes? , Up: R Basics\n2.12 Why is R named R?\nThe name is partly based on the (first) names of the first two R authors (Robert Gentleman and Ross Ihaka), and partly a play on the name of the Bell Labs language `S' (see What is S? ).\nPrevious: Why is R named R? , Up: R Basics\n2.13 What is the R Foundation?\nThe R Foundation is a not for profit organization working in the public interest. It was founded by the members of the R Core Team in order to provide support for the R project and other innovations in statistical computing, provide a reference point for individuals, institutions or commercial enterprises that want to support or interact with the R development community, and to hold and administer the copyright of R software and documentation. See http://www.R-project.org/foundation/ for more information.\nNext: What is S-PLUS? , Previous: R and S , Up: R and S\n3.1 What is S?\nS is a very high level language and an environment for data analysis and graphics. In 1998, the Association for Computing Machinery ( ACM ) presented its Software System Award to John M. Chambers, the principal designer of S, for\nthe S system, which has forever altered the way people analyze, visualize, and manipulate data\n...\nS is an elegant, widely accepted, and enduring software system, with conceptual integrity, thanks to the insight, taste, and effort of John Chambers.\nThe evolution of the S language is characterized by four books by John Chambers and coauthors, which are also the primary references for S.\nRichard A. Becker and John M. Chambers (1984), \u201cS. An Interactive Environment for Data Analysis and Graphics,\u201d Monterey: Wadsworth and Brooks/Cole.\nThis is also referred to as the \u201cBrown Book\u201d, and of historical interest only.\nRichard A. Becker, John M. Chambers and Allan R. Wilks (1988), \u201cThe New S Language,\u201d London: Chapman & Hall.\nThis book is often called the \u201cBlue Book\u201d, and introduced what is now known as S version 2.\nJohn M. Chambers and Trevor J. Hastie (1992), \u201cStatistical Models in S,\u201d London: Chapman & Hall.\nThis is also called the \u201cWhite Book\u201d, and introduced S version 3, which added structures to facilitate statistical modeling in S.\nJohn M. Chambers (1998), \u201cProgramming with Data,\u201d New York: Springer, ISBN 0-387-98503-4 ( http://cm.bell-labs.com/cm/ms/departments/sia/Sbook/ ).\nThis \u201cGreen Book\u201d describes version 4 of S, a major revision of S designed by John Chambers to improve its usefulness at every stage of the programming process.\nSee http://cm.bell-labs.com/cm/ms/departments/sia/S/history.html for further information on \u201cStages in the Evolution of S\u201d.\nThere is a huge amount of user-contributed code for S, available at the S Repository at CMU .\nNext: What are the differences between R and S? , Previous: What is S? , Up: R and S\n3.2 What is S-Plus?\nS-Plus is a value-added version of S sold by Insightful Corporation. Based on the S language, S-Plus provides functionality in a wide variety of areas, including robust regression, modern non-parametric regression, time series, survival analysis, multivariate analysis, classical statistical tests, quality control, and graphics drivers. Add-on modules add additional capabilities.\nNext: Is there anything R can do that S-PLUS cannot? , Previous: What is S-PLUS? , Up: R and S\n3.3 What are the differences between R and S?\nWe can regard S as a language with three current implementations or \u201cengines\u201d, the \u201cold S engine\u201d (S version 3; S-Plus 3.x and 4.x), the \u201cnew S engine\u201d (S version 4; S-Plus 5.x and above), and R. Given this understanding, asking for \u201cthe differences between R and S\u201d really amounts to asking for the specifics of the R implementation of the S language, i.e., the difference between the R and S engines.\nFor the remainder of this section, \u201cS\u201d refers to the S engines and not the S language.\nNext: Models , Previous: What are the differences between R and S? , Up: What are the differences between R and S?\n3.3.1 Lexical scoping\nContrary to other implementations of the S language, R has adopted an evaluation model in which nested function definitions are lexically scoped. This is analogous to the evalutation model in Scheme.\nThis difference becomes manifest when free variables occur in a function. Free variables are those which are neither formal parameters (occurring in the argument list of the function) nor local variables (created by assigning to them in the body of the function). In S, the values of free variables are determined by a set of global variables (similar to C, there is only local and global scope). In R, they are determined by the environment in which the function was created.\nConsider the following function:\ncube <- function(n) {\n       sq <- function() n * n\n       n * sq()\n     }\nUnder S, sq() does not \u201cknow\u201d about the variable n unless it is defined globally:\nS> cube(2)\n     Error in sq():  Object \"n\" not found\n     Dumped\n     S> n <- 3\n     S> cube(2)\n     [1] 18\nIn R, the \u201cenvironment\u201d created when cube() was invoked is also looked in:\nR> cube(2)\n     [1] 8\nAs a more \u201cinteresting\u201d real-world problem, suppose you want to write a function which returns the density function of the r-th order statistic from a sample of size n from a (continuous) distribution. For simplicity, we shall use both the cdf and pdf of the distribution as explicit arguments. (Example compiled from various postings by Luke Tierney.)\nThe S-Plus documentation for call() basically suggests the following:\ndorder <- function(n, r, pfun, dfun) {\n       f <- function(x) NULL\n       con <- round(exp(lgamma(n + 1) - lgamma(r) - lgamma(n - r + 1)))\n       PF <- call(substitute(pfun), as.name(\"x\"))\n       DF <- call(substitute(dfun), as.name(\"x\"))\n       f[[length(f)]] <-\n         call(\"*\", con,\n              call(\"*\", call(\"^\", PF, r - 1),\n                   call(\"*\", call(\"^\", call(\"-\", 1, PF), n - r),\n                        DF)))\n       f\n     }\nRather tricky, isn't it? The code uses the fact that in S, functions are just lists of special mode with the function body as the last argument, and hence does not work in R (one could make the idea work, though).\nA version which makes heavy use of substitute() and seems to work under both S and R is\ndorder <- function(n, r, pfun, dfun) {\n       con <- round(exp(lgamma(n + 1) - lgamma(r) - lgamma(n - r + 1)))\n       eval(substitute(function(x) K * PF(x)^a * (1 - PF(x))^b * DF(x),\n                       list(PF = substitute(pfun), DF = substitute(dfun),\n                            a = r - 1, b = n - r, K = con)))\n     }\n(the eval() is not needed in S).\nHowever, in R there is a much easier solution:\ndorder <- function(n, r, pfun, dfun) {\n       con <- round(exp(lgamma(n + 1) - lgamma(r) - lgamma(n - r + 1)))\n       function(x) {\n         con * pfun(x)^(r - 1) * (1 - pfun(x))^(n - r) * dfun(x)\n       }\n     }\nThis seems to be the \u201cnatural\u201d implementation, and it works because the free variables in the returned function can be looked up in the defining environment (this is lexical scope).\nNote that what you really need is the function closure, i.e., the body along with all variable bindings needed for evaluating it. Since in the above version, the free variables in the value function are not modified, you can actually use it in S as well if you abstract out the closure operation into a function MC() (for \u201cmake closure\u201d):\ndorder <- function(n, r, pfun, dfun) {\n       con <- round(exp(lgamma(n + 1) - lgamma(r) - lgamma(n - r + 1)))\n       MC(function(x) {\n            con * pfun(x)^(r - 1) * (1 - pfun(x))^(n - r) * dfun(x)\n          },\n          list(con = con, pfun = pfun, dfun = dfun, r = r, n = n))\n     }\nGiven the appropriate definitions of the closure operator, this works in both R and S, and is much \u201ccleaner\u201d than a substitute/eval solution (or one which overrules the default scoping rules by using explicit access to evaluation frames, as is of course possible in both R and S).\nFor R, MC() simply is\nMC <- function(f, env) f\n(lexical scope!), a version for S is\nMC <- function(f, env = NULL) {\n       env <- as.list(env)\n       if (mode(f) != \"function\")\n         stop(paste(\"not a function:\", f))\n       if (length(env) > 0 && any(names(env) == \"\"))\n         stop(paste(\"not all arguments are named:\", env))\n       fargs <- if(length(f) > 1) f[1:(length(f) - 1)] else NULL\n       fargs <- c(fargs, env)\n       if (any(duplicated(names(fargs))))\n         stop(paste(\"duplicated arguments:\", paste(names(fargs)),\n              collapse = \", \"))\n       fbody <- f[length(f)]\n       cf <- c(fargs, fbody)\n       mode(cf) <- \"function\"\n       return(cf)\n     }\nSimilarly, most optimization (or zero-finding) routines need some arguments to be optimized over and have other parameters that depend on the data but are fixed with respect to optimization. With R scoping rules, this is a trivial problem; simply make up the function with the required definitions in the same environment and scoping takes care of it. With S, one solution is to add an extra parameter to the function and to the optimizer to pass in these extras, which however can only work if the optimizer supports this.\nNested lexically scoped functions allow using function closures and maintaining local state. A simple example (taken from Abelson and Sussman) is obtained by typing\ndemo(\"scoping\")\nat the R prompt. Further information is provided in the standard R reference \u201cR: A Language for Data Analysis and Graphics\u201d (see What documentation exists for R? ) and in Robert Gentleman and Ross Ihaka (2000), \u201cLexical Scope and Statistical Computing\u201d, Journal of Computational and Graphical Statistics , 9, 491\u2013508.\nNested lexically scoped functions also imply a further major difference. Whereas S stores all objects as separate files in a directory somewhere (usually\n.Data\nunder the current directory), R does not. All objects in R are stored internally. When R is started up it grabs a piece of memory and uses it to store the objects. R performs its own memory management of this piece of memory, growing and shrinking its size as needed. Having everything in memory is necessary because it is not really possible to externally maintain all relevant \u201cenvironments\u201d of symbol/value pairs. This difference also seems to make R faster than S.\nThe down side is that if R crashes you will lose all the work for the current session. Saving and restoring the memory \u201cimages\u201d (the functions and data stored in R's internal memory at any time) can be a bit slow, especially if they are big. In S this does not happen, because everything is saved in disk files and if you crash nothing is likely to happen to them. (In fact, one might conjecture that the S developers felt that the price of changing their approach to persistent storage just to accommodate lexical scope was far too expensive.) Hence, when doing important work, you might consider saving often (see How can I save my workspace? ) to safeguard against possible crashes. Other possibilities are logging your sessions, or have your R commands stored in text files which can be read in using source().\nNote: If you run R from within Emacs (see R and Emacs ), you can save the contents of the interaction buffer to a file and conveniently manipulate it using ess-transcript-mode, as well as save source copies of all functions and data used.\n3.3.2 Models\nThere are some differences in the modeling code, such as\nWhereas in S, you would use lm(y ~ x^3) to regress y on x^3, in R, you have to insulate powers of numeric vectors (using I()), i.e., you have to use lm(y ~ I(x^3)).\nThe glm family objects are implemented differently in R and S. The same functionality is available but the components have different names.\nOption na.action is set to \"na.omit\" by default in R, but not set in S.\nTerms objects are stored differently. In S a terms object is an expression with attributes, in R it is a formula with attributes. The attributes have the same names but are mostly stored differently.\nFinally, in R y ~ x + 0 is an alternative to y ~ x - 1 for specifying a model with no intercept. Models with no parameters at all can be specified by y ~ 0.\nPrevious: Models , Up: What are the differences between R and S?\n3.3.3 Others\nApart from lexical scoping and its implications, R follows the S language definition in the Blue and White Books as much as possible, and hence really is an \u201cimplementation\u201d of S. There are some intentional differences where the behavior of S is considered \u201cnot clean\u201d. In general, the rationale is that R should help you detect programming errors, while at the same time being as compatible as possible with S.\nSome known differences are the following.\nIn R, if x is a list, then x[i] <- NULL and x[[i]] <- NULL remove the specified elements from x. The first of these is incompatible with S, where it is a no-op. (Note that you can set elements to NULL using x[i] <- list(NULL).)\nIn S, the functions named .First and .Last in the\n.Data\ndirectory can be used for customizing, as they are executed at the very beginning and end of a session, respectively.\nIn R, the startup mechanism is as follows. Unless\n--no-environ\nwas given on the command line, R searches for site and user files to process for setting environment variables. Then, R searches for a site-wide startup profile unless the command line option\n--no-site-file\nwas given. This code is loaded in package base. Then, unless\n--no-init-file\nwas given, R searches for a user profile file, and sources it into the user workspace. It then loads a saved image of the user workspace from\n.RData\nin case there is one (unless\n--no-restore-data\nor\n--no-restore\nwere specified). Next, a function .First() is run if found on the search path. Finally, function .First.sys in the base package is run. When terminating an R session, by default a function .Last is run if found on the search path, followed by .Last.sys. If needed, the functions .First() and .Last() should be defined in the appropriate startup profiles. See the help pages for .First and .Last for more details.\nIn R, T and F are just variables being set to TRUE and FALSE, respectively, but are not reserved words as in S and hence can be overwritten by the user. (This helps e.g. when you have factors with levels \"T\" or \"F\".) Hence, when writing code you should always use TRUE and FALSE.\nIn R, dyn.load() can only load shared objects, as created for example by\nR CMD SHLIB\n.\nIn R, attach() currently only works for lists and data frames, but not for directories. (In fact, attach() also works for R data files created with save(), which is analogous to attaching directories in S.) Also, you cannot attach at position 1.\nCategories do not exist in R, and never will as they are deprecated now in S. Use factors instead.\nIn R, For() loops are not necessary and hence not supported.\nIn R, assign() uses the argument\nenvir=\nThe random number generators are different, and the seeds have different length.\nR passes integer objects to C as int * rather than long * as in S.\nR has no single precision storage mode. However, as of version 0.65.1, there is a single precision interface to C/FORTRAN subroutines.\nBy default, ls() returns the names of the objects in the current (under R) and global (under S) environment, respectively. For example, given\nx <- 1; fun <- function() {y <- 1; ls()}\nthen fun() returns \"y\" in R and \"x\" (together with the rest of the global environment) in S.\nR allows for zero-extent matrices (and arrays, i.e., some elements of the dim attribute vector can be 0). This has been determined a useful feature as it helps reducing the need for special-case tests for empty subsets. For example, if x is a matrix, x[, FALSE] is not NULL but a \u201cmatrix\u201d with 0 columns. Hence, such objects need to be tested for by checking whether their length() is zero (which works in both R and S), and not using is.null().\nNamed vectors are considered vectors in R but not in S (e.g., is.vector(c(a = 1:3)) returns FALSE in S and TRUE in R).\nData frames are not considered as matrices in R (i.e., if DF is a data frame, then is.matrix(DF) returns FALSE in R and TRUE in S).\nR by default uses treatment contrasts in the unordered case, whereas S uses the Helmert ones. This is a deliberate difference reflecting the opinion that treatment contrasts are more natural.\nIn R, the argument of a replacement function which corresponds to the right hand side must be named `\nvalue\n'. E.g., f(a) <- b is evaluated as a <- \"f<-\"(a, value = b). S always takes the last argument, irrespective of its name.\nIn S, substitute() searches for names for substitution in the given expression in three places: the actual and the default arguments of the matching call, and the local frame (in that order). R looks in the local frame only, with the special rule to use a \u201cpromise\u201d if a variable is not evaluated. Since the local frame is initialized with the actual arguments or the default expressions, this is usually equivalent to S, until assignment takes place.\nIn S, the index variable in a for() loop is local to the inside of the loop. In R it is local to the environment where the for() statement is executed.\nIn S, tapply(simplify=TRUE) returns a vector where R returns a one-dimensional array (which can have named dimnames).\nIn S(-Plus) the C locale is used, whereas in R the current operating system locale is used for determining which characters are alphanumeric and how they are sorted. This affects the set of valid names for R objects (for example accented chars may be allowed in R) and ordering in sorts and comparisons (such as whether \"aA\" < \"Bb\" is true or false). From version 1.2.0 the locale can be (re-)set in R by the Sys.setlocale() function.\nIn S, missing(arg) remains TRUE if arg is subsequently modified; in R it doesn't.\nFrom R version 1.3.0, data.frame strips I() when creating (column) names.\nIn R, the string \"NA\" is not treated as a missing value in a character variable. Use as.character(NA) to create a missing character value.\nR disallows repeated formal arguments in function calls.\nIn S, dump(), dput() and deparse() are essentially different interfaces to the same code. In R from version 2.0.0, this is only true if the same control argument is used, but by default it is not. By default dump() tries to write code that will evaluate to reproduce the object, whereas dput() and deparse() default to options for producing deparsed code that is readable.\nIn R, indexing a vector, matrix, array or data frame with [ using a character vector index looks only for exact matches (whereas [[ and $ allow partial matches). In S, [ allows partial matches.\nS has a two-argument version of atan and no atan2. A call in S such as atan(x1, x2) is equivalent to R's atan2(x1, x2). However, beware of named arguments since S's atan(x = a, y = b) is equivalent to R's atan2(y = a, x = b) with the meanings of x and y interchanged. (R used to have undocumented support for a two-argument atan with positional arguments, but this has been withdrawn to avoid further confusion.)\nNumeric constants with no fractional and exponent (i.e., only integer) part are taken as integer in S-Plus 6.x or later, but as double in R.\nThere are also differences which are not intentional, and result from missing or incorrect code in R. The developers would appreciate hearing about any deficiencies you may find (in a written report fully documenting the difference as you see it). Of course, it would be useful if you were to implement the change yourself and make sure it works.\nNext: What is R-plus? , Previous: What are the differences between R and S? , Up: R and S\n3.4 Is there anything R can do that S-Plus cannot?\nSince almost anything you can do in R has source code that you could port to S-Plus with little effort there will never be much you can do in R that you couldn't do in S-Plus if you wanted to. (Note that using lexical scoping may simplify matters considerably, though.)\nR offers several graphics features that S-Plus does not, such as finer handling of line types, more convenient color handling (via palettes), gamma correction for color, and, most importantly, mathematical annotation in plot texts, via input expressions reminiscent of TeX constructs. See the help page for plotmath, which features an impressive on-line example. More details can be found in Paul Murrell and Ross Ihaka (2000), \u201cAn Approach to Providing Mathematical Annotation in Plots\u201d, Journal of Computational and Graphical Statistics , 9, 582\u2013599.\n3.5 What is R-plus?\nFor a very long time, there was no such thing.\nXLSolutions Corporation is currently beta testing a commercially supported version of R named R+ (read R plus).\nIn addition, REvolution Computing has released RPro , an enterprise-class statistical analysis system based on R, suitable for deployment in professional, commercial and regulated environments.\nRandom Technologies offers RStat , an enterprise-strength statistical computing environment which combines R with enterprise-level validation, documentation, software support, and consulting services, as well as related R-based products.\nNext: R Add-On Packages , Previous: R and S , Up: Top\n4 R Web Interfaces\nRweb is developed and maintained by Jeff Banfield . The Rweb Home Page provides access to all three versions of Rweb\u2014a simple text entry form that returns output and graphs, a more sophisticated Javascript version that provides a multiple window environment, and a set of point and click modules that are useful for introductory statistics courses and require no knowledge of the R language. All of the Rweb versions can analyze Web accessible datasets if a URL is provided.\nThe paper \u201cRweb: Web-based Statistical Analysis\u201d, providing a detailed explanation of the different versions of Rweb and an overview of how Rweb works, was published in the Journal of Statistical Software ( http://www.jstatsoft.org/v04/i01/ ).\nUlf Bartel has developed R-Online, a simple on-line programming environment for R which intends to make the first steps in statistical programming with R (especially with time series) as easy as possible. There is no need for a local installation since the only requirement for the user is a JavaScript capable browser. See http://osvisions.com/r-online/ for more information.\nRcgi is a CGI WWW interface to R by MJ Ray . It had the ability to use \u201cembedded code\u201d: you could mix user input and code, allowing the HTML author to do anything from load in data sets to enter most of the commands for users without writing CGI scripts. Graphical output was possible in PostScript or GIF formats and the executed code was presented to the user for revision. However, it is not clear if the project is still active. Currently, a modified version of Rcgi by Mai Zhou (actually, two versions: one with (bitmap) graphics and one without) as well as the original code are available from http://www.ms.uky.edu/~statweb/ .\nCGI-based web access to R is also provided at http://hermes.sdu.dk/cgi-bin/go/ . There are many additional examples of web interfaces to R which basically allow to submit R code to a remote server, see for example the collection of links available from http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/StatCompCourse .\nDavid Firth has written CGIwithR, an R add-on package available from CRAN . It provides some simple extensions to R to facilitate running R scripts through the CGI interface to a web server, and allows submission of data using both GET and POST methods. It is easily installed using Apache under Linux and in principle should run on any platform that supports R and a web server provided that the installer has the necessary security permissions. David's paper \u201cCGIwithR: Facilities for Processing Web Forms Using R\u201d was published in the Journal of Statistical Software ( http://www.jstatsoft.org/v08/i10/ ). The package is now maintained by Duncan Temple Lang and has a web page at http://www.omegahat.org/CGIwithR/ .\nRpad , developed and actively maintained by Tom Short, provides a sophisticated environment which combines some of the features of the previous approaches with quite a bit of Javascript, allowing for a GUI -like behavior (with sortable tables, clickable graphics, editable output), etc.\nJeff Horner is working on the R/Apache Integration Project which embeds the R interpreter inside Apache 2 (and beyond). A tutorial and presentation are available from the project web page at http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/RApacheProject .\nRserve is a project actively developed by Simon Urbanek. It implements a TCP/IP server which allows other programs to use facilities of R. Clients are available from the web site for Java and C++ (and could be written for other languages that support TCP/IP sockets).\nOpenStatServer is being developed by a team lead by Greg Warnes; it aims \u201cto provide clean access to computational modules defined in a variety of computational environments (R, SAS, Matlab, etc) via a single well-defined client interface\u201d and to turn computational services into web services.\nTwo projects use PHP to provide a web interface to R. R_PHP_Online by Steve Chen (though it is unclear if this project is still active) is somewhat similar to the above Rcgi and Rweb. R-php is actively developed by Alfredo Pontillo and Angelo Mineo and provides both a web interface to R and a set of pre-specified analyses that need no R code input.\nwebbioc is \u201can integrated web interface for doing microarray analysis using several of the Bioconductor packages\u201d and is designed to be installed at local sites as a shared computing resource.\nFinally, Rwui is a web application to to create user-friendly web interfaces for R scripts. All code for the web interface is created automatically. There is no need for the user to do any extra scripting or learn any new scripting techniques.\n"}, {"score": 894.81, "uuid": "9e5a59ba-96e5-5095-955a-15de81f49371", "index": "cw12", "trec_id": "clueweb12-0012wb-31-22877", "target_hostname": "projects.gnome.org", "target_uri": "http://projects.gnome.org/gnumeric/doc/sect-extending-python-writing.shtml", "page_rank": 1.2260414e-09, "spam_rank": 85, "title": "Writing your own <em>Python</em> functions", "snippet": "JK thinks that there may be no way to get the current Workbook&#x2F;Sheet in the <em>Python</em> API. * As noted, it <em>should</em> do some simple trapping to skip blank <em>or</em> text-filled cells. That can be done!", "explanation": null, "document": "18.3.4.5. Program Listings\n18.3.4.1. Prepare the spellbook\nIn many ways it would be easier to start by copying the py_func spellbook to your local .gnumeric folder, and just adding a function to that. But in general it will be more useful to be able to write your own separate spellbooks, so here we go.\nMake the folder: First we make the folders and get into the right one. As noted above, we'll call our folder (spellbook) myfuncs.\nIf they don't already exist:\nmkdir ~/.gnumeric\nmkdir ~/.gnumeric/<version>/plugins/myfuncs/\ncd ~/.gnumeric/<version>/plugins/myfuncs/\nMake the files: A spellbook has two files. The first is the python file with the functions. The second is the XML file \"plugin.xml\". The XML file holds that master spells that tell Gnumeric what functions we've defined, and what the name of the python file is, and one other important item. We'll create these as blank files.\ntouch my-func.py\ntouch plugin.xml\nWrite the master spells The good news is that you only need to do this once per spellbook. After that you just add spells to it.\nYour XML file must tell Gnumeric about your plugin. Here is a simple template. (If you want to learn about internationalization, see the example in the system's py-func spellbook.) Open up plugin.xml and insert the following lines:\n<?xml version=\"1.0\"?>\n<plugin id=\"Gnumeric_MyFuncPlugin\">\n\t<information>\n\t\t<name>Other Python functions from HOWTO</name>\n\t\t<description>A few extra python functions demonstrating the API.</description>\n\t</information>\n\t<loader type=\"Gnumeric_PythonLoader:python\">\n\t\t<attribute name=\"module_name\" value=\"my-func\"/> 3 </loader>\n\t<services>\n\t\t<service type=\"function_group\" id=\"example\"> 4 <category>Local Python</category>\n\t\t\t<functions>\n\t\t\t</functions>\n\t\t</service>\n\t</services>\n</plugin>\nThe value of \"name\" determines the name of your python script (file). In this case, it must be \"my-func.py\"\nThe value of \"id\" here determines the name of the function dictionary in your python script. In this case, it must be \"example_functions\" because here the value is \"example\".\nPrepare to write the spells: Next we'll create a minimal python file. As noted above, we must name the file my-func.py and it must have a dictionary called example_functions. So open up my-func.py and insert the following lines.\n# my-func.py\n#\n\nfrom Gnumeric import GnumericError, GnumericErrorVALUE\nimport Gnumeric\nimport string\n\t\nexample_functions = {\n}\n18.3.4.2. Writing new spells\nTo add new functions to Python, you now must do five things (three sir!):\nWrite the python function in your copy of my-func.py.\nInsert the function info into the example_functions dictionary at the end of my_func.py\nInsert the function name into the functions list at the end of plugin.xml.\nWriting a simple script: Let's do something very simple: add two numbers together. First, edit my-func.py.\n# Add two numbers together def func_add(num1, num2):\n        return num1 + num2 # Translate the func_add python function to a gnumeric function and register it example_functions = {\n        'py_add': func_add\n    }\nThen let the plugin-loader(?) know about your function. Add the following line near the end of plugin.xml (between <functions> and </functions>).\n<function name=\"py_add\"/>\nNow start Gnumeric and type py_add(2,3) into a cell. You should get \"5\". You can also use cell references. If that was in cell A1, go to cell A2 and type py_add(A1,3) and you will get \"8\". But your function won't show up in the GUI list yet.\nTell the GUI: To make your function show up in the GUI, you have to tell Gnumeric some things about it via a standard header, like this:\n# Add two numbers together def func_add(num1, num2):\n        '@FUNCTION=PY_ADD\\n'\\\n        '@SYNTAX=py_add(num1, num2)\\n'\\\n        '@DESCRIPTION=Adds two numbers together.\\n'\\\n        'Look, the description can go onto other lines.\\n\\n'\\\n        '@EXAMPLES=To add two constants, just type them in: py_add(2,3)\\n'\\\n        'To add two cells, use the cell addresses: py_add(A1,A2)\\n\\n'\\\n        '@SEEALSO='\n\n        return num1 + num2\nThe text after '@DESCRIPTION=' is the description that shows up in the function GUI. You can make it as simple or detailed as you want. I'm not sure how many other fields get used right now, as I haven't seen the EXAMPLES show up anywhere.\nBut this still isn't quite right. Gnumeric doesn't know how many arguments the function can handle, nor of what type. So the function GUI will prompt for the two values it knows about (as type \"Any\") and then keep prompting for more. But py_add cannot accept all types, nor can it handle more than two arguments, so unless you give it precisely 2 numbers, you will get an error when you click \"OK\".\nKnow your limits... We got away last time just because Gnumeric was forgiving. Now we need to say that we can accept only 2 values, of type floating-point (which will also handle ints).\nWhere before we had: 'py_add': func_add, we should now put: 'py_add': ('ff','num1,num2',func_add) This says that Gnumeric should expect two floating-point numbers ('ff') with names 'num1' and 'num2', and pass them to func_add.\n...and surpass them Of course, there is no reason an add function shouldn't be able to handle a range. The simplest way to do that is to accept a range, and then call Gnumeric's own SUM function on it! All of Gnumeric's functions are available to you in the dictionary Gnumeric.functions, keyed by name. So here is how to write py_sum.\nFirst, define func_sum (in my-func.py):\ndef func_sum(gRange):\n\t'@FUNCTION=PY_SUM\\n'\\\n\t'@SYNTAX=PY_SUM(range)\\n'\\\n\t'@DESCRIPTION=Adds a range of numbers together.'\\\n\t'Just like built-in SUM.\\n\\n'\\\n\t'@EXAMPLES=To add values in A1 to A5, just type them in:\\n'\\\n\t'    py_sum(a1:a5)\\n'\\\n\t'@SEEALSO='\n\ttry:\n\t\tsum = Gnumeric.functions['sum']\n\t\tval = sum(gRange)\n\t\t#  val = reduce(lambda a,b: a+b, vals)\n\texcept TypeError:\n\t\traise GnumericError, GnumericErrorVALUE\n\telse:\n\t\treturn val\nThen insert it into your functions dictionary. That dictionary now looks like this (with 'r' denoting a range type):\nexample_functions = {\n\t'py_add': ('ff','num1,num2',func_add),\n\t'py_sum': ('r', 'values', func_sum)\n}\nFinally, make an entry in the XML list, so that it now looks like:\n<functions>\n\t\t\t\t<function name=\"py_add\"/>\n\t\t\t\t<function name=\"py_sum\"/>\n\t\t\t</functions>\nI told you this was the easy way to do it. Obviously it's not very useful to just duplicate Gnumeric functions. But that's as far as I've made it. From what can tell, range objects are packaged as opaque pointers of type RangeRefObject. There seems to be no way to work with them from within Python, so we must rely on the Gnumeric functions.\n18.3.4.3. Do it yourself (mostly)\nAll is not lost, despite the opaque pointers. For in Gnumeric we can read about all the functions that have been defined. Some of those take references (including RangeRefs) and return useful information. For example, under \"Lookup\" we find \"Column\" and \"Row\" which return arrays of all the column (or row) indices in the range. So we can redo the sum function.\nPresume we can convert our RangeRef to a start tuple and and end tuple. Then we can write sum2:\ndef func_sum2(gRange):\n\t'@FUNCTION=PY_SUM2\\n'\\\n\t'@SYNTAX=PY_SUM2(range)\\n'\\\n\t'@DESCRIPTION=Adds a range of numbers together,'\\\n\t'without calling built-in SUM.\\n\\n'\\\n\t'@EXAMPLES=To add values in A1 to A5, just type them in:\\n'\\\n\t'    py_sum(a1:a5)\\n'\\\n\t'@SEEALSO='\n\ttry:\n\t\t[r_begin, r_end] = range_ref_to_tuples(gRange)\n\t\twb=Gnumeric.Workbooks()[0]   # Careful! This is WRONG! It doesn't\n\t\ts=wb.sheets()[0]             # use the ACTUAL workbook or sheet.\n\n\t\tval = 0\n\t\tfor col in range(r_begin[0], r_end[0]):\n\t\t\tfor row in range(r_begin[1], r_end[1]):\n\t\t\t\tcell = s[col, row]\n\t\t\t\tval = val + cell.get_value()\n\t\t\t\t# Note: this doesn't skip blank cells etc.\n\n\texcept TypeError:\n\t\traise GnumericError,GnumericErrorVALUE\n\telse:\n\t\treturn val\nThat's fine as far as it goes, but we need to define the helper function \"range_ref_to_tuples\". Although I'm rather ashamed to show this ugly literal, here's how I did it (someone suggest a better way, please!):\ndef range_ref_to_tuples(range_ref):\n\t'''I need a function to find the bounds of a RangeRef. This one\n\textracts them from the Gnumeric \"column\" and \"row\" commands, and\n\treturns them as a pair of tuples. Surely there is a better way?\n\tFor example, return a list of cells??'''\n\n\tcol  = Gnumeric.functions['column']   \n\trow  = Gnumeric.functions['row']\n\n\t# \"column\" and \"row\" take references and return an array of col or row\n\t# nums for each cell in the reference. For example, [[1, 1, 1], [2, 2, 2]]\n\t# for columns and [[2, 3, 4], [2, 3, 4]] for rows.\n\n\ttry:\n\t\tcolumns = col(range_ref)\n\t\trows    = row(range_ref)\n\n\t\tbegin_col = columns[0][0] - 1  \n\t\tbegin_row = rows[0][0] - 1     \n\n\t\tend_col = columns[-1][-1]\n\t\tend_row = rows[-1][-1]\n\n\t\t# We subtracted 1 from the begin values because in the API,\n\t\t# indexing begins at 0, while \"column\" and \"row\" begin at 1.\n\t\t# We did NOT subtract 1 from the end values, in order to make\n\t\t# them suitable for Python's range(begin, end) paradigm.\n\t\t\n\texcept TypeError:\n\t\traise GnumericError,GnumericErrorVALUE\n\texcept NameError:                     # right name?\n\t\traise GnumericError,Gnumeric.GnumericErrorNAME\n\texcept RefError:                     # right name?\n\t\traise GnumericError,Gnumeric.GnumericErrorREF\n\texcept NumError:                     # right name?\n\t\traise GnumericError,Gnumeric.GnumericErrorNUM\n\n\n\treturn [ (begin_col, begin_row), (end_col, end_row) ]\nFrom there, insert the function into the dictionary, and insert its name into plugin.xml. I leave these as exercises to the reader (answers in the sample files -- no peeking!). Restart Gnumeric and you should be able to use py_sum2!\nThere are a couple of glitches:\nIt fails the first time with \"could not import gobject\". Just run again, I don't know what that's about.\nIt will only work for Workbook 1 and Sheet 1. JK thinks that there may be no way to get the current Workbook/Sheet in the Python API. Hrm....\nAs noted, it should do some simple trapping to skip blank or text-filled cells. That can be done! I just didn't. It's late.\n18.3.4.4. More help\nRelative to the source tree:\nThe Python interface is defined in: plugins/python-loader/py-gnumeric.c That file also has good notes at the beginning.\nThere are interesting things about the way it used to be in: doc/developer/python-gnumeric.txt.\n18.3.4.5. Program Listings\n"}, {"score": 892.31104, "uuid": "83eca1b1-d8d0-5b36-86f9-9849980225d7", "index": "cw12", "trec_id": "clueweb12-0101wb-63-00328", "target_hostname": "www.astrostatistics.psu.edu", "target_uri": "http://www.astrostatistics.psu.edu/su07/R/doc/manual/R-FAQ.html", "page_rank": 1.2021157e-09, "spam_rank": 93, "title": "<em>R</em> FAQ", "snippet": "Finally, Rwui is a web application to to create user-friendly web interfaces <em>for</em> <em>R</em> scripts. All code <em>for</em> the web interface is created automatically. There is no need <em>for</em> the user to do any extra scripting <em>or</em> <em>learn</em> any new scripting techniques.", "explanation": null, "document": "Next: Introduction , Previous: (dir) , Up: (dir)\nR FAQ\nFrequently Asked Questions on R\nVersion 2.5.2007-04-23\n1.1 Legalese\nThis document is copyright \u00a9 1998\u20132007 by Kurt Hornik.\nThis document is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2, or (at your option) any later version.\nThis document is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\nA copy of the GNU General Public License is available via WWW at\nFrom there, you can obtain versions converted to plain ASCII text , DVI , GNU info , HTML , PDF , PostScript as well as the Texinfo source used for creating all these formats using the GNU Texinfo system .\nYou can also obtain the R FAQ from the doc/FAQ subdirectory of a CRAN site (see What is CRAN? ).\nNext: Notation , Previous: Obtaining this document , Up: Introduction\n1.3 Citing this document\nIn publications, please refer to this FAQ as Hornik (2007), \u201cThe R FAQ \u201d, and give the above, official URL and the ISBN 3-900051-08-9:\n@Misc{,\n       author\t= {Kurt Hornik},\n       title\t\t= {The {R} {FAQ}},\n       year\t\t= {2007},\n       note\t\t= {{ISBN} 3-900051-08-9},\n       url\t\t= {http://CRAN.R-project.org/doc/FAQ/R-FAQ.html}\n     }\nNext: Feedback , Previous: Citing this document , Up: Introduction\n1.4 Notation\nEverything should be pretty standard. R> is used for the R prompt, and a $ for the shell prompt (where applicable).\n1.5 Feedback\nFeedback via email to Kurt.Hornik@R-project.org is of course most welcome.\nIn particular, note that I do not have access to Windows or Macintosh systems. Features specific to the Windows and Mac OS X ports of R are described in the \u201cR for Windows FAQ \u201d and the \u201cR for Mac OS X FAQ . If you have information on Macintosh or Windows systems that you think should be added to this document, please let me know.\nNext: What machines does R run on? , Previous: R Basics , Up: R Basics\n2.1 What is R?\nR is a system for statistical computation and graphics. It consists of a language plus a run-time environment with graphics, a debugger, access to certain system functions, and the ability to run programs stored in script files.\nThe design of R has been heavily influenced by two existing languages: Becker, Chambers & Wilks' S (see What is S? ) and Sussman's Scheme . Whereas the resulting language is very similar in appearance to S, the underlying implementation and semantics are derived from Scheme. See What are the differences between R and S? , for further details.\nThe core of R is an interpreted computer language which allows branching and looping as well as modular programming using functions. Most of the user-visible functions in R are written in R. It is possible for the user to interface to procedures written in the C, C++, or FORTRAN languages for efficiency. The R distribution contains functionality for a large number of statistical procedures. Among these are: linear and generalized linear models, nonlinear regression models, time series analysis, classical parametric and nonparametric tests, clustering and smoothing. There is also a large set of functions which provide a flexible graphical environment for creating various kinds of data presentations. Additional modules (\u201cadd-on packages\u201d) are available for a variety of specific purposes (see R Add-On Packages ).\nR was initially written by Ross Ihaka and Robert Gentleman at the Department of Statistics of the University of Auckland in Auckland, New Zealand. In addition, a large group of individuals has contributed to R by sending code and bug reports.\nSince mid-1997 there has been a core group (the \u201cR Core Team\u201d) who can modify the R source code archive. The group currently consists of Doug Bates, John Chambers, Peter Dalgaard, Robert Gentleman, Kurt Hornik, Stefano Iacus, Ross Ihaka, Friedrich Leisch, Thomas Lumley, Martin Maechler, Duncan Murdoch, Paul Murrell, Martyn Plummer, Brian Ripley, Duncan Temple Lang, Luke Tierney, and Simon Urbanek.\nR has a home page at http://www.R-project.org/ . It is free software distributed under a GNU -style copyleft , and an official part of the GNU project (\u201c GNU S\u201d).\nNext: What is the current version of R? , Previous: What is R? , Up: R Basics\n2.2 What machines does R run on?\nR is being developed for the Unix, Windows and Mac families of operating systems. Support for Mac OS Classic ended with R 1.7.1.\nThe current version of R will configure and build under a number of common Unix platforms including cpu-linux-gnu for the i386, alpha, arm, hppa, ia64, m68k, mips/mipsel, powerpc, s390, sparc (e.g., http://buildd.debian.org/build.php?&pkg=r-base ), and x86_64 CPUs, powerpc-apple-darwin, mips-sgi-irix, rs6000-ibm-aix, and sparc-sun-solaris.\nIf you know about other platforms, please drop us a note.\nNext: How can R be obtained? , Previous: What machines does R run on? , Up: R Basics\n2.3 What is the current version of R?\nThe current released version is 2.5.0. Based on this `major.minor.patchlevel' numbering scheme, there are two development versions of R, a patched version of the current release (`r-patched') and one working towards the next minor or eventually major (`r-devel') releases of R, respectively. Version r-patched is for bug fixes mostly. New features are typically introduced in r-devel.\nNext: How can R be installed? , Previous: What is the current version of R? , Up: R Basics\n2.4 How can R be obtained?\nSources, binaries and documentation for R can be obtained via CRAN , the \u201cComprehensive R Archive Network\u201d (see What is CRAN? ).\nSources are also available via https://svn.R-project.org/R/ , the R Subversion repository, but currently not via anonymous rsync (nor CVS).\nTarballs with daily snapshots of the r-devel and r-patched development versions of R can be found at ftp://ftp.stat.math.ethz.ch/Software/R .\n2.5.1 How can R be installed (Unix)\nIf R is already installed, it can be started by typing\nR\nat the shell prompt (of course, provided that the executable is in your path).\nIf binaries are available for your platform (see Are there Unix binaries for R? ), you can use these, following the instructions that come with them.\nOtherwise, you can compile and install R yourself, which can be done very easily under a number of common Unix platforms (see What machines does R run on? ). The file INSTALL that comes with the R distribution contains a brief introduction, and the \u201cR Installation and Administration\u201d guide (see What documentation exists for R? ) has full details.\nNote that you need a FORTRAN compiler or perhaps f2c in addition to a C compiler to build R. Also, you need Perl version 5 to build the R object documentations. (If this is not available on your system, you can obtain a PDF version of the object reference manual via CRAN .)\nIn the simplest case, untar the R source code, change to the directory thus created, and issue the following commands (at the shell prompt):\n$ ./configure\n     $ make\nIf these commands execute successfully, the R binary and a shell script front-end called R are created and copied to the bin directory. You can copy the script to a place where users can invoke it, for example to /usr/local/bin. In addition, plain text help pages as well as HTML and LaTeX versions of the documentation are built.\nUse\nmake dvi\nto create DVI versions of the R manuals, such as refman.dvi (an R object reference index) and R-exts.dvi, the \u201cR Extension Writers Guide\u201d, in the doc/manual subdirectory. These files can be previewed and printed using standard programs such as xdvi and dvips. You can also use\nmake pdf\nto build PDF (Portable Document Format) version of the manuals, and view these using e.g. Acrobat. Manuals written in the GNU Texinfo system can also be converted to info files suitable for reading online with Emacs or stand-alone GNU Info; use\nmake info\nto create these versions (note that this requires Makeinfo version 4.5).\nFinally, use\nto find out whether your R system works correctly.\nYou can also perform a \u201csystem-wide\u201d installation using\nmake install\n. By default, this will install to the following directories:\n${prefix}/bin\nall the rest (libraries, on-line help system,\n...\n). This is the \u201cR Home Directory\u201d ( R_HOME) of the installed system.\nIn the above, prefix is determined during configuration (typically /usr/local) and can be set by running configure with the option\n$ ./configure --prefix=/where/you/want/R/to/go\n(E.g., the R executable will then be installed into /where/you/want/R/to/go/bin.)\nTo install DVI, info and PDF versions of the manuals, use\nmake install-dvi\nmake install-pdf\n, respectively.\nNext: How can R be installed (Macintosh) , Previous: How can R be installed (Unix) , Up: How can R be installed?\n2.5.2 How can R be installed (Windows)\nThe bin/windows directory of a CRAN site contains binaries for a base distribution and a large number of add-on packages from CRAN to run on Windows 95, 98, ME, NT4, 2000, and XP (at least) on Intel and clones (but not on other platforms). The Windows version of R was created by Robert Gentleman and Guido Masarotto, and is now being developed and maintained by Duncan Murdoch and Brian D. Ripley .\nFor most installations the Windows installer program will be the easiest tool to use.\nPrevious: How can R be installed (Windows) , Up: How can R be installed?\n2.5.3 How can R be installed (Macintosh)\nThe bin/macosx directory of a CRAN site contains a standard Apple installer package inside a disk image named R.dmg. Once downloaded and executed, the installer will install the current non-developer release of R. RAqua is a native Mac OS X Darwin version of R with a R.app Mac OS X GUI . Inside bin/macosx/powerpc/contrib/x.y there are prebuilt binary packages (for powerpc version of Mac OS X) to be used with RAqua corresponding to the \u201cx.y\u201d release of R. The installation of these packages is available through the \u201cPackage\u201d menu of the R.app GUI . This port of R for Mac OS X is maintained by Stefano Iacus . The \u201cR for Mac OS X FAQ has more details.\nThe bin/macos directory of a CRAN site contains bin-hexed (hqx) and stuffit (sit) archives for a base distribution and a large number of add-on packages of R 1.7.1 to run under Mac OS 8.6 to Mac OS 9.2.2. This port of R for Macintosh is no longer supported.\n2.6 Are there Unix binaries for R?\nThe bin/linux directory of a CRAN site contains the following packages.\nCPU\n3.2\nSusumu Tanimura\nDebian packages, maintained by Dirk Eddelbuettel and Doug Bates, have long been part of the Debian distribution, and can be accessed through APT, the Debian package maintenance tool. Use e.g. apt-get install r-base r-recommended to install the R environment and recommended packages. If you also want to build R packages from source, also run apt-get install r-base-dev to obtain the additional tools required for this. So-called \u201cbackports\u201d of the current R packages for the\nstable\ndistribution of Debian are provided by Johannes Ranke, and available from CRAN. Simply add the line\ndeb http://CRAN.R-project.org/bin/linux/debian stable/\n(feel free to use a CRAN mirror instead of the master) to the file /etc/apt/sources.list, and install as usual. More details on installing and administering R on Debian Linux can be found at http://CRAN.R-project.org/bin/linux/debian/README . These backports should also be suitable for other Debian derivatives. Native backports for Ubuntu are provided by Vincent Goulet.\nNo other binary distributions are currently publically available via CRAN .\nA \u201clive\u201d Linux distribution with a particular focus on R is\nQuantian\n, which provides a directly bootable and self-configuring \u201cLive DVD\u201d containing numerous applications of interests to scientists and researchers, including several hundred CRAN and Bioconductor packages, the \u201cESS\u201d extensions for Emacs, the \u201cJGR\u201d Java GUI for R, the Ggobi visualization tool as well as several other R interfaces. The\nQuantian\nNext: Citing R , Previous: Are there Unix binaries for R? , Up: R Basics\n2.7 What documentation exists for R?\nOnline documentation for most of the functions and variables in R exists, and can be printed on-screen by typing\nhelp(\n(or\n?\nname) at the R prompt, where name is the name of the topic help is sought for. (In the case of unary and binary operators and control-flow special forms, the name may need to be be quoted.)\nThis documentation can also be made available as one reference manual for on-line reading in HTML and PDF formats, and as hardcopy via LaTeX, see How can R be installed? . An up-to-date HTML version is always available for web browsing at http://stat.ethz.ch/R-manual/ .\nPrinted copies of the R reference manual for some version(s) are available from Network Theory Ltd, at http://www.network-theory.co.uk/R/base/ . For each set of manuals sold, the publisher donates USD 10 to the R Foundation (see What is the R Foundation? ).\nThe R distribution also comes with the following manuals.\n\u201cAn Introduction to R\u201d (R-intro) includes information on data types, programming elements, statistical modeling and graphics. This document is based on the \u201cNotes on S-Plus\u201d by Bill Venables and David Smith.\n\u201cWriting R Extensions\u201d (R-exts) currently describes the process of creating R add-on packages, writing R documentation, R's system and foreign language interfaces, and the R API .\n\u201cR Data Import/Export\u201d (R-data) is a guide to importing and exporting data to and from R.\n\u201cThe R Language Definition\u201d (R-lang), a first version of the \u201cKernighan & Ritchie of R\u201d, explains evaluation, parsing, object oriented programming, computing on the language, and so forth.\n\u201cR Installation and Administration\u201d (R-admin).\n\u201cR Internals\u201d (R-ints) is a guide to R's internal structures. (Added in R 2.4.0.)\nBooks on R include\nP. Dalgaard (2002), \u201cIntroductory Statistics with R\u201d, Springer: New York, ISBN 0-387-95475-9, http://www.biostat.ku.dk/~pd/ISwR.html .\nJ. Fox (2002), \u201cAn R and S-Plus Companion to Applied Regression\u201d, Sage Publications, ISBN 0-761-92280-6 (softcover) or 0-761-92279-2 (hardcover), http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/ .\nJ. Maindonald and J. Braun (2003), \u201cData Analysis and Graphics Using R: An Example-Based Approach\u201d, Cambridge University Press, ISBN 0-521-81336-0, http://wwwmaths.anu.edu.au/~johnm/ .\nS. M. Iacus and G. Masarotto (2002), \u201cLaboratorio di statistica con R\u201d, McGraw-Hill, ISBN 88-386-6084-0 (in Italian),\nP. Murrell (2005), \u201cR Graphics\u201d, Chapman & Hall/CRC, ISBN: 1-584-88486-X, http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html .\nThe book\nW. N. Venables and B. D. Ripley (2002), \u201cModern Applied Statistics with S. Fourth Edition\u201d. Springer, ISBN 0-387-95457-0\nhas a home page at http://www.stats.ox.ac.uk/pub/MASS4/ providing additional material. Its companion is\nW. N. Venables and B. D. Ripley (2000), \u201cS Programming\u201d. Springer, ISBN 0-387-98966-8\nand provides an in-depth guide to writing software in the S language which forms the basis of both the commercial S-Plus and the Open Source R data analysis software systems. See http://www.stats.ox.ac.uk/pub/MASS3/Sprog/ for more information.\nIn addition to material written specifically or explicitly for R, documentation for S/S-Plus (see R and S ) can be used in combination with this FAQ (see What are the differences between R and S? ). Introductory books include\nP. Spector (1994), \u201cAn introduction to S and S-Plus\u201d, Duxbury Press.\nA. Krause and M. Olsen (2005), \u201cThe Basics of S-Plus\u201d (Fourth Edition). Springer, ISBN 0-387-26109-5.\nThe book\nJ. C. Pinheiro and D. M. Bates (2000), \u201cMixed-Effects Models in S and S-Plus\u201d, Springer, ISBN 0-387-98957-0\nprovides a comprehensive guide to the use of the nlme package for linear and nonlinear mixed-effects models.\nAs an example of how R can be used in teaching an advanced introductory statistics course, see\nD. Nolan and T. Speed (2000), \u201cStat Labs: Mathematical Statistics Through Applications\u201d, Springer Texts in Statistics, ISBN 0-387-98974-9\nThis integrates theory of statistics with the practice of statistics through a collection of case studies (\u201clabs\u201d), and uses R to analyze the data. More information can be found at http://www.stat.Berkeley.EDU/users/statlabs/ .\nLast, but not least, Ross' and Robert's experience in designing and implementing R is described in Ihaka & Gentleman (1996), \u201cR: A Language for Data Analysis and Graphics\u201d, Journal of Computational and Graphical Statistics , 5, 299\u2013314.\nAn annotated bibliography (BibTeX format) of R-related publications which includes most of the above references can be found at\n2.8 Citing R\nTo cite R in publications, use\n@Manual{,\n       title        = {R: A Language and Environment for Statistical\n                       Computing},\n       author       = {{R Development Core Team}},\n       organization = {R Foundation for Statistical Computing},\n       address      = {Vienna, Austria},\n       year         = 2007,\n       note         = {{ISBN} 3-900051-07-0},\n       url          = {http://www.R-project.org}\n     }\nCitation strings (or BibTeX entries) for R and R packages can also be obtained by citation().\n2.9 What mailing lists exist for R?\nThanks to Martin Maechler , there are four mailing lists devoted to R.\nR-announce\nA moderated list for major announcements about the development of R and the availability of new code.\nR-packages\nA moderated list for announcements on the availability of new or enhanced contributed packages.\nR-help\nThe `main' R mailing list, for discussion about problems and solutions using R, announcements (not covered by `R-announce' and `R-packages') about the development of R and the availability of new code.\nR-devel\nThis list is for questions and discussion about code development in R.\nPlease read the posting guide before sending anything to any mailing list.\nNote in particular that R-help is intended to be comprehensible to people who want to use R to solve problems but who are not necessarily interested in or knowledgeable about programming. Questions likely to prompt discussion unintelligible to non-programmers (e.g., questions involving C or C++) should go to R-devel.\nConvenient access to information on these lists, subscription, and archives is provided by the web interface at http://stat.ethz.ch/mailman/listinfo/ . One can also subscribe (or unsubscribe) via email, e.g. to R-help by sending subscribe (or unsubscribe) in the body of the message (not in the subject!) to R-help-request@lists.R-project.org .\nSend email to R-help@lists.R-project.org to send a message to everyone on the R-help mailing list. Subscription and posting to the other lists is done analogously, with R-help replaced by R-announce, R-packages, and R-devel, respectively. Note that the R-announce and R-packages lists are gatewayed into R-help. Hence, you should subscribe to either of them only in case you are not subscribed to R-help.\nIt is recommended that you send mail to R-help rather than only to the R Core developers (who are also subscribed to the list, of course). This may save them precious time they can use for constantly improving R, and will typically also result in much quicker feedback for yourself.\nOf course, in the case of bug reports it would be very helpful to have code which reliably reproduces the problem. Also, make sure that you include information on the system and version of R being used. See R Bugs for more details.\nSee http://www.R-project.org/mail.html for more information on the R mailing lists.\nThe R Core Team can be reached at R-core@lists.R-project.org for comments and reports.\nMany of the R project's mailing lists are also available via Gmane , from which they can be read with a web browser, using an NNTP news reader, or via RSS feeds. See http://dir.gmane.org/index.php?prefix=gmane.comp.lang.r. for the available mailing lists, and http://www.gmane.org/rss.php for details on RSS feeds.\nNext: Can I use R for commercial purposes? , Previous: What mailing lists exist for R? , Up: R Basics\n2.10 What is CRAN ?\nThe \u201cComprehensive R Archive Network\u201d ( CRAN ) is a collection of sites which carry identical material, consisting of the R distribution(s), the contributed extensions, documentation for R, and binaries.\nThe CRAN master site at TU Wien, Austria, can be found at the URL\nhttp://cran.za.R-project.org/\n(Rhodes U, South Africa)\nSee http://CRAN.R-project.org/mirrors.html for a complete list of mirrors. Please use the CRAN site closest to you to reduce network load.\nFrom CRAN , you can obtain the latest official release of R, daily snapshots of R (copies of the current source trees), as gzipped and bzipped tar files, a wealth of additional contributed code, as well as prebuilt binaries for various operating systems (Linux, Mac OS Classic, Mac OS X, and MS Windows). CRAN also provides access to documentation on R, existing mailing lists and the R Bug Tracking system.\nTo \u201csubmit\u201d to CRAN , simply upload to ftp://CRAN.R-project.org/incoming/ and send an email to CRAN@R-project.org . Note that CRAN generally does not accept submissions of precompiled binaries due to security reasons. In particular, binary packages for Windows and Mac OS X are provided by the respective binary package maintainers.\nNote: It is very important that you indicate the copyright (license) information ( GPL , BSD , Artistic,\n...\nNext: Why is R named R? , Previous: What is CRAN? , Up: R Basics\n2.11 Can I use R for commercial purposes?\nR is released under the GNU General Public License (GPL) . If you have any questions regarding the legality of using R in any particular situation you should bring it up with your legal counsel. We are in no position to offer legal advice.\nIt is the opinion of the R Core Team that one can use R for commercial purposes (e.g., in business or in consulting). The GPL, like all Open Source licenses, permits all and any use of the package. It only restricts distribution of R or of other programs containing code from R. This is made clear in clause 6 (\u201cNo Discrimination Against Fields of Endeavor\u201d) of the Open Source Definition :\nThe license must not restrict anyone from making use of the program in a specific field of endeavor. For example, it may not restrict the program from being used in a business, or from being used for genetic research.\nIt is also explicitly stated in clause 0 of the GPL, which says in part\nActivities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program.\nMost add-on packages, including all recommended ones, also explicitly allow commercial use in this way. A few packages are restricted to \u201cnon-commercial use\u201d; you should contact the author to clarify whether these may be used or seek the advice of your legal counsel.\nNone of the discussion in this section constitutes legal advice. The R Core Team does not provide legal advice under any circumstances.\nNext: What is the R Foundation? , Previous: Can I use R for commercial purposes? , Up: R Basics\n2.12 Why is R named R?\nThe name is partly based on the (first) names of the first two R authors (Robert Gentleman and Ross Ihaka), and partly a play on the name of the Bell Labs language `S' (see What is S? ).\nPrevious: Why is R named R? , Up: R Basics\n2.13 What is the R Foundation?\nThe R Foundation is a not for profit organization working in the public interest. It was founded by the members of the R Core Team in order to provide support for the R project and other innovations in statistical computing, provide a reference point for individuals, institutions or commercial enterprises that want to support or interact with the R development community, and to hold and administer the copyright of R software and documentation. See http://www.R-project.org/foundation/ for more information.\nNext: What is S-PLUS? , Previous: R and S , Up: R and S\n3.1 What is S?\nS is a very high level language and an environment for data analysis and graphics. In 1998, the Association for Computing Machinery ( ACM ) presented its Software System Award to John M. Chambers, the principal designer of S, for\nthe S system, which has forever altered the way people analyze, visualize, and manipulate data\n...\nS is an elegant, widely accepted, and enduring software system, with conceptual integrity, thanks to the insight, taste, and effort of John Chambers.\nThe evolution of the S language is characterized by four books by John Chambers and coauthors, which are also the primary references for S.\nRichard A. Becker and John M. Chambers (1984), \u201cS. An Interactive Environment for Data Analysis and Graphics,\u201d Monterey: Wadsworth and Brooks/Cole.\nThis is also referred to as the \u201cBrown Book\u201d, and of historical interest only.\nRichard A. Becker, John M. Chambers and Allan R. Wilks (1988), \u201cThe New S Language,\u201d London: Chapman & Hall.\nThis book is often called the \u201cBlue Book\u201d, and introduced what is now known as S version 2.\nJohn M. Chambers and Trevor J. Hastie (1992), \u201cStatistical Models in S,\u201d London: Chapman & Hall.\nThis is also called the \u201cWhite Book\u201d, and introduced S version 3, which added structures to facilitate statistical modeling in S.\nJohn M. Chambers (1998), \u201cProgramming with Data,\u201d New York: Springer, ISBN 0-387-98503-4 ( http://cm.bell-labs.com/cm/ms/departments/sia/Sbook/ ).\nThis \u201cGreen Book\u201d describes version 4 of S, a major revision of S designed by John Chambers to improve its usefulness at every stage of the programming process.\nSee http://cm.bell-labs.com/cm/ms/departments/sia/S/history.html for further information on \u201cStages in the Evolution of S\u201d.\nThere is a huge amount of user-contributed code for S, available at the S Repository at CMU .\nNext: What are the differences between R and S? , Previous: What is S? , Up: R and S\n3.2 What is S-Plus?\nS-Plus is a value-added version of S sold by Insightful Corporation. Based on the S language, S-Plus provides functionality in a wide variety of areas, including robust regression, modern non-parametric regression, time series, survival analysis, multivariate analysis, classical statistical tests, quality control, and graphics drivers. Add-on modules add additional capabilities.\nNext: Is there anything R can do that S-PLUS cannot? , Previous: What is S-PLUS? , Up: R and S\n3.3 What are the differences between R and S?\nWe can regard S as a language with three current implementations or \u201cengines\u201d, the \u201cold S engine\u201d (S version 3; S-Plus 3.x and 4.x), the \u201cnew S engine\u201d (S version 4; S-Plus 5.x and above), and R. Given this understanding, asking for \u201cthe differences between R and S\u201d really amounts to asking for the specifics of the R implementation of the S language, i.e., the difference between the R and S engines.\nFor the remainder of this section, \u201cS\u201d refers to the S engines and not the S language.\nNext: Models , Previous: What are the differences between R and S? , Up: What are the differences between R and S?\n3.3.1 Lexical scoping\nContrary to other implementations of the S language, R has adopted an evaluation model in which nested function definitions are lexically scoped. This is analogous to the evalutation model in Scheme.\nThis difference becomes manifest when free variables occur in a function. Free variables are those which are neither formal parameters (occurring in the argument list of the function) nor local variables (created by assigning to them in the body of the function). In S, the values of free variables are determined by a set of global variables (similar to C, there is only local and global scope). In R, they are determined by the environment in which the function was created.\nConsider the following function:\ncube <- function(n) {\n       sq <- function() n * n\n       n * sq()\n     }\nUnder S, sq() does not \u201cknow\u201d about the variable n unless it is defined globally:\nS> cube(2)\n     Error in sq():  Object \"n\" not found\n     Dumped\n     S> n <- 3\n     S> cube(2)\n     [1] 18\nIn R, the \u201cenvironment\u201d created when cube() was invoked is also looked in:\nR> cube(2)\n     [1] 8\nAs a more \u201cinteresting\u201d real-world problem, suppose you want to write a function which returns the density function of the r-th order statistic from a sample of size n from a (continuous) distribution. For simplicity, we shall use both the cdf and pdf of the distribution as explicit arguments. (Example compiled from various postings by Luke Tierney.)\nThe S-Plus documentation for call() basically suggests the following:\ndorder <- function(n, r, pfun, dfun) {\n       f <- function(x) NULL\n       con <- round(exp(lgamma(n + 1) - lgamma(r) - lgamma(n - r + 1)))\n       PF <- call(substitute(pfun), as.name(\"x\"))\n       DF <- call(substitute(dfun), as.name(\"x\"))\n       f[[length(f)]] <-\n         call(\"*\", con,\n              call(\"*\", call(\"^\", PF, r - 1),\n                   call(\"*\", call(\"^\", call(\"-\", 1, PF), n - r),\n                        DF)))\n       f\n     }\nRather tricky, isn't it? The code uses the fact that in S, functions are just lists of special mode with the function body as the last argument, and hence does not work in R (one could make the idea work, though).\nA version which makes heavy use of substitute() and seems to work under both S and R is\ndorder <- function(n, r, pfun, dfun) {\n       con <- round(exp(lgamma(n + 1) - lgamma(r) - lgamma(n - r + 1)))\n       eval(substitute(function(x) K * PF(x)^a * (1 - PF(x))^b * DF(x),\n                       list(PF = substitute(pfun), DF = substitute(dfun),\n                            a = r - 1, b = n - r, K = con)))\n     }\n(the eval() is not needed in S).\nHowever, in R there is a much easier solution:\ndorder <- function(n, r, pfun, dfun) {\n       con <- round(exp(lgamma(n + 1) - lgamma(r) - lgamma(n - r + 1)))\n       function(x) {\n         con * pfun(x)^(r - 1) * (1 - pfun(x))^(n - r) * dfun(x)\n       }\n     }\nThis seems to be the \u201cnatural\u201d implementation, and it works because the free variables in the returned function can be looked up in the defining environment (this is lexical scope).\nNote that what you really need is the function closure, i.e., the body along with all variable bindings needed for evaluating it. Since in the above version, the free variables in the value function are not modified, you can actually use it in S as well if you abstract out the closure operation into a function MC() (for \u201cmake closure\u201d):\ndorder <- function(n, r, pfun, dfun) {\n       con <- round(exp(lgamma(n + 1) - lgamma(r) - lgamma(n - r + 1)))\n       MC(function(x) {\n            con * pfun(x)^(r - 1) * (1 - pfun(x))^(n - r) * dfun(x)\n          },\n          list(con = con, pfun = pfun, dfun = dfun, r = r, n = n))\n     }\nGiven the appropriate definitions of the closure operator, this works in both R and S, and is much \u201ccleaner\u201d than a substitute/eval solution (or one which overrules the default scoping rules by using explicit access to evaluation frames, as is of course possible in both R and S).\nFor R, MC() simply is\nMC <- function(f, env) f\n(lexical scope!), a version for S is\nMC <- function(f, env = NULL) {\n       env <- as.list(env)\n       if (mode(f) != \"function\")\n         stop(paste(\"not a function:\", f))\n       if (length(env) > 0 && any(names(env) == \"\"))\n         stop(paste(\"not all arguments are named:\", env))\n       fargs <- if(length(f) > 1) f[1:(length(f) - 1)] else NULL\n       fargs <- c(fargs, env)\n       if (any(duplicated(names(fargs))))\n         stop(paste(\"duplicated arguments:\", paste(names(fargs)),\n              collapse = \", \"))\n       fbody <- f[length(f)]\n       cf <- c(fargs, fbody)\n       mode(cf) <- \"function\"\n       return(cf)\n     }\nSimilarly, most optimization (or zero-finding) routines need some arguments to be optimized over and have other parameters that depend on the data but are fixed with respect to optimization. With R scoping rules, this is a trivial problem; simply make up the function with the required definitions in the same environment and scoping takes care of it. With S, one solution is to add an extra parameter to the function and to the optimizer to pass in these extras, which however can only work if the optimizer supports this.\nNested lexically scoped functions allow using function closures and maintaining local state. A simple example (taken from Abelson and Sussman) is obtained by typing\ndemo(\"scoping\")\nat the R prompt. Further information is provided in the standard R reference \u201cR: A Language for Data Analysis and Graphics\u201d (see What documentation exists for R? ) and in Robert Gentleman and Ross Ihaka (2000), \u201cLexical Scope and Statistical Computing\u201d, Journal of Computational and Graphical Statistics , 9, 491\u2013508.\nNested lexically scoped functions also imply a further major difference. Whereas S stores all objects as separate files in a directory somewhere (usually .Data under the current directory), R does not. All objects in R are stored internally. When R is started up it grabs a piece of memory and uses it to store the objects. R performs its own memory management of this piece of memory, growing and shrinking its size as needed. Having everything in memory is necessary because it is not really possible to externally maintain all relevant \u201cenvironments\u201d of symbol/value pairs. This difference also seems to make R faster than S.\nThe down side is that if R crashes you will lose all the work for the current session. Saving and restoring the memory \u201cimages\u201d (the functions and data stored in R's internal memory at any time) can be a bit slow, especially if they are big. In S this does not happen, because everything is saved in disk files and if you crash nothing is likely to happen to them. (In fact, one might conjecture that the S developers felt that the price of changing their approach to persistent storage just to accommodate lexical scope was far too expensive.) Hence, when doing important work, you might consider saving often (see How can I save my workspace? ) to safeguard against possible crashes. Other possibilities are logging your sessions, or have your R commands stored in text files which can be read in using source().\nNote: If you run R from within Emacs (see R and Emacs ), you can save the contents of the interaction buffer to a file and conveniently manipulate it using ess-transcript-mode, as well as save source copies of all functions and data used.\n3.3.2 Models\nThere are some differences in the modeling code, such as\nWhereas in S, you would use lm(y ~ x^3) to regress y on x^3, in R, you have to insulate powers of numeric vectors (using I()), i.e., you have to use lm(y ~ I(x^3)).\nThe glm family objects are implemented differently in R and S. The same functionality is available but the components have different names.\nOption na.action is set to \"na.omit\" by default in R, but not set in S.\nTerms objects are stored differently. In S a terms object is an expression with attributes, in R it is a formula with attributes. The attributes have the same names but are mostly stored differently.\nFinally, in R y ~ x + 0 is an alternative to y ~ x - 1 for specifying a model with no intercept. Models with no parameters at all can be specified by y ~ 0.\nPrevious: Models , Up: What are the differences between R and S?\n3.3.3 Others\nApart from lexical scoping and its implications, R follows the S language definition in the Blue and White Books as much as possible, and hence really is an \u201cimplementation\u201d of S. There are some intentional differences where the behavior of S is considered \u201cnot clean\u201d. In general, the rationale is that R should help you detect programming errors, while at the same time being as compatible as possible with S.\nSome known differences are the following.\nIn R, if x is a list, then x[i] <- NULL and x[[i]] <- NULL remove the specified elements from x. The first of these is incompatible with S, where it is a no-op. (Note that you can set elements to NULL using x[i] <- list(NULL).)\nIn S, the functions named .First and .Last in the .Data directory can be used for customizing, as they are executed at the very beginning and end of a session, respectively.\nIn R, the startup mechanism is as follows. R first sources the system startup file $R_HOME /library/base/R/Rprofile. Then, it searches for a site-wide startup profile unless the command line option --no-site-file was given. The name of this file is taken from the value of the R_PROFILE environment variable. If that variable is unset, the default is $R_HOME/etc/Rprofile.site ($R_HOME/etc/Rprofile in versions prior to 1.4.0). This code is loaded in package base. Then, unless --no-init-file was given, R searches for a file called .Rprofile in the current directory or in the user's home directory (in that order) and sources it into the user workspace. It then loads a saved image of the user workspace from .RData in case there is one (unless --no-restore was specified). If needed, the functions .First() and .Last() should be defined in the appropriate startup profiles.\nIn R, T and F are just variables being set to TRUE and FALSE, respectively, but are not reserved words as in S and hence can be overwritten by the user. (This helps e.g. when you have factors with levels \"T\" or \"F\".) Hence, when writing code you should always use TRUE and FALSE.\nIn R, dyn.load() can only load shared objects, as created for example by\nR CMD SHLIB\n.\nIn R, attach() currently only works for lists and data frames, but not for directories. (In fact, attach() also works for R data files created with save(), which is analogous to attaching directories in S.) Also, you cannot attach at position 1.\nCategories do not exist in R, and never will as they are deprecated now in S. Use factors instead.\nIn R, For() loops are not necessary and hence not supported.\nIn R, assign() uses the argument envir= rather than where= as in S.\nThe random number generators are different, and the seeds have different length.\nR passes integer objects to C as int * rather than long * as in S.\nR has no single precision storage mode. However, as of version 0.65.1, there is a single precision interface to C/FORTRAN subroutines.\nBy default, ls() returns the names of the objects in the current (under R) and global (under S) environment, respectively. For example, given\nx <- 1; fun <- function() {y <- 1; ls()}\nthen fun() returns \"y\" in R and \"x\" (together with the rest of the global environment) in S.\nR allows for zero-extent matrices (and arrays, i.e., some elements of the dim attribute vector can be 0). This has been determined a useful feature as it helps reducing the need for special-case tests for empty subsets. For example, if x is a matrix, x[, FALSE] is not NULL but a \u201cmatrix\u201d with 0 columns. Hence, such objects need to be tested for by checking whether their length() is zero (which works in both R and S), and not using is.null().\nNamed vectors are considered vectors in R but not in S (e.g., is.vector(c(a = 1:3)) returns FALSE in S and TRUE in R).\nData frames are not considered as matrices in R (i.e., if DF is a data frame, then is.matrix(DF) returns FALSE in R and TRUE in S).\nR by default uses treatment contrasts in the unordered case, whereas S uses the Helmert ones. This is a deliberate difference reflecting the opinion that treatment contrasts are more natural.\nIn R, the argument of a replacement function which corresponds to the right hand side must be named value. E.g., f(a) <- b is evaluated as a <- \"f<-\"(a, value = b). S always takes the last argument, irrespective of its name.\nIn S, substitute() searches for names for substitution in the given expression in three places: the actual and the default arguments of the matching call, and the local frame (in that order). R looks in the local frame only, with the special rule to use a \u201cpromise\u201d if a variable is not evaluated. Since the local frame is initialized with the actual arguments or the default expressions, this is usually equivalent to S, until assignment takes place.\nIn S, the index variable in a for() loop is local to the inside of the loop. In R it is local to the environment where the for() statement is executed.\nIn S, tapply(simplify=TRUE) returns a vector where R returns a one-dimensional array (which can have named dimnames).\nIn S(-Plus) the C locale is used, whereas in R the current operating system locale is used for determining which characters are alphanumeric and how they are sorted. This affects the set of valid names for R objects (for example accented chars may be allowed in R) and ordering in sorts and comparisons (such as whether \"aA\" < \"Bb\" is true or false). From version 1.2.0 the locale can be (re-)set in R by the Sys.setlocale() function.\nIn S, missing(arg) remains TRUE if arg is subsequently modified; in R it doesn't.\nFrom R version 1.3.0, data.frame strips I() when creating (column) names.\nIn R, the string \"NA\" is not treated as a missing value in a character variable. Use as.character(NA) to create a missing character value.\nR disallows repeated formal arguments in function calls.\nIn S, dump(), dput() and deparse() are essentially different interfaces to the same code. In R from version 2.0.0, this is only true if the same control argument is used, but by default it is not. By default dump() tries to write code that will evaluate to reproduce the object, whereas dput() and deparse() default to options for producing deparsed code that is readable.\nIn R, indexing a vector, matrix, array or data frame with [ using a character vector index looks only for exact matches (whereas [[ and $ allow partial matches). In S, [ allows partial matches.\nS has a two-argument version of atan and no atan2. A call in S such as atan(x1, x2) is equivalent to R's atan2(x1, x2). However, beware of named arguments since S's atan(x = a, y = b) is equivalent to R's atan2(y = a, x = b) with the meanings of x and y interchanged. (R used to have undocumented support for a two-argument atan with positional arguments, but this has been withdrawn to avoid further confusion.)\nNumeric constants with no fractional and exponent (i.e., only integer) part are taken as integer in S-Plus 6.x or later, but as double in R.\nThere are also differences which are not intentional, and result from missing or incorrect code in R. The developers would appreciate hearing about any deficiencies you may find (in a written report fully documenting the difference as you see it). Of course, it would be useful if you were to implement the change yourself and make sure it works.\nNext: What is R-plus? , Previous: What are the differences between R and S? , Up: R and S\n3.4 Is there anything R can do that S-Plus cannot?\nSince almost anything you can do in R has source code that you could port to S-Plus with little effort there will never be much you can do in R that you couldn't do in S-Plus if you wanted to. (Note that using lexical scoping may simplify matters considerably, though.)\nR offers several graphics features that S-Plus does not, such as finer handling of line types, more convenient color handling (via palettes), gamma correction for color, and, most importantly, mathematical annotation in plot texts, via input expressions reminiscent of TeX constructs. See the help page for plotmath, which features an impressive on-line example. More details can be found in Paul Murrell and Ross Ihaka (2000), \u201cAn Approach to Providing Mathematical Annotation in Plots\u201d, Journal of Computational and Graphical Statistics , 9, 582\u2013599.\n3.5 What is R-plus?\nFor a very long time, there was no such thing.\nXLSolutions Corporation is currently beta testing a commercially supported version of R named R+ (read R plus).\nIn addition, REvolution Computing has released RPro , an enterprise-class statistical analysis system based on R, suitable for deployment in professional, commercial and regulated environments.\nNext: R Add-On Packages , Previous: R and S , Up: Top\n4 R Web Interfaces\nRweb is developed and maintained by Jeff Banfield . The Rweb Home Page provides access to all three versions of Rweb\u2014a simple text entry form that returns output and graphs, a more sophisticated Javascript version that provides a multiple window environment, and a set of point and click modules that are useful for introductory statistics courses and require no knowledge of the R language. All of the Rweb versions can analyze Web accessible datasets if a URL is provided.\nThe paper \u201cRweb: Web-based Statistical Analysis\u201d, providing a detailed explanation of the different versions of Rweb and an overview of how Rweb works, was published in the Journal of Statistical Software ( http://www.jstatsoft.org/v04/i01/ ).\nUlf Bartel has developed R-Online, a simple on-line programming environment for R which intends to make the first steps in statistical programming with R (especially with time series) as easy as possible. There is no need for a local installation since the only requirement for the user is a JavaScript capable browser. See http://osvisions.com/r-online/ for more information.\nRcgi is a CGI WWW interface to R by MJ Ray . It had the ability to use \u201cembedded code\u201d: you could mix user input and code, allowing the HTML author to do anything from load in data sets to enter most of the commands for users without writing CGI scripts. Graphical output was possible in PostScript or GIF formats and the executed code was presented to the user for revision. However, it is not clear if the project is still active. Currently, a modified version of Rcgi by Mai Zhou (actually, two versions: one with (bitmap) graphics and one without) as well as the original code are available from http://www.ms.uky.edu/~statweb/ .\nCGI-based web access to R is also provided at http://hermes.sdu.dk/cgi-bin/go/ . There are many additional examples of web interfaces to R which basically allow to submit R code to a remote server, see for example the collection of links available from http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/StatCompCourse .\nDavid Firth has written CGIwithR, an R add-on package available from CRAN . It provides some simple extensions to R to facilitate running R scripts through the CGI interface to a web server, and allows submission of data using both GET and POST methods. It is easily installed using Apache under Linux and in principle should run on any platform that supports R and a web server provided that the installer has the necessary security permissions. David's paper \u201cCGIwithR: Facilities for Processing Web Forms Using R\u201d was published in the Journal of Statistical Software ( http://www.jstatsoft.org/v08/i10/ ). The package is now maintained by Duncan Temple Lang and has a web page at http://www.omegahat.org/CGIwithR/ .\nRpad , developed and actively maintained by Tom Short, provides a sophisticated environment which combines some of the features of the previous approaches with quite a bit of Javascript, allowing for a GUI -like behavior (with sortable tables, clickable graphics, editable output), etc.\nJeff Horner is working on the R/Apache Integration Project which embeds the R interpreter inside Apache 2 (and beyond). A tutorial and presentation are available from the project web page at http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/RApacheProject .\nRserve is a project actively developed by Simon Urbanek. It implements a TCP/IP server which allows other programs to use facilities of R. Clients are available from the web site for Java and C++ (and could be written for other languages that support TCP/IP sockets).\nOpenStatServer is being developed by a team lead by Greg Warnes; it aims \u201cto provide clean access to computational modules defined in a variety of computational environments (R, SAS, Matlab, etc) via a single well-defined client interface\u201d and to turn computational services into web services.\nTwo projects use PHP to provide a web interface to R. R_PHP_Online by Steve Chen (though it is unclear if this project is still active) is somewhat similar to the above Rcgi and Rweb. R-php is actively developed by Alfredo Pontillo and Angelo Mineo and provides both a web interface to R and a set of pre-specified analyses that need no R code input.\nwebbioc is \u201can integrated web interface for doing microarray analysis using several of the Bioconductor packages\u201d and is designed to be installed at local sites as a shared computing resource.\nFinally, Rwui is a web application to to create user-friendly web interfaces for R scripts. All code for the web interface is created automatically. There is no need for the user to do any extra scripting or learn any new scripting techniques.\n"}, {"score": 889.0373, "uuid": "06e91b31-b1ad-59fc-af1d-f09e672fe1d5", "index": "cw12", "trec_id": "clueweb12-0002wb-79-20820", "target_hostname": "bugs.python.org", "target_uri": "http://bugs.python.org/issue4258", "page_rank": 1.2935047e-09, "spam_rank": 95, "title": "Issue 4258: Use 30-bit digits instead of 15-bit digits <em>for</em> <em>Python</em> integers", "snippet": "The base <em>should</em> be independent of the implementation, like <em>Python</em> does with text: UTF-8 <em>for</em> files and UCS-4 in memory.", "explanation": null, "document": "Author: Mark Dickinson (mark.dickinson) *\nDate: 2008-11-04 10:45\nHere's an experimental patch, against the py3k branch, that makes Python \nrepresent its long integers internally in base 2**30 instead of base \n2**15, on platforms that have 32-bit and 64-bit integers available.\n\nOn platforms for which autoconf is unable to find both 32-bit and 64-bit \nintegers, the representation falls back to the usual one.\n\n\nSee also issue 1814 (GMP for longs), and the discussion at http://mail.python.org/pipermail/python-dev/2008-November/083315.html (note particularly Tim Peter's message at: http://mail.python.org/pipermail/python-dev/2008-November/083355.html )\nAuthor: Mark Dickinson (mark.dickinson) *\nDate: 2008-11-04 11:04\nNote that to avoid \"bad marshal data\" errors, you'll probably need to do a \n'make distclean' before rebuilding with this patch.\nAuthor: Mark Dickinson (mark.dickinson) *\nDate: 2008-11-04 15:28\nHere's an updated patch, with the following changes from the original:\n\n- make the size of a digit (both the conceptual size\nin bits and actual size in bytes) available to Python\nvia a new structseq sys.int_info.  This information\nis useful for the sys.getsizeof tests.\n\n- fix a missing cast in long_hash\n\n- better fast path for 1-by-1 multiplication that\ndoesn't go via PyLong_FromLongLong.\nAuthor: STINNER Victor (haypo) *\nDate: 2008-11-05 00:29\n> Note that to avoid \"bad marshal data\" errors, \n> you'll probably need to do a 'make distclean' \n> before rebuilding with this patch.\n\nI saw that you choosed to use the base 2^30 for marshal. For a better \nportability (be able to use .pyc generated without your patch), you \nmay keep the base 2^15. I implemented that in my GMP patch (manual \nconversion from/to base 2^15).\n\nIf we change the marshal format of long, the magic number should be \ndifferent (we might use a tag like the \"full unicode\" tag used in \nPython3 magic number) and/or the bytecode (actual bytecode is 'l'). \nThe base should be independent of the implementation, like Python does \nwith text: UTF-8 for files and UCS-4 in memory. We may use the base \n2^8 (256) or another power or 2^8 (2^16, 2^32, 2^64?). The base 256 \nsounds interresting because any CPU is able to process 8 bits digits.\n\nCons: Use a different bases makes Python slower for loading/writing \nfrom/to .pyc.\nAuthor: Gregory P. Smith (gregory.p.smith) *\nDate: 2008-11-05 00:57\noh yay, thanks.  it looks like you did approximately what i had started\nworking on testing a while back but have gone much further and added\nautoconf magic to try and determine when which size should be used.  good.\n\n(i haven't reviewed your autoconf stuff yet)\n\nAs for marhsalled data and pyc compatibility, yes that is important to\nconsider.\n\nWe should probably base the decision on which digit size to use\ninternally on benchmarks, not just if the platform can support 64bit\nints.  Many archs support 64bit numbers as a native C type but require\nmultiple instructions or are very slow when doing it.\n\n(embedded arm, mips or ppc come to mind as obvious things to test that on)\nAuthor: STINNER Victor (haypo) *\nDate: 2008-11-05 08:55\n> As for marhsalled data and pyc compatibility, yes that is important to\n> consider.\n\nThe problem is also that with the 30-bit digit patch, some Python will use 15 \nbits whereas some other will use 30 bits. The base in marshal should be the \nsame in both cases.\n\nAnd why 30 bits and not 31 bits, or 63 bits, or 120 bits? We may use other \nbases in the future. That's why I prefer to use a common base like base 256. \nAnd GMP has functions (mpz_import) to load data in base 256, but it's more \ncomplicate to load data in base 2^15.\nAuthor: Mark Dickinson (mark.dickinson) *\nDate: 2008-11-05 10:53\n[Victor Stinner]\n> I saw that you choosed to use the base 2^30 for marshal. For a better \n> portability (be able to use .pyc generated without your patch), you \n> may keep the base 2^15. I implemented that in my GMP patch (manual \n> conversion from/to base 2^15).\n\nAgreed.  I'll fix this so that the .pyc format is unchanged. Thanks!\n\n> And why 30 bits and not 31 bits, or 63 bits, or 120 bits?\n\nMostly laziness: the change from 15 to 30 bits turned out to be extraordinarily easy.  Note that the longobject.c part of \nthe patch does almost nothing except adding a bunch of casts here and there.\n\n31 bits would involve rewriting the powering algorithm, which assumes that PyLong_SHIFT is divisible by 5.  It would gain \nvery little over 30 bits, and if Pernici Mario's optimizations are considered ( issue 3944 ) multiplication would likely be \nslower with 31-bit digits than with 30-bit digits.\n\n63 (or 62, or 60) bits is simply too large right now:  you'd need access to a hardware 64 x 64 -> 128 bit multiply to make \nthis worth it, and I'd guess there are many fewer platforms that make this easy, though I don't really know.  I know it's \npossible on gcc/x86_64 by making use of the (undocumented) __uint128_t type.  But even where this type is available, base \n2**63 might still be slower than base 2**30. I've done some experiments with multiprecision *decimal* arithmetic in C that \nshowed that even on a 64-bit machine, using base 10**9 (i.e. approx 30 bits) was significantly faster than using base \n10**18.\n\n120 bits?  Does GMP even go this far?  I guess part of the attraction is that it's a multiple of 8...\n\nThe other obvious choices to consider would be 32 bits (or 16 bits, or 64 bits).  This is possible, and may even be worth \nit, but it would be a *much* bigger effort; most of the algorithms would need to be rewritten.  One problem is again the \nmismatch between C and assembler:  detecting overflow when adding two 32-bit unsigned integers is trivial in x86 assembler, \nbut it's not so obvious how to write portable C code that has a decent chance of being compiled optimally on a wide variety \nof compilers. \n\nI guess my feeling is simply that the 15-bit to 30-bit change seems incredibly easy and cheap: very little code change, and \nhence low risk of accidental breakage.  So if there are indeed significant efficiency benefits (still to be determined) then \nit looks like a clear win to me.\n\n[Gregory Smith]\n> (i haven't reviewed your autoconf stuff yet)\n\nThe changes to configure and pyconfig.h are just from rerunning autoconf and autoheader;  it's only configure.in that should \nneed looking at.\nAuthor: STINNER Victor (haypo) *\nDate: 2008-11-05 11:24\n> > And why 30 bits and not 31 bits, or 63 bits, or 120 bits?\n>\n> Mostly laziness (...)\n\nIt was an argument for changing the base used by the mashal :-)\n\n> 31 bits would involve rewriting the powering algorithm, which assumes that\n> PyLong_SHIFT is divisible by 5\n\nPowering is an simple algorithm. If it was the division, it would be much \nharder :-) Python stores the sign of the number in the first digit. Because \nof that, we are limited to 15/30 bits. Storing the sign in the size (which \nsize? no idea yet) would allows to use a bigger base (31 bits? 63 bits?).\n\n> One problem is again the mismatch between C and assembler:  detecting\n> overflow when adding two 32-bit unsigned integers is trivial in x86\n> assembler, but it's not so obvious how to write portable C code that has a\n> decent chance of being compiled optimally on a wide variety of compilers.\n\nI wrote an example to detect overflows in C on the mailing list. Copy of my \nemail:\n------------------------------- 8< ----------------------\nAbout 31, 32, 63 or 64 bits: I guess that you want to avoid integer overflow. \nIntel has an \"overflow\" flag, changed for all instructions. For other CPUs, \nyou can use emulate this flag. Example with the type int that I used in my \nGMP patch:\n\nAdd:\n  int a, b, sum;\n  sum = a + b;\n  exact = ((a < 0) ^ (b < 0)) || ((a < 0) == (sum < 0));\n\nSubstract:\n  int a, b, diff;\n  diff = a + b;\n  exact = ((a < 0) == (b < 0)) || ((a < 0) == (diff < 0));\n\nMultiply:\n  int a, b, product;\n  if (a == 0 || b == 0) {\n     product = 0;  /* exact */\n  } else if (a != INT_MIN || (b == 1)) {\n     product = a * b;\n     exact = (product / b) == a);\n  } else {\n     /* INT_MIN * -1 = -INT_MIN: overflow */\n  }\n\nDivide:\n  int a, b, q;\n  if (a != INT_MIN || b != -1) {\n     q = a / b;   /* exact */\n  } else {\n     /* INT_MIN / -1 = -INT_MIN: overflow */\n  }\n\nChecking overflow may costs more than using a smaller base. Only a benchmark \ncan answer ;-)\n------------------------------- 8< ----------------------\n\n> I guess my feeling is simply that the 15-bit to 30-bit change seems\n> incredibly easy and cheap: very little code change, and hence low risk of\n> accidental breakage.\n\nPython has an amazing regression test suite! I used it to fix my GMP patch. We \ncan experiment new bases using this suite.\n\nAnyway, i love the idea of using 30 bits instead of 15! Most computer are now \n32 or 64 bits! But it's safe to keep the 15 bits version to support older \ncomputers or buggy compilers.\n\nI started to work with GIT. You may be interrested to work together on GIT. \nIt's much easier to exchanges changeset and play with branches. I will try to \npublish my GIT tree somewhere.\nAuthor: Mark Dickinson (mark.dickinson) *\nDate: 2008-11-05 23:04\nFollowing Victor's suggestion, here's an updated patch; same as before, \nexcept that marshal now uses base 2**15 for reading and writing, \nindependently of whether PyLong_SHIFT is 15 or 30.\nAuthor: STINNER Victor (haypo) *\nDate: 2008-11-05 23:13\nMark: would it be possible to keep the \"2 digits\" hack in \nPyLong_FromLong, especially with base 2^15? Eg. \"#if PyLong_SHIFT == \n15\". The base 2^15 slow, so don't make it slower :-)\n\n-\t/* 2 digits */\n-\tif (!(ival >> 2*PyLong_SHIFT)) {\n-\t\tv = _PyLong_New(2);\n-\t\tif (v) {\n-\t\t\tPy_SIZE(v) = 2*sign;\n-\t\t\tv->ob_digit[0] = (digit)ival & PyLong_MASK;\n-\t\t\tv->ob_digit[1] = ival >> PyLong_SHIFT;\n-\t\t}\n-\t\treturn (PyObject*)v;\n-\t}\nAuthor: STINNER Victor (haypo) *\nDate: 2008-11-06 01:35\n> marshal now uses base 2**15 for reading and writing\n\nYes, it uses base 2**15 but it's not the correct conversion to base \n2**15. You convert each PyLong digit to base 2**15 but not the whole \nnumber. As a result, the format is different than the current mashal \nversion.\nAuthor: STINNER Victor (haypo) *\nDate: 2008-11-06 02:32\nPyLong_FromLong() doesn't go into the 1 digit special case for \nnegative number. You should use:\n\t/* Fast path for single-digits ints */\n\tif (!(abs_ival>>PyLong_SHIFT)) {\n\t\tv = _PyLong_New(1);\n\t\tif (v) {\n\t\t\tPy_SIZE(v) = sign;\n\t\t\tv->ob_digit[0] = abs_ival;\n\t\t}\n\t\treturn (PyObject*)v;\n\t}\nAuthor: Mark Dickinson (mark.dickinson) *\nDate: 2008-11-06 09:01\n> Yes, it uses base 2**15 but it's not the correct conversion to base \n> 2**15. You convert each PyLong digit to base 2**15 but not the whole \n> number.\n\nI don't understand:  yes, each base 2**30 digit is converted to a pair \nof base 2**15 digits, and if necessary (i.e., if the top 15 bits of the \nmost significant base 2**30 digit are zero) the size is adjusted.  How \nis this not converting the whole number?\n\n> As a result, the format is different than the current mashal version.\n\nCan you give an example of an integer n such that marshal.dumps(n) gives \nyou different results with and without the patch?  As far as I can tell, \nI'm getting the same marshal results both with the unpatched version and \nwith the patch applied.\nAuthor: Mark Dickinson (mark.dickinson) *\nDate: 2008-11-06 09:21\nOther responses...\n> It was an argument for changing the base used by the mashal :-)\n\nAh.  I think I'm with you now.  You're saying that ideally, marshal \nshouldn't have to care about how Python stores its longs:  it should \njust ask some function in longobject.c to provide an already-converted-\nto-base-256 representation.  Is that right?\n\nI also find the idea of making the marshal representation base 256 quite \nattractive.  There are already functions in longobject.c that could be \nused for this: _PyLong_{From,As}ByteArray.  And then you wouldn't need \nto touch marshal.c when swapping the GMP version of longobject.c in and \nout.\n\n> Python stores the sign of the number in the first digit. Because \n> of that, we are limited to 15/30 bits.\n\nNo: the sign is stored in the size:  if v is a PyLongObject then \nABS(Py_SIZE(v)) gives the number of digits in the absolute value of the \ninteger represented by v, and the sign of Py_SIZE(v) gives the sign of \nthe integer.\n\n> would it be possible to keep the \"2 digits\" hack in \n> PyLong_FromLong, especially with base 2^15? Eg. \"#if PyLong_SHIFT == \n> 15\". The base 2^15 slow, so don't make it slower :-)\n\nWhy don't we leave this kind of micro-optimization out until we've got \nsome benchmarks.  (I'm also tempted to get rid of the long_mul fast path \nfor now.)\n\n> PyLong_FromLong() doesn't go into the 1 digit special case for \n> negative number.\n\nWell spotted!  Yes, this should be fixed.  I have a nasty feeling that I \nwas responsible for introducing this bug some time ago...\nAuthor: Mark Dickinson (mark.dickinson) *\nDate: 2008-11-06 11:17\nHere's a pybench comparison, on OS X 10.5/Core 2 Duo/gcc 4.0.1 (32-bit \nnon-debug build of the py3k branch).  I got this by doing:\n\n[create clean build of py3k branch]\ndickinsm$ ./python.exe Tools/pybench/pybench.py -f bench_unpatched\n[apply 30bit patch and rebuild]\ndickinsm$ ./python.exe Tools/pybench/pybench.py -c bench_unpatched\n\nHighlights: SimpleLongArithmetic: around 10% faster.\n            SimpleComplexArithmetic: around 16% slower!\n            CompareFloatsIntegers: around 20% slower.\n\nI'll investigate the slowdowns.\nAuthor: STINNER Victor (haypo) *\nDate: 2008-11-06 11:30\n> I'll investigate the slowdowns\n\nThe problem may comes from int64_t on 32 bits CPU. 32x32 -> 64 may be \nemulated on your CPU and so it's slower. I improved your patch to make \nit faster, but I lost all my work because of a misuse of GIT... As I \nremember:\n - I fixed PyLong_FromLong() for small negative integer\n - I unrolled the loop in PyLong_FromLong(): the loop is at least \ncalled twice (the number has 2 digits or more)\n - I added special code for operations on two numbers of 1 digit \n(each) for long_add(), long_mul(), long_div(), long_bitwise(), etc.\n - and I don't remember the other changes...\n\nOh, I have an old patch. I will attach it to this message. About \nspeed, it was:\n * unpatched: 20600..20900 pystones\n * your patch: 19900..20100 pystones\n * + my changes: 20200..20400 pytones\nAuthor: STINNER Victor (haypo) *\nDate: 2008-11-06 12:47\nI wrote a patch to compute stat about PyLong function calls.\n\nmake (use setup.py):\n\nPyLong_FromLong: 168572 calls, min=( 0,  ), avg=(1.4,    ), max=(  3,    )\nlong_bool:        48682 calls, min=( 0,  ), avg=(0.2,    ), max=(  2,    )\nlong_add:         39527 calls, min=( 0, 0), avg=(0.9, 1.0), max=(  2,   3)\nlong_compare:     39145 calls, min=( 0, 0), avg=(1.2, 1.1), max=(  3,   3)\nPyLong_AsLong:    33689 calls, min=( 0,  ), avg=(0.9,    ), max=( 45,    )\nlong_sub:         13091 calls, min=( 0, 0), avg=(0.9, 0.8), max=(  1,   1)\nlong_bitwise:      4636 calls, min=( 0, 0), avg=(0.8, 0.6), max=(  2,   2)\nlong_hash:         1097 calls, min=( 0,  ), avg=(0.9,    ), max=(  3,    )\nlong_mul:           221 calls, min=( 0, 0), avg=(0.8, 1.1), max=(  2,   2)\nlong_invert:        204 calls, min=( 0,  ), avg=(1.0,    ), max=(  1,    )\nlong_neg:            35 calls, min=( 1,  ), avg=(1.0,    ), max=(  1,    )\nlong_format:          3 calls, min=( 0,  ), avg=(0.7,    ), max=(  1,    )\nlong_mod:             3 calls, min=( 1, 1), avg=(1.0, 1.0), max=(  1,   1)\nlong_pow:             1 calls, min=( 1, 1), avg=(1.0, 1.0), max=(  1,   1)\n\npystone:\n\nPyLong_FromLong:1587652 calls, min=( 0,  ), avg=(1.0,    ), max=(  3,    )\nlong_add:        902487 calls, min=( 0, 0), avg=(1.0, 1.0), max=(  2,   2)\nlong_compare:    651165 calls, min=( 0, 0), avg=(1.0, 1.0), max=(  3,   3)\nPyLong_AsLong:   252476 calls, min=( 0,  ), avg=(1.0,    ), max=(  2,    )\nlong_sub:        250032 calls, min=( 1, 0), avg=(1.0, 1.0), max=(  1,   1)\nlong_bool:       102655 calls, min=( 0,  ), avg=(0.5,    ), max=(  1,    )\nlong_mul:        100015 calls, min=( 0, 0), avg=(1.0, 1.0), max=(  1,   2)\nlong_div:         50000 calls, min=( 1, 1), avg=(1.0, 1.0), max=(  1,   1)\nlong_hash:          382 calls, min=( 0,  ), avg=(1.1,    ), max=(  2,    )\nlong_bitwise:       117 calls, min=( 0, 0), avg=(1.0, 1.0), max=(  1,   2)\nlong_format:          1 calls, min=( 2,  ), avg=(2.0,    ), max=(  2,    )\n\nmin/avg/max are the integer digit count (minimum, average, maximum).\n\nWhat can we learn from this numbers?\n\nPyLong_FromLong(), long_add() and long_compare() are the 3 most common \noperations on integers. \n\nExcept PyLong_FromLong(), long_compare() and long_format(), arguments of the \nfunctions are mostly in range [-2^15; 2^15].\n\nBiggest number is a number of 45 digits: maybe just one call to long_add(). \nExcept this number/call, the biggest numbers have between 2 and 3 digits. \n\nlong_bool() is never called with number bigger than 2 digits.\n\nlong_sub() is never called with number bigger than 1 digit!\nAuthor: STINNER Victor (haypo) *\nDate: 2008-11-06 13:24\nAnd now the stat of Python patched with 30bit_longdigit3.patch.\n\nmin/avg/max are now the number of bits which gives better \ninformations. \"bigger\" is the number of arguments which are bigger than 1 \ndigit (not in range [-2^30; 2^30]).\n\nmake\n====\n\n_FromLong:   169734 calls, min=( 0,  ), avg=(11.6,     ), max=(  32,     )\n  \\--> bigger=31086\nlong_bool:    48772 calls, min=( 0,  ), avg=( 0.3,     ), max=(  24,     )\nlong_add:     39685 calls, min=( 0, 0), avg=( 6.5,  3.5), max=(  19,   32)\n  \\--> bigger=1\nlong_compare: 39445 calls, min=( 0, 0), avg=( 9.3,  8.4), max=(  31,   33)\n  \\--> bigger=10438\n_AsLong:      33726 calls, min=( 0,  ), avg=( 4.9,     ), max=(1321,     )\n  \\--> bigger=10\nlong_sub:     13285 calls, min=( 0, 0), avg=( 7.6,  5.6), max=(  13,   13)\nlong_bitwise:  4690 calls, min=( 0, 0), avg=( 1.7,  1.9), max=(  16,   16)\nlong_hash:     1097 calls, min=( 0,  ), avg=( 8.1,     ), max=(  33,     )\n  \\--> bigger=4\nlong_mul:       236 calls, min=( 0, 0), avg=( 1.3,  5.4), max=(  17,   17)\nlong_invert:    204 calls, min=( 0,  ), avg=( 2.4,     ), max=(   3,     )\nlong_neg:        35 calls, min=( 1,  ), avg=( 4.3,     ), max=(   7,     )\nlong_format:      3 calls, min=( 0,  ), avg=( 2.0,     ), max=(   4,     )\nlong_mod:         3 calls, min=( 1, 2), avg=( 1.7,  2.0), max=(   2,    2)\nlong_pow:         1 calls, min=( 2, 6), avg=( 2.0,  6.0), max=(   2,    6)\n\nNotes about make:\n - PyLong_FromLong(), long_compare(), PyLong_AsLong() and long_hash() \n   gets integers not in [-2^30; 2^30] which means that all other functions\n   are only called with arguments of 1 digit!\n - PyLong_FromLong() gets ~30.000 (18%) integers of 32 bits\n - global average integer size is between 0.3 and 11.6 (~6.0 bits?)\n - There are 41.500 (12%) big integers on ~350.000 integers\n\npystone\n=======\n\n_FromLong:   1504983 calls, min=( 0,  ), avg=( 5.1,     ), max=(  31,     )\n  \\--> bigger=14\nlong_add:     902487 calls, min=( 0, 0), avg=( 3.9,  2.4), max=(  17,   17)\nlong_compare: 651165 calls, min=( 0, 0), avg=( 1.7,  1.4), max=(  31,   31)\n  \\--> bigger=27\n_AsLong:      252477 calls, min=( 0,  ), avg=( 4.6,     ), max=(  16,     )\nlong_sub:     250032 calls, min=( 1, 0), avg=( 4.0,  1.6), max=(   7,    7)\nlong_bool:    102655 calls, min=( 0,  ), avg=( 0.5,     ), max=(   7,     )\nlong_mul:     100015 calls, min=( 0, 0), avg=( 2.5,  2.0), max=(   4,   16)\nlong_truediv:  50000 calls, min=( 4, 2), avg=( 4.0,  2.0), max=(   4,    2)\nlong_hash:       382 calls, min=( 0,  ), avg=( 8.1,     ), max=(  28,     )\nlong_bitwise:    117 calls, min=( 0, 0), avg=( 6.7,  6.6), max=(  15,   16)\nlong_format:       1 calls, min=(16,  ), avg=(16.0,     ), max=(  16,     )\n\nNotes about pystone:\n - very very few numbers are bigger than one digit: 41 / ~4.000.000\n - global average integer size is between 0.5 and 6.7 (~3.0 bits?)\n - the biggest number has only 31 bits (see long_compare)\n\nShort summary:\n - pystone doesn't use big integer (1 big integer for 100.000 integers)\n   => don't use pystone!\n - the average integer size is around 3 or 6 bits, which means that most\n   integers can be stored in 8 bits (-255..255)\n   => we need to focus on the very small numbers\n   => base 2^30 doesn't help for common Python code, it only helps programs\n      using really big numbers (128 bits or more?)\nAuthor: Mark Dickinson (mark.dickinson) *\nDate: 2008-11-10 13:34\nHere's a minor update to the patch, that does some extra cleanup:\n\n- don't include longintrepr.h in Objects/abstract.c or Objects/boolobject.c --- it's not needed.\n\n- fix several places in longobject.c where int should have been size_t \nor Py_ssize_t\n\n- remove some unnecessary forward declarations in longobject.c.\n\n- fix PyLong_FromLong for small negative integers\n\nAt some point I'll try to separate the pure bugfixes (missing casts, int \nvs Py_ssize_t, etc.) from the 15-bit to 30-bit conversion.\nAuthor: STINNER Victor (haypo) *\nDate: 2008-11-11 01:58\nUsing 30bit_longdigit4.patch, I get this \nerror: \" Objects/longobject.c:700 : erreur: \"SIZE_T_MAX\" undeclared \n(first use in this function)\". You might use the type Py_ssize_t with \nPY_SSIZE_T_MAX. I used INT_MAX to compile the code.\nAuthor: STINNER Victor (haypo) *\nDate: 2008-11-11 02:24\nI like the idea of sys.int_info, but I would prefer a \"base\" attribute \nthan \"bits_per_digit\". A base different than 2^n might be used (eg. a \nbase like 10^n for fast conversion from/to string).\nAuthor: Mark Dickinson (mark.dickinson) *\nDate: 2008-11-11 16:30\nHere's a version of the 15-bit to 30-bit patch\nthat adds in a souped-up version of Mario Pernici's\nfaster multiplication.\n\nI did some testing of 100x100 digit and 1000x1000 digit\nmultiplies.  On 32-bit x86:\n  100 x 100 digits  : around 2.5 times faster\n 1000 x 1000 digits : around 3 times faster.\n\nOn x86_64, I'm getting more spectacular results:\n  100 x 100 digits : around 5 times faster\n 1000 x 1000 digits: around 7 times faster!\n\nThe idea of the faster multiplication is quite simple:\nwith 30-bit digits, one can fit a sum of 16 30-bit by\n30-bit products in a uint64_t.  This means that the\ninner loop for the basecase grade-school multiplication\ncontains one fewer addition and no mask and shift.\n\n[Victor, please don't delete the old longdigit4.patch!]\nAuthor: Antoine Pitrou (pitrou) *\nDate: 2008-12-14 00:28\nJust tested the patch, here are some benchmarks:\n\n./python -m timeit -s \"a=100000000;b=777777\" \"a//b\"\n -> 2.6: 0.253 usec per loop\n -> 3.1: 0.61 usec per loop\n -> 3.1 + patch: 0.331 usec per loop\n\n./python -m timeit -s \"a=100000000;b=777777\" \"a*b\"\n -> 2.6: 0.431 usec per loop\n -> 3.1: 0.302 usec per loop\n -> 3.1 + patch: 0.225 usec per loop\n\n./python -m timeit -s \"a=100000000;b=777777\" \"a+b\"\n -> 2.6: 0.173 usec per loop\n -> 3.1: 0.229 usec per loop\n -> 3.1 + patch: 0.217 usec per loop\n\nBut it seems there are some outliers:\n\n./python -m timeit -s \"a=100000000**5+1;b=777777**3\" \"a//b\"\n -> 2.6: 1.13 usec per loop\n -> 3.1: 1.12 usec per loop\n -> 3.1 + patch: 1.2 usec per loop\nAuthor: STINNER Victor (haypo) *\nDate: 2008-12-14 01:09\nI wrote a small benchmark tool dedicated to integer operations (+ - \n* / etc.): bench_int.py attached to issue4294 . See also Message75715 \nand Message75719 for my benchmark results. Short sum up: 2^30 base \nhelps a lot on 64 bits CPU (+26%) whereas the speedup is small (4%) on \n32 bits CPU. But don't trust benchmarks :-p\nAuthor: Mark Dickinson (mark.dickinson) *\nDate: 2009-02-14 20:32\nThe most recent patch is out of date and no longer applies cleanly.  I'm \nworking on an update.\nAuthor: Mark Dickinson (mark.dickinson) *\nDate: 2009-02-16 16:47\nUpdated patch against py3k.  I'm interested in getting this into the trunk \nas well, but py3k is more important (because *all* integers are long \nintegers).  It's also a little more complicated to do this for py3k \n(mostly because of all the small integer caching), so backporting to\n2.7 is easier than trying to forward port a patch from 2.7 to 3.1.\n\nNotes:\n\n- I've added a configure option --enable-big-digits (there are probably \nbetter names), enabled by default.  So you can use --disable-big-digits\nto get the old 15-bit behaviour.\n\n- I *think* this patch should work on Windows; confirmation would be \nappreciated.\n\n- I've removed the fast multiplication code, in the interests of keeping \nthe patch simple.  If this patch goes in, we can concentrate on speeding \nup multiplication afterwards.  For now, note that 30-bit digits give\nthe *potential* for significant speedups in multiplication and division \n(see next item).\n\n- There's a nasty 'feature' in x_divmod: the multiplication in the \ninnermost loop is digit-by-twodigits -> twodigits, when it should be \ndigit-by-digit -> twodigits;  this probably causes significant slowdown \nwith 30-bit digits.  This may explain Antoine's outlier.\nAgain, if this patch goes in I'll work on fixing x_divmod.\n\n- Re: Victor's comment about a 'base' attribute:  I tried this, but\nquickly discovered that we still need the 'bits_per_digit' for tests.\nI think that binaryness is so ingrained that it's not really worth\nworrying about the possibility of the base changing from a power of 2 to a \npower of 10.  So in the end I left base out.\n\n- It did occur to me that NSMALLPOSINTS and NSMALLNEGINTS might usefully \nbe exposed in sys.int_info, mostly for the purposes of testing.  Thoughts?\nAuthor: Mark Dickinson (mark.dickinson) *\nDate: 2009-02-16 16:51\nForgot to mention:  you'll need to rerun autoconf and autoheader after \napplying the patch and before doing ./configure\n"}, {"score": 888.0783, "uuid": "dff4e212-8a54-5a6a-9342-f3bc4a17ba1e", "index": "cw12", "trec_id": "clueweb12-0210wb-07-22363", "target_hostname": "www.linuxplanet.com", "target_uri": "http://www.linuxplanet.com/linuxplanet/tutorials/1132/3", "page_rank": 1.1895425e-09, "spam_rank": 92, "title": "<em>Python</em> Squeezes the Web - Introduction - Tutorials - LinuxPlanet", "snippet": "Debian 2.1 users <em>should</em> be able to just type &quot;apt-get install <em>python</em>&quot;, and <em>Python</em> 1.5.x is included with RedHat 5.0 <em>or</em> higher and can be installed with glint. Using your favorite text editor (<em>I</em> like VIM), pull up a chair and follow along!", "explanation": null, "document": "YahooBuzz\nWhat I Like about Python\nI've written programs in a number of different languages, including Visual Basic, C/C++, Perl, and PHP3. There are some things Python has that makes it, in my opinion, substatially more flexible than other languages:\nPowerful Datatypes and Operations -\nPython has built in strings, tuples, lists, dictionaries, and more. Want your function to return two values? Return a tuple, an immutable list of values! Want to grab elements 4-6 of list MyList? Use slice notation to write: MyList[4:6]! This slice notation works on strings, too, so \"Monty Python\"[0:5] evaluates to \"Monty\". List can also dynamically grow, too. You can easily iterate over the elements of a list with the \"for\" command, and the \"in\" and \"not in\" statements let you take advantage of Python's built-in binary search routines instead of having to code your own. Very few languages have this type of functionality built in and available as a core part of the language. Most of the time, a special add-on library (such as STL) is required to get all these features.\nRapid Development with an Interactive Interpreter -\nRather than go through the compile/test/run cycle of most traditional programming languages, or even the edit/run cycle of many scripting languages, Python has an incredibly useful interactive interpreter. During the development of the aforementioned application, I pasted a chunk of data into the interpreter, assigned to to a variable, and wrote the string parsing regular expression in about an hour. Whenever I'm curious about the built-in methods of a list, I pop into the interpreter and run dir([]). When I'm not sure exactly how some esoteric feature works, I define a test case and run it. I even build my applications bottom up, importing and testing critical functions in the interpreter before I write the top-level code that uses the functions.\nRuns on Multiple Platforms and Has the Same Implementation on Multiple Platforms -\nI don't have to extol the virtues of a multi-platform language to you; you have a tremendous amount of flexibility in where you develop and deploy your applications. You can write Python on a Mac and upload it to a Unix server, start out with a Linux server and move up to a Sun Ultra, et. al. But, unlike some \"cross-platform\" languages like ANSI C++ (which I originally wrote the uscfratingd program in) and PHP3, it supports the same features everywhere because there is only one main implementation of Python in common use. I have written GUI applications with Python that run on Windows and Linux without changing one line of code (more on this in a following article). Can any other language (aside from Perl) claim this type of functionality?\nRich Core Library -\nOut of the box, on all platforms, Python programs can use sockets to speak any protocol or use predefined classes to speak HTTP, FTP, SMTP, POP, Telnet and a variety of other Internet protocols. Built-in classes are provided to permit your app to parse XML, HTML, and SGML. Regular Expressions, a powerful feature that allows text parsing (look at the perLineRegexp variable in the program for a useful example), were borrowed from Perl and are present in Python on Windows, Mac and Linux. Python/Tk, a moderately powerful GUI framework is available for Windows, Mac and Unix, and wxPython, a wrapper to the wxWindows C++ library, are available for Unix and Windows and are under development for BeOS and the Macintosh. Overall, Python provides a lot of features for free that might require costly third-party libraries in other languages.\nThe Example Explained\nI don't have enough space to provide a complete introduction to Python (check the Python Tutorial for that), but I'll try to explain things briefly as I go. If you've done some sort of programming before, you'll find that Python is extremely easy to learn and lets you do a lot with a small amount of code. To try the code in this article, install a copy of Python for your distribution of Linux. Debian 2.1 users should be able to just type \"apt-get install python\", and Python 1.5.x is included with RedHat 5.0 or higher and can be installed with glint. Using your favorite text editor (I like VIM ), pull up a chair and follow along! Note, in order to run the example program exactly as written, you'll need to create a MySQL table called \"Players\" like this:\nCREATE TABLE Players (\n\n        PlayerId char(8) NOT NULL PRIMARY KEY,\n\n        LastName varchar(50) NOT NULL,\n\n        FirstName varchar(50) NOT NULL,\n\n        USCFRating mediumint NOT NULL,\n\n        State char(2) NOT NULL,\n\n        ExpDate date NOT NULL\n\n)\nAnd some sample data from the list above:\nINSERT INTO Players(PlayerId, LastName, FirstName, State) VALUES('21005567', 'Tarver', 'Nathan', 'TN');\n\nINSERT INTO Players(PlayerId, LastName, FirstName, State) VALUES('12613391', 'Tashie', 'Daphne', 'TN');\n\nINSERT INTO Players(PlayerId, LastName, FirstName, State) VALUES('21005567', 'Tate', 'Jeremy', 'TN');\n\nINSERT INTO Players(PlayerId, LastName, FirstName, State) VALUES('12660161','Patterson','Raphael','MS');\n\nINSERT INTO Players(PlayerId, LastName, FirstName, State) VALUES('12499243','Pattillo','Billy','MS');\n\n\n\nINSERT INTO Players(PlayerId, LastName, FirstName, State) VALUES('12660161','Pat\n\nton','Sam','MS');\n\nINSERT INTO Players(PlayerId, LastName, FirstName, State) VALUES('12739657','Pay\n\nne','Daniel','MS');\nDefining the Regular Expression\nIn developing this program, I started by looking at my data. For a couple of hours, I dabbled in the interpreter like so:\nPython 1.5.2 (#0, Sep 13 1999, 09:12:57)  [GCC 2.95.1 19990816 (release)] on lin\n\nux2\n\nCopyright 1991-1995 Stichting Mathematisch Centrum, Amsterdam\n\n>>> import urllib, re, string\n\n>>> beginDataRegexp =re.compile(r\"<pre>\\n<b>.*</b>\", re.I | re.DOTALL)\n\n>>> endDataRegexp = re.compile(r\"</pre>\")\n\n>>> test_data = urllib.urlopen(\"http://www.64.com/cgi-bin/ratings.pl?nm=T&st=TN\").read()\n\n>>> len(test_data)\n\n25286\nThus far, I've pulled in some libraries that give me string, url downloading, and regular expression functions. Then, I defined two regular expressions. Regular expressions are analagous to keys. Put simplistically, the regular expression is moved down the string one character at a time, similarly to trying a key on a hallway full of doors. When the regular expression matches, the door is opened. The first regular expression looks for the text \"<pre>\" followed by a blank line, followed by a \"<b>\" set of tags with something inside of it. The second one looks for the end of the \"<pre>\" tag. After that, I downloaded some test data from the USCF web site to play with. One line of code, that's all it takes! The original C++ version of this program required 11 lines of code to emulate the functionality of just this one! It also had to be linked with the GNOME HTTP library, which wasn't present on my target FreeBSD system.\n>>> beginMatch = beginDataRegexp.search(test_data)\n\n>>> endMatch = endDataRegexp.search(test_data)\n\n>>> test_data_lines = string.split(test_data[beginMatch.end():endMatch.\n\nstart()], \"\\n\")\n\n>>> len(test_data_lines)\nHere, I'm using the regular expression that I defined earlier to search the string for a match. Then, I used the string slicing to grab that chunk of the data. The data is then turned into a list containing the individual lines.\n>>> perLineRegexp = re.compile(r\".{5}\\s+(\\d{3,4})p?\")\n\n>>> test_data_rows[1]\n\n'12-96  523p        TN 10-96 <A HREF=/cgi-bin/ratings.pl/USCF/12659889>TAB\n\nAKOFF,ADRIAN</A>\n\n>>>  perLineRegexp.search(test_data_rows[1]).groups()\n\n('523',)\nNote that I've put \\d{3,4} in parenthesis. This represents an extremely powerful aspect of regular expressions: grouping! The regular expression parser will save any values that it finds in parenthesis, and they can be accessed via the groups function as shown above. The final part of this regular expression is \"p?\". The USCF uses \"p\" after a rating to denote it as \"provisional\", meaning that the player has not yet played 20 games. For our purposes it is not needed, so \"p?\" tells the regular expression parser \"if there is a p there, ignore it and move on.\"\nI continued this trial and error sequence, slowly expanding my regular expression until something like this:\n>>> perLineRegexp = re.compile(r\".{5}\\s+(\\d{3,4})p?\\s+(?:\\d{3,4}p?)?\\s*\\w{2}\\s+(?:(?:(\\d{2})-(\\d{2}))|Life)\\s+<A.*USCF/(\\d{8}).*/A>(?:\\s+.{5}\\s+(\\d{3,4})\\s*(?:\\d{3,4}p?)?)?\")\n\n>>> perLineRegexp.match(test_data_lines[1]).groups()\n\n('523', '10', '96', '12659889', None)\n"}, {"score": 886.0865, "uuid": "a7cfa903-a550-59b9-946b-263b1ca8246e", "index": "cw12", "trec_id": "clueweb12-1510wb-40-28311", "target_hostname": "www.drewconway.com", "target_uri": "http://www.drewconway.com/zia/?p=204", "page_rank": 1.233201e-09, "spam_rank": 70, "title": "UPDATED: Must-Have <em>Python</em> Packages <em>for</em> Social Scientists \u00ab Zero Intelligence", "snippet": "<em>I</em> do plenty of genomic <em>data</em> mining with <em>python</em>. <em>I</em> need the statistical functions of <em>R</em> but unfortunately <em>R</em> is a PAIN to <em>learn</em> and work with. <em>I</em> still dont understand its <em>data</em> structures, also its script is archaic. <em>I</em> will definitely look into rpy2.", "explanation": null, "document": "UPDATED: Must-Have Python Packages for Social Scientists\nBy Drew Conway, on June 18th, 2008\nPython is the greatest thing to happen to computer science since the Turing Machine ! Well, no, but it has inspired me into a personal renaissance for software writing. Its flexibility, widespread community support, and leveraging of legacy C and Fortran code also make it an outstanding language for social science researchers.\nIf you are a new researcher looking to get started, or experienced and willing to walk away from your [:,:] lifestyle in Matlab\u2014and licensing and training fees\u2014then equip yourself with these 10 packages and get to it!\nNumPy\nNumPy, short for Numeric Python, is the cornerstone of Python\u2019s mathematics and statistics operations. All scientific computing in Python starts and ends with NumPy!\nDownload NumPy\nSciPy\nSciPy, short for Scientific Python, is the little brother of NumPy, as it relies on NumPy data types for its operations. To distinguish itself, SciPy adds several of its own sophisticated data types, and integration and optimization techniques. Many of the packages proceeding this rely on some combination of NumPy and SciPy.\nDownload SciPy\nMatplotlib\nThe third tine on Python\u2019s scientific trident, Matplotlib (pylab) is the standard for 2D plotting. Highly extensible, and will display your results just the way you like \u2018em.\nDownload Matplotlib\nNetworkX\nThis package is what motivated me to learn Python. This is the best tool for analyzing network data\u2013period. For novice social network analysts/graph theorist, the learning curve will be steep, but taking the time to learn NX will preclude you from having to waste your time with other inferior tools . Oh, and for those of you with accreditation concerns, its subversion is maintained by Los Alamos National Laboratory.\nDownload NetworkX\nPyMC\nThis one is for all of you Bayesian/MCMC modelers out there. PyMC implements the Metropolis-Hastings algorithm as a Python class, providing flexibility when building your model. PyMC is also highly extensible, and well supported by the community.\nDownload PyMC\nSimPy\nShort for \u201cSimulation in Python\u201d, SimPy is an object-oriented, process-based discrete-event simulation language, making it a wholesale agent-based modeling environment written entirely in Python. While not as robust as REPAST or NetLogo , SimPy provides an excellent tool set for designing experiments, and because it is pure Python, the data can be fed to other analytical packages.\nDownload SimPy\nSymPy\nNot to be confused with the previous entry, SymPy is an full-featured Python library for symbolic mathematics. Oliver suggested I add Sage to the list, which is an excellent tool, but SymPy contains nearly all of the same functionality (algebraic evaluation, differentiation, expansion, complex numbers, etc.), but is contained in a pure Python distribution. This package is great for researchers who want symbolic mathematics support, but have no access to mega-expensive computer algebra systems, like Mathematica .\nUPDATE: How to use Python and SymPy to solve optimization problems .\nhtml5lib\nAfter the fall of BeautifulSoup , I was desperate for a web data parser that equaled soup\u2019s flexibility and easy of use. Enter html5lib. If you need to download and organize large amounts of data from the Internet in a quick and easy way, then html5lib is the only package you will need. This module also supports the BeautifulSoup tree type, as well as many others, making it incredibly useful across a wide range of tasks. To take advantage of its power, you will need a little background in HTML (or XML, if that happens to be what you are parsing), but there are many tutorials available online to get you up to speed quickly.\nDownload html5lib\nPycluster\nThere are many clustering algorithms available for Python, but many of these packages are designed to cluster one-dimensional data. Data collected by social scientist, however, is often of a higher dimension\u2013enter Pycluster. This package contains efficient implementations of hierarchical and k-means clustering, with several options for measuring distance. Still waiting for a clever binding to Matplotlib to draw the dendrogram, but in the meantime, you can use their Java program TreeView to display result.\nDownload Pycluster\ncjson\nThis module implements a very fast JSON encoder/decoder for Python. JSON ( JavaScript Object Notation ) is useful for many things, but most notably for social scientist is how many social networking sites use JSON to encode public data about their users and their users\u2019 relationships. JSON is also what is returned by Google\u2019s SocialGraph API , so cjson allows researchers to feed this social network data directly into Python data types.\nDownload cjson\nPyevolve\nA complete pure python genetic algorithm framework. I am wearing my computer science background on my sleeve with this one, but for people serious about designing pure Python agent-based models , Pyevolve provides the tools to create intricate experimental environments.\nDownload Pyevolve\nMySQL for Python\nA pure Python binding for MySQL, allowing the user to integrate MySQL execution into any Python script. Very straightforward and simple to use, and since many social science data sets are stored on MySQL databases, a necessity.\nUpdated 4/6/2009>: I have been negligent, as it pointed out in the comments , RPy has functionally been replaced by RPy2.\nRPy2\nThere are very few statistical calculations that the combination of NumPy and SciPy cannot handle, but there are NO statistical operations R cannot do. RPy2 is a simple Python interface for R, able to execute any R function from within a Python script.\nDownload RPy2\nI should also note that most (maybe all by now) of these packages come standard with the Enthought distribution of Python . If you are interested in using Python as a platform for scientific research, I highly recommend installing this distribution, which is free for academics .\nJune 18th, 2008 | Tags: list , Python | Category: Networks , Tools | 20 comments\nhttp://whynoti.org diN0bot\n[ Reply ]\nhttp://frikk.tk blaine\nThanks for the pyEvolve link \u2013 I\u2019ve been looking for something like this for quite awhile. Agent based simulations are very interesting indeed. Thanks\n[ Reply ]\nhttp://blogs.nyu.edu/blogs/agc282/zia/ Drew Conway\nI am glad the list was helpful. I with there was a PyGeneticProgramming counterpart to PyEvolve so models could be built that blended GP\u2019s sophisticated agent evolution; but alas, no one has done the heavy lifting for me yet.\nIf there is enough interest, I will post another update in the near future with a few additional packages I have run across since the last update.\n[ Reply ]\nVamsee\nThanks for the packages, I used to think numPy is only for high level scientific calculations. And also thanks for networkX, I initially worked on graphviz but this baby fits right into my programming environment.\n[ Reply ]\nhttp://blogs.nyu.edu/blogs/agc282/zia/ Drew Conway\nAlthough visualizations are one of NetworkX\u2019s weaknesses, it does have a very nice graphviz binding if you want to display your analysis.\n"}, {"score": 884.0134, "uuid": "1f574071-9e4b-54b4-b93e-2a09172ef70c", "index": "cw12", "trec_id": "clueweb12-0112wb-20-18169", "target_hostname": "www.fdi.vt.edu", "target_uri": "http://www.fdi.vt.edu/tracks-spring-summer/2011/spring_tracks/track-R.html", "page_rank": 1.2056093e-09, "spam_rank": 99, "title": "Track <em>R</em>: Geographic Information Systems - Fundamentals, <em>Analysis</em> and Web-based", "snippet": "You <em>should</em> also become familiar with the user interface of ESRI ArcGIS Desktop, the industry-leading GIS software package <em>for</em> which Virginia Tech has a site license. You <em>should</em> be able to develop a basic map using example <em>data</em> sources and perform basic spatial <em>analysis</em>.", "explanation": null, "document": "Torgersen 3060\nWindows / Macintosh\nRegardless of whether spatial information is to be presented on a paper map, in an interactive web-based format, or in a desktop GIS environment, understanding the fundamental principles of effective map creation and spatial data presentation will ensure that you extract the most information from your data and convey the appropriate message to map users. Cartography as a discipline predates digital mapping, GIS and the web by thousands of years, but the same principles remain just as relevant in modern computing environments. This course will cover both the technical and aesthetic aspects of creating accurate, professional, and attractive maps in a GIS environment.\nDesktop GIS Analysis Learning Path:\nDesktop GIS Part 1: Intermediate Spatial Analysis\nMonday, February 28\nTorgersen 3060\nWindows / Macintosh\nThe true power of a GIS is realized not in presentation or visualization, but in its ability to detect and quantify variation of phenomena across geographic space and identify spatial relationships between datasets. Accordingly, this course will review vector and raster analysis methods in GIS. Vector analyses such as proximity, logical operators, spatial joins, and generalization may be discussed. Raster analyses such as raster math, neighborhood / zonal statistics, and terrain-specific algorithms may be discussed. In addition to the vector and raster analysis methodologies, methods and issues associated with vector to raster conversion will be discussed.\nDesktop GIS Part 2: Geoprocessing Tools and Techniques\nMonday, March 14\nTorgersen 3060\nWindows / Macintosh\nThis course will introduce the ESRI Geoprocessing environment, and specifically, the use of Modelbuilder to create tools which automate GIS analyses and processes. Topics to be reviewed include: intended use cases for geoprocessing models, the proper use of input and output parameters in geoprocessing models, sharing geoprocessing tools with others, and applications of geoprocessing models in a web server context. The use of Python will be discussed, but not covered in detail.\nWeb Mapping Learning Path:\nWeb Mapping Part 1: Introduction to Web Mapping\nMonday, March 21\nTorgersen 3060\nWindows / Macintosh\nThis course will review the numerous options available for presenting geographic data on the web. The technical architecture features common to all web mapping applications, regardless of platform, will be identified, thus allowing students to understand the operational components or \u201cmoving parts\u201d behind the scenes of a web mapping site. The most commonly used web mapping platforms will be discussed, including the pros and cons of each in light of the application developer\u2019s goals and objectives. Multiple working web mapping applications will be presented as examples of their various platforms. The hands-on portion of this course will guide participants through the source code of a working Google Maps application.\nWeb Mapping Part 2: Participatory Mapping and Citizen Science\nMonday, March 28\nTorgersen 3060\nWindows / Macintosh\nParticipatory mappings strategies empower the public to voluntarily serve as creators of spatial data. In this way, the on-the-ground knowledge of people can be harnessed to create spatial datasets that would be prohibitively difficult to develop using traditional means. Datasets that are created using these methods are referred to as \u201cVolunteered Geographic Information (VGI). This course will cover various ways to involve large groups of people in the collection of VGI, including organized group activities, web-based interactive maps, and \u201ccrowdsourcing\u201d techniques. Strategies for developing web-based environments for read/write public interaction with maps will be discussed. Successful applications of participatory mapping will be presented as examples.\n"}, {"score": 883.9854, "uuid": "d0727954-9595-5474-aa5e-3845b253ae19", "index": "cw12", "trec_id": "clueweb12-0200wb-61-23339", "target_hostname": "www.fdi.vt.edu", "target_uri": "https://www.fdi.vt.edu/tracks-spring-summer/2011/spring_tracks/track-R.html", "page_rank": 1.2055883e-09, "spam_rank": 99, "title": "Track <em>R</em>: Geographic Information Systems - Fundamentals, <em>Analysis</em> and Web-based", "snippet": "You <em>should</em> also become familiar with the user interface of ESRI ArcGIS Desktop, the industry-leading GIS software package <em>for</em> which Virginia Tech has a site license. You <em>should</em> be able to develop a basic map using example <em>data</em> sources and perform basic spatial <em>analysis</em>.", "explanation": null, "document": "Torgersen 3060\nWindows / Macintosh\nRegardless of whether spatial information is to be presented on a paper map, in an interactive web-based format, or in a desktop GIS environment, understanding the fundamental principles of effective map creation and spatial data presentation will ensure that you extract the most information from your data and convey the appropriate message to map users. Cartography as a discipline predates digital mapping, GIS and the web by thousands of years, but the same principles remain just as relevant in modern computing environments. This course will cover both the technical and aesthetic aspects of creating accurate, professional, and attractive maps in a GIS environment.\nDesktop GIS Analysis Learning Path:\nDesktop GIS Part 1: Intermediate Spatial Analysis\nMonday, February 28\nTorgersen 3060\nWindows / Macintosh\nThe true power of a GIS is realized not in presentation or visualization, but in its ability to detect and quantify variation of phenomena across geographic space and identify spatial relationships between datasets. Accordingly, this course will review vector and raster analysis methods in GIS. Vector analyses such as proximity, logical operators, spatial joins, and generalization may be discussed. Raster analyses such as raster math, neighborhood / zonal statistics, and terrain-specific algorithms may be discussed. In addition to the vector and raster analysis methodologies, methods and issues associated with vector to raster conversion will be discussed.\nDesktop GIS Part 2: Geoprocessing Tools and Techniques\nMonday, March 14\nTorgersen 3060\nWindows / Macintosh\nThis course will introduce the ESRI Geoprocessing environment, and specifically, the use of Modelbuilder to create tools which automate GIS analyses and processes. Topics to be reviewed include: intended use cases for geoprocessing models, the proper use of input and output parameters in geoprocessing models, sharing geoprocessing tools with others, and applications of geoprocessing models in a web server context. The use of Python will be discussed, but not covered in detail.\nWeb Mapping Learning Path:\nWeb Mapping Part 1: Introduction to Web Mapping\nMonday, March 21\nTorgersen 3060\nWindows / Macintosh\nThis course will review the numerous options available for presenting geographic data on the web. The technical architecture features common to all web mapping applications, regardless of platform, will be identified, thus allowing students to understand the operational components or \u201cmoving parts\u201d behind the scenes of a web mapping site. The most commonly used web mapping platforms will be discussed, including the pros and cons of each in light of the application developer\u2019s goals and objectives. Multiple working web mapping applications will be presented as examples of their various platforms. The hands-on portion of this course will guide participants through the source code of a working Google Maps application.\nWeb Mapping Part 2: Participatory Mapping and Citizen Science\nMonday, March 28\nTorgersen 3060\nWindows / Macintosh\nParticipatory mappings strategies empower the public to voluntarily serve as creators of spatial data. In this way, the on-the-ground knowledge of people can be harnessed to create spatial datasets that would be prohibitively difficult to develop using traditional means. Datasets that are created using these methods are referred to as \u201cVolunteered Geographic Information (VGI). This course will cover various ways to involve large groups of people in the collection of VGI, including organized group activities, web-based interactive maps, and \u201ccrowdsourcing\u201d techniques. Strategies for developing web-based environments for read/write public interaction with maps will be discussed. Successful applications of participatory mapping will be presented as examples.\n"}, {"score": 882.23553, "uuid": "cbc766c6-2a2a-5f26-b97d-a2a5dceb2806", "index": "cw12", "trec_id": "clueweb12-0909wb-23-18450", "target_hostname": "python.mirocommunity.org", "target_uri": "http://python.mirocommunity.org/listing/popular/", "page_rank": 7.092599e-09, "spam_rank": 85, "title": "<em>Python</em> Miro Community - All <em>Python</em> Video, All the Time - Popular Videos", "snippet": "You&#x27;ll leave with an understanding of when to apply different tools, and <em>learn</em> about a &quot;heavy hammer&quot; <em>for</em> screen scraping that <em>I</em> picked up at a project <em>for</em> the Electronic Frontier Foundation. Atendees <em>should</em> bring a laptop, if possible, to try the examples we discuss and optionally take notes.", "explanation": null, "document": "Switching addons.mozilla.org from CakePHP to Django\nPresented by Jeff Balogh\nIn January of 2010 Mozilla started switching addons.mozilla.org (AMO) from CakePHP to Django. We see about 120 million web requests per month and 1.4 million visitors per day. This talk will be a case study of scaling and deploying a large website (and getting faster than PHP) with MySQL, memcached, virtualenv, Celery, Hudson, Redis, MongoDB, and more.\nAbstract\nTo scale the backend of AMO, we rely heavily on caching in memcached (and soon Redis) to keep the load off our database, and multidb to spread the load we can't dump off on cache. Our caching ranges from object and query caching, to template fragment caching, up to full response caching. Frontend caching is external to Django and won't be covered. In addition, we work hard to measure and improve raw speed. Hitting indexes and keeping datasets small is critical, and the Django ecosystem has many great tools to help us profile.\nWe create development environments using pip and virtualenv, so we naturally started using that setup to deploy the site in production. We ran into issues creating a reliable environment with this method; switching to a separate vendor repository solved our problems with only a small amount of added effort. We update and deploy new code at least once a week, and have to deal with the additional complication of managing and interacting with parts of the PHP site that have not been ported over.\nApart from scaling and deployment, we've customized our application in ways that could be interesting to other Django users:\nJinja is our templating engine (but we keep the admin working) Babel helps us localize the site in over 30 languages Sphinx (not the documentation tool) serves our search results Celery and RabbitMQ help us do offline processing Nose and Hudson help us get over 90% code coverage in tests\nPublished 1 year ago\n"}], [{"score": 879.2091, "uuid": "880b158b-b18d-5603-89db-ddd0fa3cc576", "index": "cw12", "trec_id": "clueweb12-1013wb-07-18331", "target_hostname": "microsoftcambridge.com", "target_uri": "http://microsoftcambridge.com/Events/PythonmeetupGettingStartedTesting/tabid/825/Default.aspx", "page_rank": 1.3739125e-09, "spam_rank": 84, "title": "<em>Python</em> meetup: Getting Started Testing", "snippet": "Come <em>learn</em> how to approach <em>Python</em> testing to make it work <em>for</em> you. Pythonistas of all levels are welcome!", "explanation": null, "document": "// Python meetup: Getting Started Testing\nEvent Details\nLocation: Microsoft New England R&D Center, One Memorial Drive, Cambridge, MA 02142\nTime: 7:00pm - 9:00pm\nTwitter: @bostonpython\nDescription: Do you have Python code that you know you should be testing, but aren't? Have you got tests, but aren't sure how to use them to full advantage? Are you not flossing as often as you should?\nWe'll start from scratch with a very first test, talk about unit test structure, how to structure your code for testability, isolating components with mocking, how to use nose to run tests, using coverage measurement to gauge test effectiveness, the testing mindset, and other topics.\nTesting will never be easy, but it doesn't have to be a burden. Come learn how to approach Python testing to make it work for you. Pythonistas of all levels are welcome!\n"}, {"score": 877.5916, "uuid": "634e2315-18ec-5cb6-ae10-6e6b98a74b24", "index": "cw12", "trec_id": "clueweb12-1102wb-65-09899", "target_hostname": "www.or-exchange.com", "target_uri": "http://www.or-exchange.com/questions/3219/classes-to-take-to-prepare-for-msc-or", "page_rank": 1.2064908e-09, "spam_rank": 82, "title": "Classes to take to prepare <em>for</em> MSc <em>OR</em> ? - <em>OR</em>-X", "snippet": "I&#x27;m a math major intent on pursuing an MSc in <em>OR</em>. I&#x27;m in my final year, and have spare time to take a couple more classes. What sort of classes <em>should</em> <em>I</em> take to prepare myself <em>for</em> a rigorous <em>OR</em> masters program?", "explanation": null, "document": "Matthew Saltzman \u2666\n2.1k\u25cf5\nI've done a course on C. What other programming languages do you suggest I try to learn before grad school?\nIn connection with this, what programming languages are taught in OR grad school?\n(Jul 18 '11 at 07:26) queueingtheory\n1\nC is a good start. The main thing about programming isn't so much the specific language as what I might call \"computational maturity\"--a basic understanding of algorithmic ideas, structures, and analysis. Once you know a couple of languages, you'll realize that you can pick them up pretty easily as needed. You see many languages in OR, from C, Java, Python to R, SAS, SPSS (statistics), to AMPL, MPL, GAMS (optimization modeling languages) and Matlab, Maple, Sage (numerical and symbolic math scripting).\n(Jul 18 '11 at 08:45) Matthew Saltzman \u2666\nI agree with almost all of Matt's reply, but number theory?\n(Jul 18 '11 at 17:32) Paul Rubin \u2666\nRe languages, I'd definitely suggest at least one object oriented language (C++, Java or maybe R), as object orientation is a nontrivial step from a procedural language such as C. (Bo: if you're reading this, I did in fact hold my nose with one hand as I typed 'C++' with the other.)\n(Jul 18 '11 at 17:34) Paul Rubin \u2666\n@Paul I am feeling your pain..\n(Jul 18 '11 at 17:36) Bo Jensen \u2666\n@Paul , number theory (beyond some of the counting theorems) isn't necessary in and of itself, but it usually comes as a section in a standard discrete math course, along with the more important stuff for OR. Also, proof by induction is often introduced in the number theory section. And anything that trains the mind in the structure of logical argument can't be bad.\n(Jul 24 '11 at 05:08) Matthew Saltzman \u2666\n@Paul , seems like Python is a good candidate these days for intro OOP, yes?\nAnd did you hold your nose the second time you typed C++?\n(Jul 24 '11 at 05:11) Matthew Saltzman \u2666\n@Matt I suggest Paul as maintainer of some of the CLP code, I am sure it's just his calling..\n(Jul 24 '11 at 05:16) Bo Jensen \u2666\n@Matt : Interesting. Math curricula apparently have changed since I went over to the dark side. I can't recall where I learned induction proofs, but certainly it was early on in my undergrad training, and I never took a number theory course (nor a discrete math course). My main regret is having passed up the graph theory course in grad school. I agree about \"structure of logical argument\", but I had that in pretty much every math course.\n(Jul 24 '11 at 12:02) Paul Rubin \u2666\n@Matt : Re Python, I like it, but I'm not sure it conveys object orientation as well as Java or (holding nose again) C++. The ability to change the structure of an object (in effect creating a one-off derivative class) is not entirely consistent with OOP (as I see OOP). And no, I no longer hold my nose when typing \"C++\"; I stuff cotton wadding in now. Frees up my other hand for the keyboard.\n@Bo : My list of languages worse than C++ is rather short, and features C prominently. So bite your tongue.\n(Jul 24 '11 at 12:05) Paul Rubin \u2666\nshowing 5 of 10 show all\n3\nI agree with most of the above, with the qualification that I would put a solid background in statistics, probability and computing ahead of business courses. I'd also recommend a course (or courses) in communications (written and presentation). They won't be necessary to get through the MS program, but they will stand you in good stead when you hit the job market, and the MS program may not leave time for them. So my prioritization (most to least critical) would be: one course in probability theory (perhaps two); one course in basic statistics (could be the same as the previous one, depending on how taught); at least one course in programming (particularly something that covers both data structures and good design/documentation techniques, and preferable at least some mention of versioning systems); and one to two courses in effective communication (I'd give a slight preference to verbal first, then written).\n"}, {"score": 875.594, "uuid": "b32fcc24-fa8c-5487-9338-56a5d5096d38", "index": "cw12", "trec_id": "clueweb12-0111wb-40-25688", "target_hostname": "www.fdi.vt.edu", "target_uri": "http://www.fdi.vt.edu/tracks-spring-summer/2012/spring_tracks/track-R.html", "page_rank": 1.2248003e-09, "spam_rank": 98, "title": "Spring Track <em>R</em>: Geographic Information Systems - Fundamentals, <em>Analysis</em>", "snippet": "such work tends more towards stand alone processing and <em>analysis</em>, <em>or</em> the dissemination of spatial <em>data</em> through interactive maps on the web.", "explanation": null, "document": "Track R: Geographic Information Systems - Fundamentals, Analysis and Web-based Mapping\nSpring Track General Requirements\nThis year, FDI is again offering alternative tracks that will meet during the spring semester. By attending the sessions that comprise a spring track, faculty can opt out of participating during the summer months and still meet the participation requirements to receive a computer.\nSpring Track R Description\nThis spring-only track, led by faculty and staff in the VTGIS office, was developed to meet the diverse needs of faculty at Virginia Tech currently using, or planning to use, geospatial technologies. The track is structured as a four-course core and two \"learning paths\" of two additional courses each, enabling track participants to establish a common base of GIS principles and then receive more in-depth instruction on either desktop GIS analysis or Web-based mapping.\nA common core of GIS principles is offered as a useful resource to all faculty interested in applying these tools, regardless of their specific application, and then the path format provides the opportunity for faculty to take additional courses beyond the core that are most relevant to their work, whether such work tends more towards stand alone processing and analysis, or the dissemination of spatial data through interactive maps on the web.\nFaculty that choose to sign up for the entire track are required to take all four of the core courses, and them may choose a minimum of two (but are certainly welcome to choose more) of the elective courses in either the web mapping or desktop GIS analysis learning path, below.\nSpring Track R Requirements Overview\nThose participating in this spring track will be asked to attend a total of seven short courses during the spring semester. \u00a0There are three requirement areas:\n1. \u00a0\u00a0Faculty participating in this track will be required to attend one of FDI's \"Campus Resources at VT\" sessions. There are five different times to choose from.\n2. \u00a0\u00a0Four required courses.\n3. \u00a0\u00a0For the last (two) sessions, participants can choose from a list of topics related to geographic information systems.\nThe table below details the requirements, with course descriptions, time and dates of the courses, and links to enroll into the course. If you plan to attend a spring alternative track, you must be able to attend required sessions from those listed below. As always, it is possible to sign up for more than the required minimum courses.\nCampus Resources at Virginia Tech\nMonday, February 6\n"}, {"score": 875.1693, "uuid": "50cab604-ef83-5b82-a9fd-9919222d742a", "index": "cw12", "trec_id": "clueweb12-0111wb-93-33642", "target_hostname": "www.fdi.vt.edu", "target_uri": "https://www.fdi.vt.edu/tracks-spring-summer/2012/spring_tracks/track-R.html", "page_rank": 1.2247745e-09, "spam_rank": 98, "title": "Spring Track <em>R</em>: Geographic Information Systems - Fundamentals, <em>Analysis</em>", "snippet": "such work tends more towards stand alone processing and <em>analysis</em>, <em>or</em> the dissemination of spatial <em>data</em> through interactive maps on the web.", "explanation": null, "document": "Track R: Geographic Information Systems - Fundamentals, Analysis and Web-based Mapping\nSpring Track General Requirements\nThis year, FDI is again offering alternative tracks that will meet during the spring semester. By attending the sessions that comprise a spring track, faculty can opt out of participating during the summer months and still meet the participation requirements to receive a computer.\nSpring Track R Description\nThis spring-only track, led by faculty and staff in the VTGIS office, was developed to meet the diverse needs of faculty at Virginia Tech currently using, or planning to use, geospatial technologies. The track is structured as a four-course core and two \"learning paths\" of two additional courses each, enabling track participants to establish a common base of GIS principles and then receive more in-depth instruction on either desktop GIS analysis or Web-based mapping.\nA common core of GIS principles is offered as a useful resource to all faculty interested in applying these tools, regardless of their specific application, and then the path format provides the opportunity for faculty to take additional courses beyond the core that are most relevant to their work, whether such work tends more towards stand alone processing and analysis, or the dissemination of spatial data through interactive maps on the web.\nFaculty that choose to sign up for the entire track are required to take all four of the core courses, and them may choose a minimum of two (but are certainly welcome to choose more) of the elective courses in either the web mapping or desktop GIS analysis learning path, below.\nSpring Track R Requirements Overview\nThose participating in this spring track will be asked to attend a total of seven short courses during the spring semester. \u00a0There are three requirement areas:\n1. \u00a0\u00a0Faculty participating in this track will be required to attend one of FDI's \"Campus Resources at VT\" sessions. There are five different times to choose from.\n2. \u00a0\u00a0Four required courses.\n3. \u00a0\u00a0For the last (two) sessions, participants can choose from a list of topics related to geographic information systems.\nThe table below details the requirements, with course descriptions, time and dates of the courses, and links to enroll into the course. If you plan to attend a spring alternative track, you must be able to attend required sessions from those listed below. As always, it is possible to sign up for more than the required minimum courses.\nCampus Resources at Virginia Tech\nMonday, February 6\n"}, {"score": 872.3422, "uuid": "affe85fd-7f4e-52ce-8d31-b78e71529e43", "index": "cw12", "trec_id": "clueweb12-0004wb-22-06128", "target_hostname": "blogs.tedneward.com", "target_uri": "http://blogs.tedneward.com/CategoryView,category,Python.aspx", "page_rank": 3.677334e-09, "spam_rank": 84, "title": "Interoperability Happens - <em>Python</em>", "snippet": "<em>I</em> almost wish all three chapters had been collapsed into one\u2014suffice it to say, <em>I</em> don&#x27;t feel like <em>I</em> know the <em>Python</em> language, and don&#x27;t feel like this book could be my <em>Python</em> reference next to me as <em>I</em> <em>learn</em> it, and <em>I</em> know that it&#x27;s not a great . NET reference, either.", "explanation": null, "document": "Is Programming Less Exciting Today?\nAs discriminatory as this is going to sound, this one is for the old-timers. If you started programming after the turn of the milennium, I don\u2019t know if you\u2019re going to be able to follow the trend of this post\u2014not out of any serious deficiency on your part, hardly that. But I think this is something only the old-timers are going to identify with. (And thus, do I alienate probably 80% of my readership, but so be it.)\nIs it me, or is programming just less interesting today than it was two decades ago?\nBy all means, shake your smartphones and other mobile devices at me and say, \u201cDude, how can you say that?\u201d, but in many ways programming for Android and iOS reminds me of programming for Windows and Mac OS two decades ago. HTML 5 and JavaScript remind me of ten years ago, the first time HTML and JavaScript came around. The discussions around programming languages remind me of the discussions around C++. The discussions around NoSQL remind me of the arguments both for and against relational databases. It all feels like we\u2019ve been here before, with only the names having changed.\nDon\u2019t get me wrong\u2014if any of you comment on the differences between HTML 5 now and HTML 3.2 then, or the degree of the various browser companies agreeing to the standard today against the \u201cbrowser wars\u201d of a decade ago, I\u2019ll agree with you. This isn\u2019t so much of a rational and logical discussion as it is an emotive and intuitive one. It just feels similar.\nTo be honest, I get this sense that across the entire industry right now, there\u2019s a sort of malaise, a general sort of \u201cBah, nothing really all that new is going on anymore\u201d. NoSQL is re-introducing storage ideas that had been around before but were discarded (perhaps injudiciously and too quickly) in favor of the relational model. Functional languages have obviously been in place since the 50\u2019s (in Lisp). And so on.\nMore importantly, look at the Java community: what truly innovative ideas have emerged here in the last five years? Every new open-source project or commercial endeavor either seems to be a refinement of an idea before it (how many different times are we going to create a new Web framework, guys?) or an attempt to leverage an idea coming from somewhere else (be it from .NET or from Ruby or from JavaScript or\u2026.). With the upcoming .NET 4.5 release and Windows 8, Microsoft is holding out very little \u201cnew and exciting\u201d bits for the community to invest emotionally in: we hear about \u201casync\u201d in C# 5 (something that F# has had already, thank you), and of course there is WinRT (another platform or virtual machine\u2026 sort of), and\u2026 well, honestly, didn\u2019t we just do this a decade ago? Where is the WCFs, the WPFs, the Silverlights, the things that would get us fired up? Hell, even a new approach to data access might stir some excitement. Node.js feels like an attempt to reinvent the app server, but if you look back far enough you see that the app server itself was reinvented once (in the Java world) in Spring and other lightweight frameworks, and before that by people who actually thought to write their own web servers in straight Java. (And, for the record, the whole event-driven I/O thing is something that\u2019s been done in both Java and .NET a long time before now.)\nAnd as much as this is going to probably just throw fat on the fire, all the excitement around JavaScript as a language reminds me of the excitement about Ruby as a language. Does nobody remember that Sun did this once already, with Phobos? Or that Netscape did this with LiveScript? JavaScript on the server end is not new, folks. It\u2019s just new to the people who\u2019d never seen it before.\nIn years past, there has always seemed to be something deeper, something more exciting and more innovative that drives the industry in strange ways. Artificial Intelligence was one such thing: the search to try and bring computers to a state of human-like sentience drove a lot of interesting ideas and concepts forward, but over the last decade or two, AI seems to have lost almost all of its luster and momentum. User interfaces\u2014specifically, GUIs\u2014were another force for a while, until GUIs got to the point where they were so common and so deeply rooted in their chosen pasts (the single-button of the Mac, the menubar-per-window of Windows, etc) that they left themselves so little room for maneuver. At least this is one area where Microsoft is (maybe) putting the fatted sacred cow to the butcher\u2019s knife, with their Metro UI moves in Windows 8\u2026 but only up to a point.\nMaybe I\u2019m just old and tired and should hang up my keyboard and go take up farming, then go retire to my front porch\u2019s rocking chair and practice my Hey you kids! Getoffamylawn! or something. But before you dismiss me entirely, do me a favor and tell me: what gets you excited these days? If you\u2019ve been programming for twenty years, what about the industry today gets your blood moving and your mind sharpened?\n"}, {"score": 870.0888, "uuid": "3f507f22-5b68-5ed4-9355-34cac0015ea9", "index": "cw12", "trec_id": "clueweb12-1314wb-68-15327", "target_hostname": "www.r-bloggers.com", "target_uri": "http://www.r-bloggers.com/r-tutorials-from-universities-around-the-world/", "page_rank": 1.2282015e-09, "spam_rank": 75, "title": "<em>R</em> Tutorials from Universities Around the World | (<em>R</em> news &amp; tutorials)", "snippet": "You can subscribe <em>for</em> e-mail updates: And get updates to your Facebook: If you are an <em>R</em> blogger yourself you are invited to add your own <em>R</em> content feed to this site (Non-English <em>R</em> bloggers <em>should</em> add themselves- here) ggplot2 <em>r</em>-project graphics programming rstats <em>R</em> Language Books visualization <em>Data</em> events", "explanation": null, "document": "R Tutorials from Universities Around the World\nFebruary 25, 2012\nBy Pairach\n(This article was first published on Pairach Piboonrungroj \u00bb R , and kindly contributed to R-bloggers)\nOnline tutorials for statistics with R University of California at Davis, Getting Started with the R Data Analysis Package by\u00c2\u00a0Professor Norm Matloff Clarkson University, R Tutorial by\u00c2\u00a0Kelly Black York University,\u00c2\u00a0Getting started with R University of Waterloo\u00c2\u00a0(Windows,\u00c2\u00a0Unix)R Tutorial For A WINDOWS Environment University of California at Los Angles, UCLA,\u00c2\u00a0Resources to help you learn and use R [...]\nTo leave a comment for the author, please follow the link and comment on his blog: Pairach Piboonrungroj \u00bb R .\n"}, {"score": 868.5067, "uuid": "e7d3c672-4891-51db-b07f-7cdee1232b0f", "index": "cw12", "trec_id": "clueweb12-1908wb-42-02037", "target_hostname": "www.ci-train.org", "target_uri": "http://www.ci-train.org/training/lectureseries/python/tutorial.html", "page_rank": 1.2184046e-09, "spam_rank": 86, "title": "<em>Python</em> <em>for</em> Scientific Computing and Education | CI-Train", "snippet": "Qualified students <em>should</em> apply to attend the 2012 Open Science Grid User School, where they will <em>learn</em> how to use high-throughput computing to harness vast amounts of computing power <em>for</em> research. Send an email to ci-info@listserv.uark.edu <em>or</em> subscribe to the RSS Feed.", "explanation": null, "document": "Training \u00bb Lecture Series \u00bb Python \u00bb Python for Scientific Computing and Education\nPython for Scientific Computing and Education\nSource Code\nSoftware Carpentry : Introductory material on Python\nIf you need help with a Python problem, but don't know where to find the answer, maybe we can be of service. Contact us at python@hpc.uark.edu.\nSource Code\nprint \"Hello World\"\n3D Contour\nfrom mpl_toolkits.mplot3d import axes3d\r\nimport matplotlib.pyplot as plt\r\n\r\nfig = plt.figure()\r\nax = axes3d.Axes3D(fig)\r\nX, Y, Z = axes3d.get_test_data(0.05)\r\ncset = ax.contour(X, Y, Z, 16, extend3d=True)\r\nax.clabel(cset, fontsize=9, inline=1)\r\n\r\nplt.show()\n3D Radial\n# By Armin Moser\r\n#Courtesy of Matplotlib examples: http://matplotlib.sourceforge.net/examples/index.html\r\n\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\nimport matplotlib\r\nimport numpy as np\r\nfrom matplotlib import cm\r\nfrom matplotlib import pyplot as plt\r\nstep = 0.04\r\nmaxval = 1.0\r\nfig = plt.figure()\r\nax = Axes3D(fig)\r\n\r\n# create supporting points in polar coordinates\r\nr = np.linspace(0,1.25,50)\r\np = np.linspace(0,2*np.pi,50)\r\nR,P = np.meshgrid(r,p)\r\n# transform them to cartesian system\r\nX,Y = R*np.cos(P),R*np.sin(P)\r\n\r\nZ = ((R**2 - 1)**2)\r\nax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.jet)\r\nax.set_zlim3d(0, 1)\r\nax.set_xlabel(r'$\\phi_\\mathrm{real}$')\r\nax.set_ylabel(r'$\\phi_\\mathrm{im}$')\r\nax.set_zlabel(r'$V(\\phi)$')\r\nax.set_xticks([])\r\nplt.show()\n3D Surface\n#Courtesy of Matplotlib examples: http://matplotlib.sourceforge.net/examples/index.html\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\nfrom matplotlib import cm\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nfig = plt.figure()\r\nax = Axes3D(fig)\r\nX = np.arange(-5, 5, 0.25)\r\nY = np.arange(-5, 5, 0.25)\r\nX, Y = np.meshgrid(X, Y)\r\nR = np.sqrt(X**2 + Y**2)\r\nZ = np.sin(R)\r\nax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.jet)\r\n\r\nplt.show()\nBasic Conditionals\nlist2=[1,2,\"cat\",\"dog\"]\r\n\r\nif \"goat\" in list2:\r\n    print \"We have a goat!\"\r\nelif \"weasel\" in list2:\r\n    print \"We have a weasel!\"\r\nelse:\r\n    print \"We have nothing!\"\nBasic Function\ndef mid(x,y):\r\n    return(x+y)/2\r\n\r\nprint mid(0,10)\r\nprint mid(0,11)\nBasic Function 2\ndef mid(x,y):\r\n    return(x+y)/2.0\r\n\r\n\r\nprint mid(0,10)\r\nprint mid(0,11)\nBasic Import\nimport math\r\nprint math.sin(3)\r\n\r\nfrom math import sin\r\nprint sin(3)\r\n\r\n#Uncomment the \"help(math)\" below to see all the function in the math module\r\n#help(math)\nBasic Lists\n#A simple list\r\nlist1=[1,2,3,4]\r\nprint list1\r\n\r\n#A list of mismatched things\r\nlist2=[1,2,\"cat\",\"dog\",12.24356]\r\nprint list2\r\n\r\n#An empty list\r\nempty=[]\r\nprint empty\r\n\r\n#We can add things to lists\r\nlist1.append(5)\r\nprint list1\r\n\r\n#Lists are indexable\r\nprint list1[1]\r\n\r\n#We can \"slice\" lists to get subsequences\r\nprint list1[2:4]\r\nprint list1[:4]\r\n\r\n#We can even index from the end of the list\r\nprint list1[-1]\r\nprint list1[-2:]\r\n\r\n#We can see if something is in a list\r\n\"dog\" in list1\r\n\"dog\" in list2\nBasic Math\n#Subtraction\r\nprint 5-12\r\nprint 5.0-12.0\r\n\r\n#Divide two integers\r\nprint 5/12\r\n\r\n#Divide two floating point numbers\r\nprint 5/12.0\r\nprint 5.0/12\r\nprint 5.0/12.0\r\n\r\n#Python supports big integers\r\nprint 2**65\r\nprint 2**128\r\n\r\n#and complex numbers\r\nprint 3+4j\r\nprint 3+4j - 1+2j\r\nprint (3+4j) - (1+2j)\nFizzBuzz\n#this is the fizzbuzz test\r\nfor i in range(16):\r\n    if i % 3 == 0 and i % 5 == 0:\r\n        print 'fizzbuzz'\r\n    elif i % 3 == 0:\r\n        print 'fizz'\r\n    elif i % 5 == 0:\r\n        print 'buzz'\r\n    else:\r\n        print i\n"}, {"score": 868.4178, "uuid": "fa017b34-dbe9-505a-b639-f5a90de7faf7", "index": "cw12", "trec_id": "clueweb12-1908wb-56-16615", "target_hostname": "www.ci-train.org", "target_uri": "http://www.ci-train.org/training/lectureseries/python/tutorial.html/", "page_rank": 1.1700305e-09, "spam_rank": 86, "title": "<em>Python</em> <em>for</em> Scientific Computing and Education | CI-Train", "snippet": "Qualified students <em>should</em> apply to attend the 2012 Open Science Grid User School, where they will <em>learn</em> how to use high-throughput computing to harness vast amounts of computing power <em>for</em> research. Send an email to ci-info@listserv.uark.edu <em>or</em> subscribe to the RSS Feed.", "explanation": null, "document": "Training \u00bb Lecture Series \u00bb Python \u00bb Python for Scientific Computing and Education\nPython for Scientific Computing and Education\nSource Code\nSoftware Carpentry : Introductory material on Python\nIf you need help with a Python problem, but don't know where to find the answer, maybe we can be of service. Contact us at python@hpc.uark.edu.\nSource Code\nprint \"Hello World\"\n3D Contour\nfrom mpl_toolkits.mplot3d import axes3d\r\nimport matplotlib.pyplot as plt\r\n\r\nfig = plt.figure()\r\nax = axes3d.Axes3D(fig)\r\nX, Y, Z = axes3d.get_test_data(0.05)\r\ncset = ax.contour(X, Y, Z, 16, extend3d=True)\r\nax.clabel(cset, fontsize=9, inline=1)\r\n\r\nplt.show()\n3D Radial\n# By Armin Moser\r\n#Courtesy of Matplotlib examples: http://matplotlib.sourceforge.net/examples/index.html\r\n\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\nimport matplotlib\r\nimport numpy as np\r\nfrom matplotlib import cm\r\nfrom matplotlib import pyplot as plt\r\nstep = 0.04\r\nmaxval = 1.0\r\nfig = plt.figure()\r\nax = Axes3D(fig)\r\n\r\n# create supporting points in polar coordinates\r\nr = np.linspace(0,1.25,50)\r\np = np.linspace(0,2*np.pi,50)\r\nR,P = np.meshgrid(r,p)\r\n# transform them to cartesian system\r\nX,Y = R*np.cos(P),R*np.sin(P)\r\n\r\nZ = ((R**2 - 1)**2)\r\nax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.jet)\r\nax.set_zlim3d(0, 1)\r\nax.set_xlabel(r'$\\phi_\\mathrm{real}$')\r\nax.set_ylabel(r'$\\phi_\\mathrm{im}$')\r\nax.set_zlabel(r'$V(\\phi)$')\r\nax.set_xticks([])\r\nplt.show()\n3D Surface\n#Courtesy of Matplotlib examples: http://matplotlib.sourceforge.net/examples/index.html\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\nfrom matplotlib import cm\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nfig = plt.figure()\r\nax = Axes3D(fig)\r\nX = np.arange(-5, 5, 0.25)\r\nY = np.arange(-5, 5, 0.25)\r\nX, Y = np.meshgrid(X, Y)\r\nR = np.sqrt(X**2 + Y**2)\r\nZ = np.sin(R)\r\nax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.jet)\r\n\r\nplt.show()\nBasic Conditionals\nlist2=[1,2,\"cat\",\"dog\"]\r\n\r\nif \"goat\" in list2:\r\n    print \"We have a goat!\"\r\nelif \"weasel\" in list2:\r\n    print \"We have a weasel!\"\r\nelse:\r\n    print \"We have nothing!\"\nBasic Function\ndef mid(x,y):\r\n    return(x+y)/2\r\n\r\nprint mid(0,10)\r\nprint mid(0,11)\nBasic Function 2\ndef mid(x,y):\r\n    return(x+y)/2.0\r\n\r\n\r\nprint mid(0,10)\r\nprint mid(0,11)\nBasic Import\nimport math\r\nprint math.sin(3)\r\n\r\nfrom math import sin\r\nprint sin(3)\r\n\r\n#Uncomment the \"help(math)\" below to see all the function in the math module\r\n#help(math)\nBasic Lists\n#A simple list\r\nlist1=[1,2,3,4]\r\nprint list1\r\n\r\n#A list of mismatched things\r\nlist2=[1,2,\"cat\",\"dog\",12.24356]\r\nprint list2\r\n\r\n#An empty list\r\nempty=[]\r\nprint empty\r\n\r\n#We can add things to lists\r\nlist1.append(5)\r\nprint list1\r\n\r\n#Lists are indexable\r\nprint list1[1]\r\n\r\n#We can \"slice\" lists to get subsequences\r\nprint list1[2:4]\r\nprint list1[:4]\r\n\r\n#We can even index from the end of the list\r\nprint list1[-1]\r\nprint list1[-2:]\r\n\r\n#We can see if something is in a list\r\n\"dog\" in list1\r\n\"dog\" in list2\nBasic Math\n#Subtraction\r\nprint 5-12\r\nprint 5.0-12.0\r\n\r\n#Divide two integers\r\nprint 5/12\r\n\r\n#Divide two floating point numbers\r\nprint 5/12.0\r\nprint 5.0/12\r\nprint 5.0/12.0\r\n\r\n#Python supports big integers\r\nprint 2**65\r\nprint 2**128\r\n\r\n#and complex numbers\r\nprint 3+4j\r\nprint 3+4j - 1+2j\r\nprint (3+4j) - (1+2j)\nFizzBuzz\n#this is the fizzbuzz test\r\nfor i in range(16):\r\n    if i % 3 == 0 and i % 5 == 0:\r\n        print 'fizzbuzz'\r\n    elif i % 3 == 0:\r\n        print 'fizz'\r\n    elif i % 5 == 0:\r\n        print 'buzz'\r\n    else:\r\n        print i\n"}, {"score": 858.1567, "uuid": "eac8735e-eabe-57ea-b544-0f21d54d311c", "index": "cw12", "trec_id": "clueweb12-1208wb-98-20461", "target_hostname": "blog.revolutionanalytics.com", "target_uri": "http://blog.revolutionanalytics.com/2010/06/learning-r.html", "page_rank": 1.2176045e-09, "spam_rank": 68, "title": "Revolutions: Learning <em>R</em>", "snippet": "So suppose you are not a statistician but belong to some other <em>data</em> <em>analysis</em> culture, <em>data</em> mining, <em>for</em> example, and you want to <em>learn</em> <em>R</em>. Well, like any of the world\u2019s great natural languages <em>R</em> approachable from many different starting points.", "explanation": null, "document": "June 17, 2010\nLearning R\nWhen R is brought up as a possibility for doing statistics or data mining or any sort of predictive analytics\u00a0among non R users, someone will invariably point out that R has a \u201csteep learning curve\u201d, and the response among those gathered usually includes a significant amount of head nodding. Even those who have put in heroic efforts to help people learn R sometimes say scary things: e.g. in his introduction to Rattle, a Data Mining GUI for R, Graham Williams writes:\nR offers a breadth and depth in statistical computing beyond what is available in commercial closed source products. Yet R remains, primarily, a programming language for the highly skilled statistician, and out of the reach of many. ( The R Journal Vol. \u00bd, December 2009 )\nAre things really that bad? Is R that difficult to learn? I think not - unless you take the statement to refer to the absolutely worst case scenario that you can think of:\u00a0the effort that would be involved in learning R by a person who doesn\u2019t have any background in statistics or data analysis and who has absolutely no interest in learning R. In this context the statement that R has a steep learning curve conveys the same truth as the assertion that Chinese or Italian or Japanese or English or any other language is difficult to learn if you don\u2019t know anything about the people who speak these languages, don\u2019t want to know, and wouldn\u2019t have an opportunity to practice the language anyway. There is some truth to this, but so what? Context is everything. If you have some background in statistics and a desire to learn some more, learning R is not going to be an insurmountable problem. Once you have some version of R installed on your favorite computing platform, any book that provides carefully worked examples of the kinds of statistical analyses that interest you should be all that is required to make you productive. These days, there are dozens or maybe even hundreds of statistics books that use R. Two of my personal favorites are John Fox\u2019s classic An R and S Plus Companion to Applied Regression (Sage Publications, 2002) and Data Analysis and Graphics Using R: An Example-Based Approach (Cambridge Series in Statistical and Probabilistic Mathematics) by John Maindonald and W. John Braun (2010). Books, of course, are only the tip of the iceberg of the resources available for the statistical cognoscenti to learn R. Have a look at the links on Inside-R as places to start on the web.\nThis discussion does raise the issue of what one means by learning a language. Knowing how to run the R scripts required to get some simple analysis done may not entitle you to say that you know R (significantly more is required to become an R developer) anymore than understanding enough Italian to get around Rome while eating well entitles you to say that you know Italian \u2013 but again, so what? It is possible to do some pretty impressive statistics using R without any deeper understanding of the R language than what is required to run the applicable models. On the other hand, if you don\u2019t know any statistics and you don\u2019t want to know more than you have to, then that might be a big problem. But, it\u2019s the statistics part that has the steep learning curve, not R. Maybe this is the point of William\u2019s comment: R remains the language of highly skilled statisticians because it is the language most capable of expressing what highly trained statisticians think about, and R is out of the reach of the many who just don\u2019t care much about statistics. But, if you are reading this post this many probably doesn\u2019t include you.\nSo suppose you are not a statistician but belong to some other data analysis culture, data mining, for example, and you want to learn R. Well, like any of the world\u2019s great natural languages R approachable from many different starting points. The entry point may be different but the path to knowledge is going to be similar. Find something you care about and start formulating simple sentences. There are fewer R language guides available for people who have some data mining skills, but that is changing. Seni and Elder have recently published a very nice little book on ensemble methods: Ensemble Methods in Data Mining: Improving Accuracy Through Combining Predictions (Synthesis Lectures on Data Mining and Knowledge Discovery) that I highly recommend.\u00a0 And, of course the masters Hastie, Tibshirani and Freidman authors of the classic text, Elements of Statistical Learning , have made both the text and much of the their code available. Moreover, if you are a data miner with a background in computer science you can probably code rings around the highly trained statisticians. If so, you may find the books by Chambers, Software for Data Analysis and Gentlemen, R Programming for Bioinformatics of great value.\nFinally, to be perfectly fair, I should acknowledge Graham Williams\u2019 quote in the context in which he wrote it. Like any other language, R is much easier to learn when you have access to the best learning tools, CDs, dictionaries, grammars etc. Rattle is one such high value learning tool and so is Revolution\u2019s R Productivity Environment . But, more about these on another day.\nPosted by Joseph Rickert at 07:00 | Permalink\nTrackBack\nTrackBack URL for this entry:\nhttp://www.typepad.com/services/trackback/6a010534b1db25970b01348435844e970c\n"}, {"score": 858.0466, "uuid": "eb43346e-c889-5486-9b27-d127d6051b33", "index": "cw12", "trec_id": "clueweb12-0208wb-70-00783", "target_hostname": "www.omegahat.org", "target_uri": "http://www.omegahat.org/SVGAnnotation/SVGAnnotationPaper/SVGAnnotationPaper.html", "page_rank": 1.2102259e-09, "spam_rank": 84, "title": "Interactive and Animated Scalable Vector Graphics and <em>R</em> <em>Data</em> Displays", "snippet": "<em>For</em> example, someone who does not know <em>R</em> <em>or</em> even have the original <em>data</em> can change the contents of the CSS file that the SVG document uses, <em>or</em> substitute a different CSS file in order to, <em>for</em> example, change the color of points in the <em>data</em> region <em>or</em> the size of the line strokes <em>for</em> the axes labels.", "explanation": null, "document": "Interactive and Animated Scalable Vector Graphics and R Data Displays\nDeborah Nolan\n< duncan@wald.ucdavis.edu >\nAbstract\nWe describe an approach to creating interactive and animated graphical displays using R's graphics engine and Scalable Vector Graphics, an XML vocabulary for describing two-dimensional graphical displays. We use the svg() graphics device inR and then post-process the resulting XML documents. The post-processing identifies the elements in the SVG that correspond to the different components of the graphical display, e.g., points, axes, labels, lines. One can then annotate these elements to add interactivity and animation effects. One can also use JavaScript to provide dynamic interactive effects to the plot, enabling rich user interactions and compelling visualizations. The resulting SVG documents can be embedded within HTML documents and can involve JavaScript code that integrates the SVG and HTML objects. The functionality is provided via the SVGAnnotation package and makes static plots generated viaR graphics functions available as stand-alone, interactive and animated plots for the Web and other venues.\nIntroduction\nThe way we view graphical representations of data has significantly changed in recent years. For example, viewers expect to interact with a graphic on the Web by clicking on it to get more information, to produce a different view, or control an animation. These capabilities are available in Scalable Vector Graphics (SVG) [ SVGEssentials ], and in this article, we describe an approach within R [ R ] to creating stand-alone interactive graphics and animations using SVG. SVG offers these capabilities through a rich set of elements that allow the graphics objects to be grouped, styled, transformed, composed with other rendered objects, clipped, masked, and filtered. SVG may not be the most ideal approach to interactive graphics, but it has advantages that come from its simplicity and increasing use on the Web and in publishing generally. The approach presented here uses R's high-level plotting functions, R's SVG graphics device engine and the third-party cairo rendering engine [ libcairo ] to create high-quality plots in SVG. Our tools allow users to post-process SVG documents/plots to provide the additional annotations to make the document interactive with, e.g., hyperlinks, tool tips, sliders, buttons, and animation to create potentially rich, compelling graphics that can be viewed outside of the R environment.\nThe SVGAnnotation package [ SVGAnnotation ] contains a suite of tools to facilitate the programmer in this process. The package provides \u201chigh-level\u201d facilities for augmenting the SVG output from the standard plotting functions in R, including lattice [ lattice ] and the traditional plotting functions. In addition to providing useful higher-levels facilities for modifying the more common R plots, the SVGAnnotation package also provides many utilities to handle other types of plots, e.g., maps and graphs. Of course, it is possible to generate interactive graphics from \u201cscratch\u201d in SVG by taking charge of all drawing, e.g., determining the coordinate system and drawing the axes, tick marks, labels, etc. However for complex data plots, we are much better off using this post-processing approach and leveraging R's excellent existing graphics facilities.\nWe do not want to suggest that the approach presented here is the definitive or dominant solution to the quest for general interactive graphics for R or statistics generally. Our focus is on making graphical displays created in R available in new ways to different audiences outside of R and within modern multi-media environments. This approach uses the XML [ OpenXML ] parsing and writing facilities in the XML package [ XMLpkg ], and the availability of XML tools make it an alternative that we believe is worth exploring. While the popularity or uptake of this approach may be debatable due to prospects of SVG or implementations of the complete SVG specification in widely used browsers, what we have implemented can be readily adapated to other formats such as Flash [ FlashPlayer ] or the JavaScript canvas [ JavaScriptGuide ]. Hence this paper also provides ideas for future work with other similar graphical formats such as Flash, Flex MXML [ Flex ], and the JavaScript canvas. The essential idea is to post-process regular R graphics output and identify and combine the low-level graphical objects (e.g., lines and text) into higher-level components within the plot (e.g., axes and labels). Given this association, we can then annotate components to create interactive, dynamic and animated plots in various formats.\nThe remainder of the paper is organized as follows. We start by explaining why SVG is a valuable format. We then give a brief introduction to several examples that illustrate different features of SVG and how they might be used for displaying statistical results. We move on to explore general facilities that one may use to add to an SVG plot generated within R. The examples serve as the context for the discussion of the functions in SVGAnnotation and the general approach used in the package. The facilities include adding tool tips and hyperlinks to elements of a display. Next, we introduce the common elements of SVG and examine the typical SVG document produced when plotting in R. This lays the ground work for using more advanced features in SVGAnnotation , such as how to handle non-standard R displays, animation, GUI components in SVG, and HTML forms. Additionally, we illustrate aspects of integrating JavaScript and SVG. We conclude the paper by discussing different directions to explore in this general area of stand-alone, interactive, animated graphics that can be displayed on the Web.\nSeveral technologies in this paper (XML, SVG, and JavaScript) may be new to readers. For this reason, we provide an introduction to XML and also to JavaScript as appendices. It may be useful for some readers to review these before reading the rest of this paper.\nWe also note that this paper can be viewed as an HTML document (at http://www.omegahat.org/SVGAnnotation/JSSPaper.html ) or as a document containing just the figures ( http://www.omegahat.org/SVGAnnotation/SVGAnnotationPaperFigures.html ). The SVG displays within the HTML version are \"live\", i.e., interactive and/or animated. The reader may choose to switch to reading this version of the paper as it is a better medium for understanding the material.\nWhy SVG?\nScalable Vector Graphics (SVG) is an XML format for describing two dimensional (2-D) graphical displays that also supports interactivity, animation, and filters for special effects on elements within the display. SVG is a vector-based system that describes an image as a series of geometric shapes. This is in contrast to a raster representation that uses a rectangular array of pixels (picture elements) to represent what appears at each location in the display. An SVG document includes the commands to draw shapes at specific sets of coordinates. These shapes are infinitely scalable because they are vector descriptions, i.e., the viewer can adjust and change the display (for example, to zoom in and refocus) and maintain a clear picture.\nSVG is a graphics format similar to PNG, JPEG, and PDF. Many commonly used Web browsers directly support SVG (Firefox, Safari, Opera), and there is a plug-in for Internet Explorer. For example, Firefox can act as a rendering engine that interprets the vector description and draws the objects in the display within the page on the screen. There are also other non-Web-browser viewers for SVG such as Inkscape ( http://www.inkscape.org/ ) and Squiggle (based on Apache's Batik http://xmlgraphics.apache.org/batik/tools/browser.html ). Similar to JPEG and PNG files, SVG documents can be included in HTML documents (they can also be in-lined within HTML content). However, quite differently from other image formats, SVG graphics, and their sub-elements, remain interactive when displayed within an HTML document and can participate as components in rich applications that interact with other graphics and HTML components. SVG graphics can also be included in PDF documents via an XML-based page description language named Formatting Objects (FO) [ FO ], which is used for high-quality typesetting of XML documents. SVG is also capable of fine-grained scaling. For these reasons, SVG is a rich and viable alternative to the ubiquitous PNG and JPEG formats.\nThe size of SVG files can be both significantly smaller and larger than the corresponding raster displays, e.g., PNG and JPEG. This is very similar to PDF and Postscript formats. For simple displays with few elements, an SVG document will have entries for just those elements and so be quite small. Bitmap formats however will have the same size regardless of the content as it is the dimensions of the canvas that determines the content of the file. As a result, for complex displays with many elements, an SVG file may be much larger than a bitmap format. Furthermore, SVG is an XML vocabulary and so suffers from the verbosity of that format. However, SVG is also a regular text format and so can be greatly compressed using standard compression algorithms (e.g., GNU zip). This means we cannot simply compare the size of compressed SVG files to uncompressed bitmap formats as we should compare sizes when both formats are compressed. JPEG files, for example, are already compressed and so direct comparisons are appropriate. Most importantly, we should compare the size of files that are directly usable. SVG viewer applications (e.g., Web browsers, Inkscape) can read compressed SVG (.svgz) files, but they typically do not read compressed bitmap formats. As a result, compressed SVG files can be significantly smaller than comparable bitmap graphics, and so utilize less bandwidth and are faster to download.\nAs just mentioned, an SVG file is a plain-text document. Since it is a grammar of XML, it is also highly structured. Being plain text means that it is relatively easy to create, view and edit, and the highly structured nature of SVG makes it easy to modify programmatically. These features are essential to the approach described here: R is used to create the initial plot; we then programmatically identify the elements in the SVG document that correspond to the components of the plot, and augment the SVG document with additional information to create the interactivity and/or animation.\nAs a grammar of XML, SVG separates content and structure from presentation, e.g., the physical appearance of components in the presentation can be modified with a Cascading StyleSheet [ CSSReference ] without the need to change or re-plot the graphic. For example, someone who does not know R or even have the original data can change the contents of the CSS file that the SVG document uses, or substitute a different CSS file in order to, for example, change the color of points in the data region or the size of the line strokes for the axes labels. While this is non-trivial, it is feasible and certain annotations on the SVG content added by the SVGAnnotation package make this simpler. An additional feature of SVG is that elements of a plot can be grouped together and operated on as a single object. This makes it relatively easy to reuse visual objects and also to apply operations to them both programmatically and interactively. This is quite different from the style of programming available in traditional R graphics which draws on the canvas and does not work with graphical objects.\nThe SVG vocabulary provides support for adding interaction and animation declarations that are used when the SVG is displayed. However, one of the powerful aspects of SVG is that we can also combine SVG with JavaScript (also known as ECMAScript) [ JavaScriptGuide ]. This allows us to provide interaction and animation programmatically during the rendering of the SVG rather than declaratively through SVG elements. Both approaches work well in different circumstances. Several JavaScript functions are provided in the SVGAnnotation package to handle common cases of interaction and animation. For more specialized graphical displays, the creator of the annotated SVG document may also need to write JavaScript code, which means working in a second language. For some users, this will be problematic, but this package and the JRSONIO package do provide some facilities to aid in this step, e.g., making objects in R available to the JavaScript code. (See addECMAScripts() and examples Example\u00a07, \u201cInteractive nearest neighbors\u201d and Example\u00a08, \u201cHighlighting nodes and edges in a graph\u201d , amongst others.) While switching between languages can be difficult, we should recognize that the SVG plots are being displayed in a very different medium and use a language (JavaScript) that is very widely used for Web content.\nThe interactivity and animation described here are very different from what is available in more commonly used statistical graphics systems such as GGobi [ GGobi ], iPlots [ iPlots ], or Mondrian [ mondrian ]. Each of these provide an interactive environment for the purpose of exploratory visualization and are intended primarily for use in the data analysis phase. While one could in theory build similar systems in SVG, this would be quite unnatural. Instead, SVG is mostly used for creating presentation graphics that typically come at the end of the data analysis stage. Rather than providing general interactive features for data exploration, we use SVG to create application-specific interactivity, including graphical interfaces for a display.\nExamples of interactivity with SVG\nTo get a sense of the possibilities for interactivity with SVG, we first present a relatively comprehensive set of examples. Later sections give the details on how these are created. We begin with very simple examples that use high-level facilities in the SVGAnnotation package and proceed to more complex cases built using the utility functions also in the package. Some of these more advanced displays require a deeper understanding of how particular plots created in R are represented in SVG and an ability to write JavaScript.\nThe examples are intended to introduce the reader to the capabilities of SVG. They are also arranged to gradually move from high-level facilities to more technical details, i.e., they increase in complexity. The aim is to provide readers with sufficient concrete examples and accompanying code to develop SVG-based interactive displays themselves. When readers have worked through these examples, they will hopefully have the facilities to create SVG and their own customized displays. The code for these and other examples appear in subsequent sections of the paper.\nTool tips and hyperlinks are very simple, but effective, forms of interactivity. With hyperlinks, the user interacts with a plot by clicking on an active region, say an axis label or a point, and in response, the browser opens the referenced Web page. An example of this is shown in the scatter plot in Figure\u00a01, \u201cHyperlinks on titles\u201d . When the mouse moves over the title of the plot, the status bar at the bottom of the screen shows the URL of the USGS map of the South Pacific that will be loaded once the mouse is clicked.\nCode\n# Load the required packages \nlibrary(\"XML\")\nlibrary(\"SVGAnnotation\")\n\n# Create the basic plot\ndoc = svgPlot(plot(mpg ~ wt, mtcars, \n                   main = \"Motor Trend Car Road Tests\", pch=19, \n                   col= \"#377EB8\"))\n\n # Annotate title and axes\nax = getAxesLabelNodes(doc)\n\n # Add link from title to motor trend website\naddLink(ax$title, \"http://www.motortrend.com\", addArea = TRUE) \n\n # Add tooltips to the x, y axes\ntips = c(\"Weight in units of lb/1000\", \"Miles per US gallon\")\naddToolTips(ax[c(\"xaxis\", \"yaxis\")], tips) \n\naddToolTips(doc, apply(mtcars, 1, function(x) \n                       paste(names(mtcars), x, sep = \" = \", \n                                   collapse = \", \")))\n\n\nsaveXML(doc, \"../mt_tips.svg\")\ndepth.col = gray.colors(100)[cut(quakes$depth, 100, label=FALSE)]\ndepth.ord = rev(order(quakes$depth))\ndoc = svgPlot(\n plot(lat ~ long, data = quakes[depth.ord, ],\n      pch = 19, col = depth.col[depth.ord],\n      xlab = \"Longitude\", ylab=\"Latitude\",\n      main = \"Fiji Region Earthquakes\") )\nax = getAxesLabelNodes(doc)\naddToolTips(ax[c(\"xaxis\", \"yaxis\")], c(\"Degrees east of the prime meridean\",\n            \"Degrees south of the equator\"), addArea = TRUE)\nusgs = \"http://earthquake.usgs.gov/eqcenter/recenteqsww/\"\nregion = \"Maps/region/S_Pacific.php\"\naddAxesLinks(ax$title, paste(usgs, region, sep = \"\"))\naddToolTips(doc, \n             apply(quakes[depth.ord, ], 1, \n                    function(x)\n                       paste(names(quakes), x, \n                             sep = \" = \", collapse = \", \")))\n# This is a relative path to the CSS file.\n# The default path will be absolute.\n#addCSS(doc, css = \"../../inst/CSS/RSVGPlot.css\")\nsaveXML(doc, \"quakes_tips.svg\")\nThis scatter plot was produced by a call to the plot() function in R. After the plot was created, a hyperlink was added to the title of the plot using the addAxesLinks() function in SVGAnnotation . When viewed in a browser, a mouse click on the title will make the browser open up the url shown in the status bar of the screen shot. (This plot is adapted from [ Sarkar ].)\nWith tool tips, the user interacts with a plot by pausing the pointer over a portion of the plot, say an axis label or a point. This \u201cmouse-over\u201d action causes a small window to pop up with additional information. Figure\u00a02, \u201c Tool tips on a hexbin plot \u201d shows an example where the mouse has been placed on an hexagonal bin in the plot and a tool tip has consequently appeared to provide a count of the number of observations in that bin. Note that these forms of interactivity do not require R or JavaScript within the browser; they merely require an SVG-compliant browser.\nlibrary(\"SVGAnnotation\")\ndata(\"traffic\")\nOccupancy = unlist(traffic[ c(\"Occ1\", \"Occ2\", \"Occ3\")])\nFlow = unlist(traffic[c(\"Flow1\", \"Flow2\", \"Flow3\")])\nlibrary(\"hexbin\")\nhbin = hexbin(Occupancy, Flow)\ndoc = svgPlot(\n        plot(hbin, \n             main = \"Loop Detector #313111 on I-80E Sacramento\"))\nptz = getPlotPoints(doc)\nlength(ptz)\ntips = paste(\"Count: \", hbin@count)\naddToolTips(ptz, tips, addArea = TRUE)\nsaveXML(doc, \"hexbin.svg\")\nThe hexagonal-bin plot shown here was generated by a call to the hexbin() function. After the plot was created, the SVG file was then post-processed with addToolTips() to add a tool tip to each of the shaded hexagonal regions. When the mouse hovers over a hexagon, the number of observations in that bin appears as a tool tip.\nIt is also possible to link observations across sub-plots. For example, Figure\u00a03, \u201cLinked scatter plots\u201d shows the mouse hovering over a point in one plot. This mouse-over action causes that point and the corresponding point in a second plot to be colored red. When the mouse moves off the point, both points return to their original color(s). Another example found later in this paper demonstrates how to link across conditional plots, where the mouse-over action on a plot legend causes the corresponding group of points to be highlighted in the panels ( Figure\u00a011, \u201c Linking a legend to a group of points in a conditional plot \u201d ). This linking of points across plots uses reasonably generic JavaScript code that is provided in the SVGAnnotation package.\nCode\nlibrary(\"SVGAnnotation\")\n\ndoc = svgPlot({ par(mfrow = c(1, 2))\n  plot(mpg ~ wt, mtcars, main=\"MPG\", ylab=\"\", cex = 1.4)\n  plot(hp ~ wt, mtcars, main = \"Horsepower\", ylab=\"\", cex = 1.4)\n})\n\nlinkPlots(doc)\nsaveXML(doc, \"../mt_link.svg\")\ndoc = svgPlot({par(mfrow = c(1,2))\n    plot(Murder ~ UrbanPop, USArrests, main=\"\", cex = 1.4)\n    plot(Rape ~ UrbanPop, USArrests, main = \"\", cex = 1.4)\n    }, \n  width = 14, height = 7)\nlinkPlots(doc)\nsaveXML(doc, \"USArrests_linked.svg\")\nThe points in the two scatter plots shown here are linked. When the mouse covers a point in one plot, that point changes color as does the point in the other plot that corresponds to the same observation. When the mouse moves off the point, then the linked points return to their original color(s). The linkPlots() function in SVGAnnotation takes care of embedding the necessary annotations and JavaScript code in the SVG file to change the color of the points.\nSVG provides basic animation capabilities that can be used to animate R graphics. For example, it is possible to create animations similar in spirit to the well-known Gapminder animation [ GapMinder ], where points move across a canvas according to a time variable. See Figure\u00a04, \u201cScatter plot animation\u201d as an example. This animation is created by simply adding SVG elements/commands to the SVG content generated by R. In other words, no JavaScript code is needed to create the animation effects.\nCode\nlibrary(\"RColorBrewer\")\nlibrary(\"SVGAnnotation\")\nlibrary(\"XML\")\n\nfixNAs = function(x) {\n  na = is.na(x)\n  if(!any(na))\n    return(x)\n\n  NApos = which(na)\n  notNA = which(!na)\n  \n  if (any(NApos < min(notNA))) {\n    NApos = NApos[ NApos > min(notNA)]\n  }\n\n  if (length(NApos))\n    x[ NApos ] =  x[sapply(NApos, function(i) notNA[max(which(notNA < i))] )]\n\n  return(x)\n}\n\nfirstNAs = function(x) {\n na = is.na(x)\n if (!any(na))\n   return(x)\n\n NApos = which(na)\n notNA = which(!na)\n\n if (length(NApos)) {\n    x[NApos] = x[notNA[1]] \n }\n\n return(x)\n}\n\nload(system.file(\"examples\", \"GapMinder\", \"gapminder.rda\", package = \"SVGAnnotation\"))\nctry = c(\"Argentina\", \"Australia\", \"Austria\", \"Bangladesh\", \"Belgium\",\"Brazil\", \n        \"Bulgaria\", \"Canada\",  \"Chile\", \"China\", \"Colombia\", \"Costa Rica\", \"Cuba\", \"Cyprus\",\n        \"Denmark\", \"Fiji\", \"Finland\",\"France\", \"Germany\", \"Ghana\", \"Greece\", \"Guatemala\",\n        \"Hungary\", \"India\", \"Indonesia\", \"Ireland\", \"Italy\", \"Jamaica\", \"Japan\", \n        \"Netherlands\", \"Norway\", \"Peru\", \"Philippines\", \n        \"Portugal\", \"United Kingdom\", \"United States\")\nyrA = cut(dd$year, \"year\")\nyr4 = gsub(\"-.*\", \"\", yrA)\nyrA = trunc(as.integer(yr4)/10) * 10\nyr = yrA\n\ncont = c(\"AM\", \"EA\", \"EU\", \"SA\", \"EU\", \"AM\", \n         \"EU\", \"AM\", \"AM\", \"EA\", \"AM\", \"AM\", \"AM\", \"EU\", \n         \"EU\", \"EA\", \"EU\", \"EU\", \"EU\", \"SS\", \"EU\", \"AM\", \n         \"EU\", \"SA\", \"EA\", \"EU\", \"EU\", \"AM\", \"EA\", \n         \"EU\", \"EU\", \"AM\", \"EA\", \n         \"EU\", \"EU\", \"AM\")\nkeep = (dd$country %in% ctry) & (yr >= 1890)\nyr = yr[keep]\ndd = dd[keep, ]\n\nvars = c(\"longevity\", \"income\", \"population\")\ngapM = list()\n\n\nfor(i in vars) {\n  x = dd[[i]]\n  for (j in ctry) {\n     x[dd$country == j] = fixNAs(x[dd$country == j])\n  }\n  y = tapply(x, data.frame(yr, dd$country), max, na.rm = TRUE)\n\n  w = y[, levels(dd$country) %in% ctry]\n  w = as.vector(t(w[-1, ]))\n  w[w == -Inf] = NA\n  gapM[[i]] = w \n}\n  \ngapM = as.data.frame(gapM) \n\nnc  = length(unique(dd$country))\nny  = length(unique(yr[yr>1890]))\n\ndecade = rep(unique(yr[yr>1890]), rep(nc, ny))\ngapM$yr = decade   \ncountry = rep(unique(dd$country), ny)\ngapM$country = country\nhead(gapM)\nlongevity    income population   yr    country\n1     39.70 4708.2323    6584000 1900  Argentina\n2     63.22 6740.9394    4278000 1900  Australia\n3     42.00 5163.9268    6550000 1900    Austria\n4     22.00  794.8383   31786000 1900 Bangladesh\n5     51.11 4746.9662    7478000 1900    Belgium\n6     32.90  705.4758   21754000 1900     Brazil\ntail(gapM)\nlongevity   income population   yr        country\n391    80.550 48264.67    4610820 2000         Norway\n392    69.906  6875.51   28302603 2000           Peru\n393    70.303  3030.88   89468677 2000    Philippines\n394    78.920 20149.08   10605870 2000       Portugal\n395    78.471 32334.53   60609153 2000 United Kingdom\n396    77.890 42445.70  298444215 2000  United States\nfor (i in vars) {\n  for (j in ctry) {\n    gapM[gapM$country == j, i] = firstNAs(gapM[gapM$country == j, i])\n  }\n}\nrad = 1 + 10 * sqrt(gapM$population)/max(sqrt(gapM$population))\ndisappear = is.na(gapM$longevity) | is.na(gapM$income) | \n            is.na(gapM$population)\nrad[disappear] = 0.00001 \nradL = lapply(ctry, function(i) rad[gapM$country == i])\nnames(radL) = ctry\ncolI = c(\"#E41A1C80\", \"#377EB880\", \"#4DAF4A80\", \"#984EA380\", \"#FF7F0080\", \"#FFFF3380\")\ncolB = c(\"#E41A1CFF\", \"#377EB8FF\", \"#4DAF4AFF\", \"#984EA3FF\", \"#FF7F00FF\", \"#FFFF33FF\")\nnames(colB) = c(\"EA\", \"AM\", \"ME\", \"SA\", \"SS\", \"EU\")\nnames(colI) = names(colB)\ncolsB = colB[cont]\ncolsI = colI[cont]\nlongCont = c(\"East Asia\", \"America\", \"Middle East\", \"South Asia\", \n              \"Sub-Sahara Africa\", \"Europe & Central Asia\")\ndoc = svgPlot( {\n  plot(longevity ~ income, \n          subset(gapM, yr == 1900 & country %in% ctry), \n    pch = 21, col = colsB, bg = colsI,\n    xlab = \"Income\", ylab = \"Life Expectancy\", \n    axes = FALSE, \n    xlim = c(-400, 50000), ylim = c(20, 85) )\n    box()\n    y.at = seq(20, 85, by = 5)\n    x.at = c(200, 400, 1000, 2000, 4000, 10000, 20000, 40000) \n    axis(1, at = x.at, labels = formatC(x.at, big.mark = \",\", \n                                         format = \"d\"))\n    axis(2, at = y.at, labels = formatC(y.at) )\n    abline(h = y.at, col=\"gray\",  lty = 3)\n    abline(v = x.at, col=\"gray\",  lty = 3)\n    legend(35000, 40, longCont, col = colB, fill = colB, \n           bty = \"n\", cex = 0.75 )\n   })\naddToolTips(doc,\n   as.character(gapM$country[gapM$yr == 1900 & gapM$country %in% ctry]))\nanimate(doc, \n        gapM[gapM$country %in% ctry, c(\"income\", \"longevity\")], \n        gapM$yr[gapM$country %in% ctry], \n        dropFirst = TRUE,\n        labels = seq(1900, length = 11, by = 10),\n        begin = 0, interval = 3,  radii = radL[ctry])\nsaveXML(doc, \"gapM.svg\")\nThis screen shot shows a scatter plot at the end of an animation. Each circle represents one country. The location of the circle's center corresponds to (income, life expectancy) and the area is proportional to the size of the population. Time is represented through the movement of the circle center from the (x,y) value for one decade to the next. The animate() function in SVGAnnotation provides the basic functionality to create this scatter plot animation. Reloading the page will restart the animation. At the time of writing, this animation is only visible in the Opera browser [ opera ].\nThe Carto:Net project [ CartoNet ] has made available a Graphical User Interface (GUI) library for SVG. For convenience, this library is distributed as part of the SVGAnnotation package. It can be used to add controls such as sliders, radio boxes, and choice menus to an SVG plot. These controls provide an interface for the viewer to have more complex interactions with the graphic. For example, in Figure\u00a05, \u201cA slider for choosing a parameter value\u201d the location of the slider thumb specifies the bandwidth used to smooth data. When the viewer moves the slider thumb, the corresponding smoothed curve is displayed in the left-hand plot, and the right-hand plot is updated to display the residuals from the newly fitted curve. The interactivity of the GUI controls are provided via JavaScript functionality in Carto:Net. Additional JavaScript is required to perform the plot-specific functionality, i.e., to display the correct curve and residuals in the figure. We note that R is not involved when the plot is being viewed and the different curves are being displayed. This is done entirely via SVG and JavaScript because all fitted values are precomputed within R and serialized to the JavaScript code.\nCode\nlibrary(\"XML\")\nlibrary(\"SVGAnnotation\")\ndata(\"rat.diet\", package=\"fields\")\nlambdas= 2:floor(0.6 * length(unique(rat.diet$t)))\nxrange = range(rat.diet$t)\nxinterps = seq(xrange[1], xrange[2], by = 0.5)\ndoc = svgPlot({\n          par(mfrow = c(1, 2))\n\n          plot(con ~ t, data = rat.diet, log = \"\",\n                xlim = xrange, \n                xlab = \"Time (days)\", ylab = \"Median Food Intake\", \n                main = \"Control group\")\n\n          predicted = lapply(lambdas, function(lam) {\n                         spl = smooth.spline(rat.diet$t, rat.diet$con, df = lam)\n                        lines(predict(spl, xinterps), col=\"green\", lwd = 2)\n                        predict(spl, rat.diet$t)\n                                     })\n\n          range.y = range(unlist(sapply(predicted, function(pred) {\n                       range(rat.diet$con - pred$y)})))\n          \n          plot(y = 0, x = 0, xlim = xrange,  ylim = range.y, type = \"n\",\n                main = paste(\"Residual plot\"), ylab = \"Residuals\", \n                xlab = \"Time (days)\")\n          abline(h = 0, col = \"lightgray\", lwd = 2, lty = 3)\n          sapply(predicted, function(p) points(p$x, rat.diet$con - p$y))\n       })\nplots = getPlotRegionNodes(doc)\nlines = getNodeSet(doc, \"//x:path[contains(@style, 'rgb(0%,100%,0%)')]\", \"x\")\nlength(lines) == length(lambdas)\ninvisible(mapply(function(lam, node) {\n                    xmlAttrs(node, append =TRUE) = c(id = lam, visibility = \"hidden\")\n                 }, paste(\"curve-lambda-\", lambdas, sep = \"\"), lines))\n\nxmlAttrs(lines[[1]], append = TRUE) = c(visibility = \"visible\")\nnumPoints = nrow(rat.diet) \n points = xmlChildren(plots[[3]])[-1]\n lambdaVal = rep(lambdas, each = numPoints)\nindex = matrix(1:length(points), , length(lambdas))\n  at = plots[[3]]\n  nodes = sapply(seq(along = lambdas),\n              function(i) {\n              g = newXMLNode(\"g\", attrs = \n                        c(id = paste(\"residual-group\", lambdas[i], sep = \"-\"),\n                          visibility = \"hidden\"), \n                         parent = at, \n                         namespaceDefinitions = c(xlink = \"http://www.w3.org/1999/xlink\") )\n              removeNodes(points[index[,i]])\n              addChildren(g, points[index[,i]])\n \n         })\n  xmlAttrs(nodes[[1]], TRUE) = c(visibility = \"visible\")\n#jscript = c(\"../Javascript/linkedSmootherInit.js\",\n#            \"../Javascript/linkedSmootherSet.js\")\nsvgRoot = xmlRoot(doc)\nenlargeSVGViewBox(doc, y = 100, svg = svgRoot)\nonl = sprintf(\"init(evt, %d);\", max(lambdas) )\njscript = list.files(path = system.file(\"examples\", \"Javascript\", \n                              package = \"SVGAnnotation\"), \n                     full.names = TRUE, pattern = \"linkedSmoother\")\naddSlider(doc, onload = onl, svg = svgRoot,\n          javascript = jscript, id = \"slider-lambda\")\nsaveXML(doc, \"linkedSmoother.svg\")\nThis pair of plots were made in R. Hidden within the plot on the left are curves from the fit for many different values of the smoothing parameter, and hidden in the right-hand plot are the residuals for each of those fits. The slider displayed across the bottom of the image is provided by Carto:Net [ CartoNet ]. It is added to the SVG display using addSlider() in SVGAnnotation . When the viewer changes the location of the slider thumb, the corresponding curve and residuals are displayed and the previously displayed curve and residuals are hidden. JavaScript added to the SVG document responds to the change in the position of the slider thumb and takes care of hiding and showing the various parts of the two plots.\nAn alternative to SVG GUI controls is to embed the SVG graphic in an (X)HTML page and use controls provided by an HTML form to control the plot. Again, JavaScript is required to respond to the user input via the HTML controls. The basic set of HTML UI controls is quite limited, so the author must either use JavaScript to provide a slider (e.g., using the YUI toolkit from Yahoo http://developer.yahoo.com/yui/ ) or change the interface to use a simpler control.\nA complication from embedding an SVG document within an (X)HTML document stems from the JavaScript code within the HTML code being separate from the code and objects within the SVG document. In this case, a simple extra step is necessary to access the SVG elements from the JavaScript code within the HTML document. An example is shown in Figure\u00a06, \u201c SVG embedded within HTML connects regions in a map to HTML tables. \u201d . The map of the state-level outcome of the United States presidential election is embedded in an HTML <div> tag. When the viewer clicks on a state in the map, JavaScript code located in the HTML document pulls up the state's summary information and dynamically places it in a table in the region above the map.\nlibrary(SVGAnnotation)\ndata(election2008)\ncapitalize =\nfunction(x)\n{\n  e = strsplit(x, \"[[:space:]]\")[[1]]\n  paste(sapply(e, function(o) paste(toupper(substring(o, 1, 1)), \n        substring(o,2), sep = \"\")), collapse = \" \")\n}\nmakeStateTable =\nfunction(info, name, asString = TRUE)\n{\n  totals = colSums(info[,1:2])\n  total = sum(totals)\n\n  obama.won = info[,1] > info[,2]\n\n  div = newXMLNode(\"div\")\n  newXMLNode(\"h2\", capitalize(name), parent = div)\n  diva = newXMLNode(\"div\", parent = div, attrs = c(id = \"diva\"))  \n  p = newXMLNode(\"p\", \"Winner: \",  \n      newXMLNode(\"b\", if(totals[1] > totals[2]) \"Obama\" else \"McCain\",\n      attrs = c(class = if(totals[1] > totals[2]) \"obama\" else \"mccain\")), \n      parent = diva)  \n  p = newXMLNode(\"p\", \"Number of votes:\", parent = diva)\n  \n  tb = newXMLNode(\"table\", parent = p)\n  tr = newXMLNode(\"tr\", newXMLNode(\"th\", \"Total\"),\n                   newXMLNode(\"td\", total),\n                   parent = tb, attrs = c(align = \"right\"))  \n  tr = newXMLNode(\"tr\", newXMLNode(\"th\", \"Obama\"),\n                  newXMLNode(\"td\", totals[1]),\n                  newXMLNode(\"td\", sprintf(\"  (%.0f%%)\", 100*totals[1]/total)),\n                  parent = tb, attrs = c(align = \"right\"))\n tr = newXMLNode(\"tr\", newXMLNode(\"th\", \"McCain\"),\n                 newXMLNode(\"td\", totals[2]),\n                 newXMLNode(\"td\", sprintf(\"  (%.0f%%)\", 100*totals[2]/total)),\n                 parent = tb, attrs = c(align = \"right\"))\n\n  divb = newXMLNode(\"div\", parent = div, attrs = c(id = \"divb\"))    \n  p = newXMLNode(\"p\", \"Number of counties:\", parent = divb)\n  tb = newXMLNode(\"table\", parent = p)\n  tr = newXMLNode(\"tr\", newXMLNode(\"th\", \"Total\"),\n                  newXMLNode(\"td\", nrow(info)),\n                  parent = tb, attrs = c(align = \"right\"))  \n  tr = newXMLNode(\"tr\", newXMLNode(\"th\", \"Obama\"),\n                  newXMLNode(\"td\", sum(obama.won)),\n                  parent = tb, attrs = c(align = \"right\"))\n  tr = newXMLNode(\"tr\", newXMLNode(\"th\", \"McCain\"),\n                  newXMLNode(\"td\", sum(!obama.won)),\n                  parent = tb, attrs = c(align = \"right\"))\n\n  if(asString)\n     saveXML(div)\n  else\n     div\n}\nmakeCountiesTable =\nfunction(info, name, asString = TRUE)\n{\n  obama.won = info[,1] > info[,2]\n  \n  tb = newXMLNode(\"table\")\n  newXMLNode(\"tr\", newXMLNode(\"th\"), newXMLNode(\"th\", \"County\"), \n             newXMLNode(\"th\", \"Obama\"), newXMLNode(\"th\", \"McCain\"), \n             parent = tb)\n\n  sapply(seq(length = nrow(info)),\n          function(i) {\n            x = info[i,]\n            tr = newXMLNode(\"tr\", \n                 attrs = c(align = \"left\", \n                   class = if(obama.won[i]) \"obama\" else \"mccain\"), parent = tb)\n            newXMLNode(\"td\", i, parent = tr)            \n            newXMLNode(\"td\", info[i, 3], parent = tr)\n            newXMLNode(\"td\", info[i, 1], attrs = c(align = \"right\"), \n                       parent = tr)\n            newXMLNode(\"td\", info[i, 2], attrs = c(align = \"right\"), \n                       parent = tr)                        \n          })\n\n  if(asString)\n    saveXML(tb)\n  else\n    tb\n}\nlibrary(maps)\nm = map('state', fill = TRUE, col = \"red\")\nstateNames = gsub(\":.*\", \"\", m$names)\npolyNames = gsub(\"'\", \"\", m$names)\n\nobama.won = sapply(states, function(x) diff(colSums(x[, 1:2]))) < 0\nnames(obama.won) = gsub(\"-\", \" \", names(states))\n\n\nsetdiff(stateNames, names(obama.won))\nsetdiff(names(obama.won), stateNames)\n\ndoc = svgPlot(map('state', fill = TRUE, \n              col = c(\"red\", \"blue\")[obama.won[stateNames] + 1]))\np = getPlotPoints(doc)\nmapply(function(node, id) xmlAttrs(node) = c(id = id),\n        p, polyNames)\naddToolTips(p, stateNames, addArea = 2)\n#mapply(function(node, id) newXMLNode(\"title\", id, parent = node),  p, stateNames)\nsaveXML(doc, \"stateMap.svg\")\nlibrary(XML)\n names(states) = gsub(\"-\", \" \", names(states)) \n stateTables = mapply(makeStateTable, states, names(states))\n stateTables[\"national\"] = makeStateTable(allStates, \"National\")\n countyTables = mapply(makeCountiesTable, states, names(states))\n \n library(RJSONIO)\n cat(\"var stateSummaryTables = \", toJSON(stateTables), \";\",\n     \"var countyTables = \",  toJSON(countyTables), \";\",\n     \"var polyIds = \", toJSON(polyNames), \";\",\n     \"var polyStates = \", toJSON(stateNames), \";\",\n     file = \"stateHTMLTables.js\")\nThis screen shot shows a canonical red-blue map of the results of US presidential election embedded within a <div> tag in an HTML page. Interactivity within the map is controlled via JavaScript located in the HTML page. When the viewer clicks on a state, the table displaying the summary of votes in the state is rendered within the light-grey region above the map. Also, the county-level results are available in another HTML table (not shown in this screen shot).\nThese six figures show the variety of interactivity possible with SVG. In the following sections we introduce the functionality available within SVGAnnotation to create these and other interactive graphics. The examples are chosen to demonstrate our philosophy behind creating interactivity with R and SVG. The package offers the R programmer a new mode of displaying R graphics in an interactive, dynamic manner on the Web. Table\u00a01, \u201c Examples of Interactive SVG plots\u201d contains a comprehensive list of the 14 examples in this paper. The table includes a brief summary of the type of interactivity in the graphical display and the functions used to make the SVG interactive. These examples can be found in the XMLExamples directory in SVGAnnotation , and they can be run with xmlSource() , a function in the XML package, e.g., xmlSource(\"exToolTipsAndLinks.xml\").\nTable\u00a01.\u00a0 Examples of Interactive SVG plots\nExample\nExample\u00a04, \u201cInteractive hexagonal bin plots\u201d\nTool tips are added to the hexagonal bins in a hexbin plot. To do this, we use the helper function getPlotPoints() to locate the bins in the SVG document and then apply addToolTips() to the hexagonal bin elements in the SVG. Source: exHexbinToolTips.xml\nExample\u00a06, \u201cLinking across lattice plots\u201d\nPoints in lattice panels are linked to a legend for the plot. This customized plot is created using the getPlotRegionNodes() to access the panels in the plot, getLatticeLegendNodes() to access the legend, addAttributes() to add JavaScript calls in response to mouse-over events, and finally addECMAScripts() places the JavaScript code in the SVG document. Source: exLegendLatticeLink.xml\nExample\u00a07, \u201cInteractive nearest neighbors\u201d\nThis plot requires greater understanding of the internal workings of the SVG document. It takes the approach of dynamically constructing and adding line segments to a scatter plot at the time of viewing. All of the computations of nearest neighbors within a set of observations are computed in R and stored as JavaScript variables in the SVG document (using addECMAScripts() ). These JavaScript functions respond to mouse-over events at viewing time and create line segments connecting points in the scatter plot while the image is being viewed. The points have been annotated with unique identifiers using getPlotPoints() and addAttributes() . Source: exKNN.xml\nExample\u00a08, \u201cHighlighting nodes and edges in a graph\u201d\nIn this example, a graph is annotated to respond to mouse-over events on the nodes of the graph. The edges and connecting nodes are brought to the forefront by a change in color when the mouse moves over the node. This interactivity is created in a similar manner as the nearest neighbor display. Source: exGraphviz.xml\nExample\u00a09, \u201cAnimating scatter plots through snapshots in time\u201d\nThe high-level function animate() adds to a scatter plot the capability of moving a point to different locations according to the change in the (x,y) values of the corresponding observation over different time intervals. The animate() function uses the animation features available in SVG, i.e., no JavaScript is needed to create the animation. Source: exWorldAnimation.xml\nExample\u00a010, \u201cAnimation with JavaScript and SVG\u201d\nThis example demonstrates how to use JavaScript facilities to animate a map. This hands-on approach uses a JavaScript timer to change the colors at regular intervals of states in a map of the US. Several helper functions are employed, including getPlotPoints() , addECMAScripts() , addToolTips() and addAttributes() . Source: exJSAnimateElectionMapPaper.xml\nExample\u00a011, \u201cControlling smoothness with a slider\u201d\nA Graphical User Interface (GUI) control is added to the canvas. The GUI is a slider that controls the bandwidth parameter for fitting a curve to the data. The slider is provided by Carto:Net, and is easily added to the display with a call to addSlider() . The connection between the slider and the plots is made via application specific JavaScript functions. Source: exLinkedSmoother.xml\nExample\u00a012, \u201cShowing and hiding currency exchange series with check boxes\u201d\nSimilar to the smoother example, SVG checkboxes are added to a time series plot. These checkboxes are also supplied by Carto:Net. The function radioShowHide() takes care of the details, and display-specific JavaScript is used to show or hide a particular time series curve in the plot. Source: exEUseries.xml\nExample\u00a013, \u201c Using forms to highlight points in a lattice plot \u201d\nAn earlier example ( Example\u00a06, \u201cLinking across lattice plots\u201d ) is recreated using an HTML form to control the highlighting of points in panels of a lattice plot. The JavaScript functions that earlier responded to mouse events on the lattice legend, now are called when the viewer changes the selection in an HTML choice menu. The SVG image is embedded within the HTML document along with the form to control it. The JavaScript sits within the HTML document, rather than the SVG document as with the earlier example. Source: exLatticeChoiceHTML.xml\nThis example continues the ideas from Example\u00a013, \u201c Using forms to highlight points in a lattice plot \u201d and uses mouse events on regions in a map (drawn in R) to display alternative HTML tables. Source: exStateElectionTable.xml\nSimple annotations\nIn this section, we turn our attention to how a user can create the simplest displays shown in the section called \u201c Examples of interactivity with SVG \u201d . We introduce several high-level functions from the SVGAnnotation package that make it easy to add interactivity to SVG plots; these are briefly described in Table\u00a0A.1, \u201c High-level functions for adding interactivity to SVG plots \u201d . We also explain the basic approach we take to create these graphical displays.\nIn order to produce SVG graphics in R (using the built-in graphics device), libcairo [ libcairo ] must be installed when R is built. You can determine whether an R installation supports SVG with the expression capabilities()[\"cairo\"]. If this yields TRUE, then the libcairo-based SVG support is present. Assuming this is the case, we create/open an SVG graphics device with a call to the svg() function and then issue R plotting commands to the SVG device as with any other graphics device. We must remember to close the device with a call to dev.off() when the commands to generate the display are complete. For example,\nsvg(\"foo.svg\")\nplot(density(rnorm(100)), type = \"l\")\nabline(v = 0, lty = 2, col = \"red\")\ncurve(dnorm(x), -3, 3, add = TRUE, col = \"blue\")\ndev.off()\ncreates the file named foo.svg, which contains the SVG that renders a smoothed density of 100 random normal observations.\nThe SVGAnnotation package provides a convenience layer to this process. The svgPlot() function opens the device, evaluates the code to create the plot(s), and then closes the device, all in a single function call. For example,\nsvgPlot({\n         plot(density(rnorm(100)), type = \"l\")\n         abline(v = 0, lty = 2, col = \"red\")\n         curve(dnorm(x), -3, 3, add = TRUE, col = \"blue\")\n        },\n          \"foo.svg\")\nperforms the same function calls as in the above example. We recommend using svgPlot() because it inserts the R code that generated the plot as meta-data into the SVG file and provides provenance and reflection information. This can be convenient for the programmer who is post-processing the resulting SVG. Also important is the information svgPlot() adds to the SVG for lattice plots, such as the number of panels, strips, conditioning variables, levels, and details about the legend. This extra information allow us to more easily and reliably identify the SVG elements that correspond to the components of the R plot(s) we want to annotate.\nAn additional reason for using the svgPlot() function is that it can hide the use of a file. As shown below, often we want the SVG document generated by R's graphics commands as an XML tree and not written to a file. The return value of svgPlot() is a tree structure, if no file name is specified by the caller, e.g.,\ndoc = svgPlot({\n                plot(density(rnorm(100)), type = \"l\")\n                abline(v = 0, lty = 2, col = \"red\")\n                curve(dnorm(x), -3, 3, add = TRUE, col = \"blue\")\n              })\nThe higher-level facilities in SVGAnnotation make it quite easy to add simple forms of interactivity to an SVG display. To add this interactivity, we follow a three-step process.\nPlotting Stage: First, we create the base SVG document. That is, we create an SVG graphics device and plot to it using R plotting tools. Again, we recommend using svgPlot() because it adds the plotting commands and other information to the SVG document.\nAnnotation Stage: Here, the SVG file created in the Plotting Stage is modified. That is, SVG content (and possibly JavaScript code) is added to the document to make the display interactive or animated. Depending on the type of plot to be annotated and the kind of annotations desired, the high-level functions described in this section may be able to handle all aspects of these annotations. Some cases, however, may require the intermediate functions in SVGAnnotation to, for example, add tool tips to a non-standard plot (see the section called \u201cTools for customizing interactivity \u201d ). Also, the annotation stage may possibly require working directly with the SVG document to, for example, add GUI controls to the plot ( the section called \u201cCreating GUIs\u201d ). All of these approaches use the XML package to parse and manipulate the SVG document from within R. We'll see examples of how this is used in later sections when we directly manipulate the SVG for more specialized annotations.\nOnce the annotation is completed, we save the SVG document to a file.\nViewing Stage: Now the enhanced SVG document is loaded into an SVG viewer, such as Opera, Firefox, Safari, or Squiggle, and the reader views and interacts with the image. In this stage, R is not available. The interactivity is generated by SVG elements themselves and/or JavaScript code that has been added in the annotation stage.\nSimple interactivity: Tool tips and links\nIn this section we provide simple examples of how to use the high-level functions to add tool tips ( Example\u00a01, \u201cAdding tool tips to points and labels\u201d ) and hyperlinks ( Example\u00a02, \u201cAdding hyperlinks to a plot title\u201d ) to scatter plots. The R user does not need to understand much about SVG to add these simple features to plots.\nExample\u00a01.\u00a0Adding tool tips to points and labels\nIn this example, we demonstrate how to add tool tips to the SVG plot shown in Figure\u00a07, \u201cTool tips for points in a scatter plot\u201d . The screen shot shows one of the effects that we are attempting to create: when the mouse is over a point, then a tool tip with additional information about that point appears.\nCode\n# Load the required packages \nlibrary(\"XML\")\nlibrary(\"SVGAnnotation\")\n\n# Create the basic plot\ndoc = svgPlot(plot(mpg ~ wt, mtcars, \n                   main = \"Motor Trend Car Road Tests\", pch=19, \n                   col= \"#377EB8\"))\n\n # Annotate title and axes\nax = getAxesLabelNodes(doc)\n\n # Add link from title to motor trend website\naddLink(ax$title, \"http://www.motortrend.com\", addArea = TRUE) \n\n # Add tooltips to the x, y axes\ntips = c(\"Weight in units of lb/1000\", \"Miles per US gallon\")\naddToolTips(ax[c(\"xaxis\", \"yaxis\")], tips) \n\naddToolTips(doc, apply(mtcars, 1, function(x) \n                       paste(names(mtcars), x, sep = \" = \", \n                                   collapse = \", \")))\n\n\nsaveXML(doc, \"../mt_tips.svg\")\ndepth.col = gray.colors(100)[cut(quakes$depth, 100, label=FALSE)]\ndepth.ord = rev(order(quakes$depth))\ndoc = svgPlot(\n plot(lat ~ long, data = quakes[depth.ord, ],\n      pch = 19, col = depth.col[depth.ord],\n      xlab = \"Longitude\", ylab=\"Latitude\",\n      main = \"Fiji Region Earthquakes\") )\nax = getAxesLabelNodes(doc)\naddToolTips(ax[c(\"xaxis\", \"yaxis\")], c(\"Degrees east of the prime meridean\",\n            \"Degrees south of the equator\"), addArea = TRUE)\nusgs = \"http://earthquake.usgs.gov/eqcenter/recenteqsww/\"\nregion = \"Maps/region/S_Pacific.php\"\naddAxesLinks(ax$title, paste(usgs, region, sep = \"\"))\naddToolTips(doc, \n             apply(quakes[depth.ord, ], 1, \n                    function(x)\n                       paste(names(quakes), x, \n                             sep = \" = \", collapse = \", \")))\n# This is a relative path to the CSS file.\n# The default path will be absolute.\n#addCSS(doc, css = \"../../inst/CSS/RSVGPlot.css\")\nsaveXML(doc, \"quakes_tips.svg\")\nThis screen shot shows a scatter plot where each point has a tool tip on it. The tool tips were added using the addToolTips() function in SVGAnnotation . When viewed in a browser, as the mouse passes over the point, information about the values of each variable for that observation is provided in a pop-up window. (Another screen shot of this plot is shown in Figure\u00a01, \u201cHyperlinks on titles\u201d .)\nThe first step is to make the plot with the SVG graphics device.\ndepth.col = gray.colors(100)[cut(quakes$depth, 100, label=FALSE)]\ndepth.ord = rev(order(quakes$depth))\ndoc = svgPlot(\n plot(lat ~ long, data = quakes[depth.ord, ],\n      pch = 19, col = depth.col[depth.ord],\n      xlab = \"Longitude\", ylab=\"Latitude\",\n      main = \"Fiji Region Earthquakes\") )\nAs noted earlier, the function svgPlot() is a wrapper for R's own svg() function. We did not specify a value for the file parameter for svgPlot() and as a result, the function returns the parsed XML/SVG tree, which we can then post-process and enhance. (The code for this plot of quakes is adapted from [ Sarkar ].)\nThe default operation for addToolTips() is to add tool tips on each of the points in a scatter plot. We simply pass the SVG document to addToolTips() along with the text to be displayed. For example, adding the row names from the data frame as tool tips for the points is as simple as\naddToolTips(doc, rownames(quakes[depth.ord, ]))\nIf we wanted the tool tip to provide information about the value of each of the variables for that observation, we could do this with\naddToolTips(doc, \n             apply(quakes[depth.ord, ], 1, \n                    function(x)\n                       paste(names(quakes), x, \n                             sep = \" = \", collapse = \", \")))\nWe can also use addToolTips() to provide tool tips on the axes labels. This time, since it is not the default operation, we need to pass the particular axes label nodes to be annotated rather than the entire SVG document. We find these axes label nodes using another function in SVGAnnotation , getAxesLabelNodes() . This function locates the nodes in the SVG document that correspond to the title and axes labels:\nax = getAxesLabelNodes(doc)\nThe first element returned is the title. We discard the title node, e.g., ax[ -1], and call addToolTips() to place tool tips on just the axis nodes:\naddToolTips(ax[c(\"xaxis\", \"yaxis\")], c(\"Degrees east of the prime meridean\",\n            \"Degrees south of the equator\"), addArea = TRUE)\nThe addToolTips() function will operate on any SVG element passed to it via the function's first argument. The addArea parameter is set to TRUE to indicate that the rectangle surrounding the characters in the label should be made active, i.e., when the mose moves into this region the tool tip will pop up. When FALSE, only the characters in the label will be responsive to the mouse movement.\nThe addCSS parameter of addToolTips() controls the addition to the document of a cascading style sheet for controlling the appearance of the tooltip rectangle. If TRUE, then the default CSS is added. The default value for addCSS is NA , and in this case, the function determines whether or not the CSS is needed, e.g., if a tool tip is to be placed on a rectangular area surrounding the text in an axis label then a CSS is added. The code adds the specified default CSS file to the document only once. However, a warning message is issued if there are multiple requests to add the CSS to the document. The programmer also can add a specific CSS file via a call to addCSS() .\nNow that the SVG has been annotated, we save the modified document:\nsaveXML(doc, \"quakes_tips.svg\")\nThis document can then be opened in an SVG viewer (e.g., a Web browser) and the user can interact with it by mousing over the points and axes labels to read the tool tips that we have provided.\nExample\u00a02.\u00a0Adding hyperlinks to a plot title\nWe sometimes want to click on a phrase or an individual point in a plot and have the browser jump to a different view or a Web page. SVG supports hyperlinks on elements so we can readily add this feature to R plots.\nWe continue with the plot that we annotated in Example\u00a01, \u201cAdding tool tips to points and labels\u201d and add a hyper-link to the title. We add a feature to the plot that will allow viewers to click on the phrase \u201cFiji Region Earthquakes\u201d and have their Web browser display the USGS web page containing a map of recent earthquakes in the South Pacific. We have already seen that the title of the plot is in the first element of the return value from the call to getAxesLabelNodes() , which was saved in the R variable ax (see note below). We simply associate the target URL with this element via a call to addAxesLinks() as follows:\nusgs = \"http://earthquake.usgs.gov/eqcenter/recenteqsww/\"\nregion = \"Maps/region/S_Pacific.php\"\naddAxesLinks(ax$title, paste(usgs, region, sep = \"\"))\nNow, when the viewer clicks on the rectangular region that surrounds the title, the browser will open the USGS website.\nNote\nThe parsed SVG document is a C-level structure, and the return value from svgPlot() is a reference to this structure. Further, the functions, such as getAxesLabelNodes() , return pointers to the nodes in this C-level structure. The consequence of this is that when we assign the return value from, say, getAxesLabelNodes() to an R object, we are not getting a new copy of the nodes. Hence, any modification to the returned value modifies the original parsed document. For example, in Example\u00a02, \u201cAdding hyperlinks to a plot title\u201d an assignment to the R variable ax modifies the axes label nodes in the C-level structure. This is an important distinction from the usual way R handles assignments. The call to saveXML() will save the modified, parsed document as an XML file.\nNote\nCurrently, many, but not all, common plots in R are handled at the high level described in this section. Some may require low-level manipulation of the XML in the same manner we have illustrated and implemented within the examples in the later sections of this paper and in the SVGAnnotation package.\nExample\u00a03.\u00a0Adding hyperlinks to polygons\nIn this example, we create a popular map of the USA where we color the states red or blue according to whether McCain or Obama, respectively, received the majority of votes cast in that state in the 2008 presidential election (see Figure\u00a08, \u201cHyperlinks on regions of a map\u201d ). These data were \u201cscraped\u201d from the New York Times Web Site http://elections.nytimes.com/2008/results/president/map.html . The data are total vote counts at the county level for each presidential candidate. We first aggregate the counts to the state level and determine the winner as follows:\nstateO = sapply(states, function(x) sum(x$Obama))\nstateM = sapply(states, function(x) sum(x$McCain))\nwinner = 1 + (stateO > stateM)\nlibrary(\"XML\")\nlibrary(\"SVGAnnotation\")\nlibrary(\"maps\")\ndata(\"election2008\")\nstateO = sapply(states, function(x) sum(x$Obama))\nstateM = sapply(states, function(x) sum(x$McCain))\nwinner = 1 + (stateO > stateM)\nregions = gsub(\"-\", \" \", names(winner)) \nstateInd = match.map(\"state\", regions)\npolyWinners = winner[stateInd]\nstateColors =  c(\"#E41A1C\", \"#377EB8\")[polyWinners]\ndoc = svgPlot({\n                map('state', fill = TRUE, col = stateColors)\n                title(\"Election 2008\")\n              }\n             )\npolygonPaths = getPlotPoints(doc)\nlength(polygonPaths)\nurls = paste(\"http://elections.nytimes.com/2008/results/states/\", \n             names(winner)[stateInd],\".html\", sep=\"\")\naddLink(polygonPaths, urls, css = character())\npolygonAlt = getNodeSet(doc, \"/svg:svg/svg:g/svg:g//svg:path\", \"svg\")\n\nlength(polygonAlt)\n[1] 63\nptz = xmlChildren(getPlotRegionNodes(doc)[[1]])\n addLink(ptz, urls, css = character())\nThe map shown here was produced by the map() function in the maps package [ maps ]. The SVG file was then post-processed using the addLink() function in SVGAnnotation to add hyperlinks to the state regions. When the user clicks on a state, the browser links to the corresponding state's Web page of election results on the New York Times site, e.g., http://elections.nytimes.com/2008/results/states/california .\nWe use map() in the maps package to draw the map, and match.map() to identify the polygonal regions so that we can color them correctly.\nregions = gsub(\"-\", \" \", names(winner)) \nstateInd = match.map(\"state\", regions)\npolyWinners = winner[stateInd]\nstateColors =  c(\"#E41A1C\", \"#377EB8\")[polyWinners]\ndoc = svgPlot({\n                map('state', fill = TRUE, col = stateColors)\n                title(\"Election 2008\")\n              }\n             )\nWe then use getPlotPoints() , one of the facilities in the SVGAnnotation for handling SVG elements, to find the polygons regions for the states:\npolygonPaths = getPlotPoints(doc)\nlength(polygonPaths)\nlength(stateInd)\n[1] 63\nNote that there are more than 50 polygons because some states are drawn using multiple polygons, e.g., Long Island and Manhattan are drawn as separate polygons and belong to New York. Notice that the number of regions found matches the length of stateInd.\nNow that we have polygonPaths, we can easily add links to the polygons in the SVG with addLink() . We simply provide a vector of target URLs to the function. These are created by pasting the state names to the New York Times base URL, as follows:\nurls = paste(\"http://elections.nytimes.com/2008/results/states/\", \n             names(winner)[stateInd],\".html\", sep=\"\")\nThen we add the links:\naddLink(polygonPaths, urls, css = character())\nNote that the order of the polygon paths in the SVG document corresponds to the order in which the polygons were plotted in R.\nWhen the saved document is viewed in a Web browser, or a dedicated SVG viewer such as batik [ batik ], a mouse click on a state will display the state's page on the New York Times Website.\nFinally, we should note that if we had not filled the polygons with color, then the interiors would not be explicitly drawn and therefore, only the boundaries (i.e., the paths) would be active. This means that the interiors of the states would not respond to a mouse click.\nWe next annotate another slightly non-standard plot, the hexagonal bin plot. As in the previous example, getPlotPoints() finds the elements in the SVG document that correspond to the \u201cpoints\u201d in the plot. In general, the functions in SVGAnnotation are intelligent enough to handle many diverse types of plots and should be used in preference to low-level processing of nodes with XPath and getNodeSet() .\nExample\u00a04.\u00a0Interactive hexagonal bin plots\nThe goal of this example is to add tool tips to a hexagonal bin plot [ hexbin ] such that mouse movement over a shaded hexagonal region results in the display of information about that bin, e.g., the number of points included in the bin (see Figure\u00a02, \u201c Tool tips on a hexbin plot \u201d ).\nThe data have been extracted from the Performance Measurement System (PeMS) Web site http://pems.dot.ca.gov . These data provide the occupancy (percent) and flow (count) of vehicles (measured in 5 minute intervals for one week) over one loop detector embedded beneath the surface of Interstate 80 in California. We begin by collapsing the measurements for the three lanes of traffic into one set of measurements.\nlibrary(\"SVGAnnotation\")\ndata(\"traffic\")\nOccupancy = unlist(traffic[ c(\"Occ1\", \"Occ2\", \"Occ3\")])\nFlow = unlist(traffic[c(\"Flow1\", \"Flow2\", \"Flow3\")])\nWe proceed to make the hexagonal bin plot and save the return value from the call to hexbin() , which is an S4 object.\nlibrary(\"hexbin\")\nhbin = hexbin(Occupancy, Flow)\ndoc = svgPlot(\n        plot(hbin, \n             main = \"Loop Detector #313111 on I-80E Sacramento\"))\nThe count slot in the hbin object is of interest to us because it contains a vector of cell counts for all of the bins, which we will use as the tool tips for the bins.\nA call to getPlotPoints() locates the hexbins within the SVG.\nptz = getPlotPoints(doc)\nlength(ptz)\nlength(hbin@count)\n[1] 276\nThe length of hbin@count shows that there are 276 bins in the plot. This figure matches the number of \u201cpoints\u201d found in the SVG, which confirms that the bins have been correctly located by getPlotPoints() .\nNow we can easily add tool tips to these regions in the plot as follows:\ntips = paste(\"Count: \", hbin@count)\naddToolTips(ptz, tips, addArea = TRUE)\nAdditional annotations that might be included in the tool tip are the xcm and ycm slots in hbin, which hold the x and y coordinates (in data rather than SVG coordinates) for the center of mass of the cell.\nMouse events that change the style of graphics elements\nOur examples so far in this section have added SVG in the post-processing stage to create interactive effects with tooltips and hyperlinks. Here, with the help of JavaScript, we create plots with more complex interactivity. We us linkPlots() to add JavaScript code to an SVG display so that the style of an element can be programmatically changed while viewing it. Specifically, points in a scatter plot change color in response to a mouse event. In general, whether it is a point's color, visibility, location, or size, mouse actions can initiate the changing of SVG \u201con the fly\u201d thus enabling a rich set of user interactions with the plot.\nExample\u00a05.\u00a0Point-wise linking across plots\nIn this example, we show how to use the high-level linkPlots() function to link points across plots (see Figure\u00a03, \u201cLinked scatter plots\u201d ). We start by creating the collection of plots. We might use a simple call to the pairs() function to create a draftsman's display of pairwise-scatter plots. Alternatively, one can create the plots with individual R commands and arrange them in arbitrary layouts with par() or layout() . We'll use the latter approach to create two plots:\ndoc = svgPlot({par(mfrow = c(1,2))\n    plot(Murder ~ UrbanPop, USArrests, main=\"\", cex = 1.4)\n    plot(Rape ~ UrbanPop, USArrests, main = \"\", cex = 1.4)\n    }, \n  width = 14, height = 7)\nThe high-level function linkPlots() does the hard work for us:\nlinkPlots(doc)\nsaveXML(doc, \"USArrests_linked.svg\")\nWe can then view this and mouse over points in either plot to see the linking. The default color to change a point is red; alternative colors can be specified with col.\nDependency on the svg() function and libcairo\nOne of the features of the approach we describe is that we can post-process the output of an existing R graphics device to provide interactivity and animation. We do not have to replace the device with our own to intercept R graphics operations and assemble the results within the device. This is made possible and relatively easy because of the XML structure of the generated SVG. It is not nearly as simple to provide interactivity with binary formats such as PDF or rasterized formats such as PNG and JPEG.\nThe post-processing approach does however give rise to a potential problem. Since we post-process the output from the svg() function and associated device, we are exploiting a format and structure that may change. There are two layers of software, each of which may change. The first is the implementation of the SVG device in R. The second is the third-party C-level libcairo library on which the R graphics device is based. Since the generic R graphics engine and also the device structure are well-defined and very stable, it is unlikely that there will be significant changes to the format of the SVG generated by R-related code. Changes to libcairo are more likely to cause changes to the SVG. Very old versions of libcairo (e.g., 1.2.4) do yield SVG documents that are non-trivially different from more recent versions (e.g., version 1.10.0) and have caused errors in the SVGAnnotation package. Specifically, <rect> elements are used for describing rectangles rather than the generic <path>. Future versions of libcairo may introduce new changes to the format and cause issues for SVGAnnotation . We do not however expect significant changes.\nThe problem of relying on a particular format generated by libcairo is similar to interfacing to a particular application programming interface (API) of a C/C++ library. Any changes in that API will cause the interface to break. Ideally, the API is fixed. However, new major versions can lead to such breaks. The potential for the interface to break across new versions of the software do not make the interface useless. The situation here with libcairo is similar although somewhat more complex. We have to identify a change in the format and this might be subtle.\nIn many regards, the approach we have taken in designing SVGAnnotation is intended to cause minimal disruption to the existing tool chain. We do not require the user to deploy a different graphics device. We do not modify the code of the existing graphics device. Instead, we work to make sense of the output (e.g., identify shapes) rather than knowing the graphical operations, e.g., circle, polygon, rectangle, text. This introduces the perils of a change in the output structure, but is a worthwhile goal of direct software reuse. It synchronizes the annotation facilities to the primary SVG graphics device in use within R. The alternative is to provide our own device and generate the SVG content ourselves and control its format. This would protect us from changes in the format. However, we would not benefit from passively incorporating enhancements to the R graphics device or libcairo. To address this issue, we could use a \"proxy\" device that acts as a \"front\" for the regular SVG device. This device would identify the different R-level components and then forward the device calls to the existing SVG libcairo-based graphics device. This would help us to map the high-level R components of the graphical displays to the lower-level SVG content. This would be of marginal benefit and would require direct knowledge of the SVG graphics device C code. In many regards this would be more of a problem than the problem we are trying to address, i.e., potential changes to the SVG format.\nThis reliance on the format generated by the svg() function is an issue of which users should be aware. Changes to this format might make functions in the SVGAnnotation package either fail or create erroneous annotations. Such changes are likely to be firstly rare and secondly, relatively minor. The required changes to the SVGAnnotation package should be relatively easy to implement. Importantly, we are not expecting or claiming that the SVGAnnotation package will work for all plots generated in R. Instead, we are reporting an approach of post-processing the SVG content generated from R graphics commands. The SVGAnnotation package provides high-level functions for many types of plots, but not all. Users may have to deal with the SVG directly or be able to use some of the medium- and low-level functions to manipulate the content. The contribution of this work is the approach of post-processing arbitrary R plots and the scheme for mapping low-level graphical primitive operations/elements to higher level graphical components such as axes, data points, legends, etc.\nThe SVG grammar\nIn this section we provide a brief overview of the commonly used XML elements/nodes in SVG and the basics of the drawing model. For more detailed information on SVG, readers should consult [ SVGEssentials , W3 Schools SVG Tutorial ], and readers unfamiliar with XML should read Appendix\u00a0B, Basics of XML or [ XMLNutshell ]. We also explore the basic layout of an SVG document created by the SVG device in R. In particular, we examine the SVG document produced from a simple call to plot() with two numeric vectors. If we wish to annotate other types of plots, i.e., one that is not covered by the high-level functions in SVGAnnotation , such as a ternary plot, then we will need to understand the structure of documents produced by the SVG device.\nWe include here a sample SVG file that will make our description of the SVG elements more concrete. This document was created manually (not with R) and is rendered in Figure\u00a09, \u201cSample SVG\u201d .\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<svg xmlns = \"http://www.w3.org/2000/svg\" \n     xmlns:xlink = \"http://www.w3.org/1999/xlink\" \n     width = \"300pt\" height = \"300pt\" \n     viewBox = \"0 0 300 300\" version = \"1.1\">\n <defs>\n  <g id=\"circles\">\n    <circle id = \"greencirc\" cx = \"15\" cy=\"15\" r = \"15\" \n            fill = \"lightgreen\"/>\n    <circle id = \"pinkcirc\" cx= \"50\" cy=\"50\"  r = \"15\" \n            fill = \"pink\"/>\n  </g>\n  <style type=\"text/css\">  \n   < ![CDATA[\n   .recs {fill: rgb(50%, 50%, 50%); fill-opacity: 1; stroke: red;}\n   ]] >\n  </style> \n </defs>\n <g id = \"main\">\n    <rect x = \"10\" y = \"20\" width = \"50\" height = \"100\" \n          class = \"recs\"/>\n    <use  x = \"100\" y = \"100\" xlink:href = \"#circles\"/>\n    <use  x = \"200\" y = \"50\" xlink:href = \"#circles\" />\n    <image xlink:href = \"examples/pointer.jpg\" \n           x = \"70\" y = \"50\" width = \"10\" height = \"10\"/>\n    <path style = \"fill: rgb(100, 149, 237);                    \n            fill-opacity: 0.5;stroke-width: 0.75; \n            stroke-linecap: round; stroke-linejoin: round;             \n            stroke: rgb(0%,0%,0%); stroke-opacity: 1;\" \n        d = \"M 102.8 174.6 L 102.8 174.6\n             L 106.1 178.0 L 100.9 189.3 L 102.8 191.2 \n             L 102.1 195.1 L 102.1 209.9 L 53.9 209.9 \n             L 51.5 210.0 L 50.0 206.0 L 49.3 202.9 L 52.0 191.5 \n             L 52.7 185.0 L 52.9 184.1 L 53.4 179.0 L 53.3 173.0 \n             L 54.5 173.1 L 55.1 173.1 L 56.0 172.8 L 60.5 174.0 \n             L 61.4 177.2 L 63.5 178.4 L 67.7 177.5 L 72.5 177.7 \n             L 88.5 174.6 L 102.8 174.6\n             Z\" \n          id = \"oregon\"/>\n    <text x = \"110\" y = \"200\" fill = \"navy\" font-size = \"15\">\n        Oregon\n    </text>\n </g>\n</svg>\nFigure\u00a09.\u00a0Sample SVG\nThis simple SVG image is composed of a gray rectangle with a red border, two pairs of green and pink circles, a jpeg image of a pointer, a path that draws the perimeter of Oregon and fills the resulting polygon with blue, and the text \u201cOregon\u201d.\nThe image uses the most common SVG tags, which we describe below.\nSVG documents begin with the root tag <svg>. Possible elements it can have are: a <title> tag that contains the text to be displayed in the title bar of the SVG viewer; a <desc> tag, which holds a description of the document; and other tags for grouping and drawing elements.\nInstructions to draw basic shapes are provided via the <line>, <rect>, <circle>, <ellipse>, and <polygon> tags. In our sample document,\n<rect x = \"10\" y = \"20\" width = \"50\" height = \"100\" \n       class = \"recs\" />\nis an instruction to draw a rectangle with upper left corner at (10, 20), a width of 50, and height of 100. (The class attribute contains style information about the color of the interior of the rectangle and its border.) These shape elements are specific families of shapes that can also be rendered with the more general <path> element. Note the size of the rectangle is relative to the size of the viewBox in the canvas, which in our example is 300 by 300.\nThe <path> tag provides the information needed to draw a \u201ccurve\u201d. It contains instructions for the placement of a pen on a canvas and the movement of the pen from one point to the next in a connect-the-dot manner. The instructions for drawing the path are specified via a character string that is provided in the d attribute of <path>. For example, the path for the Oregon border in Figure\u00a09, \u201cSample SVG\u201d is as follows\nd = \"M 102.8 174.6 L 102.8 174.6\n      L 106.1 178.0 L 100.9 189.3 L 102.8 191.2 L 102.1 195.1 \n      L 102.1 209.9 L 53.9 209.9 L 51.5 210.0 L 50.0 206.0 \n      L 49.3 202.9 L 52.0 191.5 L 52.7 185.0 L 52.9 184.1 \n      L 53.4 179.0 L 53.3 173.0 L 54.5 173.1 L 55.1 173.1 \n      L 56.0 172.8 L 60.5 174.0 L 61.4 177.2 L 63.5 178.4 \n      L 67.7 177.5 L 72.5 177.7 L 88.5 174.6 L 102.8 174.6 \n      Z\"\nThe drawing of the Oregon border begins by picking up the pen and moving it to the starting position (102.8, 174.6). This position is given either as an absolute position, i.e., \u201cM x,y\u201d, or a relative position, i.e., \u201cm x,y\u201d. Note that the capitalization of the letter determines whether the position is relative (m) or absolute (M). Either a comma or blank space can be used to separate the x and y coordinates. From the starting point, the pen draws line segments from one point to the next. The segment may be a straight line (L or l) or a quadratic (Q or q) or cubic (C or c) Bezier curve. The Z command closes a path by drawing a line segment from the pen's current position back to the starting point. These paths provide very succinct notation for drawing curves.\nThe libcairo rendering engine (and hence the svg() device in R) uses <path> for rendering all shapes including characters and text. One benefit to this approach of using a <path> command to draw each letter is that there is no reliance on fonts when the SVG document is viewed. It also means that scaling the SVG preserves the shape of the letters with very high accuracy. However, when you add text and shapes to an SVG document, you may want to use the simpler higher-level short-cut tags, e.g., <text> and <ellipse>.\nElements can be grouped using the <g> tag. This is helpful when for example, you want to treat the collection of objects as a single object in order to transform it as a unit, or place the same appearance characteristics on a collection of elements. The style placed on <g> will apply to all of its sub-elements. Grouped elements can also be defined and then inserted multiple times in the document (see the description of the <defs> element below). It is also possible to nest other <svg> elements within a <g> element. This allows us to create compositions reusing previously and separately created displays.\nThe <defs> node is a container for SVG elements that are defined and given a label, but not immediately put in the SVG display. These definitions can be augmented, displayed and reused within the SVG display through a reference to the element's unique identifier. The <defs> element acts as a dictionary of template elements.\nFor example, in the sample SVG code we defined a pair of circles, one green and the other pink. These are grouped together into a single unit and identified by the id of \u201ccircles\u201d as shown here:\n<g id=\"circles\">\n <circle id = \"greencirc\" cx = \"15\" cy=\"15\" r = \"15\" \n      fill = \"lightgreen\"/>\n <circle id = \"pinkcirc\" cx= \"50\" cy=\"50\" r = \"15\" \n      fill = \"pink\"/>\n</g>\nThis pair of circles is defined in the <defs> element, and rendered via the <use> tag. The pair is rendered twice, at two different locations on the canvas as follows,\n<use  x = \"100\" y = \"100\" xlink:href = \"#circles\"/>\n<use  x = \"200\" y = \"50\" xlink:href = \"#circles\" />\nAs with HTML, we specify the reference to an internal element (or \"anchor\") by prefixing the name/id of the desired element with a '#', i.e., \"#circles\". This suggests that we can link to elements in other files, and indeed we can. In fact, we have the full power of another XML technology, named XLink [ XPathXPointer ], available in SVG.\nMany times, we want to create style definitions to be used by multiple SVG elements. The style of an element can be specified in four ways:\nIn-line styles. One approach is to place the style information directly in the element (e.g., <circle>, <path>) by setting the value of a style attribute. For example, the style of the Oregon polygon,\n<path style = \"fill: rgb(100, 149, 237);                    \n         fill-opacity: 0.5;stroke-width: 0.75; \n         stroke-linecap: round; stroke-linejoin: round;             \n         stroke: rgb(0%,0%,0%); stroke-opacity: 1;\" \n...\n/>\nprovides the color (cornflower blue) and opacity for filling the polygon, the color of the border (black), and details about the border, such as the thickness of the line. With this approach, the style attribute value holds a string of Cascading Style Sheet (CSS) properties. Note that, colors in SVG can be represented by text name, e.g., \"cornflowerblue\", the red-green-blue triple rgb(100, 149, 237), or the hexadecimal representation of the triple, e.g., \u201c#6495ED\u201d.\nInternal Stylesheet. The style information can be placed in a stylesheet that is stored within the file in the <defs> node of the document. As an example, the rectangle in our Figure\u00a09, \u201cSample SVG\u201d uses the \"recs\" class within the internal stylesheet. This connection is specified via the class attribute on the <rect> element:\n<rect x = \"10\" y = \"20\" width = \"50\" height = \"100\" \n        class = \"recs\" />\nThe CSS style sheet and its classes are found within the <defs> portion of the file, in the <style> node:\n<style type=\"text/css\">  \n < ![CDATA[\n   .recs {fill: rgb(50%,50%,50%); fill-opacity: 1; stroke: red;}\n  ] ]>\n</style>\nFor more information about Cascading StyleSheets see [ CSSReference ].\nExternal Stylesheet. The stylesheets may also be located in an external file. It can be included via the xml-stylesheet processing instruction such as\n<?xml-stylesheet type=\"text/css\" href=\"RSVGPlot.css\" ?>\nPresentation Attributes. An alternative to using a stylesheet, whether in-line, internal, or external, is to provide individual presentation attributes directly in the SVG element. As an example, the pink circle's color in Figure\u00a09, \u201cSample SVG\u201d is specified through a fill attribute in the <circle> tag as follows,\n<circle id = \"pinkcirc\" cx= \"50\" cy=\"50\"  r = \"15\" \n     fill = \"pink\"/>\nThe presentation attributes are very straightforward and easy to use. We can just add a simple attribute, e.g., fill, on an element and avoid the extra layer of indirectness. This approach allows us to easily modify the presentation of an element in response to a user action. However, the downside of this approach is that presentation is mixed with content. For this reason, the in-line, internal and external cascading style sheets are preferable to presentation attributes. These various approaches can be mixed; that is, style information can be provided from a combination of in-line, internal, and external stylesheets, as well as presentation attributes.\nThe SVG display for an R plot\nWe next examine a typical document produced from the SVG graphics device in R. The SVG produced in R is highly structured, and we use this structure to locate particular elements and enhance them with additional attributes, parent them with new elements, and insert sibling elements in order to create various forms of interactivity and animation. We examine the SVG generated for the following call to plot() that was used to make the scatter plot in Example\u00a01, \u201cAdding tool tips to points and labels\u201d .\ndepth.col = gray.colors(100)[cut(quakes$depth, 100, label=FALSE)]\ndepth.ord = rev(order(quakes$depth))\ndoc = svgPlot(\n plot(lat ~ long, data = quakes[depth.ord, ],\n      pch = 19, col = depth.col[depth.ord],\n      xlab = \"Longitude\", ylab=\"Latitude\",\n      main = \"Fiji Region Earthquakes\") )\nWe can explore the contents of the resulting XML/SVG document programmatically by using the tools available in the XML package. The tree in Figure\u00a010, \u201cSVG document tree\u201d provides a conceptual image of the hierarchy of the SVG nodes for the plot and its annotations.\nFigure\u00a010. SVG document tree\nThis tree provides a visual representation of the organization and structure of the SVG document produced by the call to plot() in Example\u00a01, \u201cAdding tool tips to points and labels\u201d . SVG elements are shown as nodes in the tree. The style-sheet and the <display> element on the left of the tree are added by svgPlot() . The <CDATA> child of <display> contains the R code passed in the call to svgPlot() . The nodes with dotted lines are those that have been added to the SVG document by the addLink() and addToolTips() functions. For readability, not all nodes are displayed, and in some cases the number of nodes is provided to make it clear to which part of the plot these elements correspond. For example, the \u201c1000 children\u201d refers to the elements that plot the points in the scatter plot in Figure\u00a07, \u201cTool tips for points in a scatter plot\u201d ; they correspond to the 1000 observations in the quakes data frame.\nThe XML package provides several tools that aid us in examining the structure and content of an SVG document. We demonstrate some of these as we explore the resulting SVG document in doc. We begin by retrieving the top node and assigning it to root,\nroot = xmlRoot(doc)\nWe can use other functions such as xmlName() , xmlSize() , and xmlValue() to query the name, number of children, and the text content of an element, respectively. With them, we determine that the root has three children, the first contains the R code that is in the call to svgPlot() , and the following two are the <defs> and the main <g> tag that contains the plotting elements.\nxmlSize(root)\n$display\n[1] \"display\"\n\n$defs\n[1] \"defs\"\n\n$g\n[1] \"g\"\nor more simply\ndisplay      defs         g \n\"display\"    \"defs\"       \"g\"\nxmlValue(root[[1]])\n[1] \"plot(lat ~ long, data = quakes[depth.ord, ], pch = 19, \n col = depth.col[depth.ord], \\n    xlab = \\\"Longitude\\\",\n ylab = \\\"Latitude\\\", main = \\\"Fiji Region Earthquakes\\\")\"\nTo examine the <rect> element, we can also use either of the following approaches,\nroot[[3]][[1]]\n<rect x=\"0\" y=\"0\" width=\"504\" height=\"504\" \n      style=\"fill:rgb(100%,100%,100%); fill-opacity: 1; \n      stroke: none;\"/>\nroot[[\"g\"]][[\"rect\"]]\n<rect x=\"0\" y=\"0\" width=\"504\" height=\"504\" \n      style=\"fill:rgb(100%,100%,100%); fill-opacity: 1; \n      stroke: none;\"/>\nAlso, we can use the xmlChildren() function to extract each child node into a list of regular XML nodes, which can make it easier to explore and manipulate them.\nkids = xmlChildren(root[[3]])\nlength(kids)\n[1] 4\nkids[[1]]\n<rect x=\"0\" y=\"0\" width=\"504\" height=\"504\" \n      style=\"fill:rgb(100%,100%,100%); fill-opacity: 1; \n      stroke: none;\"/>\nAlternatively, the function getNodeSet() extracts elements from the XML tree, doc, and is a very general mechanism for querying the entire tree or sub-trees. It requires an XPath expression that specifies how to locate nodes. XPath [ XPathXPointer , XPathW3 ] is an extraction tool for locating content in an XML document. It uses the hierarchy of a well-formed XML document to specify the desired elements to extract. XPath is not an XML vocabulary; it has a syntax that is similar to the way files are located in a hierarchy of directories in a computer file system, but it is much more flexible and general. Rather than locating a single node in a tree, XPath extracts node-sets, which are collections of nodes that meet the criteria in the XPath expression. The node-set may be empty when no nodes satisfy the XPath expression. Likewise, when multiple nodes match the expression, a collection of nodes make up the node-set. For example,\n\"/x:svg/x:g/*\"\nlocates all grandchildren of the root <svg> that have a <g> parent. Note that we must specify the name space for the <svg> tag. Since the SVG elements use the default name space in this document, getNodeSet() allows us to use any name space abbreviation without defining it or matching it to the name space prefix in the document. In this case we simply chose \"x\". We use getNodeSet() to extract these elements, and then request their names via a call to sapply() ,\nkids = getNodeSet(doc, \"/x:svg/x:g/*\", \"x\") \nsapply(kids, xmlName)\n[1] \"rect\" \"g\"    \"g\"    \"g\"\nFor more details on how to use XPath to retrieve nodes from an XML document see [ XPathXPointer ]. The return value from getNodeSet() is a list, and we access the first element by the standard indexing methods for a list:\nkids[[1]]\n<rect x=\"0\" y=\"0\" width=\"504\" height=\"504\" \n  style=\"fill: rgb(100%,100%,100%); fill-opacity: 1; \n         stroke: none;\"/>\nOf the following three approaches,\nroot[[3]][[1]]\nroot[[\"g\"]][[\"rect\"]]\ngetNodeSet(doc, \"/x:svg/x:g/x:rect\", \"x\")\nthe first two return an object of class XMLInternalNode, whereas the call to getNodeSet() returns a list where each element is an XMLInternalNode .\nR's plotting functions are very regular and predictable so we can determine which nodes corresponds to which graphics objects quite easily. The approach we use here capitalizes on understanding the default operation of the plotting functions. To see how, notice that the first <g> sibling of <rect> has 1000 children.\nsapply(kids, xmlSize)\n[1]    0 1000   25    3\nThis number exactly matches the number of points plotted.\ndim(quakes)\n[1] 1000  5\nThe element with 25 children corresponds to the axes of the data region of the plot and their tick marks. The element with just 3 children contains the title and the two axes labels.\nThe high-level functions described in the section called \u201cSimple annotations\u201d make use of these default locations in the SVG output for the most common plots. If you need to annotate less common plots, then you may need to use the other functions in SVGAnnotation , or directly handle the XML nodes yourself with the functionality available in the XML package.\nNext, let's examine the first of these 1000 nodes. We see that it is a <path> element.\nkids[[2]][[1]]\n<path style=\"fill-rule: nonzero; \n   fill: rgb(90.196078%,90.196078%,90.196078%); \n   fill-opacity: 1;stroke-width: 0.75; stroke-linecap: round; \n   stroke-linejoin: round; \n   stroke: rgb(90.196078%,90.196078%,90.196078%);\n   stroke-opacity: 1;stroke-miterlimit: 10; \"\n   d=\"M 337.144531 191.292969 C 337.144531 194.894531 \n        331.746094 194.894531 ...  \" />\nAlthough the symbol used to represent a point in the plot is a circle, the SVG graphics device in R (via libcairo) does not use the <circle> element to draw it. Instead, the <path> node provides instructions for drawing the circle using Bezier curves to connect the points supplied in the d attribute. (The letter C between the (x,y) pairs means the points are to be connected by a cubic Bezier).\nThe x, y coordinates used to specify the path are in the coordinate system of the SVG canvas, not the coordinate system of the data. This system places the smallest values at the upper left corner of the canvas and the maximum values for x and y at the lower right corner, i.e., y increases as you move down the canvas and x increases as you move right. As a result, we cannot directly use the values in our data to directly identify SVG elements in the document; the data values first need to be converted into this alternative coordinate system. The SVG coordinate system supports various units of measurement, but the device in R uses only points (abbreviated as `pt' or `pts'). A point is approximately 1/72 of an inch. The size of the canvas is provided via the width and height attributes on the <svg> root node of the document.\nThe libcairo engine used by R generates all shapes exclusively with the <path> tag. This holds true as well for the text in axes and plot labels; that is, the cairo rendering engine in R creates the text by explicitly drawing the letters via SVG paths. The resulting letters scale extremely well and do not rely on special fonts which may not be available at the time of rendering. More specifically, the path for the glyphs that correspond to the text are created and placed in a <defs> element, and a <use> element brings in the glyph at the proper location in the plot. This representation introduces some difficulties for us because the text for legends and axes labels do not appear as plain text in the SVG document and so are not easily located for post-processing. The placement of the glyphs in the <defs> means that there is one additional level of indirection that needs to be handled when annotating text.\nTo make this concrete, let's consider our scatter plot example again. The last child of the main graphing node contains the information for drawing the title and axes labels. It has three children, one each for the title, y axis, and x axis, respectively. We identify the x axis with the following XPath expression,\ngetNodeSet(doc, \"/x:svg/x:g/x:g[3]/*[last()]\", \"x\")\n[[1]]\n<g style=\"fill: rgb(0%,0%,0%); fill-opacity: 1;\" type=\"axis-label\">\n  <use xlink:href=\"#glyph1-7\" x=\"14.398438\" y=\"266.152344\"/>\n  <use xlink:href=\"#glyph1-8\" x=\"14.398438\" y=\"259.478516\"/>\n  <use xlink:href=\"#glyph1-9\" x=\"14.398438\" y=\"252.804688\"/>\n  <use xlink:href=\"#glyph1-10\" x=\"14.398438\" y=\"249.470703\"/>\n  <use xlink:href=\"#glyph1-9\" x=\"14.398438\" y=\"246.804688\"/>\n  <use xlink:href=\"#glyph1-11\" x=\"14.398438\" y=\"243.470703\"/>\n  <use xlink:href=\"#glyph1-12\" x=\"14.398438\" y=\"236.796875\"/>\n  <use xlink:href=\"#glyph1-13\" x=\"14.398438\" y=\"230.123047\"/>\n</g> \n\nattr(,\"class\")\n[1] \"XMLNodeSet\"\nThis XPath expression starts at the root node, proceeds down one step to the root's <g> child, then down another level in the tree to select the third <g> element, and finally, one more level to the last child of the third <g>. Notice that we use the XPath predicate [3] to select the third <g> and the XPath function last() to get the last child element. This particular <g> element contains the instructions for drawing the axes label \"Latitude\". It references eight glyphs that are located in the <defs> node of the document. The references are via the href attribute. Note that there is one glyph per letter so, for example, the letter `a' appears in the document as \"glyph1-8\".\ngetNodeSet(doc, \"/x:svg/x:defs/x:g/x:symbol[@id ='glyph1-8']\", \"x\")\n[[1]]\n<symbol overflow=\"visible\" id=\"glyph1-8\">\n  <path style=\"stroke: none;\" d=\"M -1.671875 -1.578125\n    C -1.367188 -1.578125 -1.128906 -1.6875 -0.953125 -1.90625 \n    C -0.773438 -2.132812 -0.6875 -2.398438 -0.6875 -2.703125\n    ...\n    Z M -6.421875 -3.265625 \"/>\n</symbol> \n\nattr(,\"class\")\n[1] \"XMLNodeSet\"\nThe high-level functions in SVGAnnotation add elements and/or attributes to the SVG produced by the graphics device. For example, addToolTips() adds a type attribute with value of \"plot-point\" so the <path> element can be more easily identified as instructions to draw a point in a plot. This addition makes it easier for the programmer to extract and annotate elements.\naddToolTips(doc, apply(quakes[depth.ord, ], 1, function(x)\n            paste(names(quakes), x, sep = \" = \", collapse = \", \")),\n            addArea = TRUE)\nkids[[2]][[1]]\n<path style=\"fill-rule: nonzero;\n             fill: rgb(90.196078%,90.196078%,90.196078%); \n             fill-opacity: 1;stroke-width: 0.75; \n             stroke-linecap: round; \n             stroke-linejoin: round; stroke:\n             rgb(90.196078%,90.196078%,90.196078%); \n             stroke-opacity: 1;stroke-miterlimit: 10; \" \n      d=\"M 337.144531 191.292969 C 337.144531 194.894531 \n        331.746094 194.894531 ... \" \n      type=\"plot-point\" \n      xlink:title=\"lat = -20.32, long = 180.88, depth = 680, \n                    mag = 4.2, stations = 22\">\n  <title>lat = -20.32, long = 180.88, depth = 680, mag = 4.2, \n            stations = 22\n  </title>\n</path>\nIn the call to addToolTips() , the child element, <title>, was added to <path>; it contains the information that will be displayed when the mouse hovers the point. Similarly, adding tool tips to the axes and a hyperlink to the plot title further modifies the SVG. The addition of the hyperlink results in the insertion of an <a> tag as the parent of the <g> element that contains the title information and a <rect> element as a sibling to this <g>. All elements added after the creation of the SVG are denoted by dashed lines in Figure\u00a010, \u201cSVG document tree\u201d .\nTools for customizing interactivity\nWe have seen that SVGAnnotation provides facilities for identifying elements of the SVG document that correspond to particular parts of plots, e.g., in the Example\u00a04, \u201cInteractive hexagonal bin plots\u201d the function getPlotPoints() locates the SVG elements that correspond to the hexagonal bins in the R display. These functions can assist developers in creating new high-level functions for annotating the output from \u201cnon-standard\u201d plotting functions in R. In this section, we demonstrate the lower-level details of how a developer might post-process SVG to add annotations and JavaScript in order to enable interactivity. Table\u00a0A.2, \u201c Intermediate-level functions for working with SVG elements \u201d lists additional functions available for working with various parts of the SVG document.\nTo begin, we explain how linkPlots() , which was demonstrated in Example\u00a05, \u201cPoint-wise linking across plots\u201d , modifies the SVG to perform the linking action. The key idea is that the SVG is annotated to add a unique identifier to each SVG element that represents a point in a plot. This identifier includes information as to which plot region and observation the point belongs. The linkPlots() function retrieves the plot regions by using getPlotRegionNodes() . Whether we use pairs() or par(mfrow = ...), the points within each plotting region appear in a regular order; that is, the first element in each plotting region corresponds to the first row in the data frame, the second element to the second row, and so on. Thus elements across plots can be matched by simply using the order in which they appear in their respective plotting regions, assuming that they come from the same data frame or have a correspondence by row. The identifiers added to the points are of the form \"plotI-J\" where \"I\" is the plot index and \"J\" is the observation index within the plot. This unique identifier is added as an id attribute on each of the SVG elements, which makes it easy for the JavaScript function to find the elements to be linked.\nIn addition, the linkPlots() function in SVGAnnotation adds onmouseover and onmouseout attributes to each point element in the SVG document. The value of each of these attributes is a JavaScript function call to perform the linking and unlinking action, e.g., to change the color of the linked points. Below is the augmented SVG node that corresponds to the 10th point in the first plot. This is the point colored red in the left-hand plot in the screen shot in Figure\u00a03, \u201cLinked scatter plots\u201d .\n<path style=\"stroke-width:0.75; ... \" \n      d=\"M 260.417969 72.800781 C 260.417969 77.839844 252.855 ...\" \n      id=\"plot1-10\" \n      onmouseover=\"color_point(evt, 10 , 2 , 'red' )\" \n      onmouseout=\"reset_color(evt, 10 , 2 )\" \n      fill=\"none\" originalFill=\"none\"/>\nThe linkPlots() function adds to the SVG document the JavaScript code for the two functions color_point and reset_color. The code appears in a <script> node within the SVG document. Note that these JavaScript functions are relatively general and available in the package for making customized plots. (For more information about JavaScript, see Appendix\u00a0C, Basics of JavaScript .)\nThe principles we established for annotating the SVG to link points across plots carry over to more general settings. The basic ideas used in linkPlots() are to: post-process the document so that elements in the SVG are easily identified and changed at the time of viewing; create JavaScript functions, which are not dependent on a particular set of data to respond to mouse events and make the requested changes to the display. This approach can be summarized by the following set of tasks performed in the annotation stage:\nWithin R, add unique identifiers to the relevant graphics elements in the SVG document so they can be retrieved easily in the viewing stage (via the JavaScript function getElementById).\nCreate special attributes to store the default values for settings that are subject to change, e.g., original-style. These attribute names are made up by the developer, and should not conflict with SVG attribute names.\nFor the \"action\", set up event attributes, such as onmouseover and onmouseout, on the appropriate elements. The attribute values are JavaScript function calls. Our philosophy is to pass all element-specific information needed to respond to a request/mouse-event in the call. This way, the JavaScript functions can be used in other situations, e.g., with other data and other types of plots, and not rely on auxiliary global variables.\nEmbed in the document the JavaScript functions that modify and reset the element attributes.\nWe demonstrate how to use the basic approach just outlined to create a customized event handler for lattice plots. The interactivity in this example extends the notion of linked plots to lattice plots where points are linked via an interactive legend.\nExample\u00a06.\u00a0Linking across lattice plots\nIn this example, we follow the steps listed above to customize a JavaScript event handler. In particular, we show how you might extend the linking type of interactivity in Example\u00a05, \u201cPoint-wise linking across plots\u201d to lattice plots. The data used are the familar mtcars, and the lattice plot is constructed with the formula mpg ~ disp | am with cyl as the groups argument. We add a legend to a lattice plot for a fifth variable (gear). A mouse-over event on the legend results in the highlighting of the observations in each panel that belong to the corresponding level of gear. In this way, information from an additional variable is added to the lattice plot.\nCode\nlibrary(\"XML\")\nlibrary(\"SVGAnnotation\")\nlibrary(\"lattice\")\nmtcars$cyl = factor(mtcars$cyl,\n                    labels = paste(c(\"four\", \"six\", \"eight\"), \n                                   \"cylinder\"))\nmtcars$am = factor(mtcars$am, labels = c(\"automatic\", \"manual\"))\ngearGroups = sort(unique(mtcars$gear))\ngearLabels = paste(gearGroups, \"gears\", sep = \" \")\ncolors = \n trellis.par.get(\"superpose.symbol\")$col[seq(along = \n                                             levels(mtcars$cyl))]\ntopArgs = list(fun = draw.key, \n               args = list(key = list(text = list(gearLabels), \n                                      columns = 3)))\nbotArgs = \n  list(fun = draw.key,\n       args = list(key = list(text = list(levels(mtcars$cyl)),\n                              points = list(pch = 21,\n                                            col = colors),\n                              columns = 3)))\ndoc = \n svgPlot(xyplot(mpg ~ disp| am, groups = cyl, data = mtcars,\n                legend = list(top = topArgs, bottom = botArgs)))\npanels = getPlotRegionNodes(doc)\npoints = unlist(lapply(panels, xmlChildren), recursive = FALSE)\nids = by(mtcars, list(mtcars$gear, mtcars$am), \n         function(x) paste(as.numeric(x$am), x$gear, 1:nrow(x), \n                             sep = \"-\"))\nuids = unlist(ids) \nmapply(function(node, id)\n        addAttributes(node, id = id), points, uids)\ncounts = table(mtcars$am, mtcars$gear)\ncounts\n3  4  5\n  automatic 15  4  0\n  manual     0  8  5\nnodes = getLatticeLegendNodes(doc, panels, 1)\nsapply(seq(along = 1:length(gearGroups)), \n       function(i) {\n        cts = paste(\"[\", paste(counts[,i], collapse = \", \"), \"]\", \n                    sep = \"\")\n        addAttributes(nodes[[i]], \n          onmouseover = paste(\"highlight(\", gearGroups[i], \",\", \n                              cts, \", true)\"),\n          onmouseout = paste(\"highlight(\", gearGroups[i], \",\", \n                              cts, \", false)\")\n        )\n       }\n      )\n#jscript=c(\"../Javascript/multiLegendHighlight.js\",\n#          \"../Javascript/multiLegendHighlightPoint.js\")\njscript = list.files(system.file(\"examples\", \"Javascript\", \n                       package=\"SVGAnnotation\"), \n                     full.names = TRUE, pattern = \"multiLegend\")\naddECMAScripts(doc, jscript)\nsaveXML(doc, \"mt_lattice.svg\")\naddECMAScripts(doc, I(\"highlight(4, [4, 8], false);\"))\nsaveXML(doc, \"mt_lattice_gears.svg\")\nother = getNodeSet(panels[[length(panels)]], \n                      \"./following-sibling::*\")\nnodesAlt = other[-(1:(length(other) - length(gearGroups)))]\nnn = length(getNodeSet(doc, \"/x:svg/x:g/x:g\", \"x\"))\n nodes = getNodeSet(doc, paste(\"/x:svg/x:g/x:g[position() >\", nn-length(gearGroups), \"]\"), \"x\")\nrm(mtcars)\nThe scatter plots show the relationship between horsepower and miles per gallon for automatic and manual transmission cars. The points within each panel are colored according to the number of cylinders the engine has (4 cyan, 6 pink, and 8 green). The legend above the plots lists the possible number of gears, 3, 4, and 5. When for example, the mouse hovers over the 4-gear group, the points belonging to that group are highlighted. This interactivity is accomplished by changing the style attribute of the points via JavaScript. When the mouse moves off the legend, the original styles for these points are restored.\nWe begin by making the plot. We convert cyl and am into factors with more meaningful labels for the plot, and we use simpleKey() to add a legend to the lattice plot:\nlibrary(\"lattice\")\nmtcars$cyl = factor(mtcars$cyl,\n                    labels = paste(c(\"four\", \"six\", \"eight\"), \n                                   \"cylinder\"))\nmtcars$am = factor(mtcars$am, labels = c(\"automatic\", \"manual\"))\ngearGroups = sort(unique(mtcars$gear))\ngearLabels = paste(gearGroups, \"gears\", sep = \" \")\ncolors = \n trellis.par.get(\"superpose.symbol\")$col[seq(along = \n                                             levels(mtcars$cyl))]\ntopArgs = list(fun = draw.key, \n               args = list(key = list(text = list(gearLabels), \n                                      columns = 3)))\nbotArgs = \n  list(fun = draw.key,\n       args = list(key = list(text = list(levels(mtcars$cyl)),\n                              points = list(pch = 21,\n                                            col = colors),\n                              columns = 3)))\ndoc = \n svgPlot(xyplot(mpg ~ disp| am, groups = cyl, data = mtcars,\n                legend = list(top = topArgs, bottom = botArgs)))\nThere are 2 plotting regions in the SVG document, one for each panel. Each plotting-region element contains its points as child elements and we locate the entire collection of the corresponding SVG elements with\npanels = getPlotRegionNodes(doc)\npoints = unlist(lapply(panels, xmlChildren), recursive = FALSE)\nNow that we have the point elements, we can proceed with the first step in creating our own event handler, and augment these elements with unique identifiers. We construct identifiers based on the point's index within its gear-group and panel, e.g., id=\"2-4-1\" is the identifier for the first observation in the four-gear group in the second panel (the manual transmission panel). We use addAttributes() , a function in the XML package, to add the id attribute to the point elements as follows:\nids = by(mtcars, list(mtcars$gear, mtcars$am), \n         function(x) paste(as.numeric(x$am), x$gear, 1:nrow(x), \n                             sep = \"-\"))\nuids = unlist(ids) \nmapply(function(node, id)\n        addAttributes(node, id = id), points, uids)\nWe are ready for the third step in the process: augmentation of the legend labels with JavaScript calls for mouse events. We skipped the second step, that of preserving the default style values, because we will take care of it at viewing time with our JavaScript function highlight. That is, highlight will highlight or un-highlight the relevant points, and it will save and restore the default styles of the points. highlight is called with 3 arguments: the first argument indicates the gear-group; the second is an array giving the number of elements within that group in each panel; and, the third indicates whether the event is a mouse over (true) or a mouse out (false). Thus for the third step, we need to add onmouseover and onmouseout attributes that have JavaScript function calls such as highlight(4, [4, 8], true). Notice that JavaScript uses square brackets to delimit arrays (e.g., [1, 2]). We construct these simple arrays with direct calls to paste() . For more complex R objects, we would be advised to use the RJSONIO package to serialize R objects to JavaScript object notation (JSON).\nTo generate the values for these arrays, we need to know how many points are in each group within each panel. We get a frequency table with this information via\ncounts = table(mtcars$am, mtcars$gear)\ncounts\n3  4  5\n  automatic 15  4  0\n  manual     0  8  5\nWe use these counts to construct the JavaScript calls as follows:\nnodes = getLatticeLegendNodes(doc, panels, 1)\nsapply(seq(along = 1:length(gearGroups)), \n       function(i) {\n        cts = paste(\"[\", paste(counts[,i], collapse = \", \"), \"]\", \n                    sep = \"\")\n        addAttributes(nodes[[i]], \n          onmouseover = paste(\"highlight(\", gearGroups[i], \",\", \n                              cts, \", true)\"),\n          onmouseout = paste(\"highlight(\", gearGroups[i], \",\", \n                              cts, \", false)\")\n        )\n       }\n      )\nNote that getLatticeLegendNodes() locates the elements corresponding to the legend in the SVG.\nFor the fourth and last step, we add to the SVG document the JavaScript function definition for highlight and its helper function highlightPoint. The addECMAScripts() function takes care of this for us:\njscript = list.files(system.file(\"examples\", \"Javascript\", \n                       package=\"SVGAnnotation\"), \n                     full.names = TRUE, pattern = \"multiLegend\")\naddECMAScripts(doc, jscript)\nsaveXML(doc, \"mt_lattice.svg\")\nFor completeness, we examine the JavaScript functions highlight and highlightPoint to see how they handle the interactivity. Recall that highlight is called with the index of the group to be highlighted/un-highlighted and the array holding the counts of the number of points in that group within each panel. The function, shown below, iterates over each panel and each point within the desired group in that panel and constructs the corresponding id value for the affected points. It then retrieves the SVG element by identifier, using the Javascript method getElementById. This method is very convenient because it retrieves a node in the SVG document by specifying the value of its id attribute. Our highlight function is defined as\nfunction highlight(group, pointCounts, status)\n{\n    var el;\n    var i, numPanels = pointCounts.length;\n\n       /*  we want the group */\n     for(panel = 1; panel <= numPanels; panel++) {\n       for(i = 1; i <= pointCounts[panel-1]; i++) {\n         var id = panel + \"-\"+ group + \"-\" + i;\n         el = document.getElementById(id);\n         if(el == null) {\n           alert(\"can't find element \" + id)\n           return(1);\n         }\n         highlightPoint(el, status);\n       }\n     }\n }\nOnce an element is retrieved, highlightPoint is called to change its appearance.\nThe highlightPoint function modifies the value of the style attribute for an element. We could have handled the style change by adding a specific presentation attribute to the element, e.g., adding a fill attribute to change the fill color. Here we use regular expressions (in Perl format) to process the value of the style attribute. We break the character string into individual components, change the fill component, and create the new in-line style string. At the same time, we make sure to save the original style information in order to restore it as needed. It is saved in an attribute we made especially for this purpose, dubbed original-style. The highlightPoint function is defined as follows.\nfunction highlightPoint(el, status)\n{\n  var old = el.getAttribute('original-style');\n\n  if(status && old == null) \n    el.setAttribute('original-style', el.getAttribute('style'));\n\n  if(status) {\n    /* Have to set the attribute within the style attribute, \n       i.e. a sub-attribute which makes things more complex. */\n    var cur = el.getAttribute('style');\n    var tmp = cur.replace(/fill:?[^;]+/, \"fill: black\");\n    var tmp = tmp.replace(/stroke-width:?[^;]+/, \n                          \"stroke-width: 2\");\n    el.setAttribute('style', tmp);\n  }\n  else \n    el.setAttribute('style', old);\n}\nWe conclude the example, with a brief discussion as to how we determined the ordering of the points in the SVG for xyplot() . We programmatically changed the color of a point in the SVG, and viewed the modified document to determine that the points for each group appear sequentially within the document. Further exploration of the document confirmed that all of the elements of the plot's legend are sibling nodes of the plotting region. A simple XPath expression was used to locate them:\nother = getNodeSet(panels[[length(panels)]], \n                      \"./following-sibling::*\")\nnodesAlt = other[-(1:(length(other) - length(gearGroups)))]\nThis basic approach was then incorporated into getLatticeLegendNodes() , which does this more robustly by matching against the type of node. (svgPlot() uses the plotting calls to annotate the SVG elements with this additional information.)\nComplex/non-standard examples\nIn this section, we use a different approach to creating interactivity with JavaScript. In the section called \u201cMouse events that change the style of graphics elements\u201d , we used JavaScript to simply alter the attributes of existing elements. That is, all of the computations on the data were done in advance, in R, in either the plotting stage or the annotation stage, and the JavaScript functions simply changed and reset attributes. Here, in the annotation stage, we place R objects in the SVG document as JavaScript variables, and in the viewing stage, we use JavaScript and these variables to create new shapes in the display. We provide two examples. In the first, we draw line segments on a scatter plot to connect a point to its nearest neighbors. The information as to which points are nearest others is calculated in R and placed in the SVG document as JavaScript variables. These examples demonstrate that drawing can be done in two places: in R via the plotting routines and in JavaScript at the time the document is viewed (and R is no longer available). If we had made this plot using the earlier approach in the section called \u201cTools for customizing interactivity \u201d , we would have drawn all possible line segments in R, hidden them within the plot, and the JavaScript code would respond to mouse-over and mouse-out events by modifying element attributes in order to show and hide the line segments. In the approach here, we draw new lines in response to a mouse-over event, and then we discard them when the mouse moves off the element.\nIn the section called \u201cThe SVG grammar\u201d , we determined the structure of an SVG document generated via libcairo. To do this, we used information about the data being plotted. For example, knowing the number of observations helped us find the SVG nodes corresponding to the points. We matched the number of rows in the data frame against the number of elements in various parts of the SVG document. The examples in this section continue this approach of determining which elements in the SVG document correspond to particular components of a plot. In one case, in order to uncover the structure in the SVG image, we compare the graphics object returned from the call to the R plotting function with the graph object returned from randomGraph() . This is a deterministic process that need only be done once because the SVG output for a particular type of plot has a very regular structure. Although regular, the structure is not a formal one that allows us to immediately identify the SVG elements corresponding to R components. The SVGAnnotation package does most of this automatically. We describe it here to illustrate the underlying mechanism and approach so it is clearly understood and can be adapted for new types of plots.\nExample\u00a07.\u00a0Interactive nearest neighbors\nIn this example, we use JavaScript to dynamically (i.e., at viewing time) augment a scatter plot so that it displays the four nearest-neighbors to a point. That is, when the mouse moves over a point, new line segments are drawn from that point to its four nearest neighbors (see Figure\u00a012, \u201c Interactive display of a point's 4 nearest neighbors. \u201d ). Also, when the mouse moves off the point, the lines are removed from the display. To do this, we use JavaScript to lookup the elements in the SVG display that correspond to the nearest points, and we add new <line> elements from the active point to its neighbors. Note that we are not adding XML content to the original SVG document, but rather adding line objects to the JavaScript rendering of the SVG display.\nlibrary(\"SVGAnnotation\")\nlibrary(\"XML\")\ndoc = svgPlot(plot(mpg ~ wt, mtcars, \n                   main = \"Motor Trend Car Road Tests\", \n                   pch=19, col= c(\"green\", \"blue\")[(am+1)]))\nptz = getPlotPoints(doc, simplify = FALSE)[[1]]\nsapply(seq(along = ptz), function(i) \n          addAttributes(ptz[[i]], id = i - 1))\nsapply(seq(along = ptz), \n       function(i) { \n        addAttributes(ptz[[i]], \n                      onmouseover = \n                        \"showNeighbors(evt, k, neighbors)\", \n                      onmouseout = \"hideLines(evt)\") \n       })\nDD = as.matrix(dist(mtcars[, c(\"mpg\", \"wt\")]))\nD = t(apply(DD, 1, order)) - 1\n#jscript=c(\"../Javascript/knnMain.js\", \"../Javascript/knnAddLines.js\",\n#          \"../Javascript/knnHideLines.js\")\ndimnames(D) = list(NULL, NULL)\njscript = list.files(path = system.file(\"examples\", \"Javascript\", \n                                        package = \"SVGAnnotation\"), \n                     full.names = TRUE, pattern=\"knn\")\naddECMAScripts(doc, jscript, TRUE, neighbors = D)\nsaveXML(doc, \"mt_knn.svg\")\nThis interactive scatter plot can display the four-closest neighbors of any point. When the mouse moves over a point, a JavaScript function is called to determine the coordinates of the 4 nearest neighbors and draw line segments from the active point to its neighbors. The SVG document contains JavaScript variables that hold the indices of the neighbors for each point, in order, from closest to farthest. The nearest neighbors are defined by Euclidean distance in the mpg and wt dimensions. Notice that the scales on the two axes are not the same and so the aspect ratio for the plot is not 1. This can yield a somewhat misleading display of the nearest neighbors.\nWe begin by creating the basic scatter plot:\ndoc = svgPlot(plot(mpg ~ wt, mtcars, \n                   main = \"Motor Trend Car Road Tests\", \n                   pch=19, col= c(\"green\", \"blue\")[(am+1)]))\nIn the annotation stage, we add unique identifiers to the points in the plot. Specifically, we add an id attribute to each point that simply has the index of the observation in the data frame. Since JavaScript uses 0-based indexing, we find it easier to start the indexing at 0, e.g.,\nptz = getPlotPoints(doc, simplify = FALSE)[[1]]\nsapply(seq(along = ptz), function(i) \n          addAttributes(ptz[[i]], id = i - 1))\nWe also add calls to the JavaScript functions (showNeighbors and hideLines) to handle the mouse-over and mouse-out events on each point:\nsapply(seq(along = ptz), \n       function(i) { \n        addAttributes(ptz[[i]], \n                      onmouseover = \n                        \"showNeighbors(evt, k, neighbors)\", \n                      onmouseout = \"hideLines(evt)\") \n       })\nAlso in the annotation stage, we calculate the distances between points, and use these distances to identify the nearest neighbors as follows:\nDD = as.matrix(dist(mtcars[, c(\"mpg\", \"wt\")]))\nD = t(apply(DD, 1, order)) - 1\nNotice that we use the order of the observation in the data frame so that it will match the identifier that we added to the point in the plot.\nWe make this information about a point's neighbors available to the JavaScript code as a two-dimensional JavaScript array, serialized from R. The following call to addECMAScripts() illustrates how we serialize the R matrix D to JavaScript as the variable neighbors, in addition to adding the JavaScript code from several files:\ndimnames(D) = list(NULL, NULL)\njscript = list.files(path = system.file(\"examples\", \"Javascript\", \n                                        package = \"SVGAnnotation\"), \n                     full.names = TRUE, pattern=\"knn\")\naddECMAScripts(doc, jscript, TRUE, neighbors = D)\nThe addECMAScripts() function has a ... argument that accepts R objects in the form name = value. These objects are added to the SVG document as JavaScript variables with the argument name used as the name for the JavaScript variable. The TRUE value for the third argument indicates that we want the contents of the JavaScript file to be copied into the SVG rather than supplied via a link. We choose to copy the contents to make the resulting SVG file independent of auxiliary files.\nThe JavaScript function showNeighbors has three arguments: the event object, the number of neighbors of interest (k), and a two-dimensional array identifying the nearest neighbors for all points (neighbors). The array is a JavaScript variable that contains the contents of the R matrix D. Notice that the data are separated from the function and explicitly passed as arguments in the function call so that the function can be used with other data in other contexts. With its helper functions, showNeighbors retrieves the index of each of the k neighboring points from the neighbors array; extracts the corresponding JavaScript SVG object and gets its coordinates in the SVG canvas; and then creates a new line segment between the neighboring point and the active point. The line segments run from the center of the active point's symbol to the center of each of its neighbors. The function puts these line segments inside a new group element (corresponding to a <g> node); this facilitates removing the set of lines when the mouse moves off the point.\nWe provide here the complete code for one of the helper functions, addLines. We include it to show how JavaScript methods are used to construct new elements in the display:\nfunction addLines(obj, neighbors, numNeighbors)\n{\n    var x, y, x1, y1;\n\n    var tmp = obj.getBBox();\n    x = tmp.x + tmp.width/2;\n    y = tmp.y + tmp.height/2;\n\n    lineGroup = document.createElementNS(svgNS, \"g\");      \n    obj.parentNode.appendChild(lineGroup);\n    var ids = obj.getAttribute('id') + \": \";\n    for(var i = 1; i <= numNeighbors ; i++) {\n      var target;\n      target = document.getElementById(neighbors[i]);\n      ids = ids + \" \" + neighbors[i];\n\n      tmp = target.getBBox();\n      x1 = tmp.x + tmp.width/2;\n      y1 = tmp.y + tmp.height/2;\n\n      var line = document.createElementNS(svgNS, \"line\");      \n      line.setAttribute('x1', x);\n      line.setAttribute('y1', y);\n      line.setAttribute('x2', x1);\n      line.setAttribute('y2', y1);\n      line.setAttribute('style', \"fill: red; stroke: red;\");\n\n      line.setAttribute('class', \"neighborLine\");\n      lineGroup.appendChild(line);\n    }\n    window.status = ids;\n}\nNote that although libcairo uses only <path> elements to draw objects, we can use higher-level elements such as <line> in our JavaScript drawing. Also note that when we create the new nodes, we must include the name space or otherwise the node is not recognized as SVG. The line segments are put into a group that is stored as a global variable in JavaScript. This makes it easy to remove the lines in one operation when the mouse moves off the point. This is illustrated by the definition of the hideLines function which simply checks the value of the variable lineGroup and removes its children if it is an existing node:\nfunction hideLines()\n{\n    if(typeof lineGroup != \"undefined\") {\n      lineGroup.parentNode.removeChild(lineGroup);\n      lineGroup = document.createElementNS(svgNS, \"g\");      \n    }\n}\nThis example has demonstrated how to use JavaScript to dynamically manipulate an SVG display using data created in R when the plot was originally created. Again, in the viewing stage, when the JavaScript is running, the original data in R are not available. The matrix of ordered neighbors for each point that was computed in R is available within the JavaScript code. If the script needs any of the additional data, then they must be placed in the document in the annotation stage.\nAn alternative approach to the above example is to add, in the annotation stage, all of the nearest neighbor lines for all of the points. We would set the visibility style attribute to \"hidden\" so these line segments would not be displayed when the document is loaded by the viewer. Then, a point's onmouseover function call would identify the appropriate line segments emanating from the point to its nearest neighbors and change their visibility attribute to \"visible\". Similarly, the onmouseout event would change them back to \u201chidden\u201d. This alternative approach is similar in spirit to those examples in the section called \u201cMouse events that change the style of graphics elements\u201d , i.e., JavaScript functions do little other than change attribute values on nodes in the document. There are run-time benefits to this approach because we don't have to create the lines each time the user moves over a point. However, there are many more lines in the display, and the loading of the SVG file may be slower. This trade-off involves the issue of where the computations are performed. The approach that leaves the line creation to JavaScript also generalizes to allow dynamic specification of the number of nearest neighbors, e.g., with a slider or HTML spin box.\nExample\u00a08.\u00a0Highlighting nodes and edges in a graph\nIn this example, we explore how to annotate an SVG document created via Rgraphviz [ Rgraphviz ] to make the graph interactive. Specifically, when the mouse moves over a node in the graph, the node and its connections to other nodes are highlighted. In addition, the other connections between nodes in the graph recede in appearance (see Figure\u00a013, \u201cInteractive nodes in a graph\u201d ).\nlibrary(\"Rgraphviz\")\nlibrary(\"RJSONIO\")\nset.seed(123)\nV = letters[1:10]\nM = 1:4\ng1 = randomGraph(V, M, 0.8)\ndoc = svgPlot(plot(g1, \"twopi\", \n              attrs = list(node = list(fillcolor = \"white\"))))\ntop = xmlRoot(doc)[[\"g\"]][[\"g\"]]\ntable(names(top))\ng path \n  10   55\nlayout2pi = agopen(g1, layoutType = \"twopi\", name = \"bob\")\n#ids = addGraphIds(doc, layout2pi)\ncat(toJSON(getEdgeInfo(g1)))\n{\n \"a\": [ \"edge:a-c\", \"edge:a-d\", \"edge:a-e\", \"edge:a-g\", \"edge:a-h\", \n        \"edge:a-i\", \"edge:a-j\", \"edge:a-b\", \"edge:a-f\" ],\n \"b\": [ \"edge:b-c\", \"edge:b-d\", \"edge:b-e\", \"edge:b-f\", \"edge:b-g\", \n        \"edge:b-h\", \"edge:b-i\", \"edge:b-j\", \"edge:a-b\" ],\n...\n \"j\": [ \"edge:a-j\", \"edge:c-j\", \"edge:d-j\", \"edge:e-j\", \"edge:g-j\", \n        \"edge:h-j\", \"edge:i-j\", \"edge:b-j\", \"edge:f-j\" ] \n}\nids = addGraphIds(doc, layout2pi)\nels = getNodeElements(doc)  \nsapply(seq(along = els),\n          function(i)\n            addAttributes(els[[i]], \n             onmouseover = paste(\"highlightEdges(evt, \", i- 1, \n                                  \", 'chartreuse');\"),\n             onmouseout = paste(\"highlightEdges(evt, \", i-1, \");\")\n                          ))\ninfo = getEdgeInfo(g1) \nnames(info) = seq(from = 0, length = length(info))\notherEdges = lapply(info, function(x) setdiff(ids$edgeIds, x))\nmapply(addLink, els, ids$nodeIds, MoreArgs = list(silent = TRUE))\njscript = c(system.file(\"examples\", \"Javascript\", \n                        \"highlightEdges.js\", \n                         package = \"SVGAnnotation\"),\n            system.file(\"examples\", \"Javascript\", \n                        \"setEdgeStyle.js\",\n                        package = \"SVGAnnotation\")\n           )\naddECMAScripts(doc, jscript, TRUE, edgeTable = info,\n                 edgeDiff = otherEdges)\nsaveXML(doc, \"graphviz.svg\")\nThis graph is made using the Rgraphviz [ Rgraphviz ] package. The graph has been annotated to make it interactive, where a mouse-over event on a node brings that node and its edges to the forefront, and the remaining edges recede into the background. This interactivity relies on JavaScript. The mouse-over event triggers a call to a JavaScript function that finds the edges and nodes that need to be modified and changes their color by modifying the corresponding SVG elements. The information identifying the connections between nodes is supplied via JavaScript variables whose values were computed in R and stored in the SVG document as JavaScript variables. This particular graph is not of great interest, being a simple random graph. However, the ideas apply to more interesting data such as package dependencies and call graphs illustrating which functions call which other functions.\nThis example is adapted from [ HowToUseRgraphviz ]. We follow their example and make a simple graph with 10 nodes using the graph package.\nlibrary(\"Rgraphviz\")\nlibrary(\"RJSONIO\")\nset.seed(123)\nV = letters[1:10]\nM = 1:4\ng1 = randomGraph(V, M, 0.8)\nWe use a circular \u201ctwopi\u201d layout of the nodes with\ndoc = svgPlot(plot(g1, \"twopi\", \n              attrs = list(node = list(fillcolor = \"white\"))))\nWe then examine the resulting SVG content\ntop = xmlRoot(doc)[[\"g\"]][[\"g\"]]\ntable(names(top))\ng path \n  10   55\nWe see that we have 65 nodes; 10 are <g> nodes and the remainder are <path> elements. In this layout, the first 10 SVG elements in the graph specify the text labels for the graph's nodes. The remaining 55 correspond to the 10 circles, one for each node, and the 45 edges corresponding to the 10 choose 2 undirected connections.\nWe can find out more about the structure of the graph with agopen() .\nlayout2pi = agopen(g1, layoutType = \"twopi\", name = \"bob\")\nThe layout2pi variable contains all the information about the positions of the nodes and edges. renderGraph() uses this to draw the graph using R's graphics primitives (circle, line, text, etc.). So rather than using R's graphics and post-processing the resulting SVG document, we might consider creating the SVG directly from the layout information returned by agopen() . We wouldn't have to identify the node and edge elements in the SVG tree as we would explicitly create them. We could give them meaningful id attributes to make annotating the SVG content easier. This is a reasonable approach, but it requires that we permit the R user to specify the color, font family and size, line types and widths, etc. for the nodes and edges. Furthermore, the R user would not be able to place additional elements on the plot with regular R graphics functions so it is best to work with the SVG document that R creates.\nWe can however make use of the Ragraph object (returned by agopen() ) to identify the elements in the SVG. For example, we can look at the edges in the AgEdge slot of the layout. This is a list with 45 elements. The order of the elements in the top-level SVG <g> element corresponds to the order of the edges in this list.\nNow that we have some understanding of the structure of the SVG document and how to identify the nodes and edges, we can proceed with our goal of making the display interactive. We add code to the document that allows the user to mouse-over/click on a node and highlight the associated edges. The action is done via JavaScript. We start by identifying each node by a number, its index in the list of nodes. To highlight the node and edge, we call the function highlightEdges, passing it that index. The highlightEdges function determines the ids of the edges associated with that node and uses these to retrieve the corresponding element in the display. It then sets their color attribute to, e.g., chartreuse. The function also changes the color of the other edges and nodes to make them less visible. When the mouse leaves the node, we set the color of the edges back to their original colors.\nThe first step is to add an id attribute on each of the SVG elements corresponding to the nodes and edges. The function addGraphIds() does this for us, e.g.,\n#ids = addGraphIds(doc, layout2pi)\nThis uses the node labels as the node ids. The edge ids are of the form, \"edge:sourceId-destinationId\", e.g., \"edge:a-b\", identifying the two end points with their ids and indicating the direction of the edge.\nTo implement highlightEdges, we need information for each node giving the ids of the associated edges. It is natural to represent this in R as a list with each element being a character vector giving the edge identifiers corresponding to the node. This is relatively straightforward to create in R using getEdgeInfo() (in SVGAnnotation ). However, we need it in JavaScript. The addECMAScripts() function will take care of serializing this R object in a suitable form so that it can be used directly in JavaScript code. It uses the toJSON() function in the RJSONIO package [ RJSONIO ] to do this. The following illustrates the form in which it will appear in the JavaScript code:\ncat(toJSON(getEdgeInfo(g1)))\n{\n \"a\": [ \"edge:a-c\", \"edge:a-d\", \"edge:a-e\", \"edge:a-g\", \"edge:a-h\", \n        \"edge:a-i\", \"edge:a-j\", \"edge:a-b\", \"edge:a-f\" ],\n \"b\": [ \"edge:b-c\", \"edge:b-d\", \"edge:b-e\", \"edge:b-f\", \"edge:b-g\", \n        \"edge:b-h\", \"edge:b-i\", \"edge:b-j\", \"edge:a-b\" ],\n...\n \"j\": [ \"edge:a-j\", \"edge:c-j\", \"edge:d-j\", \"edge:e-j\", \"edge:g-j\", \n        \"edge:h-j\", \"edge:i-j\", \"edge:b-j\", \"edge:f-j\" ] \n}\nNow that we have arranged for the node-edge information to be available to the highlightEdges function, the JavaScript code to manipulate this is, at its very basic, like the following:\nfunction highlightEdges(evt, row, color)\n{\n  var labels = edgeTable[row];\n  var reset = false;\n  var el;\n\n   /* If no color was specified, we reset the original values. */\n  if(typeof color == 'undefined') {\n    reset = true; \n    color=black;\n  } else {\n    currentStyle = evt.target.getAttribute('style');\n  }\n\n    /* Loop over the edges associated with this node */\n  for(var i = 0; i < labels.length; i++) {\n      el = document.getElementById(labels[i]);\n      setEdgeStyle(el, \"stroke: \" + color);\n  }\n \n  labels = edgeDiff[row];\n  var stroke = \"lightgray\";\n  if(reset) stroke = \"black\";\n\n    /* hide the other edges */\n  for(var i = 0 ; i < labels.length; i++) {\n    el = document.getElementById(labels[i]);\n    setEdgeStyle(el, 'stroke: ' + stroke);\n  }\n\n   /* Restore or set the style for the target node. */\n  if(reset) evt.target.setAttribute('style', currentStyle);\n  else evt.target.setAttribute('style', 'fill: ' + color);\n}\nGiven this JavaScript function, we are ready to put all the pieces together to annotate the SVG document. The steps are: i) put mouse event handlers on the SVG elements corresponding to the nodes in the graph, ii) compute the information about the edges for each node, iii) add the code and this edge information data to the SVG document. We do this as\nids = addGraphIds(doc, layout2pi)\nels = getNodeElements(doc)  \nsapply(seq(along = els),\n          function(i)\n            addAttributes(els[[i]], \n             onmouseover = paste(\"highlightEdges(evt, \", i- 1, \n                                  \", 'chartreuse');\"),\n             onmouseout = paste(\"highlightEdges(evt, \", i-1, \");\")\n                          ))\ninfo = getEdgeInfo(g1) \nnames(info) = seq(from = 0, length = length(info))\notherEdges = lapply(info, function(x) setdiff(ids$edgeIds, x))\nmapply(addLink, els, ids$nodeIds, MoreArgs = list(silent = TRUE))\njscript = c(system.file(\"examples\", \"Javascript\", \n                        \"highlightEdges.js\", \n                         package = \"SVGAnnotation\"),\n            system.file(\"examples\", \"Javascript\", \n                        \"setEdgeStyle.js\",\n                        package = \"SVGAnnotation\")\n           )\naddECMAScripts(doc, jscript, TRUE, edgeTable = info,\n                 edgeDiff = otherEdges)\nsaveXML(doc, \"graphviz.svg\")\nNote we pass the edge information to JavaScript via addECMAScripts() and create JavaScript variables named edgeTable and edgeDiff.\nAnimation\nSVG supports two types of animation: declarative, which solely uses SVG facilities; and scripted, which relies on JavaScript. In this section, the focus is on the declarative approach, however, we present examples of both. The SVG animation facilities make it quite easy to produce animations similar to GapMinder, by simply annotating a scatter plot display with animation elements.\nThe basic concept of a \u201cpure\u201d SVG animation is that animation elements provide directions on how to move an object, or group of objects, and how to change the shape or appearance/style of an object. These animation elements are added as children of the object that is being animated. The animation tag names are <animate>, <animateMotion>, <animateTransform>, <animateColor>, and <set>. These tags act on a particular attribute of the parent object. For example, the <animateTransform> tag can be used to change the width attribute of its parent, which causes the parent object to grow or shrink. As another example, <animateTransform> can change the visibility attribute in order to make an object become hidden or visible. Furthermore, with <animateMotion>, we can specify a new location for the object, which causes the object to move to this new position.\nTo get an idea as to how this works, the following SVG animation takes two seconds to move a circle from its original location at the time of loading to a new location and simultaneously shrink the circle from a radius of 5.4 to 3. This mini-animation is the building-block for the scatter-plot animation shown in Figure\u00a04, \u201cScatter plot animation\u201d . There, each point represents a country, and the x and y locations correspond to the country's average life expectancy and income for a particular decade. As the points move, they grow or shrink according to the change in population size from one decade to the next. The animation is described in more detail in Example\u00a09, \u201cAnimating scatter plots through snapshots in time\u201d .\n<circle x=\"95.1\" y=\"369.8\" r=\"5.4\" style=\"fill-rule: nonzero; fill: rgb(100%,0%,0%); ...\"> <animateMotion id=\"move1\" from=\"95.1, 369.8\" to=\"66.7, 412.7\" fill=\"freeze\" dur=\"2s\" begin=\"0s\"/> <animateTransform attributeName=\"transform\" type=\"scale\" fill=\"freeze\" to=\"3\" dur=\"2s\" begin=\"0s\"/> </circle>\nThe <animateMotion> tag provides the details for moving the circle across the canvas, and the <animateTransform> tag provides information to resize the circle. The attributes from and to on <animateMotion> provide the beginning and ending location for the circle. Whereas to on <animateTransform> indicates the final size of the circle. The type attribute of \"scale\" indicates that the circle is to be scaled; other possible values for this attribute are rotate, skewX, skewY, and translate. The fill attribute is not to be confused with the fill attribute of the circle. It refers to what is to happen at the end of the animation. The value of \"freeze\" indicates that the final value should be kept. Without it, the attribute being animated would return to its original value, i.e., the circle would return to its starting position or it would return to its original size. The attributes begin and dur, indicate that the movement of the circle and its change in size are to both start at \"0s\", i.e., when the page is loaded, and last for 2 seconds.\nThe movement of the circle and the changing of its size are coordinated by specifying that both animations are to begin at the same time (begin is at 0 seconds) and take the same amount of time (2 seconds). With <animateMotion>, it is also possible to specify the path the circle would take in traveling from one location to the other. Any animation element requires a dur attribute to specify its duration. The value can be provided in seconds, e.g., dur=\"10s\", or minutes, e.g., dur=\"2min\". It is also possible to use clock values, e.g., dur=\"2:10\" for 2 minutes and 10 seconds. The SVG clock starts ticking when the document has completed loading.\nIn addition to specifying the duration of an animation, it is also possible to specify when the animation is to begin or end. The values of these attributes can be absolute values, as with dur, or they can be relative to the start or end of another animation. For example, if we change the begin value in our <animateTransform> to \"move1.end+3s\", then the circle would start shrinking 3 seconds after it finished moving to its new location, i.e., 3 seconds after the animation with an id attribute value of \"move1\" ends. This can be useful when synchronizing parts of an animation.\nThe <set> element behaves somewhat differently in that it is used to change attribute values such as fill. For example, the color of the circle could be changed when it reaches its new location by embedding the following element as a child of <circle>:\n<set attributeName=\"fill\" fill = \"freeze\" attributeType = \"CSS\" to = \"#FFDB00FF\" begin = \"move1.end\"/>\nThe attribute called attributeName specifies which attribute of the parent node is to be \u201canimated\u201d. (Note that attributeName in <animateTransform> is \"transform\", which is not an explicit attribute on the circle.) The to attribute indicates the new color of the circle. The change of color begins when the circle has finished moving because begin has a value of \"move1.end\". As mentioned already, fill refers to what is to happen at the end of the animation; the value of \"freeze\" indicates that the final color should remain.\nOne additional point to note is that the attributeType attribute in the <set> element is used to indicate where the attribute being changed can be found. A value of \"CSS\" means that the fill color is located in the style attribute of <circle>. If the circle had specified the value of its fill color in a presentation attribute, then we would have given attributeType a value of \"XML\".\nScatter plot animation\nThe animate() function in the SVGAnnotation package takes a scatter plot and moves the points around the canvas through a sequence of steps. In each step, a point moves continuously from one position to another; then in the next step, the point moves from there to yet another location. We implement it by adding <animateMotion> elements to each of the SVG elements corresponding to points in the plot. Each \u201cpoint\u201d in the SVG document will have as many <animateMotion> elements as there are time steps. In addition, if we want the size of the points to represent the value of an additional variable, which also changes in time, then we can specify the radii of the circles in each stage. In this case, the animate() function will add <animateTransform> elements to the display to change the circle sizes. Also, if the point is to change color, <set> elements are also added. These various options are specified in the function call.\nThe animate() function takes as an argument the SVG display of the initial scatter plot. It could in fact create the plot itself based on the data for the first time step, but the caller will most likely want to create the plot in advance in order to customize its appearance (e.g., title, axes labels, lines, etc.). A second argument provides the data to be animated. The data are provided as a data frame giving the locations of the points in the scatter plot for the different steps. It is structured as a column of x values and a column of y values for each step stacked on top of each other. In addition, the which parameter indicates to which step the points belong. We can provide radii for the circles via the radii parameter, if we want to change the size of the circles. In addition, we can supply colors for each stage via the colors parameter. We demonstrate some of these features and some implementation issues in the next example, Example\u00a09, \u201cAnimating scatter plots through snapshots in time\u201d .\nExample\u00a09.\u00a0Animating scatter plots through snapshots in time\nIn this example, we display 11 decades of United Nations data via a scatter plot animation shown in Figure\u00a04, \u201cScatter plot animation\u201d . The following is a snippet of the data we use,\nhead(gapM)\nlongevity    income population   yr    country\n1     39.70 4708.2323    6584000 1900  Argentina\n2     63.22 6740.9394    4278000 1900  Australia\n3     42.00 5163.9268    6550000 1900    Austria\n4     22.00  794.8383   31786000 1900 Bangladesh\n5     51.11 4746.9662    7478000 1900    Belgium\n6     32.90  705.4758   21754000 1900     Brazil\ntail(gapM)\nlongevity   income population   yr        country\n391    80.550 48264.67    4610820 2000         Norway\n392    69.906  6875.51   28302603 2000           Peru\n393    70.303  3030.88   89468677 2000    Philippines\n394    78.920 20149.08   10605870 2000       Portugal\n395    78.471 32334.53   60609153 2000 United Kingdom\n396    77.890 42445.70  298444215 2000  United States\nWe want to plot the variable income along the x axis and longevity along the y axis; we use yr to indicate the time/step; and population determines the radius of the circle.\nThese data have many missing values, and most of the work for us is in cleaning up that data and formatting it for animate() . Below is a snippet of the data preparation that we need to do. Here we are computing the radii for the circles. When the data are missing, we set the radii to a small value.\nrad = 1 + 10 * sqrt(gapM$population)/max(sqrt(gapM$population))\ndisappear = is.na(gapM$longevity) | is.na(gapM$income) | \n            is.na(gapM$population)\nrad[disappear] = 0.00001 \nradL = lapply(ctry, function(i) rad[gapM$country == i])\nnames(radL) = ctry\nHere ctry is a vector of the names of the countries to be included in the animation.\nOnce the data are prepared, we plot the points for the first decade, which creates the starting frame of the animation. We control much of the layout, placing the tick marks, grid lines, and labels ourselves, as shown here.\ndoc = svgPlot( {\n  plot(longevity ~ income, \n          subset(gapM, yr == 1900 & country %in% ctry), \n    pch = 21, col = colsB, bg = colsI,\n    xlab = \"Income\", ylab = \"Life Expectancy\", \n    axes = FALSE, \n    xlim = c(-400, 50000), ylim = c(20, 85) )\n    box()\n    y.at = seq(20, 85, by = 5)\n    x.at = c(200, 400, 1000, 2000, 4000, 10000, 20000, 40000) \n    axis(1, at = x.at, labels = formatC(x.at, big.mark = \",\", \n                                         format = \"d\"))\n    axis(2, at = y.at, labels = formatC(y.at) )\n    abline(h = y.at, col=\"gray\",  lty = 3)\n    abline(v = x.at, col=\"gray\",  lty = 3)\n    legend(35000, 40, longCont, col = colB, fill = colB, \n           bty = \"n\", cex = 0.75 )\n   })\nHere colB holds the colors for each circle border, colI has the colors for the interior of the circles, and longCont has that names of the continents. We would also like to provide tooltips for each of the points that display the particular country's name. We annotate the plot using addToolTips() as follows\naddToolTips(doc,\n   as.character(gapM$country[gapM$yr == 1900 & gapM$country %in% ctry]))\nThe initial view or starting point of the animation is now in the variable doc. We provide this to animate() , along with the subsequent slices of the data for each decade. We specify the duration of the animation via the interval parameters ( the total duration of the animation can also be specified via dur), and the radii of the circle at each time period (e.g., decade) is provided in radii. The call to animate() is\nanimate(doc, \n        gapM[gapM$country %in% ctry, c(\"income\", \"longevity\")], \n        gapM$yr[gapM$country %in% ctry], \n        dropFirst = TRUE,\n        labels = seq(1900, length = 11, by = 10),\n        begin = 0, interval = 3,  radii = radL[ctry])\nThe animate() function also needs to know the range of the horizontal and vertical data used to create the initial plot. If this is not the range of the entire data, the caller must specify these explicitly. Furthermore, background labels and point colors can be changed at each time period; these can be specified via labels and colors, respectively.\nThe animate() function handles the complication that arises from the coordinate system of the data being different from the SVG coordinate system. The data values must be transformed into the SVG coordinate system before we add an <animateMotion> node to move the points to new locations. Below is a snippet of the code in the animate() function that does this.\nrect = getBoundingBox(plotRegion)\ntransformX = function(vals, xlim, rect) {\n     (vals - xlim[1]) * \n       (rect[2, 1]- rect[1, 1])/(xlim[2] - xlim[1]) + rect[1, 1]\n}\nThe getBoundingBox() function returns the limits of the plotting region in the SVG coordinate system, and these are used to transform the data values (in vals). If the data are to be plotted on a log scale, then the data must be transformed as well. That is, the parameter log in plot() will create a mismatch between the scale of the data and the plotting region. Instead, the caller will need to take logs of the data and handle the specification of the tick mark labels if they are to appear in unlogged units.\nA second complication arises due to the SVG element for drawing the plotting symbol being a <path> rather than a <circle>. The animate() function examines the path to determine whether or not the plotting symbol is a circle. If the radii parameter is provided and the plotting symbol is a circle, then the <path> is replaced by a circle so that it can easily grow and shrink as it moves through the stages of the animation. Currently, all other symbols will move around the canvas and radii will be ignored, but a scaling transformation is possible.\nControlling animation with JavaScript\nIn this section, we examine how to animate plots using JavaScript explicitly via programmatic changes to the display at viewing time. There are some kinds of animations that are difficult to achieve with the declarative approach of the SVG model; for example, to change a path dynamically (i.e., at run/viewing-time) requires scripting. We provide a simple example to illustrate a few of the ideas in the JavaScript approach.\nThe setInterval function is key to using JavaScript for animation. This function sets a timer to go off at repeated intervals. When it does, previously specified JavaScript code is evaluated. For example, in the code\nanimationHandle = setInterval(\"animationStep(polyIds, \n                                             stateResultsByYear, \n                                             yearLabels, \n                                             polyStateNames)\", \n                              Interval);\nthe setInterval function is called with the values \"animationStep(polyIds, stateResultsByYear, yearLabels, polyStateNames)\" and Interval. This means that every Interval milliseconds, the specified call to animationStep will be made. The return value from the setInterval function is a reference to the timer just established. We assign this to the global variable animationHandle. To stop the animation, we can call the JavaScript function clearInterval with animationHandle as its argument, and the timer will be removed.\nPreviously, we encountered the use of scripts to interact with an SVG graphic in the section called \u201cMouse events that change the style of graphics elements\u201d . There, a command was called as the result of a mouse event, such as a mouse-over and mouse-out. In addition, there are the click, mouse-down, and mouse-up events associated with the mouse button. Another event that can be used to trigger a function call is \u201cload\u201d, which occurs when the SVG document has been loaded and rendered in the viewer. Each of these events has a corresponding attribute, e.g., onmouseover, onclick, and onload, and the value of the attribute is one or more JavaScript statements to be executed when the event occurs (see Example\u00a07, \u201cInteractive nearest neighbors\u201d and Example\u00a05, \u201cPoint-wise linking across plots\u201d ). Any of these events can also be used to initiate an animation. Below, we place setInterval within an init function, and have it called at the time the SVG document is loaded, i.e., the top of the document appears as\n<svg xmlns=\"http://www.w3.org/2000/svg\"\n     xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n     width=\"504pt\" height=\"504pt\" viewBox=\"0 0 504 504\"\n     version=\"1.1\" onload=\"init(evt)\">\nThe key attribute is onload. The corresponding JavaScript code is\nvar animationHandle; \nfunction init(evt) { \n...  \n  animationHandle = setInterval(\"animationStep(polyIds, \n                                               stateResultsByYear, \n                                               yearLabels, \n                                               polyStateNames)\",\n                                Interval);\n}\nThe following example demonstrates how to use the timer to create an animation.\nlibrary(\"SVGAnnotation\")\nlibrary(\"maps\")\ndata(\"electionHistory\")\ndoc = svgPlot({\n        m <- map('state', fill = TRUE, col = 'grey66')\n        title('Presidential Election Results by State 1900-2008')\n        text(m$range[1] + 3, m$range[3] + 1, \"Start\", col = \"green\")\n      })\nlabels = getAxesLabelNodes(doc)\ntitle = asTextNode(labels$title, \n           'Presidential Election Results by State 1900-2008')\nxmlAttrs(title) = c(id = \"title\")\nstart = getTextNodes(doc)[[1]]\nstart = asTextNode(start, \"Start\")\nxmlAttrs(start) = c(id = \"Start\", onclick = \"toggleAnimation(evt)\")\naddToolTips(start, \"Start or pause the animation\")\npts = getPlotPoints(doc)\nmapply(function(node, id)\n            xmlAttrs(node) = c(id = id),\n        pts, m$names)\naddToolTips(pts, m$names)\npolyStateNames = gsub(\":.*$\", \"\", m$names)\npolyStateNames[ polyStateNames ==\"district of columbia\"] = \"d. c.\"\naddECMAScripts(doc, \n               system.file(\"examples\", \"JavaScript\", \n                           \"animateElectionMap.js\", \n                            package=\"SVGAnnotation\"),\n               stateResultsByYear = electionHistory, \n               yearLabels = names(electionHistory),\n               polyIds = m$names,\n               polyStateNames = polyStateNames,\n               insertJS = TRUE)\nconvertCSSStylesToSVG(pts)\nsaveXML(doc, \"exJSAnimateElectionMap.svg\")\nThis screen shot was captured in the midst of an animation where the colors of the states (red for Republican, blue for Democrat, and green for Independent) change to reflect the party of the presidential candidate who received the most votes in the state. The animation is initiated with a mouse-click on the term \u201cStart\u201d, and once it starts, it can be paused with a mouse-click on \u201cPause\u201d. The animation is controlled by a JavaScript timer, where at regular intervals the SVG is modified. In particular, the state polygons are filled with color for the next election year and the map title is changed to the next date.\nExample\u00a010.\u00a0Animation with JavaScript and SVG\nIn this example, we animate a political map of the states within the USA. Each state is painted according to the party of the winning presidential candidate, with Republican red, Democrat blue, and Independent green. For any given year, this gives us a visualization of the geographical voting patterns. We obtained this information for 1900 to 2008 from onwards http://electionatlas.org .\nThe data are in electionHistory, a named list with an element for each of the presidential elections. Each such element is a character vector giving the colors for each of the states. The names on this character vector identify each state. The names of the electionHistory list identify the election year, e.g., 1900, 1904, ..., 2008.\nWe start by plotting a grey state map of the US. Also, since we do not want the animation to start when the page is loaded, we add a label titled \"Start\" which we will have respond to a mouse click to start the animation.\nlibrary(\"SVGAnnotation\")\nlibrary(\"maps\")\ndata(\"electionHistory\")\ndoc = svgPlot({\n        m <- map('state', fill = TRUE, col = 'grey66')\n        title('Presidential Election Results by State 1900-2008')\n        text(m$range[1] + 3, m$range[3] + 1, \"Start\", col = \"green\")\n      })\nAt regular intervals, e.g., 1 second, we will update the title to the next election year and change the color of the polygon(s) for each state. The title element is reasonably easy to find using the function getAxesLabelNodes() :\nlabels = getAxesLabelNodes(doc)\nHowever, we change the nature of this title node to be a regular text element rather than a path element. The reason for this is that it is significantly simpler to modify the value of the text node than that of a path which represents text:\ntitle = asTextNode(labels$title, \n           'Presidential Election Results by State 1900-2008')\nWe put an id attribute on the new title element, which is the first element of the labels list, so that we more easily can retrieve it with JavaScript code at viewing time.\nxmlAttrs(title) = c(id = \"title\")\nWe add an id attribute to the \u201cStart\u201d label, and make it respond to onclick events. A mouse-click will causes the state of the animation to change from stopped to running, running to paused, or from paused to running, and to also change the text of this label appropriately. We do this with the JavaScript function toggleAnimation and add the call to the label with\nstart = getTextNodes(doc)[[1]]\nstart = asTextNode(start, \"Start\")\nxmlAttrs(start) = c(id = \"Start\", onclick = \"toggleAnimation(evt)\")\nWe'll also add a tooltip to this label which will both provide information to the viewer but also make the entire label (and not just the path) active. So we add this and the CSS file which takes care of the appearance of the underlying rectangle:\naddToolTips(start, \"Start or pause the animation\")\nThe next step is to add an identifier to each polygon on the map. The map contains 63 polygons, as some states such as Washington are drawn using multiple polygons. Each polygon has a unique name, which we add as ids to the elements:\npts = getPlotPoints(doc)\nmapply(function(node, id)\n            xmlAttrs(node) = c(id = id),\n        pts, m$names)\nWe also put a tooltip on each polygon to identify the state:\naddToolTips(pts, m$names)\nFinally, we add the necessary variables to the JavaScript code in the SVG document along with the JavaScript code for handling the animation. We will assume the JavaScript code is in a file named animateElectionMap.js, and add the code and variables to the SVG document using\npolyStateNames = gsub(\":.*$\", \"\", m$names)\npolyStateNames[ polyStateNames ==\"district of columbia\"] = \"d. c.\"\naddECMAScripts(doc, \n               system.file(\"examples\", \"JavaScript\", \n                           \"animateElectionMap.js\", \n                            package=\"SVGAnnotation\"),\n               stateResultsByYear = electionHistory, \n               yearLabels = names(electionHistory),\n               polyIds = m$names,\n               polyStateNames = polyStateNames,\n               insertJS = TRUE)\nNote that we are adding 4 variables: stateResultsByYear, which holds the colors for the states; yearLabels, the years of the elections; polyIds, the unique names for the polygons; and polyStateNames, the state names for each of the polygons (so Washington appears multiple times, once for each polygon).\nThe JavaScript code will update the value of fill for each polygon. It is easiest to override the value for fill whene all the style information are in separate attributes on the element. To this end, we will change the way the SVG style attributes are stored for each of the polygon nodes. We use convertCSSStylesToSVG() for this:\nconvertCSSStylesToSVG(pts)\nThat is all we need to do to our SVG document at this point so we can save it to a file:\nsaveXML(doc, \"exJSAnimateElectionMap.svg\")\nAll that remains in our example is to write the relevant JavaScript code. We need the toggleAnimation function and the code that does the actual changing of the color of each state/polygon. This code also checks to see if the animation is complete and, if so, changes the label from \u201cPause\u201d to \u201cStart\u201d.\nWe begin by defining some global variables for controlling the animation. These are the identifier for the interval, the current year being displayed and the amount of time between updates in the animation:\nvar animationHandle = null;\nvar currentYear = 0;\nvar Interval = 1000;\nNext we define the function that updates the display. This function increments the value of currentYear and checks to see if the animation is done. If the animation has completed, we reset the variables and the display. If not, we update the display with the values for this election year by calling displayYear. The entire animationStep appears here.\nfunction animationStep(ids, colors, yearLabels, stateNames)\n{\n  currentYear++;\n\n  if(currentYear >= yearLabels.length) {\n     var el = document.getElementById(\"Start\");\n     setTextValue(el, \"Start\");\n     clearInterval(animationHandle);\n     animationHandle = null;\n     for(var i = 0; i < ids.length; i++) {\n        var el = document.getElementById(ids[i]);\n        if(el)\n          el.setAttribute('style', 'fill: ' + '#A8A8A8');\n     }\n     var title = document.getElementById('title');\n     setTextValue(title, \n            'Presidential Election Results by State 1900-2008');\n     return;\n  }\n\n  displayYear(currentYear, ids, colors, yearLabels, stateNames);\n}\nThe function displayYear updates the colors of the polygons and and changes the year displayed in the title node. The simple <style> attribute contains fill information only because the other style details have been provided as indiviual attributes and so will remain in effect. It is defined as,\nfunction displayYear(year, ids, colors, yearLabels, stateNames)\n{\n  for(var i = 0; i < ids.length; i++) {\n     var el = document.getElementById(ids[i]);\n      /* Lookup the year by name. Then within the year, look up\n         the state, using the actual name not the polygon id. */\n     var col = colors[yearLabels[year]][ stateNames[ i ] ] ;\n     if(el && col)\n        el.setAttribute('style', 'fill: ' + col);\n  }\n\n  var title = document.getElementById('title');\n  setTextValue(title, yearLabels[year]);\n}\nThe toggleAnimation function is in charge of starting and pausing the animation. It is called when there is a mouse-click on the Start/Pause \u201cbutton\u201d. The function examines the value of the animation handle and if this is non-null, terminates the animation. Otherwise, it starts the animation and shows the first year. It also updates the value displayed on the \u201cStart\u201d button as shown below.\nfunction toggleAnimation(evt)\n{\n  var label;\n  if(animationHandle) {\n    clearInterval(animationHandle);\n    animationHandle = null;\n    label = currentYear > 0 ? \"Restart\" : \"Start\";\n  } else {\n    animationHandle= setInterval(\"animationStep(polyIds, \n                                     stateResultsByYear, \n                                     yearLabels, polyStateNames)\", \n                                 Interval);\n    if(currentYear >= yearLabels.length) currentYear = 0;\n    displayYear(currentYear, polyIds, stateResultsByYear, \n                yearLabels, polyStateNames);\n    label = \"Pause\";\n  }\n\n  var start = document.getElementById('Start');\n  setTextValue(start, label);\n}\nThe utility function which changes the value/label of a <text> node is given by\nfunction setTextValue(node, val)\n{\n node.firstChild.data = val;\n}\nCreating GUIs\nIn Example\u00a010, \u201cAnimation with JavaScript and SVG\u201d , we added a rectangle, containing the text \u201cStart\u201d, to act as a button, where clicking on this region in the plot started an animation. In this section, we extend this idea and demonstrate how to provide interactivity through user controls, such as buttons, check boxes, and sliders that are drawn on the display using SVG & JavaScript rather than R. The Carto:Net community [ CartoNet ] has developed an open source library of SVG graphical user interface (GUI) controls to facilitate interactivity in SVG documents. The library consists of JavaScript functions to build the controls as SVG elements and respond to user interaction. This library is available from http://carto.net/ , and is also distributed with the SVGAnnotation package, for convenience. It includes controls such as a check box, radio button, button, slider, text box, and selection menu, which are rendered using native SVG elements and are quite different from their JavaScript equivalents.\nBy using these SVG GUI components, we can create applications that have rich user interfaces. An alternative approach is to embed the SVG graphic in an HTML document and control it through buttons and boxes in an HTML form. We show how to do this in the section called \u201cInterfacing with (X)HTML\u201d . With SVG, there is the potential to design unique, sophisticated GUI elements, such as sliders, dial knobs, and color choosers. Another potentially useful feature of SVG-based GUI components is that they can be rendered with SVG filters and transformations, e.g., at an angle, with different filters (e.g., Gaussian blurring), or even animated. The main disadvantage is complexity. SVG GUI elements are somewhat more complex to use in programs than the \u201cbuilt-in\u201d HTML form elements. Fortunately, the Carto:Net library offers a variety of GUI elements that can be easily embedded in the document.\nIn this section, we provide examples of how to use two of these GUI controls, a slider and check box. The first example adds a slider to a display in order to give the viewer control of a smoothing parameter. The second example uses check boxes to add and remove groups of data (time series) from a plot.\nAlthough the GUI control is being drawn with JavaScript commands, the basic approach to annotating the SVG remains essentially the same. The annotation stage now typically requires the following additional tasks:\nEnlarge the viewing area so that there is room for the GUI control (i.e., change the viewBox attribute on the root element).\nAdd an initialization script to the document in order to create the GUI object when the document is loaded (i.e., add a call to a JavaScript initialization function via the onload attribute on the <svg> element). This script needs to call the JavaScript function provided by Carto:Net that creates the GUI object.\nPopulate the <defs> node with elements which contain the generic drawing instructions for the control that are provided by Carto:Net. Plus, locate the GUI in the display by adding an empty <g> tag that Carto:Net scripts use to draw the graphical representation of the control.\nAdd the Carto:Net scripts to the document along with the application specific JavaScript functions that connect GUI events to changes in the display, e.g., a change in the location of the slider's thumb calls a function that reveals a new curve on a plot and hides from view the old curve.\nSlider widget\nIn this section, we consider an example of how to embed a Carto:Net slider in an SVG document. The SVGAnnotation package provides a generic addSlider() function to add a slider to an SVG document. The addSlider() function takes care of setting up the SVG content according to the Carto:Net requirements. It enlarges the \u201cviewBox\u201d to make room for the slider, adds the initialization script to the <onload> attribute of the root node, adds the required <g> parent node for the slider, and inserts the required JavaScript. In addition, it places the slider thumb drawing instructions in the <defs> portion of the document. The declaration of the function is shown below.\naddSlider =\nfunction(doc, onload, javascript,\n          id = \"slider\", svg = NULL,\n          defaultJavascripts = c(\"mapApp.js\",\n                                 \"helper_functions.js\", \n                                 \"slider.js\"),\n          side = \"bottom\", ...) {}\nThe onload argument gives the JavaScript code that will be invoked when the document is loaded. Also, javascript and defaultJavascripts take the file names for the JavaScript code (file names or text content) that contain, respectively, application specific functions and the required Carto:Net code. Finally, any JavaScript variables needed in the document are supplied via the ... argument.\nThe JavaScript slider can be queried at various times to find the location of the slider thumb, to set its value, to move the slider thumb, and to remove the slider from the display. This functionality is provided via the JavaScript slider object's methods getValue, setValue, moveTo, and removeSlider, respectively. These methods are invoked as, e.g., slider.getValue(). The next example, demonstrates how to use the Carto:Net slider.\nExample\u00a011.\u00a0Controlling smoothness with a slider\nIn this example, we build a slider to interactively control the bandwidth of a kernel smoother (see Figure\u00a05, \u201cA slider for choosing a parameter value\u201d ). When the viewer moves the slider thumb to a new position, the corresponding fitted curve is overlaid on the scatter plot on the left-hand side of the display, and a scatter plot of the residuals for that curve is rendered in the plot on the right-hand side.\nSimilar to the approach of many of the previous examples, the computations for fitting the curve to the data are performed in R in advance for every possible selectable value of the smoothing parameter. The resulting curves and residuals are plotted in the SVG document. Then in the annotation stage, the SVG elements corresponding to each curve are given unique ids so that they can be identified. In this case, we use the identifier, \u201ccurve-lambda-N\u201d, where N is the parameter value associated with the curve. These nodes are also annotated with a visibility attribute so the curves can be exposed or hidden according to the value of the smoothing parameter selected by the viewer via the slider. The residuals are similarly annotated. To do this, we group together the residuals from the fit for a particular smoothing parameter, and place them within a <g> node that has a unique id, namely \u201cresidual-group-N\u201d. We also add a visibility attribute of \"hidden\" to each <g>.\nThe final annotation task is to add the slider to the document. A call to addSlider() does this for us. The value of the onload argument is a call to the init function that creates the slider. When the document is loaded, this function will be called with the number of smoothing parameter values, which is also the number of fitted curves, and the number of positions on the slider. This extra information is used to coordinate the slider value with the curve and residual points to be displayed. We add this onload attribute and the supporting JavaScript code to the SVG document with\nsvgRoot = xmlRoot(doc)\nenlargeSVGViewBox(doc, y = 100, svg = svgRoot)\nonl = sprintf(\"init(evt, %d);\", max(lambdas) )\njscript = list.files(path = system.file(\"examples\", \"Javascript\", \n                              package = \"SVGAnnotation\"), \n                     full.names = TRUE, pattern = \"linkedSmoother\")\naddSlider(doc, onload = onl, svg = svgRoot,\n          javascript = jscript, id = \"slider-lambda\")\nsaveXML(doc, \"linkedSmoother.svg\")\nThe id parameter identifies the id of the SVG element in the SVG document that corresponds to the slider, i.e., the identifier on the <g> element that is added to support the slider.\nThe init function simply instantiates the slider. It is defined as\nvar myMapApp = new mapApp(false,undefined);\nvar cur_lambda = 2;\nvar numPoints;\n\n/*  Create the slider.  */\nfunction init(evt, maxLambda, n) {\n   var sliderStyles = {\"stroke\" : \"blue\", \"stroke-width\" : 3};\n   var invisSliderWidth = 15;\n   new slider(\"lambda\", \"slider-lambda\", 100, 510, 2, 475, 510, \n              maxLambda, 2, sliderStyles, invisSliderWidth, \n              \"sliderSymbol\", setLambda, false);\n   numPoints = n;\n}\nA call to init creates the slider via the JavaScript constructor function,\nvar slider = new slider(id, parentNode, x1, y1, value1, x2, y2, \n                        value2, startVal, sliderStyles, \n                        invisSliderWidth, sliderSymb, \n                        functionToCall, mouseMoveBool);\nIn our example, the id of \u201clambda\u201d specifies a unique identifier for the slider that is used internally within Carto:Net; the parentNode, \u201cslider-lambda\u201d, gives the id for the <g> element that will contain the slider; the triple x1, y1, value1 provides the (x,y) coordinates for the location of the start of the slider and the starting value of the slider; similarly, x2, y2, value2 provide this information for the end of the slider; and startVal provides the initial value of the slider. We further customize the slider by specifying presentation attributes via CSS. In our example, we set the sliderStyles so the slider will be drawn with a thick blue line. Finally, the events on the slider are handled by the JavaScript callback function, setLambda, which we provide via the functionToCall argument, and the final argument, mouseMoveBool is \u201cfalse\u201d, indicating not to trigger updates as the mouse drags the slider, i.e., only call setLambda when the mouse is released and the action of dragging the slider thumb has stopped.\nThe setLambda function has a helper function setVisibility. Both are shown below. They are similar in spirit to JavaScript functions we have used in other examples in that they simply reset the visibility attribute for the appropriate group of points and curve (e.g., Example\u00a06, \u201cLinking across lattice plots\u201d ). They are defined as\nfunction setLambda(evType, group, val)\n{\n        /* If it is the same value, don't do anything*/\n    if(Math.floor(val) == cur_lambda) return(0);\n    setVisibility(cur_lambda, 'hidden', numPoints);\n    cur_lambda = Math.floor(val);\n    setVisibility(cur_lambda, 'visible', numPoints);\n}\n\nfunction setVisibility(lambda, state, numPoints)\n{\n     var el;\n     lambda = Math.floor(lambda);\n     el = document.getElementById(\"curve-lambda-\" + lambda);\n     el.setAttribute('visibility', state);\n\n     el = document.getElementById(\"residual-group-\" + lambda);\n     if(el) {\n          el.setAttribute('visibility', state);\n          return(0);\n     }\n\n     for(i = 0 ; i < numPoints; i++) {\n         el = document.getElementById(\"residual-\" + lambda + \n                                       \"-\" + (i+1));\n         el.setAttribute('visibility', state);\n     }\n}\nWe have put this code in the linkedSmootherSet.js file and so have already included it in the SVG document.\nCheck boxes for toggling elements in a plot\nWe can use a similar approach to create interactivity with other SVG GUI controls offered in Carto:Net. For users unfamiliar with event handling, we provide one additional example involving check boxes. The check boxes in Carto:Net support toggle events, i.e., when a box is checked or unchecked then a user-supplied function is called to perform some action. For example, in Example\u00a012, \u201cShowing and hiding currency exchange series with check boxes\u201d , several time series are overlaid on a plot and made visible or invisible according to whether or not a corresponding box is checked. That is, when a check box is toggled, this event then triggers a function call to change the visibility attribute of the associated times series.\nThe SVG document needs to be modified in the same way we did for the slider so as to include the check box GUI controls. In particular, the document should have an empty group <g> that will hold the elements for the check boxes and their labels. This needs to be added with a unique id to the appropriate place in the document; the checkBox JavaScript object needs to be initialized via JavaScript in the onload attribute on the root element; and JavaScript helper functions need to be included in the document.\nThe radioShowHide() function in R sets up a group of check boxes for toggling on and off curves in a plot. Once the initial SVG plot has been made, radioShowHide() post-processes the SVG in the annotation stage. This function handles all of the setup. Similar to the slider, radioShowHide() expands the viewing area to make room for drawing the check boxes; sets up the drawing areas for the check boxes; adds to the document the initialization script that creates the JavaScript check box objects when the document is loaded in the browser; and includes the JavaScript functions provided by Carto:Net that respond to the viewer actions and the application specific JavaScript functions that change the graphical display in response to viewers actions.\nThe function signature/declaration for the radioShowHide() function provides the information needed to perform these tasks:\nradioShowHide =\nfunction(doc, insertScripts = TRUE, within = FALSE, group = FALSE,\n          labels = paste(\"Series\", seq(length = numSeries)),\n          extraWidth = 15 * (max(nchar(labels)) + 1),\n          save = !is(doc, \"XMLInternalDocument\"),\n          id.prefix = \"series\",\n          checkboxCallback = if(is.na(within)) \"togglePanel\"\n                             else \"toggle\",\n          jscripts = c(\"helper_functions.js\", \"timer.js\", \n                       \"checkbox_and_radiobutton.js\", \n                       \"hideSVGElements.js\"),\n          numPanels = 1) {}\nBriefly, doc specifies the SVG document to be annotated; within indicates whether a regular plot (FALSE) or matplot (TRUE) call was used to create the plot, which helps in finding the series; group allows the series to be matched up across lattice panels so that a series can be toggled on and off in all panels; labels contains the text labels for the check boxes; and checkboxCallback holds the JavaScript for the call back function.\nExample\u00a012.\u00a0Showing and hiding currency exchange series with check boxes\nIn this example, we create an interactive time series plot, where several times series curves are super-imposed on the same plot and check boxes along side the plot are used to toggle the display of the series on and off (see Figure\u00a015, \u201cTogglable exchange rate display\u201d ). The check boxes are ordered according to the median value of the time series to make it easier to visually connect the check box with its time series. The text labels could also have their colors coordinated with the series to make this connection clearer. We create the SVG plot with the command\nsvgPlot({matplot(eu$Date, as.data.frame(lapply(eu[,-1], log)),\n                 type = \"l\", xlab = \"Year\", xaxt = \"n\",\n                 ylab = \"log(exchange rate)\",\n                 main = \"European Exchange Rate\")\n         startYr = min(eu$Date)\n         endYr = max(eu$Date)\n         axis.POSIXct(1, at=seq(startYr, endYr, by=\"year\"), \n                     format =\"%y\")\n         abline(h = 1, col = \"gray\")\n         }, \n        \"euSeries.svg\")\nNote that in this case we are explicitly writing the SVG to a file rather than returning the SVG tree.\nCode\nlibrary(\"SVGAnnotation\")\ndata(\"eu\")\n\neu$Date = as.POSIXct(strptime(eu$Date, \"%Y-%m-%d\"))\n\n  # Discard currencies that are mostly missing\neu = eu[ , sapply(eu, function(x) sum(is.na(x)))/nrow(eu) < .5]\neu = eu[ !(names(eu) %in% c(\"KRW\", \"TRL\", \"BGN\", \"ROL\")) ]\n\no = order(sapply(eu[-1], median, na.rm = TRUE), decreasing = TRUE)\neu = eu[c(1, o+1)]\nsvgPlot({matplot(eu$Date, as.data.frame(lapply(eu[,-1], log)),\n                 type = \"l\", xlab = \"Year\", xaxt = \"n\",\n                 ylab = \"log(exchange rate)\",\n                 main = \"European Exchange Rate\")\n         startYr = min(eu$Date)\n         endYr = max(eu$Date)\n         axis.POSIXct(1, at=seq(startYr, endYr, by=\"year\"), \n                     format =\"%y\")\n         abline(h = 1, col = \"gray\")\n         }, \n        \"euSeries.svg\")\ndoc = \n  radioShowHide(\"euSeries.svg\", within = TRUE, \n                 labels = currency.names[ match(names(eu)[-1], \n                                          names(currency.names))]\n               )\nThis screen shot shows a time-series plot (via matplot() ) of exchange rates against the Euro for 24 different currencies. The SVG is post processed to add check boxes to the right of the plot. These check boxes are interactive and allow the viewer to toggle on/off the display of the corresponding currency's time series. The SVG GUI controls shown here are provided by Carto:Net.\nA simple call to radioShowHide() will do all the post-processing of the SVG document created in the matplot() call:\ndoc = \n  radioShowHide(\"euSeries.svg\", within = TRUE, \n                 labels = currency.names[ match(names(eu)[-1], \n                                          names(currency.names))]\n               )\nWe can also extend this functionality to plots with multiple panels. For example, we might draw the daily time series for the different currencies for each year in a separate panel by conditioning on year. Then toggling the currency check box would show/hide the corresponding curves in each of the panels. Again, the reader should view the example by visiting the package's Web site and viewing it there.\nInterfacing with (X)HTML\nThe interactivity for the examples considered so far have been entirely contained within the SVG document. That is, the SVG or JavaScript code that responds to user interaction has been located in the SVG file. In this section, we demonstrate that it is also possible to control SVG using JavaScript that is placed within an HTML document, where the SVG display is also embedded in the HTML.\nThere are several benefits to this approach. For example, HTML forms can be used to control the interactivity, and these controls can be easily visually arranged using layout facilities in HTML, e.g., lists, and tables. Other advantages are that embedding the SVG document in HTML allows us to control the width and height of the area in which it is displayed; multiple SVG documents can be embedded in one HTML document; and SVG documents can be linked together via JavaScript in the HTML document. There can be separate JavaScript code in the different components, i.e., in the HTML document and in each SVG document. This increased locality and flexibility can also be slightly more complicated.\nIn this section, we provide two examples of this approach. The first revisits a previous example and creates interactivity through HTML forms rather than with onmouseover calls to JavaScript functions. It serves as a comparison between these two approaches. In the second example, we move the process of annotating the SVG entirely into the viewing stage, specifically at the point where the document is loaded into the browser. As such, JavaScript, rather than R, is used to augment the SVG elements with the necessary annotations for interactivity, e.g., adding onmouseclick attributes to plot elements.\n(X)HTML forms\nHTML forms [ XHTML ] provide built-in controls, such as a choice menu, check box, radio button, textbox, and button, that receive input from the user. The controls may not be as rich as those available in Carto:Net, e.g., a slider is only available via JavaScript rather than a built-in HTML form element, however they are simple to deploy in an HTML page and make available to readers.\nWith forms, we can mix controls in HTML with a plot in SVG. We embed the SVG display in an HTML document using the HTML element <object> such as\n<object id=\"PlotID\" data=\"plot.svg\" \n        type=\"image/svg+xml\" width=\"960\" height=\"800\"/>\nwhich specifies the name of the SVG file via the data attribute, its MIME type, and dimensions. When we want to operate on pieces of the SVG document (e.g., when the viewer clicks on an HTML button), we use JavaScript to retrieve the resulting SVG object. Once we have the SVG object, we can operate on it with JavaScript, just as we have done when we placed the scripts in the SVG document. The JavaScript code below shows how to retrieve the embedded object and treat it as an SVG document.\ndoc = document.getElementById('PlotID');\ndoc = doc.getSVGDocument();\nWe can then deploy JavaScript callback functions that respond to viewer input via the controls in an HTML form where these functions interact with an SVG document embedded in the HTML document. For example, when a viewer selects an option from a choice menu, a JavaScript function can be called to modify an SVG display in the document. We illustrate this approach in the next example.\nExample\u00a013.\u00a0 Using forms to highlight points in a lattice plot\nThis example re-implements Example\u00a06, \u201cLinking across lattice plots\u201d . In that example, we have a legend on a lattice plot. The legend displays the levels of the variable gear, which is not contained in the display. The display shows mpg, disp, am, and cyl via points, color, or location in a panel. When the viewer mouses over an entry in the legend, the observations corresponding to this value of gear are highlighted in the different panels of the lattice plot. In contrast, the example here removes the legend from the plot, and controls the highlighting action through a choice menu in an HTML form. See Figure\u00a016, \u201c Linking an HTML select menu to groups of points in a conditional plot \u201d . That is, the SVG is embedded in an HTML document and this document also contains an HTML form with a choice menu for selecting a gear value.\nCode\nlibrary(\"XML\")\nlibrary(\"SVGAnnotation\")\nlibrary(\"lattice\")\nlibrary(\"RJSONIO\")\nmtcars$cyl = factor(mtcars$cyl, \n                    labels = c(\"four cyl\", \"six cyl\", \"eight cyl\"))\nmtcars$am = factor(mtcars$am, labels = c(\"automatic\", \"manual\"))\ndoc = svgPlot(xyplot(mpg ~ hp | am, groups =cyl, data = mtcars))\npanels = getPlotRegionNodes(doc)\npoints = unlist(lapply(panels, xmlChildren), recursive = FALSE)\nids = by(mtcars, list(mtcars$gear, mtcars$am), \n         function(x) paste(as.integer(x$am), x$gear - 2, 1:nrow(x), \n                           sep = \"-\")\n        )\nuids = unlist(ids) \nmapply(function(node, id) addAttributes(node, id = id),\n       points, uids)\nsaveXML(doc, \"mt_lattice_Choice.svg\")\ncounts = table(mtcars$am, mtcars$gear)\nrownames(counts) = NULL\ndfCounts = list()\nfor (i in 1:ncol(counts)) \n{\n  dfCounts[[i]] = counts[, i]\n}\n#htmlSkel = htmlParse(\"mt_lattice_Choice_Skel.html\")\n#jscript = c(\"../Javascript/latticeChoiceHighlight.js\",\n#            \"../Javascript/latticeChoiceHighlightPoint.js\",\n#            \"../Javascript/latticeChoiceShowChoice.js\")\nhtmlSkel = htmlParse(system.file(\"examples\", \"HTML\",\n              \"mt_lattice_Choice_Skel.html\", package=\"SVGAnnotation\"))\njscript = list.files(path = system.file(\"examples\", \"Javascript\", \n                              package = \"SVGAnnotation\"), \n                     full.names = TRUE, pattern=\"latticeChoice\")\n            \naddECMAScripts(htmlSkel, scripts = jscript, \n                insertJS = TRUE, .jsvars = list(pointCounts = dfCounts))\n\nsaveXML(htmlSkel, \"mt_lattice_Choice.html\")\nrm(mtcars)\nThis HTML page contains a form and an SVG lattice plot. The SVG is controlled by the choice menu in the form. It offers the same functionality as the SVG document in Figure\u00a011, \u201c Linking a legend to a group of points in a conditional plot \u201d . However, when the viewer changes the choice in the HTML form, a JavaScript function in the HTML document is called to respond to this event, i.e., to highlight the appropriate set of points.\nThe HTML document rendered in Figure\u00a016, \u201c Linking an HTML select menu to groups of points in a conditional plot \u201d contains the SVG plot by including it using the following <object> tag:\n<object id=\"latticePlot\"  data=\"mt_lattice_Choice.svg\" \n        type=\"text/svg+xml\" width=\"960\" height=\"768\" />\nThe SVG document is the same as the one created in the earlier example with two important differences. Firstly, it no longer contains any JavaScript code. Secondly, the legend has been removed from the lattice plot as the HTML form is replacing the mouse-over capability. However, all of the plotting elements still have their appropriate identifier attributes (i.e., the id attributes indicate to which group they belong). We show the code here:\nmtcars$cyl = factor(mtcars$cyl, \n                    labels = c(\"four cyl\", \"six cyl\", \"eight cyl\"))\nmtcars$am = factor(mtcars$am, labels = c(\"automatic\", \"manual\"))\ndoc = svgPlot(xyplot(mpg ~ hp | am, groups =cyl, data = mtcars))\npanels = getPlotRegionNodes(doc)\npoints = unlist(lapply(panels, xmlChildren), recursive = FALSE)\nids = by(mtcars, list(mtcars$gear, mtcars$am), \n         function(x) paste(as.integer(x$am), x$gear - 2, 1:nrow(x), \n                           sep = \"-\")\n        )\nuids = unlist(ids) \nmapply(function(node, id) addAttributes(node, id = id),\n       points, uids)\nsaveXML(doc, \"mt_lattice_Choice.svg\")\nAlong with the SVG display of the scatter plot, the HTML document also contains a <form> with a <select> menu that has four options as shown below.\n<form>\nChoose number of gears: \n <select name=\"gear\" onchange=\"showChoice(this)\">\n   <option value=\"0\" default=\"true\">Select</option>\n   <option value=\"1\">three</option>\n   <option value=\"2\">four</option>\n   <option value=\"3\">five</option>\n </select>\n</form>\nNotice that when the selection is changed, the showChoice function is called.\nThe showChoice function is passed the JavaScript object corresponding to the choice menu, and the function uses this object to query the value the viewer has selected. The showChoice function then calls highlight, passing it the information it needs to highlight the new group of points. showChoice is\nvar oldgroup = 0;\nvar group = 0;\nvar doc;\n\nfunction showChoice(obj)\n{\n   doc = document.getElementById('latticePlot');\n   doc = doc.getSVGDocument();\n   group = Math.floor(obj.value);\n   if (group > 0) {\n    highlight(group, pointCounts[(group - 1)], true);\n   }\n\n   if (oldgroup > 0) {\n    highlight(oldgroup, pointCounts[(oldgroup - 1)], false);\n   }\n  \n   oldgroup = group;\n}\nThe highlight function called from showChoice and its helper function highlightPoint are identical to the functions by the same name that were placed in the <script> tag within the SVG document in Example\u00a06, \u201cLinking across lattice plots\u201d . They extract the elements in the SVG document corresponding to the points that belong to the selected group, and change their style attributes. These functions rely on the plotting elements having unique ids that indicate to which group the points belong. Note that the embedded SVG document is in the global variable doc, which must be extracted from the HTML for highlight.\nfunction highlight(group, pointCts, status)\n{\n    var el;\n    var i, numPanels = pointCts.length;\n\n       /*  we want the group */\n    for(panel = 1; panel <= numPanels; panel++) {\n      for(i = 1; i <= pointCts[panel-1]; i++) {\n        var id = panel + \"-\"+ group + \"-\" + i;\n        el = doc.getElementById(id);\n        if(el == null) {\n          alert(\"can't find element \" + id)\n          return(1);\n        }\n        highlightPoint(el, status);\n      }\n    }\n }\nhighlight and highlightPoint now appear in the <head> of the HTML document rather than in the SVG. We can programmatically add them to the HTML document in much the same way we did for the SVG document using addECMAScripts() .\nNote that the SVG document created for this example had no JavaScript in it and no calls to JavaScript functions. The interactive functionality relies on the plot elements in the SVG having unique identifiers. In general, provided the plot elements have appropriate ids, \u201cplain\u201d SVG can be made interactive via JavaScript embedded in HTML rather than in the SVG. The advantage of this approach is that plots made for some general purpose can be made to have interactive capabilities.\nAdding interactivity to SVG embedded in HTML\nThere is yet another approach to providing interactivity for SVG documents that are embedded within HTML. The idea is that we use JavaScript code in the HTML document to programmatically modify elements in the SVG document at the time it is loaded in the browser. This approach can be useful when the plots have been produced in SVG but the creator has no motivation to add any interactivity. If the JavaScript code can identify the appropriate elements in the SVG document, either by contextual knowledge or using known id attribute values, it can add onmouseover, onmouseout and onclick event attributes to these elements and so dynamically (i.e., at viewing time) add the interactivity.\nAs an example of this approach, we consider the case where we place mouse capabilities on a map so that a mouse click triggers the display of an HTML table.\nExample\u00a014.\u00a0 Communication between SVG and an HTML page\nIn this example, we create an HTML page that displays the canonical red-blue map of the United States, where the states are colored red or blue according to whether the majority of votes in the 2008 presidential election were Republican or Democrat, respectively. The viewer interacts with the map by clicking on a state in the map, and as a result, summary statistics for the selected state are loaded into the web page.\nWe layout the HTML page using nested <div> tags as follows\n<body onload=\"loaded()\">\n<center>\n<h3>Presidential Election 2008</h3>\n\n<p>\nThe map is interactive in that you can click on\nanywhere within a state to see more detailed\nresults about the voting in that state.\n</p>\n</center>\n\n<div id=\"main\">\n <div id=\"summaryMap\">\n    <div id=\"stateSummary\"></div>\n    <div id=\"map\">\n       <object id=\"svgMap\" data=\"stateMap.svg\" \n          type=\"image/svg+xml\" width=\"700\" height=\"700\"> \n       </object>\n    </div>\n </div>\n\n <div id=\"countySummary\">\n  <a id=\"toggleCounty\" \n     onclick=\"toggleCountyView()\">+ Click to see county results</a>\n  <div id=\"countySummaryContent\" class=\"hidden\">\n     Click on a state to see more detailed information about the\n     election results for that state.\n  </div>\n </div>\n</div>\nEach of these <div>s has an associated style which we place in the <head>.\nThe SVG map is embedded in the <div> that has an id of \u201cmap\u201d. The plot was created with R using the maps package. We add id attributes to the polygon elements that correspond to the polygon name used in map() . (See Example\u00a03, \u201cAdding hyperlinks to polygons\u201d for an example of how to do this.) Although we place unique identifiers on each polygon, we do not add any JavaScript interactivity in the annotation stage of the plot creation. Recall that the first stage is when we create the graphical display in R; the second stage, the annotation stage, is when we use R to modify the resulting SVG document to add tags, attributes, and JavaScript to support interactivity; and the third stage is the viewing stage, when we are in the browser, R is no longer available, and JavaScript functions respond to events. In this example, we use JavaScript that is located in the HTML page to annotate the SVG with onclick attributes on each of the polygon elements. This annotation occurs at the time the page is loaded into the browser. We do this with the following JavaScript code:\n<script type=\"text/javascript\">\nfunction loaded() {\n    show(\"national\");\n    \n    var doc;\n    doc = document.getElementById('svgMap');\n    doc = doc.getSVGDocument();\n\n    for(var i = 0; i < polyIds.length ; i++) {\n        var el = doc.getElementById(polyIds[i]);\n        var txt = \"parent.show('\" + polyStates[i] + \"')\";\n        el.setAttribute(\"onclick\", txt);\n    }\n}\n</script>\nThe mouse clicks occur on elements in the SVG document, and since the JavaScript to handle these events are in the HTML document, not the SVG, we need to call the show method of the parent document using parent.show. Notice that the mouse click function call passes the state name, not the polygon name. The JavaScript variables polyIds and polyStates contain the unique names of the polygons in the map and the names of the states that the polygons belong to, respectively. (Recall that there are 63 polygons in the map because some states are drawn with multiple polygons, e.g., Manhattan in New York).\nThe show function appears in a <script> tag in the <head> of the HTML file. We also place the JavaScript variables polyIds and polyStates there. show is defined as\n<script type=\"text/javascript\">\nfunction show(stateName)\n{\n  var val;\n  var div;\n  val = stateSummaryTables[stateName];\n  div = document.getElementById(\"stateSummary\");\n  div.innerHTML = val;\n\n  val = countyTables[stateName];\n  if(val) {\n    div = document.getElementById(\"countySummaryContent\");\n    div.innerHTML = val;\n  }\n}\n</script>\nWhen show is called, it retrieves a reference to the embedded element in the HTML document named \u201cstateSummary\u201d, which is where it will place the table of statistics for the selected state. We modify the contents of this <div> element, adding the new HTML table. In addition, the county level information is retrieved and placed in the <div> called \u201ccountySummaryContent\u201d. Finally, additional JavaScript variables are loaded into the document with\n<script type=\"text/javascript\" src=\"stateHTMLTables.js\"></script>\nThese variables were created in R and contain the HTML table content for the states and counties information.\nThe main difference between the approach presented here and that found in the earlier examples is that some of the post-plotting annotations have been lifted into the viewing stage. One result of this approach is that the code that responds to user interaction will be more indirect as it is no longer within the SVG document. Also, the post-processing occurs outside of the R environment, at the time the document is loaded. Thus, we can take SVG documents not made for interactivity and modify them with JavaScript. A downside to this approach is that the interactivity may require access from the browser to the data and statistical routines used to generate the plot.\nRelated approaches\nThe SVGAnnotation package allows R users to leverage the sophisticated and flexible static graphics frameworks in R to create views of data and models and then display them in new and interesting ways in new emerging media. With SVGAnnotation , we can add interactivity and animation to the displays. The mechanism relies on post-processing the output from the R graphics engine and associating elements within the SVG output with the high-level components of the display. This is entirely deterministic based on the nature of the R plot, but is slightly different for each type of plot since there is no simple format or model for representing all plots. The purpose of SVGAnnotation is to find the SVG elements corresponding to the high-level graphical elements and allow the R user to easily augment these.\nThe package provides high-level facilities for \u201cstandard\u201d plots in which we can readily identify the R elements. It also allows an R programmer to use intermediate-level facilities to operate on axes, legends, etc. There are also low-level facilities for identifying the shape of visual elements, e.g., polygon, line, vertical line, text. The functions in the package can also add high-level type identifiers to nodes in the SVG document such as identifying data points, axes, labels, titles, frames. These facilities allow us to handle cases from regular R graphics, lattice [ lattice ], ggplot2 [ ggplot ] and grid [ grid ]. By leveraging the XML facilities in R, SVGAnnotation offers capabilities for creating rich new plots. Most importantly, the approach and the concept it implements is readily extended to other contexts and types of plots. The abstract idea and approach is the main contribution of the paper and package.\nOur approach in the SVGAnnotation package is to use the high-quality rendering engine provided by libcairo from within R. There are two other SVG graphics devices available for use within R. These are found in the RSvgDevice [ RSvgDevice ] and the derived RSVGTipsDevice [ RSVGTipsDevice ] packages. The former generates an SVG document that contains SVG elements corresponding to the graphical primitives the R graphics engine emits, e.g., lines, rectangles, circles, text. However, the libcairo system is more widely used and more robust. It also deals with text in more advanced and sophisticated ways (specifically drawing letters as infinitely scalabe shapes/paths rather than using font sets). This is the primary reason we use the libcairo approach even though the format of the SVG documents that RSvgDevice produces is simpler and more direct.\nThe RSVGTipsDevice package builds on the code from RSvgDevice . It allows R programmers to add SVG annotations and does so when the SVG is being created, rather than via post-processing additions. R users can set text for tooltips and URLs for hyperlinks that are applied to the next graphical primitive that the R graphics engine emits. This works well when the R programmer is creating directly in their own code all graphical elements of a display. However, it does not work when calling existing plotting functions that create many graphical elements. The computational model does not provide the appropriate resolution for associating an annotation with a particular element, but just the next one that will be created. As a result, we cannot annotate just the vertical axis label when calling a function that creates an entire plot.\nOur post-processing approach is not limited to using libcairo. We could also use RSVGTipsDevice to generate the SVG content and then programmatically manipulate that. The documents generated by the two different devices will be somewhat different (e.g., the use of groups of characters for strings, in-line CSS styles versus separate classes, SVG primitives for circles rather than paths). However, because the elements of the graphical display were generated by the R graphics engine with the same R expressions, the order of the primitive elements and the general structure will be very similar.\nThe package gridSVG [ gridSVG ] is an earlier and quite different approach to taking advantage of SVG. As the name suggests, it is focused on R's grid graphics system. For this reason, it cannot work with displays created with the traditional and original graphics model, but it does handle anything based on grid such as all of the plot types in lattice and ggplot2 . The package gridSVG takes advantage of the structured self-describing information contained in grid's graphical objects. As one creates the grid display, the information about the locations and shapes are stored as objects in R. These can then be translated to SVG without using the R graphics device system. New graphics primitives have been added to the grid system to add hyperlinks, animations, etc. corresponding to the facilities provided in SVG.\nThe approach provided by gridSVG is quite rich. It removes the need to post-process the graphics that we presented here. Instead, one proactively and explicitly specifies graphical concepts. One limitation that is similar to that of RSVGTipsDevice is that we still run into problems with interleaving SVG annotations. While a user can issue grid commands that add SVG facilities to the output, higher-level functions that create plots produce entire sequences/hierarchies of grid objects in a single operation. If these do not add the desired annotations, we have to post-process the grid hierarchy to add them. This post-processing would be very similar to what we are doing in SVGAnnotation , however it would be done on R objects.\nOf course, gridSVG is restricted to the grid graphics system, and higher-level graphics facilities based on grid such as ggplot2 and lattice . Our approach however works with any R graphical output, although it needs to be \u201ctrained\u201d to understand the content. The combination of the two approaches: gridSVG and SVGAnnotation appear to give a great deal of flexibility that allows both pre-processing and post-processing. If all graphics were grid-based, gridSVG may well be the most appropriate approach. Since many widely used graphics functionality in R are based on the traditional graphics system, SVGAnnotation is necessary.\nThe animation package [ animation ] provides functionality to create movies (e.g., animated GIF files or MPEG movies) entirely within R, using some additional external software. The idea is that one draws a sequence of plots in R. These are combined and then displayed at regular intervals as frames of the movie to give the appearance of motion. R users can draw each frame using R code and without regard for particular graphics device or formats. This simplifies the process for many users. The approach does, however, involve redrawing each frame rather than animating individual objects. It also provides no interactive facilities within the plot. It involves a different programming model where we redraw entire displays rather than working on individual graphical elements. In different circumstances, each model has advantages. So while animated displays are common to both the animation and SVGAnnotation packages, the goals and infrastructure are very different. Being able to work at the level of graphical objects within the plot is richer and more efficient for many applications.\nFinally, the imagemap package [ imagemap ] offers another approach to creating interactivity within R graphical displays. The basic concept is that a plot is created in R in the usual manner. Then, the creator specifies different regions within the plot that correspond to areas of interest to viewers, e.g., the axes labels, points in a scatter plot, bars in a histogram. These regions are specified using the coordinate system of the data. These regions define an image map that can be displayed within an HTML document. The creator specifies actions in the form of JavaScript code that are triggered as the viewer moves the mouse over the different regions or clicks within a region. This approach does not map the elements of the plot, such as a title, hexagonal bin, or state polygon, to mouse events. It instead creates an overlay for the image from separately identified regions. In addition, the elements in the image cannot be programmatically changed at viewing time. For example, we cannot change the color of a line, or the size of a point or rectangle in response to a viewer's actions. We also cannot move elements to achieve animation effects.\nFuture directions\nThe motivation behind the SVGAnnotation package is the ability to exploit and harness new media such as interactive Web pages. The aim is to enable statisticians to readily create new graphical displays of data and models using existing tools that can be displayed in rich, interactive and animated venues such as Web browsers, Google Earth, Google Maps, GIS applications. The SVGAnnotation package focuses on facilitating R users in leveraging existing R graphical functionality by post-processing the output to make it interactive and/or animated. It is a complete computational model in that it enables the author of a display (or a third-party) to modify all elements in that display. This approach can be used for other modern formats.\nAnother format for graphical displays and applications is Adobe's Flash & Flex ( http://www.adobe.com/products/flash ). This is an alternative to SVG that is a very widely used framework (the ActionScript programming language, collection of run-time libraries, and compiler suite) for creating interactive, animated general interfaces and displays. The range of applications include rich graphical user interfaces and interactive, animated business charts. Flash and Flex are freely available, but not Open Source. The format and tools are mostly controlled by Adobe. The Adobe chart libraries for Flash are only available in commercial distributions.\nThere are publicly available Open Source libraries for creating certain types of common plots, e.g flare http://flare.prefuse.org . Alternatively, we can use R to create statistical graphical displays by generating ActionScript code to render the different elements in the display. We have developed a prototype R graphics device (in the FlashMXML package) that creates Flash plots within R using the regular graphics framework. We can then post-process the generated content in much the same way we do with the SVGAnnotation package in order to provide interaction and animation, connect the plot to GUI components, etc.,\nRather than considering Flash and JavaScript as competitors to SVG, we think that each has its own strengths. SVG is more straightforward and direct than Flash and JavaScript for creating graphical displays. For one, we have an excellent R graphics device that creates the initial SVG content. We can add GUI components, but Flash is better for this as it provides a much richer collection of GUI components such as a data grid. Drawing displays with JavaScript avoids depending on the presence of support for either SVG or Flash and so can be deployed in more environments.\nAnother approach is to use the HTML5 canvas element that has been recently introduced in several browsers. We can create JavaScript objects for drawing, e.g., circles, lines, text on a canvas. Again, we have developed a prototype R graphics device that generates such code for an R plot. We can also annotate this to add animation and interaction. We also mention the Vector Markup Language (VML) ( http://msdn.microsoft.com/en-us/library/bb263898(VS.85).aspx ) which is quite similar to SVG. It is used within Microsoft products such as Word, Excel and PowerPoint. However, it is not widely supported by other applications.\nA fundamental aspect of what we have described with respect to the SVGAnnotation package is that R is used to create the display but is not available at viewing time when the SVG document is rendered. If R were available for the SVG viewer, then the JavaScript/ECMAScript code within an SVG or HTML document could make use of R at run-time. It could invoke R functions and access data to update the display in response to user interaction and animation. Additionally, we could use R code to manipulate the SVG and HTML content and so enable programming in both ECMAScript and R. One approach is to have R available on the server side of a client-server setup via, e.g., RApache . Alternatively, R could be plugged into the HTML viewer and perform R computations on the client side. We are developing such an extension for Firefox which embeds R within a user's browser. This modernizes previous work in 2000 on the SNetscape package that embedded R and R graphics devices within the Netscape browser. We believe the combination of SVG (or Flash or the HTML5 canvas) with R at viewing-time will prove to be a very rich and flexible environment for creating new types of dynamic, interactive and animated graphics and also allow Google Earth and Google Maps displays to be combined with R plots and computations.\nIn conclusion, the increasing importance of Web based presentations is a space where statisticians need to be engaged. To accomplish this, we need more tools for creating these presentations. SVGAnnotation offers one approach; we are working on others. We ask that the reader think of the package as a rich starting point that enables a new mode of displaying R graphics in an interactive, dynamic manner on the Web. It establishes a foundation on which we and others can build even higher-level facilities for annotating SVG content and providing rich plots in several different media (i.e., SVG, HTML, JavaScript).\nAcknowledgements. We thank the referees for their detailed comments, which led to an improved version of this article. We also thank Gabe Becker for assistance with the examples. This material is based in part upon work supported by the National Science Foundation under Grant Number DUE-0618865.\nBibliography\n[1] SVG Essentials. J David Eisenberg. O'Reilly Media, Inc. Sebastopol CA. 2002.\n[2] R: A Language and Environment for Statistical Computing. R Development Core Team. 2009. R Foundation for Statistical Computing. ViennaAustria. http://www.r-project.org\n[3] Gapminder: World. Hans Rosling. 2008. http://www.gapminder.org/world\n[4] JavaScript: The Definitive Guide. David Flanagan. O'Reilly Media, Inc. Sebastopol CA. 2006.\n[5] HTML and XHTML: The Definitive Guide. Bill Kennedy and Chuck Musciano. O'Reilly Media, Inc. Sebastopol CA. 2006.\n[6] XML in a Nutshell. Elliotte Rusty Harold and W. Scott Means. O'Reilly Media, Inc. Sebastopol CA. 2004.\n[7] XPath and XPointer. Locating Content in XML Documents. John E Simpson. O'Reilly Media, Inc. Sebastopol CA. 2002.\n[8] CSS Pocket Reference. Eric A Meyer. O'Reilly Media, Inc. Sebastopol CA. 2004.\n[9] The XML C parser and toolkit of Gnome. Daniel Veillard. http://www.xmlsoft.org\n[10] Carto:Net Software. Alex Berger, Alex Pucher, Alexandra Medwedeff, Andreas Neumann, Andre Winter, Christian Furpass, Christian Resch, Florent Chuffart, Florian Jurgeit, Georg Held, Greg Sepesi, Iris Fibinger, Klaus Forster, Martin Galanda, Nedjo Rogers, Nicole Ueberschar, Peter Sykora, Sudhir Kumar Reddy Maddirala, Thomas Mailander, Til Voswinckel, Tobias Bruehlmeier, Torsten Ullrich, and Yvonne Barth. http://www.carto.net 2010.\n[11] GGobi Software, Version 2.1. Debby Swayne, Dianne Cook, Duncan Temple Lang, and Andreas Buja. http://www.ggobi.org 2010.\n[12] Open XML. The Markup Explained. Wouter van Vugt. http://openxmldeveloper.org/attachment/1970.ashx 2007.\n[13] gridSVG : Export grid graphics as SVG. R package, version 0.5. Paul Murrell. http://www.stat.auckland.ac.nz/~paul/R/gridSVG/gridSVG_0.5-10.tar.gz 2010.\n[14] gridBase : Integration of base and grid graphics. R package, version 0.4. Paul Murrell. http://www.stat.auckland.ac.nz/~paul/grid/grid.html 2006.\n[15] ggplot2 : An implementation of the Grammar of Graphics. R package, version 0.8. Hadley Wickham. http://cran.r-project.org/web/packages/ggplot2/ 2010.\n[16] Rgraphviz : Provides plotting capabilities for R graph objects. R package, version 1.30. Jeff Gentry, Li Long, Robert Gentleman, Seth Falcon, Florian Hahne, Deepayan Sarkar, and Kaspar Hansen. http://www.bioconductor.org/packages/2.6/bioc/html/Rgraphviz.html 2011.\n[17] How To Plot A Graph Using Rgraphviz. Jeff Gentry, Robert Gentleman, and Wolfgang Huber. http://bioconductor.org/packages/2.6/bioc/vignettes/Rgraphviz/inst/doc/Rgraphviz.pdf 2010.\n[18] RJSONIO : Serialize R objects to JSON, JavaScript Object Notation. R package, version 0.95. Duncan Temple Lang. http://www.omegahat.org/RJSONIO 2011.\n[19] XML : Tools for parsing and generating XML within R and S-PLUS. R package, version 3.4. Duncan Temple Lang. http://www.omegahat.org/RSXML 2011.\n[20] SVGAnnotation : Tools for post-processing SVG plots created in R. R package, version 0.9. Deborah Nolan and Duncan Temple Lang. http://www.omegahat.org/SVGAnnotation 2011.\n[21] Cairo Software, Version 1.1. Cairo Graphics. . http://www.cairographics.org 2010.\n[22] Flash Player Software, Version 10.3. Adobe. . http://get.adobe.com/flashplayer/ 2011.\n[23] maps: Draw Geographical Maps . R package, version 2.1. Richard Becker, Allan Wilks, Ray Brownrigg, and Thomas Minka. 2011. http://cran.r-project.org/maps/\n[24] lattice : Lattice Graphics. R package, version 0.19. Deepayan Sarkar. http://cran.r-project.org/lattice/ 2011.\n[25] hexbin : Hexagonal Binning Routines. R package, version 1.22. Dan Carr, Nicholas Lewin-Koh, and Martin Maechler. http://www.bioconductor.org/packages/2.6/bioc/html/hexbin.html 2009.\n[26] Opera Software, Version 11.51. Opera. . http://www.opera.com/browser/ 2011.\n[27] Batik: Java SVG Toolkit, Version 1.7. Apache Software Foundation. . http://xmlgraphics.apache.org/batik/ 2008.\n[28] JavaScript Tutorial. W3 schools. http://www.w3schools.com/JS/default.asp\n[29] XML Tutorial. W3 schools. http://www.w3schools.com/xml/default.asp\n[30] SVG Tutorial. W3 schools. http://www.w3schools.com/svg/default.asp\n[31] XPath Tutorial. W3schools. http://www.w3schools.com/XPath/default.asp\n[32] Programming Flex 3: The Comprehensive Guide to Creating Rich Internet Applications with Adobe Flex . Chafic Kazoun and Joey Lott. O'Reilly Media, Inc. Sebastopol CA. 2008.\n[33] XSL FO: Making XML Look Good in Print. Dave Pawson. O'Reilly Media, Inc. Sebastopol CA. 2002.\n[34] Interactive Data Visualization Using Mondrian. Martin Theus. 2002. http://www.jstatsoft.org/v07/i11/ Journal of Statistical Software. 7. 11.\n[35] RSvgDevice : An R SVG graphics device. R package, version 0.6. Jake Luciani. 2009. http://cran.r-project.org/web/packages/RSvgDevice/\n[36] RSVGTipsDevice : An R SVG graphics device with dynamic tips and hyperlink. Tony Plate. R package, version 1.0. http://cran.r-project.org/web/packages/RSVGTipsDevice/ 2011.\n[37] animation : A Gallery of Animations in Statistics and Utilities to Create Animations. R package, version 2.0. Yihui Xie. http://cran.r-project.org/web/packages/animation/ 2011.\n[38] imagemap : Create HTML imagemaps. R package, version 0.9. Barry Rowlingson. http://www.maths.lancs.ac.uk/Software/Imagemap/ 2004.\n[39] iPlots : iPlots - interactive graphics for R. R package, version 1.1. Simon Urbanek and Tobias Wichtrey. http://cran.r-project.org/web/packages/iplots/ 2011.\n[40] Lattice: Multivariate Data Visualization with R. Deepayan Sarkar. Springer-Verlag. New York . 2008. http://lmdvr.r-forge.r-project.org/figures/figures.html\nA.\u00a0Functions in SVGAnnotation\nThere are approximately 60 functions available in SVGAnnotation for annotating plots. Among these are high-level functions that add interactivity through a single function call. These functions are listed in Table\u00a0A.1, \u201c High-level functions for adding interactivity to SVG plots \u201d . Other intermediate level functions are described in Table\u00a0A.2, \u201c Intermediate-level functions for working with SVG elements \u201d . These roughly fall into two types of functions: those that find nodes in the SVG that correspond to particular parts of a plot e.g., points, legend, panel; and those that annotate or add a node. The functions that locate the nodes begin with the prefix \"get\" and those that annotate a node or add a node begin with \"add\".\nWe recommend creating the SVG plot with the function svgPlot() in SVGAnnotation . It opens the SVG device, evaluates the commands to generate the plot, and closes the device. It also adds the R plotting commands to the SVG document and can either return the parsed XML document as a tree-like structure or write it to a file.\nTable\u00a0A.1.\u00a0 High-level functions for adding interactivity to SVG plots\nFunction\nBrief Description\naddAxesLinks()\nAssociates target URLs with axes labels and plot titles so that a mouse click on an axis label or title jumps to the specified URL in the viewer's Web browser.\naddToolTips()\nAssociates text with elements of the SVG document so that the text can be viewed in a tool tip when the mouse is placed over that object/element in the display. The default action of this function is to add tool tips to points in a scatter plot.\nlinkPlots()\nImplements a simple linking mechanism of points within an R plot consisting of multiple sub-panels/plots. When the mouse is moved over a point in a sub-plot, the color of all corresponding points in other plots/panels also changes. The sub-plots can be created by, for example, arranging plots via the mfrow parameter in par() , or via the functions pairs() or splom() in the lattice package.\nanimate()\nCreates an animated scatter plot where points in the scatter plot move at different points in time, possibly changing size and color as they move.\nTable\u00a0A.2.\u00a0 Intermediate-level functions for working with SVG elements\nFunction\nBrief Description\naddLink()\nAssociates target URLs with any SVG element so that a mouse click on an axis label, title, point or some other plot component displays the specified URL in the viewer's Web browser. Set the addArea parameter to TRUE if you want the link to be associated with a bounding rectangle around the SVG element and not just the pixels along the path of the SVG element.\naddECMAScripts() and addCSS()\nAdd JavaScript/ECMAScript and CSS code to an SVG document. These function either directly insert the content or put a reference (href attribute) to the file/URL in the document.\naddSlider()\nAdd an interactive slider to the SVG document. Uses the Carto.Net GUIlibrary.\nradioShowHide()\nAdd radio buttons to the SVG document. Uses the Carto.Net GUI library.\ngetPlotPoints()\nExamines the SVG document and returns the SVG elements/nodes which represent the \u201cpoints\u201d within the R graphic. These may be in multiple plots or panels, and they may be, e.g., the polygonal regions in a map or hexagons in a hexbin plot.\ngetAxesLabelNodes()\nExamines the SVG document and returns the SVG elements/nodes which represent the text of the main title and the X and Y axes for each sub-plot within the R graphic.\ngetPlotRegion()\nRetrieve the SVG elements which correspond to the plotting regions, i.e., the high-level frames or panels, within the display. This returns multiple elements as appropriate for traditional and lattice plots.\ngetPlotRegionNodes()\nRetrieve the SVG elements which house the contents of the data regions of the sub-plots within the display. This works for traditional and lattice plots, and also for histograms and bar plots.\ngetStyle() , setStyle() , modifyStyle()\nThese functions query, set, and reorganize the value of a style attribute of an SVG node, respectively. Use these functions to determine and set the vector of CSS-based style settings.\ngetLatticeLegendNodes()\nRetrieve the legend in a lattice plot.\nenlargeSVGViewBox()\nChange the dimensions of the viewing box so that extra elements can be added to the document and displayed within its viewing area, e.g., a slider below a plot or check boxes alongside a plot.\nconvertCSSStylesToSVG()\nConverts a CSS style attribute into presentation attributes. The purpose is to make it easier to change a single attribute in response to a user event. See the section called \u201cThe SVG grammar\u201d for a description of these types of style specifications.\nasTextNode()\nReplaces a <g> element (that contains directions for drawing text with <path> tags) with a <text> node. This makes it easier to modify text in a display. Although it does not have the benefit of scalability and the variety of fonts, it is much simpler for creating interactivity. (See the section called \u201cThe SVG grammar\u201d for more details on the text created by R's graphics system(s) and libcairo).\nThere are many more \"get\" functions available in SVGAnnotation . These can make it easier for the developer to build other high-level functions for annotation. Here is a list of all the get functions currently in the package:\nobjects(2)[substr(objects(2), 1, 3) == \"get\"]\n[1] \"getAxesLabelNodes\"            \"getAxesLabelNodes.mosaic\"    \n [3] \"getBoundingBox\"               \"getCategoryLabelNodes.mosaic\"\n [5] \"getCSS\"                       \"getECMAScript\"               \n [7] \"getEdgeElements\"              \"getEdgeInfo\"                 \n [9] \"getGeneralPath\"               \"getJavaScript\"               \n[11] \"getLatticeLegendNodes\"        \"getLatticeObject\"            \n[13] \"getMatplotSeries\"             \"getNodeElements\"             \n[15] \"getPanelCoordinates\"          \"getPanelDataNodes\"           \n[17] \"getPlotPoints\"                \"getPlotRegion\"               \n[19] \"getPlotRegionNodes\"           \"getRCommand\"                 \n[21] \"getRect\"                      \"getShape\"                    \n[23] \"getStripNodes\"                \"getStyle\"                    \n[25] \"getSVGNodeTypes\"              \"getTextPoints\"               \n[27] \"getTopContainer\"              \"getTopG\"                     \n[29] \"getUSR\"                       \"getViewBox\"\nB.\u00a0Basics of XML\nThe basic unit in an XML document is an element, also known as a node or chunk. An element can contain textual content and/or additional XML elements. By content, we mean the simple text, such as the word \u201cOregon\u201d that appears in the simple SVG document shown in Figure\u00a09, \u201cSample SVG\u201d . An element begins with a start-tag, which has the format <tagname>, and the element ends with </tagname>. The end-tag, where the tag name matches the start tag's name. Those readers who have read or composed HTML (HyperText Markup Language) will recognize this format. For more in-depth information about XML see [ XMLNutshell , XMLW3 ].\nSVG, like HTML or WordProcessingML, is an example of a specific grammar of XML. With SVG, the focus is on describing graphical displays. The SVG document rendered in Figure\u00a09, \u201cSample SVG\u201d includes element/tag names such as <circle> and <rect> for drawing circles and rectangles respectively. Notice that some of the elements in the sample SVG document are nested within other start and end tags. This hierarchical structure gives us great flexibility in describing complex, nested data with arbitrary depth.\nFor more details about the specific tags and how to create SVG graphics, see the section called \u201cThe SVG grammar\u201d .\nAn XML element can have attributes associated with it. These are supplied in the tag itself as name-value pairs as follows: <tagname attributeName=\"value\">. For example, the <text> tag,\n<text x = \"110\" y = \"200\" fill = \"navy\" font-size = \"15\">\n        Oregon\n </text>\nhas an attribute, x, which specifies the horizontal position of the text on the SVG canvas. The value for x in this example is \"110\", indicating position 110 along the x-axis of the canvas (which in our example is 300 by 300 points). The syntax rules for elements and their tags are provided below in the section called \u201c Well-formed XML\u201d .\nWe should note one important short cut for start and end tags. If an XML element contains no text content or elements within it, i.e., there are no text or sub-elements contained between the start and end tags of the element, then it can be written as <tagname/> rather than the longer <tagname ></tagname>. For example, notice that the <rect> element:\n<rect x = \"10\" y = \"20\" width = \"50\" height = \"100\"  \n      class = \"recs\"/>\nis empty. All of the relevant information for drawing the rectangle is contained in the attributes, i.e., its width, height, location on the canvas, and color. Hence the end tag is omitted and the start tag also acts as a closing tag.\nWell-formed XML\nFor XML to be properly processed it must obey some basic syntax rules, and we say the document is well-formed when it satisfies these rules. The following list is a subset of the most important syntax rules. This list covers the vast majority of XML documents. Notice that they are very general and do not pertain to a specific grammar of XML. XML documents that are not well-formed typically produce fatal errors when processed.\nAn XML document must have a single root element that completely contains all other elements. In our example, <svg> is the root element.\nXML tags are case sensitive so start and end tag names must match exactly. For example,\n<text> \nOregon  \n</Text>\nis not well-formed because the start-tag begins with a lower-case \u201ct\u201d that does not match the capital \u201cT\u201d in the end-tag.\nAll start tags must have a closing tag, unless the tag is empty and so can be contracted to a single tag of the form <tagname />.\nElements must nest properly. That is, when one element contains another element then both the start and end tags of the inner element must be between the start and end tags of the outer element. For example, the following XML content is three nodes deep:\n<defs>\n  <g id=\"circles\">\n     <circle id = \"pinkcirc\" cx= \"50\" cy=\"50\"  r = \"15\" \n         fill = \"pink\"/>\n </g>\n</defs>\nNote the use of indentation is optional, but it makes it easier to see the nesting of elements. (The white space is part of the XML document and is typically preserved during processing.)\nAll attribute values must appear in quotes in a name=\"value\" format in the start tag. For example the following <g> element has a value of main for the identifier attribute id: <g id = \"main\"> It is common to omit the quotes around attributes within HTML documents, but this is an error for XML.\nTree structure\nThe conceptual model of the XML document as a tree can be very helpful when processing and navigating it. The SVG shown in the section called \u201cThe SVG grammar\u201d uses indentation to make the nesting of elements clear, and demonstrates that the elements are logically structured into a hierarchical tree. Each element can be thought of as a node in the tree where branches emanate from the node to those elements that it immediately contains. Figure\u00a0B.1, \u201cSample SVG document tree\u201d shows the tree representation of this SVG document.\nFigure\u00a0B.1.\u00a0Sample SVG document tree\nThis tree provides a conceptual model for the organization of the SVG document rendered in Figure\u00a09, \u201cSample SVG\u201d . Each element is represented by a node in the tree and the branches from one node to another show the connections between the nodes that are one layer apart from each other in the nesting of nodes. For example, there are two <circle> nodes and both are nested directly as children of (i.e., within) the <g> node; the branch between these nodes indicates this connection. That particular <g> node is a child of the <defs> element. Text content nodes are actually elements but without a tag name and are included in the tree. They are displayed via rectangles rather than ovals to distinguish them from regular tags, e.g., \u201cOregon\u201d is a text node in this tree.\nThe root of the tree is also referred to as the document node, which in this case is the <svg> element. As mentioned earlier, there is only one root node per document. Notice that the root node of this tree has two children, a <defs> node and a <g> node. The <g> has six children - a <rect> node, two <use> nodes, and one each of <image>, <path> and <text>.\nThe relative position of these nodes in the tree are described using family tree terminology. For example, the <defs> element is the parent of a <g> node, and <circle> is the child of <g>. The <rect> element is a sibling to <image>, and <defs> and <svg> are both ancestors of each of the <circle> nodes. Also, note that <path> can be referred to as a \u201cfollowing sibling\u201d to <image> because it comes after (to the right in the image and below in the actual document) of <image>.\nThe character content of an element is placed in a \u201ctext\u201d node. That is, text content is also represented as a node in the tree. In our example, the text Oregon is a child node of <text>. The terminal nodes in a tree are those that have no children and are known as leaf nodes. By design, text content will always be in a leaf node.\nAdditional markup\nIn addition to elements, XML markup includes the XML declaration, processing instructions, comments, and CDATA section delimiters.\nXML declaration. An XML document must start with the XML declaration that identifies it as an XML document and provides the XML version number,\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\nThe declaration appears outside of the root element.\nProcessing instructions. Similar to the XML declaration, processing instructions must begin with <? and end with ?>. Immediately following the <? is the target of the instruction, i.e., the agent/application for which the instruction is intended. The following processing instruction is for an xml-stylesheet, which is a standard name that means the parameters in this processing instruction are intended for an XML viewer, e.g., a Web browser or XML editor, that can apply a style sheet to the document.\n<?xml-stylesheet type=\"text/css\"\n   href= '~/Rpackages/SVGAnnotation/CSS/RSVGPlot.css'?>\nNotice that the processing instructions for the xml-stylesheet are provided via name-value pairs in a format that imitates the syntax for attributes, e.g., type=\"text/css\". This format is optional, and other applications could expect the processing instructions to be in some other format. Different applications support different processing instructions. Most applications simply ignore any processing instruction whose target they don't recognize. (Technically, the XML declaration is not a processing instruction). The XML-Stylesheet processing instructions are always placed in the document prolog between the XML declaration and the root element start tag. Other processing instructions may also be placed in the prolog or at almost any other location in the XML document, i.e., within sub-nodes.\nComments. A comment must appear between <!-- and -->, and it can contain < and > because all text between these two delimiters is ignored by the XML processor and so is neither rendered nor read. For example,\n<!-- This is a comment which is so long that \nit appears on three lines of\nthe document before it ends with -- followed by >. \n-->\nComments can appear anywhere after the XML declaration, e.g., they can appear outside the root element of the document.\nEntities. Because XML uses the < character to start or end an XML element, we need an alternative way to include the literal character < as text within an XML node. We use XML entities for this. An entity is a named symbol within XML that corresponds to some fixed content. We refer to them by prefixing the name of the entity with & and adding the suffix ;. There are numerous built-in entities to which we can refer, e.g., the < character can be written as &lt; and > is expressed as &gt; and & as &amp;.\nCDATA. Entities are convenient for escaping content from XML processing, especially individual characters. There are occasions, however, where we have a significant amount of content that we want to escape from XML processing and to be treated as literal or verbatim content. For example, when we insert the R code used to generate an SVG plot into the SVG document, we want to avoid having to worry about characters an XML processor will interpret. We do this by enclosing the content between between <![CDATA[ and ] ]> markers. For example,\n<![CDATA[ \n plot(y ~ x, myData[ (myData$var1 < 10 | myData$var1 > 20) & \n                       myData$var2 %in% c(\"A\", \"B\", \"&\")])\n ]]>\nThis is then treated as a verbatim block of characters and not parsed by the XML processor for XML elements.\nC.\u00a0Basics of JavaScript\nWe provide here a brief introduction to JavaScript for programmers who are familiar with the S language. JavaScript (the official name is ECMAScript) is an interpreted scripting language that is widely used within HTML documents and also within SVG displays and Flash applications (using a slight variant named ActionScript). JavaScript can be used within HTML to respond to user actions on buttons, menus, checkboxes, etc. in HTML forms, or to dynamically validate the content of a form before submitting it to a remote server. JavaScript can also be used to dynamically and programmatically construct the content of an HTML document. Within SVG, we use JavaScript to respond to user events on elements of the display (e.g., circles, lines, rectangles, ...) and to provide animation. Similar to HTML, JavaScript can be used to dynamically create SVG elements within the display.\nAs with R, one does not need to compile JavaScript code since it is interpreted. However, JavaScript is more like C++, Python and Java in its computational model. Much of the use of the language will focus on classes and instances (objects) of these classes. We frequently invoke an object's methods rather than call top-level functions, although, like R, we can have regular functions unattached to objects.\nJavaScript is not a vectorized language like R, i.e., it does not operate on vectors element-wise unless explicitly programmed to do so using loop constructs.\nJavaScript requires variables to be declared within the scope in which they are used. Unlike C++ or Java, one does not need to declare the type of a variable and a variable can take on values of different types during its lifetime.\nTo make use of JavaScript within an HTML or SVG document, we must connect that code with the document. We can do this by either inserting the code as content in the document or alternatively adding a reference to the file containing the JavaScript code. That code file may be located locally or on a remote server, subject to certain security restrictions. Both approaches use the <script> node within the HTML or SVG document. We can insert the code content between the start and end tag of the <script> node. Alternatively, we can use an attribute in the <script> element to refer to the JavaScript file. For SVG, we use an attribute named href and for HTML, we use src. The value in both cases is a local file name or a URL. The following illustrates the mechanism for SVG:\n<script xmlns:xlink=\"http://www.w3.org/1999/xlink\" \n          xmlns=\"http://www.w3.org/2000/svg\"\n          type=\"text/ecmascript\" \n          xlink:href=\"SVGAnnotation/tests/multiLegend.js\"/>\nFor HTML, we refer to a JavaScript file as\n<script type=\"text/javascript\" src=\"alert.js\"/>\nTo in-line the JavaScript content, we use\n<script type=\"text/ecmascript\">\n\nvar neighbors = [[0,  1, 31, 20,  3, 29], [2, 8, 9, 24]]; \nvar k = 4;\n\nfunction showNeighbors(evt, k, neighbors)\n{\n    var idx = 1 * evt.target.getAttribute('id');\n    window.status = \"Showing \" + idx;\n    addLines(evt.target, neighbors[idx], k);\n}\n</script>\nNote that we have to specify the type attribute but can use either JavaScript or ECMAScript in most HTML browsers and SVG viewers.\nOften, JavaScript code that defines functions that will be used in event handler attributes on HTML or SVG elements are placed in a script element near the top of the HTML or SVG document. For HTML, these function definitions and global variables often appear in the <head> element. JavaScript code can also appear within the <body> of an HTML document and is evaluated when it is processed and so can access previously created elements in the document.\nWe provide here a brief summary of many of the basic syntax features of the language. For more detailed information about JavaScript see [ JavaScriptGuide , JavaScriptW3 ]\nExecutable statements typically end with a semicolon. Although it is not strictly required, it is good practice to use the semicolon.\nLike R, curly braces group statements together into executable blocks and are used for defining the body of a function or a multi-expression if, while or for clause.\nVariables are declared within a specific scope via the var prefix, e.g.,\nvar global = 1;\nvar debug = false;\nfunction foo(N) {\n var i;\n for(i = 0; i < N; i++) {\n    ...\n }\n}\nThe declaration can assign an initial value to the variable as in the first two expressions immediately above. These are global variables; the variable i is local to the function calls for foo. Note that the values for boolean variables are true and false, not TRUE and FALSE as in R.\nJavaScript supports arrays, including multi-dimensional arrays which can be \u201cragged\u201d, i.e., the sub-arrays can have different length/dimensions. In the JavaScript code above, the variable neighbors contains two arrays, one of length 6 and the other of length 4. JavaScript uses 0-based indexing for arrays. For example, in the two-dimensional array neighbors, neighbors[0] returns the first element of the top-level array and this is itself an array of six integer values. The expression neighbors[1][0] returns the first element in the second array, and the value is the scalar 2.\nMulti-line comments are delimited by /* and */ and single line comments begin with //, e.g.,\n/*  This is a multi-line \n    comment.\n*/\n\n// This is a comment\nvar debug = false; // display debug info with alert().\nThe equal sign (=) is the assignment operator. In addition, x += y is equivalent to the assignment x = x + y, and -=, *=, and /= are similarly defined. Also, the operator ++ increments the referenced variable in-place by 1 and -- decrements by 1, i.e., ++x is equivalent to x = x + 1,\nAdding two character variables, pastes the two strings together. If a number and string are \u201cadded\u201d, the result will be a string, e.g., \"K is \" + k results in the character string \"K is 4\".\nComparison operators are the same as in R, e.g.,\nx == 1\nabc < 10 \nstr != \"this is a string\"\nThe logical operators are &&, ||, and !; they correspond to and, or, and not, respectively.\nThe control flow syntax is similar to that in R:\nif (x < 2) { \n  ... \n} else if( x > 2 && x < 10) {\n   ... \n} else \n   ...\nJavaScript provides an if-else construct for conditional evaluation. We cannot assign the value of an if-else statement to a variable as we can in R.\nx = if(y > 2) 3 else 10;\nis a syntax error and will terminate the processing of the JavaScript code. JavaScript does provide the ternary operator for these simple situations:\nvarname = (condition) ? value1 : value2\nJavaScript supports for and do...while loops. In the code below, the JavaScript within the curly braces will be executed N + 1 times, as i takes on the values 0, 1, ..., N.\nfor(var i = 0; i <= N ; i++) {\n    var target;\n    target = document.getElementById(neighbors[i]);\n    ids = ids + \" \" + neighbors[i];\n...\n}\nAgain, note that JavaScript is not vectorized, as R is.\nJavaScript code can be used in-line within <script> elements within a document or as the value of attributes of HTML or SVG elements, e.g., event handlers such as onclick, onmouseover, onmouseout. The JavaScript code is one or more JavaScript expressions separated by ';' or new lines.\nTypically these expressions call JavaScript functions. This is especially true of event handler code that invokes a function with specific arguments to respond to the event in a particular way. For example, the SVG attribute onmouseover = \"showNeighbors(evt, k, neighbors)\" results in a call to the JavaScript function showNeighbors when the mouse moves over the corresponding element in the SVG display (see Example\u00a07, \u201cInteractive nearest neighbors\u201d for more details). The call passes three arguments: the event object and the global variables k and neighbors.\nUnlike R, JavaScript functions are not defined and assigned to a variable. Instead, functions are defined using the keyword function as a prefix to the definition as in\nfunction functionName(var1, var2, ..., varN)\n{\n   body-code\n}\nLike R, the parameters correspond to local variables within each call to the function. Since (non-primitive) variables in JavaScript are passed by reference, changes to these objects are made to the objects in the calling frame and are accessible after the specific function call is completed, i.e., back in the calling frame.\nIn addition to the syntactic rules for JavaScript above, we spend a great deal of time focused on objects and their methods. JavaScript provides a large library of classes and we either explicitly create instances of these via the new operator, e.g., rx = new Regexp(\"^[ACGT]+$\") or work with existing objects provided to us. We operate on these objects via their methods. We invoke these methods as if they were functions belonging to the object, e.g., rx.exec(\"my string\") or node.removeChildren(). Objects also have properties or fields and we can query and set these, e.g., button.value = \"My Label\" and document.links.\nThere are many classes and each has many methods. This is what makes JavaScript useful, but also difficult to learn because you need to find the classes and methods of use for your task. There are a few classes and methods, however, that arise very commonly in JavaScript code for SVG and HTML documents. One of these is the Document that provides access to the contents of the document being displayed. Another is the general concept of a document element represented by the Node and Element classes and the more specific SVGElement and HTMLElement classes and their sub-classes (e.g., SVGCircleElement, HTMLHeadingElement). These classes allow us to not only query the contents of the document, but also to programmatically modify this content, adding new elements or changing the characteristics of existing elements.\nWhen JavaScript code is evaluated as part of an HTML or SVG document, there is an implicit global variable named document. This is the top-level Document object and we can refer to it without any declarations or additional code. Perhaps the most important method this object provides for our use is getElementById which searches the entire SVG/HTML tree and finds the element which has an id attribute with the specified value. Another method, getElementsByTagName, allows us to find SVG/HTML elements based on the name of the element/tag, e.g., \u201ch1\u201d or \u201cpath\u201d.\nOnce we have an element in the document, we can use its methods to operate on it. We can retrieve the values of the element's attributes via getAttribute, e.g., circle.getAttribute(\"r\"). Similarly, we can set the value of an existing or new attribute using setAttribute. We can query the child nodes of a Node/Element via its childNodes field, and the parent node via parentNode field. We can add nodes via the appendChild and insertBefore methods, and we can remove nodes via removeChild.\nDebugging and display messages\nAs with any programming language, the JavaScript code we write often contains bugs, especially when we are learning the language. Since the code is being run within a Web browser or specialized SVG viewer, it is being run asynchronously (i.e., when an event occurs such as the document is loaded or the viewer clicks on a shape in the plot) rather than explicitly by our direct commands. As a result, debugging JavaScript can seem somewhat difficult. There are several tools to aid us, however. It is essential to look at the error console within the Web browser, e.g., the error cosole is selected in Firefox via the menu Tools -> Error Console, and in Opera under Tools -> Advanced -> Error Console.\nWhile the \u201cprint\u201d approach to debugging is not generally a good one, it is a common approach in asynchronous JavaScript programming. The idea is that we display the values of variables or computations as we evaluate the code and display it for the viewer or programmer to see in order to understand how the code is behaving. We can display the message in the viewer's status bar or in a pop-up window. In addition to helping the programmer debug code, this same approach can be used to provide information to the viewer. The alert function is the primary one used to display information in a pop-up window. We construct the text of the message to display and pass this as the sole argument in the call to alert. The viewer must click on the \"Okay\" button to dismiss the window and continue with the calculations.\nIn an HTML browser, we can display text in the status bar by merely assigning a string containing the desired text to the variable status. For example, we can display the number of links in a document with\nstatus = \"# of links \" + document.links.length\nA more sophisticated approach to debugging is to use a browser extension such as Firebug ( http://getfirebug.com ) for Firefox, or Firebug Lite ( http://getfirebug.com/lite.html ) for various browsers (including Firefox). Firebug is an extension that the programmer has to install. Firebug lite is a JavaScript file that one can include by reference in an HTML document, e.g.,\n<script type='text/javascript' \n        src='http://getfirebug.com/releases/lite/1.2/\n                                      firebug-lite-compressed.js'>\n</script>\nThese provide a separate panel within the browser's tab and show many aspects of the JavaScript code and HTML document. We can write messages to the console part of this panel via\nconsole.log(\"a string\")\nand use this to track how code is running.\nWe have seen how we can display information to the viewer via either a pop-up window or the status bar. There are also facilities for getting feedback from the viewer via pop-up windows. The confirm function displays a message and allows the viewer to proceed or cancel an operation. For example,\nfunction doConfirm()\n{\nvar con = confirm(\"Do you really want to do this?\");\nif (con == true)\n  {  ...  }\nelse\n  { ... }\n}\nWe can also get information via HTML forms and other active elements of the document rather than via separate (pop-up) windows.\nFor more information on debugging and programming in JavaScript, see [ JavaScriptGuide ]\n"}], [{"score": 852.1408, "uuid": "31e96bbb-e5f1-5121-bdec-a279f2f11647", "index": "cw12", "trec_id": "clueweb12-0916wb-66-15188", "target_hostname": "home.badc.rl.ac.uk", "target_uri": "http://home.badc.rl.ac.uk/lawrence/blog/2010/10/13/the_choice_is_python", "page_rank": 1.1758532e-09, "spam_rank": 79, "title": "Bryan Lawrence : The choice is <em>python</em>", "snippet": "What <em>should</em> an organisation like ourselves, <em>or</em> even the Met Office <em>for</em> example, consider <em>for</em> their main &quot;toolset&quot; development language? Clearly on the table we have Matlab and <em>Python</em> (given the results above).", "explanation": null, "document": "The choice is python\nSummary\nThis longer piece summarises my thinking as to what language folks like ourselves should use to develop new data processing (including manipulation and visualisation) tools. The context is clearly that we have data producers and data consumers - who are not the same communities - and both of whom ideally would use the same toolset. As scientists they need to be able to unpick the internals and be sure they trust them, but they'll also be lazy; once trusted,tools need to be simultaneously easy and extensible. Ideally of course, one wants to develop toolsets that the community will start to own themselves, so that the ongoing maintenance and development doesn't become an unwelcome burden (even as we might invest ourselves in ongoing support, we want that support to be manageable, and even better, we might want collaborators to take some of that on too)! The bottom line is that I think there are two players: Python and Matlab with and R and IDL as also rans, and that for me, Python is the clear winner - especially since with the right kind of library structure, users can mix and match between R, Python and IDL.\nIntroduction\nFor nearly a decade now, the BADC has been mainly a Python shop, even as much of, but not all, the NERC climate community has been exploiting IDL. The motivation for that has been my contention that\nPython is easy to learn (particularly on one's own using a book supplemented by the web) - and that's important when we are mostly hiring scientists who we want to code, not software engineers to do science,\nThe Python syntax is conducive to writing \"easier to maintain\" code (although obviously it's possible to write obscure code in Python, the syntax, at least, promotes easier-to-read code).\nPython can be deployed at all levels: from interaction with the system, building workflow, scientific processing and visualisation, and for web services (both backend services and front end GUIs via tools like Django and Pylons). In principle that means staff should be more flexible in what they can do (both in terms of their day jobs and in backing up others) without learning a plethora of languages.\nOf course, one might make arguments like those about other languages, and folks do, but mostly I get arguments about two particular languages:\nIDL- which is obviously familiar to many (but far from all) of both our data suppliers and consumers, and\nJava - particularly given the Unidata toolsets, and because some of my software engineers complain about various (arcane) aspects of Python.\nWe'll get to the IDL arguments below, but w.r.t. Java: it's not really a contender, it's simply not suitable as a general purpose language in our environment. It's too verbose, it requires too much \"expertise\", and it's a nightmare to maintain. Some supporting arguments for that position are here (10 minute video) and here (interesting blog article).\nIn the remainder of this piece, I introduce some context: some results from a recent user survey at the BADC, a quick (and incomplete) survey of what is taught in a few UK university physics departments - with a few adhoc and non-attributable comments from someone involved with a much wider group of UK physics departments.\nI'll then report on a few experiences in the BADC, before summarising with my conclusions - which of course are both overtly subjective and come with considerable input bias.\nContext: User Surveys\n(This section is based on material collected and analysed by my colleague: Graham Parton.)\nWe surveyed our users and asked them about their proficiency with a variety of programming languages/packages: the basic results are depicted in this bar chart:\nThe results are from around 280 responses (Red means: geek level; orange: happy to use it; yellow: use it on and off; green: aware of, but not used lately: and blues : complete mystery or no response).\nIf we look at this, we see that\nThe common scripting languages (Perl and Python) are not that commonly used by our community (but active Python usage is more prevalent than Perl and we can ignore TCL/Tk).\nOf the high level programming languages (Fortran, java, C and friends), Fortran is the team leader (as you might expect for our community).\nThe big packages (Matlab, IDL, R) rank in that order (but note that R is more commonly used than python).\nGrADS has usage comparable to R and python, but Ferret isn't much in use in our community.\nExcel and MS friends are common (but so is the influenza, and neither can do big data processing tasks).\nIf we split all the responses into those from our \"target\" community (users who claimed to be atmospheric or climate related - roughly half of the total responses):\nwe find broadly similar results, except that IDL is marginally stronger than Matlab (at least as far as the usage goes - even if there is still more folk who are aware of Matlab). However, IDL still only hits half the audience!!!\nContext: University Undergraduate teaching\nObviously most of the folks who use our data do so in postgraduate or other environments, and at least for NCAS, most of those will have IDL in the vicinity, if not on their desktop. However, what skills do they enter with?\nAs a proxy for entry level into our community, we (ok, Graham Parton again), did a quick survey as to what programming is taught in Russell group universities (why physics, why Russel group? Physics: graduates who are more likely to go under the hood ... we'll get back to that ... and Russell: a small number of identified universities which we might a priori assume to have high quality courses).\nThe results that we could get in an afternoon are here:\n(Key: Red: integrated courses. Green: taught, Orange: accepted but not taught, P: project work, 1/2/3: year in which it is taught, if known). (We asked about some other languages too, but these are the main responses.)\nWhat we find is that most of them offer programming courses to some level as an introduction to computational physics. There has been a move away from FORTRAN as the language of choice to other languages such as C++ and Python. Southampton, Cardiff and Nottingham have focused particularly on concentrating on one language that is integrated into wider course material (Matlab in Nottingham, and Python in Cardiff and Sheffield). These three universities have focused on one language to avoid confusion with others, focusing on aiming for fluency in programming that can be later translated to other languages as opposed to exposure to many languages. Oxford, on the other hand, is a notable exception where a wide number of languages are introduced in years 1 and 2. Imperial is reviewing programming provision and there is a strong lobby for Matlab within their department.\nMost departments reported using a wide range of programming languages/packages (e.g FORTRAN, C++, IDL, Matlab) depending on what was the predominant processing package in the research group/field, e.g. IDL for astronomy, C++ for particle physics.\nOverall, it appears that a ranking of programming language provision would be:\nC++\nMatLab\nPython\nOff the cuff comments from a member of the Institute of Physics asked if they had any insight into the provision of programming languages in a wider group of physics departments suggest these results aren't unique to the Russell group departments (but also that Python, having been off the radar, is increasing rapidly). That person had not heard of IDL (which is mostly used in research departments, and then mainly in astrophysics/solar-terrestrial/astronomy and atmospheric physics).\n(Common feedback on why Matlab was chosen indicated that one of the drivers was the relatively pain-free path from programming to getting decent graphics at the other end.)\nDiscussion\nAt this point we need to focus down to some contenders. What should an organisation like ourselves, or even the Met Office for example, consider for their main \"toolset\" development language? Clearly on the table we have Matlab and Python (given the results above).\nGiven the importance of statistics to our field, and the fact that R is in relatively common usage and has an outlet for publishing code we should also keep it in the mix. However, if using R libraries is important, we can do that from Python ... and it's not a natural language for complex workflow development, so we'll park R in the \"useful addendum to python\" corner ... (that said, for a class of problems, we have used, and continue to use, R in production services at the BADC.)\nWhat about IDL then? Well, clearly it's useful, and clearly folks will use it for a long time to come. However, most ordinary IDL users are likely to be able to read Python very easily - even if they have never seen Python before: For a time we used to give candidates for jobs at the BADC a bit of Python code and ask them to explain what it did, and we only did that to folk who hadn't seen python before. We had intended it as a discriminator of folks ability to interpret something they hadn't seen before, but in most cases they just \"got it right\". We obviously needed something a bit more complicated (in which case the more obscure Python syntax might have got in the way), but as it was, what we learned from that exercise was mostly that \"Python is easy to read\"!\nWhat about writing IDL? Well, yes, it's relatively straightforward, but it's not a great language for maintaining code in, and it's commercial (and not cheap!). The IDL community of use is rather limited in comparison to Python - and, you can call Python from IDL anyway. So if you really want IDL, but wanted \"my new toolset\", (if we wrote it properly) you could call it from IDL anyway. (In this context, it's worth noting that calling C and Fortran from Python is apparently much easier than doing so from IDL.)\nThere is clearly a lot of momentum:\nfolk moving from IDL to Python, and some pretty coherent analyses of why one might use Python in comparison to IDL (e.g. here )\nThere are also lots of web pages which provide information for folk migrating to Python from IDL ( example ).\nWe've seen that I believe python is easy to learn, and that at least two UK departments have built their courses around it. But what about the wider community?\nA number of computer science departments are now teaching Python as their first programming language as well (S. Easterbrook in private conversation).\nProbably more importantly for my thesis, is that the well regarded software carpentry course which provides an introduction to things working scientists most need to know uses Python.\nClear climate code are using Python of course!\nWhich leaves us with Matlab. In truth, I don't know that much about Matlab. My feeling is that the big advantage of Python over matlab is the integration with all the other bits and pieces one wants as soon as a workflow gets sufficiently interesting (GUIs, Databases, XML parsers, other people's libraries etc), and the easy extensibility. You can use R from Python. You can even use the NCAR graphics library from Python (via PyNGL even if some are curmudgeonly about the interface).\nThe other thing that I believe to be a killer reason for using Python: proper support for unit testing: if we could inculcate testing into the scientific development workflow, I, for one, believe a lot of time would be saved in scientific coding. I might even rest happier about many of the results in the literature.\nThe Bottom Line\nSo, I'm still convinced that that the community should migrate away from IDL to Python, and the way to do that is to build a library that can be called from IDL, but is in native Python.\nI appreciate that there may be some resistance to this, particularly from those scientists who like to look under the hood and understand and extend library functions. Some of those scientists are very familiar with IDL - but my gut feeling is that those are also the very same ones, that, if they spent an afternoon familiarising themselves with Python, would find they can go faster and further with Python. (Many of those folks are going to have been physicists, which was why I started by looking at what Physics courses have been up to.) My suspicion is that those that don't look under the hood wont care, provided it's easy to use, and well documented. Python helps with the latter too: with documentation utilities vastly superior to anything available in the IDL (and I suspect, Matlab) space.\nSo, after all that: the choice is (still) Python!\nNB: I will update this entry over time if folk give me useful feedback.\n"}, {"score": 849.0213, "uuid": "e78e2bf2-843e-5304-bfbb-d3dea7f6cc71", "index": "cw12", "trec_id": "clueweb12-0804wb-87-05827", "target_hostname": "www.oscon.com", "target_uri": "http://www.oscon.com/oscon2009/public/schedule/detail/8404", "page_rank": 1.2046539e-09, "spam_rank": 94, "title": "Open Source Analytics: Visualization and Predictive Modeling of Big <em>Data</em>", "snippet": "He is the co-chair of the Bay Area <em>R</em> Users Group, and has used <em>R</em> extensively <em>for</em> the visualization and <em>analysis</em> of genome <em>data</em>, GIS <em>data</em>, and macroeconomic <em>data</em> sets. Michael has a Ph.", "explanation": null, "document": "Location: Exhibit Hall 3\nPresentation File:\nOpen Source Analytics_ Visualization and Predictive Modeling of Big Data with the R Programming Language Presentation 1 [PPT]\nThe economics of data aggregation and analysis are being disrupted by falling costs for storage and CPU power, the continuing shift of business processes online, and the deluge of data that is being generated as a consequence.\nInnovative technologies have emerged to cope with the storage and retrieval of Big Data, yet analysis tools have been less emphasized. Many emerging data sets do not fit within existing software paradigms: either their size overwhelms traditional desktop tools such as Excel, or their range of data types (geocodes, for example) prevent them from being pipelined into more powerful, but narrowly designed tools. Most importantly, closed-source tools cannot keep pace with the leading edge of innovation in statistical and machine-learning algorithms.\nEnter the open source programming language R. R has been dubbed the lingua franca for statistical computing and graphical analysis, with a pedigree tracing back several decades at Bell Labs. Though its million-plus users are concentrated within academia, R is gaining currency within several high-profile quantitative analysis groups, including Google\u2019s Customer Insights team and Barclays Global Investors. In addition, R\u2019s extensibility via user-contributed packages has spawned an active developer community.\nIn this session, I will focus on applying R\u2019s powerful visualization and analysis capabilities to the kinds of large, multidimensional data sets that increasingly confront developers. Along the way, I will highlight R\u2019s functional programming features, its compact syntax for statistical modeling, and its ease of connectivity with persistent data stores.\nIn particular, I will present the following two case studies applying R to large, freely available data sets:\n- an analysis of NASA\u2019s Landsat imagery of Brazil\u2019s center-west agricultural regions to detect correlates for soybean harvest yields, and a derived predictor of the Brazilian soybean market based in part on these correlates.\n- a validation of Bill James\u2019 sabermetrics approach to batting performance using 30 years of Major League Baseball statistics, and a derived predictor for batters\u2019 salaries.\nFor all of its strengths, R has an admittedly steep learning curve. While source code for these examples will be provided, this talk will emphasize techniques and approach over detail. This session seeks to give developers the courage to learn R, the confidence to include it in their OSS arsenal, and the wisdom to recognize opportunities for its use.\nPeople planning to attend this session also want to see:\n"}, {"score": 848.52124, "uuid": "de1e82c2-b2a8-5492-afe8-9e5d9979b1ee", "index": "cw12", "trec_id": "clueweb12-1315wb-09-01174", "target_hostname": "www.r-bloggers.com", "target_uri": "http://www.r-bloggers.com/tag/stocks/", "page_rank": 2.558787e-09, "spam_rank": 81, "title": "Stocks (<em>R</em> news &amp; tutorials)", "snippet": "After reading the fine article Style <em>Analysis</em> from Systematic Investor and What we can <em>learn</em> from Bill Miller and the Legg Mason Value Trust from Asymmetric Investment Returns, <em>I</em> thought <em>I</em> <em>should</em> combine the two in <em>R</em> with the FactorAnalytics package.&amp;n...", "explanation": null, "document": "By klr\nWhen we learn the efficient frontier, most are misled to believe that the frontier is static and unchanging.\u00a0 However, we should have all learned by recent experience that the frontier is as volatile as the assets that construct it.\u00a0 If we lo...\nPage 1 of 7 1 2 3 4 5 ... 7 \u00bb\nTop 7 articles of the week\n"}, {"score": 846.5616, "uuid": "9aff119c-6d9f-5317-b312-ee1c4347b94d", "index": "cw12", "trec_id": "clueweb12-0704wb-93-01024", "target_hostname": "www.wingware.com", "target_uri": "http://www.wingware.com/psupport/python-manual/3.2/faq/design.html", "page_rank": 1.2098994e-09, "spam_rank": 74, "title": "Design and History FAQ \u2014 <em>Python</em> v3.2.1 documentation", "snippet": "o Can <em>Python</em> be compiled to machine code, C <em>or</em> some other language? o Why are there separate tuple and list <em>data</em> types? o How do you specify and enforce an interface spec in <em>Python</em>? o Why can\u2019t raw strings (<em>r</em>-strings) end with a backslash?", "explanation": null, "document": "Design and History FAQ \u00b6\nWhy does Python use indentation for grouping of statements? \u00b6\nGuido van Rossum believes that using indentation for grouping is extremely elegant and contributes a lot to the clarity of the average Python program. Most people learn to love this feature after a while.\nSince there are no begin/end brackets there cannot be a disagreement between grouping perceived by the parser and the human reader. Occasionally C programmers will encounter a fragment of code like this:\nif (x <= y) x++; y--; z++;\nOnly the x++ statement is executed if the condition is true, but the indentation leads you to believe otherwise. Even experienced C programmers will sometimes stare at it a long time wondering why y is being decremented even for x > y.\nBecause there are no begin/end brackets, Python is much less prone to coding-style conflicts. In C there are many different ways to place the braces. If you\u2019re used to reading and writing code that uses one style, you will feel at least slightly uneasy when reading (or being required to write) another style.\nMany coding styles place begin/end brackets on a line by themselves. This makes programs considerably longer and wastes valuable screen space, making it harder to get a good overview of a program. Ideally, a function should fit on one screen (say, 20-30 lines). 20 lines of Python can do a lot more work than 20 lines of C. This is not solely due to the lack of begin/end brackets \u2013 the lack of declarations and the high-level data types are also responsible \u2013 but the indentation-based syntax certainly helps.\nWhy am I getting strange results with simple arithmetic operations? \u00b6\nSee the next question.\nWhy are floating point calculations so inaccurate? \u00b6\nPeople are often very surprised by results like this:\n>>> 1.2 - 1.0 0.199999999999999996\nand think it is a bug in Python. It\u2019s not. This has nothing to do with Python, but with how the underlying C platform handles floating point numbers, and ultimately with the inaccuracies introduced when writing down numbers as a string of a fixed number of digits.\nThe internal representation of floating point numbers uses a fixed number of binary digits to represent a decimal number. Some decimal numbers can\u2019t be represented exactly in binary, resulting in small roundoff errors.\nIn decimal math, there are many numbers that can\u2019t be represented with a fixed number of decimal digits, e.g. 1/3 = 0.3333333333.......\nIn base 2, 1/2 = 0.1, 1/4 = 0.01, 1/8 = 0.001, etc. .2 equals 2/10 equals 1/5, resulting in the binary fractional number 0.001100110011001...\nFloating point numbers only have 32 or 64 bits of precision, so the digits are cut off at some point, and the resulting number is 0.199999999999999996 in decimal, not 0.2.\nA floating point number\u2019s repr() function prints as many digits are necessary to make eval(repr(f)) == f true for any float f. The str() function prints fewer digits and this often results in the more sensible number that was probably intended:\n>>> 1.1 - 0.9 0.20000000000000007 >>> print(1.1 - 0.9) 0.2\nOne of the consequences of this is that it is error-prone to compare the result of some computation to a float with ==. Tiny inaccuracies may mean that == fails. Instead, you have to check that the difference between the two numbers is less than a certain threshold:\nepsilon = 0.0000000000001 # Tiny allowed error expected_result = 0.4 if expected_result-epsilon <= computation() <= expected_result+epsilon: ...\nPlease see the chapter on floating point arithmetic in the Python tutorial for more information.\nWhy are Python strings immutable? \u00b6\nThere are several advantages.\nOne is performance: knowing that a string is immutable means we can allocate space for it at creation time, and the storage requirements are fixed and unchanging. This is also one of the reasons for the distinction between tuples and lists.\nAnother advantage is that strings in Python are considered as \u201celemental\u201d as numbers. No amount of activity will change the value 8 to anything else, and in Python, no amount of activity will change the string \u201ceight\u201d to anything else.\nWhy must \u2018self\u2019 be used explicitly in method definitions and calls? \u00b6\nThe idea was borrowed from Modula-3. It turns out to be very useful, for a variety of reasons.\nFirst, it\u2019s more obvious that you are using a method or instance attribute instead of a local variable. Reading self.x or self.meth() makes it absolutely clear that an instance variable or method is used even if you don\u2019t know the class definition by heart. In C++, you can sort of tell by the lack of a local variable declaration (assuming globals are rare or easily recognizable) \u2013 but in Python, there are no local variable declarations, so you\u2019d have to look up the class definition to be sure. Some C++ and Java coding standards call for instance attributes to have an m_ prefix, so this explicitness is still useful in those languages, too.\nSecond, it means that no special syntax is necessary if you want to explicitly reference or call the method from a particular class. In C++, if you want to use a method from a base class which is overridden in a derived class, you have to use the :: operator \u2013 in Python you can write baseclass.methodname(self, <argument list>). This is particularly useful for __init__() methods, and in general in cases where a derived class method wants to extend the base class method of the same name and thus has to call the base class method somehow.\nFinally, for instance variables it solves a syntactic problem with assignment: since local variables in Python are (by definition!) those variables to which a value is assigned in a function body (and that aren\u2019t explicitly declared global), there has to be some way to tell the interpreter that an assignment was meant to assign to an instance variable instead of to a local variable, and it should preferably be syntactic (for efficiency reasons). C++ does this through declarations, but Python doesn\u2019t have declarations and it would be a pity having to introduce them just for this purpose. Using the explicit self.var solves this nicely. Similarly, for using instance variables, having to write self.var means that references to unqualified names inside a method don\u2019t have to search the instance\u2019s directories. To put it another way, local variables and instance variables live in two different namespaces, and you need to tell Python which namespace to use.\nWhy can\u2019t I use an assignment in an expression? \u00b6\nMany people used to C or Perl complain that they want to use this C idiom:\nwhile (line = readline(f)) { // do something with line }\nwhere in Python you\u2019re forced to write this:\nwhile True: line = f.readline() if not line: break ... # do something with line\nThe reason for not allowing assignment in Python expressions is a common, hard-to-find bug in those other languages, caused by this construct:\nif (x = 0) { // error handling } else { // code that only works for nonzero x }\nThe error is a simple typo: x = 0, which assigns 0 to the variable x, was written while the comparison x == 0 is certainly what was intended.\nMany alternatives have been proposed. Most are hacks that save some typing but use arbitrary or cryptic syntax or keywords, and fail the simple criterion for language change proposals: it should intuitively suggest the proper meaning to a human reader who has not yet been introduced to the construct.\nAn interesting phenomenon is that most experienced Python programmers recognize the while True idiom and don\u2019t seem to be missing the assignment in expression construct much; it\u2019s only newcomers who express a strong desire to add this to the language.\nThere\u2019s an alternative way of spelling this that seems attractive but is generally less robust than the \u201cwhile True\u201d solution:\nline = f.readline() while line: ... # do something with line... line = f.readline()\nThe problem with this is that if you change your mind about exactly how you get the next line (e.g. you want to change it into sys.stdin.readline()) you have to remember to change two places in your program \u2013 the second occurrence is hidden at the bottom of the loop.\nThe best approach is to use iterators, making it possible to loop through objects using the for statement. For example, file objects support the iterator protocol, so you can write simply:\nfor line in f: ... # do something with line...\nWhy does Python use methods for some functionality (e.g. list.index()) but functions for other (e.g. len(list))? \u00b6\nThe major reason is history. Functions were used for those operations that were generic for a group of types and which were intended to work even for objects that didn\u2019t have methods at all (e.g. tuples). It is also convenient to have a function that can readily be applied to an amorphous collection of objects when you use the functional features of Python (map(), apply() et al).\nIn fact, implementing len(), max(), min() as a built-in function is actually less code than implementing them as methods for each type. One can quibble about individual cases but it\u2019s a part of Python, and it\u2019s too late to make such fundamental changes now. The functions have to remain to avoid massive code breakage.\nNote\nFor string operations, Python has moved from external functions (the string module) to methods. However, len() is still a function.\nWhy is join() a string method instead of a list or tuple method? \u00b6\nStrings became much more like other standard types starting in Python 1.6, when methods were added which give the same functionality that has always been available using the functions of the string module. Most of these new methods have been widely accepted, but the one which appears to make some programmers feel uncomfortable is:\n\", \".join(['1', '2', '4', '8', '16'])\nwhich gives the result:\n\"1, 2, 4, 8, 16\"\nThere are two common arguments against this usage.\nThe first runs along the lines of: \u201cIt looks really ugly using a method of a string literal (string constant)\u201d, to which the answer is that it might, but a string literal is just a fixed value. If the methods are to be allowed on names bound to strings there is no logical reason to make them unavailable on literals.\nThe second objection is typically cast as: \u201cI am really telling a sequence to join its members together with a string constant\u201d. Sadly, you aren\u2019t. For some reason there seems to be much less difficulty with having split() as a string method, since in that case it is easy to see that\n\"1, 2, 4, 8, 16\".split(\", \")\nis an instruction to a string literal to return the substrings delimited by the given separator (or, by default, arbitrary runs of white space).\njoin() is a string method because in using it you are telling the separator string to iterate over a sequence of strings and insert itself between adjacent elements. This method can be used with any argument which obeys the rules for sequence objects, including any new classes you might define yourself. Similar methods exist for bytes and bytearray objects.\nHow fast are exceptions? \u00b6\nA try/except block is extremely efficient. Actually catching an exception is expensive. In versions of Python prior to 2.0 it was common to use this idiom:\ntry: value = mydict[key] except KeyError: mydict[key] = getvalue(key) value = mydict[key]\nThis only made sense when you expected the dict to have the key almost all the time. If that wasn\u2019t the case, you coded it like this:\nif mydict.has_key(key): value = mydict[key] else: mydict[key] = getvalue(key) value = mydict[key]\nFor this specific case, you could also use value = dict.setdefault(key, getvalue(key)), but only if the getvalue() call is cheap enough because it is evaluated in all cases.\nWhy isn\u2019t there a switch or case statement in Python? \u00b6\nYou can do this easily enough with a sequence of if... elif... elif... else. There have been some proposals for switch statement syntax, but there is no consensus (yet) on whether and how to do range tests. See PEP 275 for complete details and the current status.\nFor cases where you need to choose from a very large number of possibilities, you can create a dictionary mapping case values to functions to call. For example:\ndef function_1(...): ... functions = {'a': function_1, 'b': function_2, 'c': self.method_1, ...} func = functions[value] func()\nFor calling methods on objects, you can simplify yet further by using the getattr() built-in to retrieve methods with a particular name:\ndef visit_a(self, ...): ... ... def dispatch(self, value): method_name = 'visit_' + str(value) method = getattr(self, method_name) method()\nIt\u2019s suggested that you use a prefix for the method names, such as visit_ in this example. Without such a prefix, if values are coming from an untrusted source, an attacker would be able to call any method on your object.\nCan\u2019t you emulate threads in the interpreter instead of relying on an OS-specific thread implementation? \u00b6\nAnswer 1: Unfortunately, the interpreter pushes at least one C stack frame for each Python stack frame. Also, extensions can call back into Python at almost random moments. Therefore, a complete threads implementation requires thread support for C.\nAnswer 2: Fortunately, there is Stackless Python , which has a completely redesigned interpreter loop that avoids the C stack. It\u2019s still experimental but looks very promising. Although it is binary compatible with standard Python, it\u2019s still unclear whether Stackless will make it into the core \u2013 maybe it\u2019s just too revolutionary.\nWhy can\u2019t lambda forms contain statements? \u00b6\nPython lambda forms cannot contain statements because Python\u2019s syntactic framework can\u2019t handle statements nested inside expressions. However, in Python, this is not a serious problem. Unlike lambda forms in other languages, where they add functionality, Python lambdas are only a shorthand notation if you\u2019re too lazy to define a function.\nFunctions are already first class objects in Python, and can be declared in a local scope. Therefore the only advantage of using a lambda form instead of a locally-defined function is that you don\u2019t need to invent a name for the function \u2013 but that\u2019s just a local variable to which the function object (which is exactly the same type of object that a lambda form yields) is assigned!\nCan Python be compiled to machine code, C or some other language? \u00b6\nNot easily. Python\u2019s high level data types, dynamic typing of objects and run-time invocation of the interpreter (using eval() or exec() ) together mean that a \u201ccompiled\u201d Python program would probably consist mostly of calls into the Python run-time system, even for seemingly simple operations like x+1.\nSeveral projects described in the Python newsgroup or at past Python conferences have shown that this approach is feasible, although the speedups reached so far are only modest (e.g. 2x). Jython uses the same strategy for compiling to Java bytecode. (Jim Hugunin has demonstrated that in combination with whole-program analysis, speedups of 1000x are feasible for small demo programs. See the proceedings from the 1997 Python conference for more information.)\nInternally, Python source code is always translated into a bytecode representation, and this bytecode is then executed by the Python virtual machine. In order to avoid the overhead of repeatedly parsing and translating modules that rarely change, this byte code is written into a file whose name ends in \u201d.pyc\u201d whenever a module is parsed. When the corresponding .py file is changed, it is parsed and translated again and the .pyc file is rewritten.\nThere is no performance difference once the .pyc file has been loaded, as the bytecode read from the .pyc file is exactly the same as the bytecode created by direct translation. The only difference is that loading code from a .pyc file is faster than parsing and translating a .py file, so the presence of precompiled .pyc files improves the start-up time of Python scripts. If desired, the Lib/compileall.py module can be used to create valid .pyc files for a given set of modules.\nNote that the main script executed by Python, even if its filename ends in .py, is not compiled to a .pyc file. It is compiled to bytecode, but the bytecode is not saved to a file. Usually main scripts are quite short, so this doesn\u2019t cost much speed.\nThere are also several programs which make it easier to intermingle Python and C code in various ways to increase performance. See, for example, Cython , Pyrex and Weave .\nHow does Python manage memory? \u00b6\nThe details of Python memory management depend on the implementation. The standard C implementation of Python uses reference counting to detect inaccessible objects, and another mechanism to collect reference cycles, periodically executing a cycle detection algorithm which looks for inaccessible cycles and deletes the objects involved. The gc module provides functions to perform a garbage collection, obtain debugging statistics, and tune the collector\u2019s parameters.\nJython relies on the Java runtime so the JVM\u2019s garbage collector is used. This difference can cause some subtle porting problems if your Python code depends on the behavior of the reference counting implementation.\nIn the absence of circularities, Python programs do not need to manage memory explicitly.\nWhy doesn\u2019t Python use a more traditional garbage collection scheme? For one thing, this is not a C standard feature and hence it\u2019s not portable. (Yes, we know about the Boehm GC library. It has bits of assembler code for most common platforms, not for all of them, and although it is mostly transparent, it isn\u2019t completely transparent; patches are required to get Python to work with it.)\nTraditional GC also becomes a problem when Python is embedded into other applications. While in a standalone Python it\u2019s fine to replace the standard malloc() and free() with versions provided by the GC library, an application embedding Python may want to have its own substitute for malloc() and free(), and may not want Python\u2019s. Right now, Python works with anything that implements malloc() and free() properly.\nIn Jython, the following code (which is fine in CPython) will probably run out of file descriptors long before it runs out of memory:\nfor file in very_long_list_of_files: f = open(file) c = f.read(1)\nUsing the current reference counting and destructor scheme, each new assignment to f closes the previous file. Using GC, this is not guaranteed. If you want to write code that will work with any Python implementation, you should explicitly close the file or use the with statement; this will work regardless of GC:\nfor file in very_long_list_of_files: with open(file) as f: c = f.read(1)\nWhy isn\u2019t all memory freed when Python exits? \u00b6\nObjects referenced from the global namespaces of Python modules are not always deallocated when Python exits. This may happen if there are circular references. There are also certain bits of memory that are allocated by the C library that are impossible to free (e.g. a tool like Purify will complain about these). Python is, however, aggressive about cleaning up memory on exit and does try to destroy every single object.\nIf you want to force Python to delete certain things on deallocation use the atexit module to run a function that will force those deletions.\nWhy are there separate tuple and list data types? \u00b6\nLists and tuples, while similar in many respects, are generally used in fundamentally different ways. Tuples can be thought of as being similar to Pascal records or C structs; they\u2019re small collections of related data which may be of different types which are operated on as a group. For example, a Cartesian coordinate is appropriately represented as a tuple of two or three numbers.\nLists, on the other hand, are more like arrays in other languages. They tend to hold a varying number of objects all of which have the same type and which are operated on one-by-one. For example, os.listdir('.') returns a list of strings representing the files in the current directory. Functions which operate on this output would generally not break if you added another file or two to the directory.\nTuples are immutable, meaning that once a tuple has been created, you can\u2019t replace any of its elements with a new value. Lists are mutable, meaning that you can always change a list\u2019s elements. Only immutable elements can be used as dictionary keys, and hence only tuples and not lists can be used as keys.\nHow are lists implemented? \u00b6\nPython\u2019s lists are really variable-length arrays, not Lisp-style linked lists. The implementation uses a contiguous array of references to other objects, and keeps a pointer to this array and the array\u2019s length in a list head structure.\nThis makes indexing a list a[i] an operation whose cost is independent of the size of the list or the value of the index.\nWhen items are appended or inserted, the array of references is resized. Some cleverness is applied to improve the performance of appending items repeatedly; when the array must be grown, some extra space is allocated so the next few times don\u2019t require an actual resize.\nHow are dictionaries implemented? \u00b6\nPython\u2019s dictionaries are implemented as resizable hash tables. Compared to B-trees, this gives better performance for lookup (the most common operation by far) under most circumstances, and the implementation is simpler.\nDictionaries work by computing a hash code for each key stored in the dictionary using the hash() built-in function. The hash code varies widely depending on the key; for example, \u201cPython\u201d hashes to -539294296 while \u201cpython\u201d, a string that differs by a single bit, hashes to 1142331976. The hash code is then used to calculate a location in an internal array where the value will be stored. Assuming that you\u2019re storing keys that all have different hash values, this means that dictionaries take constant time \u2013 O(1), in computer science notation \u2013 to retrieve a key. It also means that no sorted order of the keys is maintained, and traversing the array as the .keys() and .items() do will output the dictionary\u2019s content in some arbitrary jumbled order.\nWhy must dictionary keys be immutable? \u00b6\nThe hash table implementation of dictionaries uses a hash value calculated from the key value to find the key. If the key were a mutable object, its value could change, and thus its hash could also change. But since whoever changes the key object can\u2019t tell that it was being used as a dictionary key, it can\u2019t move the entry around in the dictionary. Then, when you try to look up the same object in the dictionary it won\u2019t be found because its hash value is different. If you tried to look up the old value it wouldn\u2019t be found either, because the value of the object found in that hash bin would be different.\nIf you want a dictionary indexed with a list, simply convert the list to a tuple first; the function tuple(L) creates a tuple with the same entries as the list L. Tuples are immutable and can therefore be used as dictionary keys.\nSome unacceptable solutions that have been proposed:\nHash lists by their address (object ID). This doesn\u2019t work because if you construct a new list with the same value it won\u2019t be found; e.g.:\nmydict = {[1, 2]: '12'} print(mydict[[1, 2]])\nwould raise a KeyError exception because the id of the [1, 2] used in the second line differs from that in the first line. In other words, dictionary keys should be compared using ==, not using is .\nMake a copy when using a list as a key. This doesn\u2019t work because the list, being a mutable object, could contain a reference to itself, and then the copying code would run into an infinite loop.\nAllow lists as keys but tell the user not to modify them. This would allow a class of hard-to-track bugs in programs when you forgot or modified a list by accident. It also invalidates an important invariant of dictionaries: every value in d.keys() is usable as a key of the dictionary.\nMark lists as read-only once they are used as a dictionary key. The problem is that it\u2019s not just the top-level object that could change its value; you could use a tuple containing a list as a key. Entering anything as a key into a dictionary would require marking all objects reachable from there as read-only \u2013 and again, self-referential objects could cause an infinite loop.\nThere is a trick to get around this if you need to, but use it at your own risk: You can wrap a mutable structure inside a class instance which has both a __eq__() and a __hash__() method. You must then make sure that the hash value for all such wrapper objects that reside in a dictionary (or other hash based structure), remain fixed while the object is in the dictionary (or other structure).\nclass ListWrapper: def __init__(self, the_list): self.the_list = the_list def __eq__(self, other): return self.the_list == other.the_list def __hash__(self): l = self.the_list result = 98767 - len(l)*555 for i, el in enumerate(l): try: result = result + (hash(el) % 9999999) * 1001 + i except Exception: result = (result % 7777777) + i * 333 return result\nNote that the hash computation is complicated by the possibility that some members of the list may be unhashable and also by the possibility of arithmetic overflow.\nFurthermore it must always be the case that if o1 == o2 (ie o1.__eq__(o2) is True) then hash(o1) == hash(o2) (ie, o1.__hash__() == o2.__hash__()), regardless of whether the object is in a dictionary or not. If you fail to meet these restrictions dictionaries and other hash based structures will misbehave.\nIn the case of ListWrapper, whenever the wrapper object is in a dictionary the wrapped list must not change to avoid anomalies. Don\u2019t do this unless you are prepared to think hard about the requirements and the consequences of not meeting them correctly. Consider yourself warned.\nWhy doesn\u2019t list.sort() return the sorted list? \u00b6\nIn situations where performance matters, making a copy of the list just to sort it would be wasteful. Therefore, list.sort() sorts the list in place. In order to remind you of that fact, it does not return the sorted list. This way, you won\u2019t be fooled into accidentally overwriting a list when you need a sorted copy but also need to keep the unsorted version around.\nIn Python 2.4 a new built-in function \u2013 sorted() \u2013 has been added. This function creates a new list from a provided iterable, sorts it and returns it. For example, here\u2019s how to iterate over the keys of a dictionary in sorted order:\nfor key in sorted(mydict): ... # do whatever with mydict[key]...\nHow do you specify and enforce an interface spec in Python? \u00b6\nAn interface specification for a module as provided by languages such as C++ and Java describes the prototypes for the methods and functions of the module. Many feel that compile-time enforcement of interface specifications helps in the construction of large programs.\nPython 2.6 adds an abc module that lets you define Abstract Base Classes (ABCs). You can then use isinstance() and issubclass() to check whether an instance or a class implements a particular ABC. The collections modules defines a set of useful ABCs such as Iterable, Container, and MutableMapping.\nFor Python, many of the advantages of interface specifications can be obtained by an appropriate test discipline for components. There is also a tool, PyChecker, which can be used to find problems due to subclassing.\nA good test suite for a module can both provide a regression test and serve as a module interface specification and a set of examples. Many Python modules can be run as a script to provide a simple \u201cself test.\u201d Even modules which use complex external interfaces can often be tested in isolation using trivial \u201cstub\u201d emulations of the external interface. The doctest and unittest modules or third-party test frameworks can be used to construct exhaustive test suites that exercise every line of code in a module.\nAn appropriate testing discipline can help build large complex applications in Python as well as having interface specifications would. In fact, it can be better because an interface specification cannot test certain properties of a program. For example, the append() method is expected to add new elements to the end of some internal list; an interface specification cannot test that your append() implementation will actually do this correctly, but it\u2019s trivial to check this property in a test suite.\nWriting test suites is very helpful, and you might want to design your code with an eye to making it easily tested. One increasingly popular technique, test-directed development, calls for writing parts of the test suite first, before you write any of the actual code. Of course Python allows you to be sloppy and not write test cases at all.\nWhy are default values shared between objects? \u00b6\nThis type of bug commonly bites neophyte programmers. Consider this function:\ndef foo(mydict={}): # Danger: shared reference to one dict for all calls ... compute something ... mydict[key] = value return mydict\nThe first time you call this function, mydict contains a single item. The second time, mydict contains two items because when foo() begins executing, mydict starts out with an item already in it.\nIt is often expected that a function call creates new objects for default values. This is not what happens. Default values are created exactly once, when the function is defined. If that object is changed, like the dictionary in this example, subsequent calls to the function will refer to this changed object.\nBy definition, immutable objects such as numbers, strings, tuples, and None, are safe from change. Changes to mutable objects such as dictionaries, lists, and class instances can lead to confusion.\nBecause of this feature, it is good programming practice to not use mutable objects as default values. Instead, use None as the default value and inside the function, check if the parameter is None and create a new list/dictionary/whatever if it is. For example, don\u2019t write:\ndef foo(mydict={}): ...\nbut:\ndef foo(mydict=None): if mydict is None: mydict = {} # create a new dict for local namespace\nThis feature can be useful. When you have a function that\u2019s time-consuming to compute, a common technique is to cache the parameters and the resulting value of each call to the function, and return the cached value if the same value is requested again. This is called \u201cmemoizing\u201d, and can be implemented like this:\n# Callers will never provide a third parameter for this function. def expensive (arg1, arg2, _cache={}): if (arg1, arg2) in _cache: return _cache[(arg1, arg2)] # Calculate the value result = ... expensive computation ... _cache[(arg1, arg2)] = result # Store result in the cache return result\nYou could use a global variable containing a dictionary instead of the default value; it\u2019s a matter of taste.\nWhy is there no goto? \u00b6\nYou can use exceptions to provide a \u201cstructured goto\u201d that even works across function calls. Many feel that exceptions can conveniently emulate all reasonable uses of the \u201cgo\u201d or \u201cgoto\u201d constructs of C, Fortran, and other languages. For example:\nclass label: pass # declare a label try: ... if (condition): raise label() # goto label ... except label: # where to goto pass ...\nThis doesn\u2019t allow you to jump into the middle of a loop, but that\u2019s usually considered an abuse of goto anyway. Use sparingly.\nWhy can\u2019t raw strings (r-strings) end with a backslash? \u00b6\nMore precisely, they can\u2019t end with an odd number of backslashes: the unpaired backslash at the end escapes the closing quote character, leaving an unterminated string.\nRaw strings were designed to ease creating input for processors (chiefly regular expression engines) that want to do their own backslash escape processing. Such processors consider an unmatched trailing backslash to be an error anyway, so raw strings disallow that. In return, they allow you to pass on the string quote character by escaping it with a backslash. These rules work well when r-strings are used for their intended purpose.\nIf you\u2019re trying to build Windows pathnames, note that all Windows system calls accept forward slashes too:\nf = open(\"/mydir/file.txt\") # works fine!\nIf you\u2019re trying to build a pathname for a DOS command, try e.g. one of\ndir = r\"\\this\\is\\my\\dos\\dir\" \"\\\\\" dir = r\"\\this\\is\\my\\dos\\dir\\ \"[:-1] dir = \"\\\\this\\\\is\\\\my\\\\dos\\\\dir\\\\\"\nWhy doesn\u2019t Python have a \u201cwith\u201d statement for attribute assignments? \u00b6\nPython has a \u2018with\u2019 statement that wraps the execution of a block, calling code on the entrance and exit from the block. Some language have a construct that looks like this:\nwith obj: a = 1 # equivalent to obj.a = 1 total = total + 1 # obj.total = obj.total + 1\nIn Python, such a construct would be ambiguous.\nOther languages, such as Object Pascal, Delphi, and C++, use static types, so it\u2019s possible to know, in an unambiguous way, what member is being assigned to. This is the main point of static typing \u2013 the compiler always knows the scope of every variable at compile time.\nPython uses dynamic types. It is impossible to know in advance which attribute will be referenced at runtime. Member attributes may be added or removed from objects on the fly. This makes it impossible to know, from a simple reading, what attribute is being referenced: a local one, a global one, or a member attribute?\nFor instance, take the following incomplete snippet:\ndef foo(a): with a: print(x)\nThe snippet assumes that \u201ca\u201d must have a member attribute called \u201cx\u201d. However, there is nothing in Python that tells the interpreter this. What should happen if \u201ca\u201d is, let us say, an integer? If there is a global variable named \u201cx\u201d, will it be used inside the with block? As you see, the dynamic nature of Python makes such choices much harder.\nThe primary benefit of \u201cwith\u201d and similar language features (reduction of code volume) can, however, easily be achieved in Python by assignment. Instead of:\nfunction(args).mydict[index][index].a = 21 function(args).mydict[index][index].b = 42 function(args).mydict[index][index].c = 63\nwrite this:\nref = function(args).mydict[index][index] ref.a = 21 ref.b = 42 ref.c = 63\nThis also has the side-effect of increasing execution speed because name bindings are resolved at run-time in Python, and the second version only needs to perform the resolution once.\nWhy are colons required for the if/while/def/class statements? \u00b6\nThe colon is required primarily to enhance readability (one of the results of the experimental ABC language). Consider this:\nif a == b print(a)\nversus\nif a == b: print(a)\nNotice how the second one is slightly easier to read. Notice further how a colon sets off the example in this FAQ answer; it\u2019s a standard usage in English.\nAnother minor reason is that the colon makes it easier for editors with syntax highlighting; they can look for colons to decide when indentation needs to be increased instead of having to do a more elaborate parsing of the program text.\nWhy does Python allow commas at the end of lists and tuples? \u00b6\nPython lets you add a trailing comma at the end of lists, tuples, and dictionaries:\n[1, 2, 3,] ('a', 'b', 'c',) d = { \"A\": [1, 5], \"B\": [6, 7], # last trailing comma is optional but good style }\nThere are several reasons to allow this.\nWhen you have a literal value for a list, tuple, or dictionary spread across multiple lines, it\u2019s easier to add more elements because you don\u2019t have to remember to add a comma to the previous line. The lines can also be sorted in your editor without creating a syntax error.\nAccidentally omitting the comma can lead to errors that are hard to diagnose. For example:\nx = [ \"fee\", \"fie\" \"foo\", \"fum\" ]\nThis list looks like it has four elements, but it actually contains three: \u201cfee\u201d, \u201cfiefoo\u201d and \u201cfum\u201d. Always adding the comma avoids this source of error.\nAllowing the trailing comma may also make programmatic code generation easier.\n"}, {"score": 844.75824, "uuid": "ea3e1e16-11c6-5536-bb9b-df97f7560a69", "index": "cw12", "trec_id": "clueweb12-0410wb-66-20748", "target_hostname": "wingide.com", "target_uri": "http://wingide.com/psupport/python-manual/3.1/faq/design.html", "page_rank": 1.2112906e-09, "spam_rank": 74, "title": "Design and History FAQ \u2014 <em>Python</em> v3.1.4 documentation", "snippet": "o Can <em>Python</em> be compiled to machine code, C <em>or</em> some other language? o Why are there separate tuple and list <em>data</em> types? o How do you specify and enforce an interface spec in <em>Python</em>? o Why can\u2019t raw strings (<em>r</em>-strings) end with a backslash?", "explanation": null, "document": "Design and History FAQ \u00b6\nWhy does Python use indentation for grouping of statements? \u00b6\nGuido van Rossum believes that using indentation for grouping is extremely elegant and contributes a lot to the clarity of the average Python program. Most people learn to love this feature after a while.\nSince there are no begin/end brackets there cannot be a disagreement between grouping perceived by the parser and the human reader. Occasionally C programmers will encounter a fragment of code like this:\nif (x <= y) x++; y--; z++;\nOnly the x++ statement is executed if the condition is true, but the indentation leads you to believe otherwise. Even experienced C programmers will sometimes stare at it a long time wondering why y is being decremented even for x > y.\nBecause there are no begin/end brackets, Python is much less prone to coding-style conflicts. In C there are many different ways to place the braces. If you\u2019re used to reading and writing code that uses one style, you will feel at least slightly uneasy when reading (or being required to write) another style.\nMany coding styles place begin/end brackets on a line by themselves. This makes programs considerably longer and wastes valuable screen space, making it harder to get a good overview of a program. Ideally, a function should fit on one screen (say, 20-30 lines). 20 lines of Python can do a lot more work than 20 lines of C. This is not solely due to the lack of begin/end brackets \u2013 the lack of declarations and the high-level data types are also responsible \u2013 but the indentation-based syntax certainly helps.\nWhy am I getting strange results with simple arithmetic operations? \u00b6\nSee the next question.\nWhy are floating point calculations so inaccurate? \u00b6\nPeople are often very surprised by results like this:\n>>> 1.2 - 1.0 0.199999999999999996\nand think it is a bug in Python. It\u2019s not. This has nothing to do with Python, but with how the underlying C platform handles floating point numbers, and ultimately with the inaccuracies introduced when writing down numbers as a string of a fixed number of digits.\nThe internal representation of floating point numbers uses a fixed number of binary digits to represent a decimal number. Some decimal numbers can\u2019t be represented exactly in binary, resulting in small roundoff errors.\nIn decimal math, there are many numbers that can\u2019t be represented with a fixed number of decimal digits, e.g. 1/3 = 0.3333333333.......\nIn base 2, 1/2 = 0.1, 1/4 = 0.01, 1/8 = 0.001, etc. .2 equals 2/10 equals 1/5, resulting in the binary fractional number 0.001100110011001...\nFloating point numbers only have 32 or 64 bits of precision, so the digits are cut off at some point, and the resulting number is 0.199999999999999996 in decimal, not 0.2.\nA floating point number\u2019s repr() function prints as many digits are necessary to make eval(repr(f)) == f true for any float f. The str() function prints fewer digits and this often results in the more sensible number that was probably intended:\n>>> 1.1 - 0.9 0.20000000000000007 >>> print(1.1 - 0.9) 0.2\nOne of the consequences of this is that it is error-prone to compare the result of some computation to a float with ==. Tiny inaccuracies may mean that == fails. Instead, you have to check that the difference between the two numbers is less than a certain threshold:\nepsilon = 0.0000000000001 # Tiny allowed error expected_result = 0.4 if expected_result-epsilon <= computation() <= expected_result+epsilon: ...\nPlease see the chapter on floating point arithmetic in the Python tutorial for more information.\nWhy are Python strings immutable? \u00b6\nThere are several advantages.\nOne is performance: knowing that a string is immutable means we can allocate space for it at creation time, and the storage requirements are fixed and unchanging. This is also one of the reasons for the distinction between tuples and lists.\nAnother advantage is that strings in Python are considered as \u201celemental\u201d as numbers. No amount of activity will change the value 8 to anything else, and in Python, no amount of activity will change the string \u201ceight\u201d to anything else.\nWhy must \u2018self\u2019 be used explicitly in method definitions and calls? \u00b6\nThe idea was borrowed from Modula-3. It turns out to be very useful, for a variety of reasons.\nFirst, it\u2019s more obvious that you are using a method or instance attribute instead of a local variable. Reading self.x or self.meth() makes it absolutely clear that an instance variable or method is used even if you don\u2019t know the class definition by heart. In C++, you can sort of tell by the lack of a local variable declaration (assuming globals are rare or easily recognizable) \u2013 but in Python, there are no local variable declarations, so you\u2019d have to look up the class definition to be sure. Some C++ and Java coding standards call for instance attributes to have an m_ prefix, so this explicitness is still useful in those languages, too.\nSecond, it means that no special syntax is necessary if you want to explicitly reference or call the method from a particular class. In C++, if you want to use a method from a base class which is overridden in a derived class, you have to use the :: operator \u2013 in Python you can write baseclass.methodname(self, <argument list>). This is particularly useful for __init__() methods, and in general in cases where a derived class method wants to extend the base class method of the same name and thus has to call the base class method somehow.\nFinally, for instance variables it solves a syntactic problem with assignment: since local variables in Python are (by definition!) those variables to which a value is assigned in a function body (and that aren\u2019t explicitly declared global), there has to be some way to tell the interpreter that an assignment was meant to assign to an instance variable instead of to a local variable, and it should preferably be syntactic (for efficiency reasons). C++ does this through declarations, but Python doesn\u2019t have declarations and it would be a pity having to introduce them just for this purpose. Using the explicit self.var solves this nicely. Similarly, for using instance variables, having to write self.var means that references to unqualified names inside a method don\u2019t have to search the instance\u2019s directories. To put it another way, local variables and instance variables live in two different namespaces, and you need to tell Python which namespace to use.\nWhy can\u2019t I use an assignment in an expression? \u00b6\nMany people used to C or Perl complain that they want to use this C idiom:\nwhile (line = readline(f)) { // do something with line }\nwhere in Python you\u2019re forced to write this:\nwhile True: line = f.readline() if not line: break ... # do something with line\nThe reason for not allowing assignment in Python expressions is a common, hard-to-find bug in those other languages, caused by this construct:\nif (x = 0) { // error handling } else { // code that only works for nonzero x }\nThe error is a simple typo: x = 0, which assigns 0 to the variable x, was written while the comparison x == 0 is certainly what was intended.\nMany alternatives have been proposed. Most are hacks that save some typing but use arbitrary or cryptic syntax or keywords, and fail the simple criterion for language change proposals: it should intuitively suggest the proper meaning to a human reader who has not yet been introduced to the construct.\nAn interesting phenomenon is that most experienced Python programmers recognize the while True idiom and don\u2019t seem to be missing the assignment in expression construct much; it\u2019s only newcomers who express a strong desire to add this to the language.\nThere\u2019s an alternative way of spelling this that seems attractive but is generally less robust than the \u201cwhile True\u201d solution:\nline = f.readline() while line: ... # do something with line... line = f.readline()\nThe problem with this is that if you change your mind about exactly how you get the next line (e.g. you want to change it into sys.stdin.readline()) you have to remember to change two places in your program \u2013 the second occurrence is hidden at the bottom of the loop.\nThe best approach is to use iterators, making it possible to loop through objects using the for statement. For example, file objects support the iterator protocol, so you can write simply:\nfor line in f: ... # do something with line...\nWhy does Python use methods for some functionality (e.g. list.index()) but functions for other (e.g. len(list))? \u00b6\nThe major reason is history. Functions were used for those operations that were generic for a group of types and which were intended to work even for objects that didn\u2019t have methods at all (e.g. tuples). It is also convenient to have a function that can readily be applied to an amorphous collection of objects when you use the functional features of Python (map(), apply() et al).\nIn fact, implementing len(), max(), min() as a built-in function is actually less code than implementing them as methods for each type. One can quibble about individual cases but it\u2019s a part of Python, and it\u2019s too late to make such fundamental changes now. The functions have to remain to avoid massive code breakage.\nNote\nFor string operations, Python has moved from external functions (the string module) to methods. However, len() is still a function.\nWhy is join() a string method instead of a list or tuple method? \u00b6\nStrings became much more like other standard types starting in Python 1.6, when methods were added which give the same functionality that has always been available using the functions of the string module. Most of these new methods have been widely accepted, but the one which appears to make some programmers feel uncomfortable is:\n\", \".join(['1', '2', '4', '8', '16'])\nwhich gives the result:\n\"1, 2, 4, 8, 16\"\nThere are two common arguments against this usage.\nThe first runs along the lines of: \u201cIt looks really ugly using a method of a string literal (string constant)\u201d, to which the answer is that it might, but a string literal is just a fixed value. If the methods are to be allowed on names bound to strings there is no logical reason to make them unavailable on literals.\nThe second objection is typically cast as: \u201cI am really telling a sequence to join its members together with a string constant\u201d. Sadly, you aren\u2019t. For some reason there seems to be much less difficulty with having split() as a string method, since in that case it is easy to see that\n\"1, 2, 4, 8, 16\".split(\", \")\nis an instruction to a string literal to return the substrings delimited by the given separator (or, by default, arbitrary runs of white space).\njoin() is a string method because in using it you are telling the separator string to iterate over a sequence of strings and insert itself between adjacent elements. This method can be used with any argument which obeys the rules for sequence objects, including any new classes you might define yourself. Similar methods exist for bytes and bytearray objects.\nHow fast are exceptions? \u00b6\nA try/except block is extremely efficient. Actually catching an exception is expensive. In versions of Python prior to 2.0 it was common to use this idiom:\ntry: value = mydict[key] except KeyError: mydict[key] = getvalue(key) value = mydict[key]\nThis only made sense when you expected the dict to have the key almost all the time. If that wasn\u2019t the case, you coded it like this:\nif mydict.has_key(key): value = mydict[key] else: mydict[key] = getvalue(key) value = mydict[key]\nFor this specific case, you could also use value = dict.setdefault(key, getvalue(key)), but only if the getvalue() call is cheap enough because it is evaluated in all cases.\nWhy isn\u2019t there a switch or case statement in Python? \u00b6\nYou can do this easily enough with a sequence of if... elif... elif... else. There have been some proposals for switch statement syntax, but there is no consensus (yet) on whether and how to do range tests. See PEP 275 for complete details and the current status.\nFor cases where you need to choose from a very large number of possibilities, you can create a dictionary mapping case values to functions to call. For example:\ndef function_1(...): ... functions = {'a': function_1, 'b': function_2, 'c': self.method_1, ...} func = functions[value] func()\nFor calling methods on objects, you can simplify yet further by using the getattr() built-in to retrieve methods with a particular name:\ndef visit_a(self, ...): ... ... def dispatch(self, value): method_name = 'visit_' + str(value) method = getattr(self, method_name) method()\nIt\u2019s suggested that you use a prefix for the method names, such as visit_ in this example. Without such a prefix, if values are coming from an untrusted source, an attacker would be able to call any method on your object.\nCan\u2019t you emulate threads in the interpreter instead of relying on an OS-specific thread implementation? \u00b6\nAnswer 1: Unfortunately, the interpreter pushes at least one C stack frame for each Python stack frame. Also, extensions can call back into Python at almost random moments. Therefore, a complete threads implementation requires thread support for C.\nAnswer 2: Fortunately, there is Stackless Python , which has a completely redesigned interpreter loop that avoids the C stack. It\u2019s still experimental but looks very promising. Although it is binary compatible with standard Python, it\u2019s still unclear whether Stackless will make it into the core \u2013 maybe it\u2019s just too revolutionary.\nWhy can\u2019t lambda forms contain statements? \u00b6\nPython lambda forms cannot contain statements because Python\u2019s syntactic framework can\u2019t handle statements nested inside expressions. However, in Python, this is not a serious problem. Unlike lambda forms in other languages, where they add functionality, Python lambdas are only a shorthand notation if you\u2019re too lazy to define a function.\nFunctions are already first class objects in Python, and can be declared in a local scope. Therefore the only advantage of using a lambda form instead of a locally-defined function is that you don\u2019t need to invent a name for the function \u2013 but that\u2019s just a local variable to which the function object (which is exactly the same type of object that a lambda form yields) is assigned!\nCan Python be compiled to machine code, C or some other language? \u00b6\nNot easily. Python\u2019s high level data types, dynamic typing of objects and run-time invocation of the interpreter (using eval() or exec() ) together mean that a \u201ccompiled\u201d Python program would probably consist mostly of calls into the Python run-time system, even for seemingly simple operations like x+1.\nSeveral projects described in the Python newsgroup or at past Python conferences have shown that this approach is feasible, although the speedups reached so far are only modest (e.g. 2x). Jython uses the same strategy for compiling to Java bytecode. (Jim Hugunin has demonstrated that in combination with whole-program analysis, speedups of 1000x are feasible for small demo programs. See the proceedings from the 1997 Python conference for more information.)\nInternally, Python source code is always translated into a bytecode representation, and this bytecode is then executed by the Python virtual machine. In order to avoid the overhead of repeatedly parsing and translating modules that rarely change, this byte code is written into a file whose name ends in \u201c.pyc\u201d whenever a module is parsed. When the corresponding .py file is changed, it is parsed and translated again and the .pyc file is rewritten.\nThere is no performance difference once the .pyc file has been loaded, as the bytecode read from the .pyc file is exactly the same as the bytecode created by direct translation. The only difference is that loading code from a .pyc file is faster than parsing and translating a .py file, so the presence of precompiled .pyc files improves the start-up time of Python scripts. If desired, the Lib/compileall.py module can be used to create valid .pyc files for a given set of modules.\nNote that the main script executed by Python, even if its filename ends in .py, is not compiled to a .pyc file. It is compiled to bytecode, but the bytecode is not saved to a file. Usually main scripts are quite short, so this doesn\u2019t cost much speed.\nThere are also several programs which make it easier to intermingle Python and C code in various ways to increase performance. See, for example, Psyco , Pyrex , PyInline , Py2Cmod , and Weave .\nHow does Python manage memory? \u00b6\nThe details of Python memory management depend on the implementation. The standard C implementation of Python uses reference counting to detect inaccessible objects, and another mechanism to collect reference cycles, periodically executing a cycle detection algorithm which looks for inaccessible cycles and deletes the objects involved. The gc module provides functions to perform a garbage collection, obtain debugging statistics, and tune the collector\u2019s parameters.\nJython relies on the Java runtime so the JVM\u2019s garbage collector is used. This difference can cause some subtle porting problems if your Python code depends on the behavior of the reference counting implementation.\nIn the absence of circularities, Python programs do not need to manage memory explicitly.\nWhy doesn\u2019t Python use a more traditional garbage collection scheme? For one thing, this is not a C standard feature and hence it\u2019s not portable. (Yes, we know about the Boehm GC library. It has bits of assembler code for most common platforms, not for all of them, and although it is mostly transparent, it isn\u2019t completely transparent; patches are required to get Python to work with it.)\nTraditional GC also becomes a problem when Python is embedded into other applications. While in a standalone Python it\u2019s fine to replace the standard malloc() and free() with versions provided by the GC library, an application embedding Python may want to have its own substitute for malloc() and free(), and may not want Python\u2019s. Right now, Python works with anything that implements malloc() and free() properly.\nIn Jython, the following code (which is fine in CPython) will probably run out of file descriptors long before it runs out of memory:\nfor file in very_long_list_of_files: f = open(file) c = f.read(1)\nUsing the current reference counting and destructor scheme, each new assignment to f closes the previous file. Using GC, this is not guaranteed. If you want to write code that will work with any Python implementation, you should explicitly close the file or use the with statement; this will work regardless of GC:\nfor file in very_long_list_of_files: with open(file) as f: c = f.read(1)\nWhy isn\u2019t all memory freed when Python exits? \u00b6\nObjects referenced from the global namespaces of Python modules are not always deallocated when Python exits. This may happen if there are circular references. There are also certain bits of memory that are allocated by the C library that are impossible to free (e.g. a tool like Purify will complain about these). Python is, however, aggressive about cleaning up memory on exit and does try to destroy every single object.\nIf you want to force Python to delete certain things on deallocation use the atexit module to run a function that will force those deletions.\nWhy are there separate tuple and list data types? \u00b6\nLists and tuples, while similar in many respects, are generally used in fundamentally different ways. Tuples can be thought of as being similar to Pascal records or C structs; they\u2019re small collections of related data which may be of different types which are operated on as a group. For example, a Cartesian coordinate is appropriately represented as a tuple of two or three numbers.\nLists, on the other hand, are more like arrays in other languages. They tend to hold a varying number of objects all of which have the same type and which are operated on one-by-one. For example, os.listdir('.') returns a list of strings representing the files in the current directory. Functions which operate on this output would generally not break if you added another file or two to the directory.\nTuples are immutable, meaning that once a tuple has been created, you can\u2019t replace any of its elements with a new value. Lists are mutable, meaning that you can always change a list\u2019s elements. Only immutable elements can be used as dictionary keys, and hence only tuples and not lists can be used as keys.\nHow are lists implemented? \u00b6\nPython\u2019s lists are really variable-length arrays, not Lisp-style linked lists. The implementation uses a contiguous array of references to other objects, and keeps a pointer to this array and the array\u2019s length in a list head structure.\nThis makes indexing a list a[i] an operation whose cost is independent of the size of the list or the value of the index.\nWhen items are appended or inserted, the array of references is resized. Some cleverness is applied to improve the performance of appending items repeatedly; when the array must be grown, some extra space is allocated so the next few times don\u2019t require an actual resize.\nHow are dictionaries implemented? \u00b6\nPython\u2019s dictionaries are implemented as resizable hash tables. Compared to B-trees, this gives better performance for lookup (the most common operation by far) under most circumstances, and the implementation is simpler.\nDictionaries work by computing a hash code for each key stored in the dictionary using the hash() built-in function. The hash code varies widely depending on the key; for example, \u201cPython\u201d hashes to -539294296 while \u201cpython\u201d, a string that differs by a single bit, hashes to 1142331976. The hash code is then used to calculate a location in an internal array where the value will be stored. Assuming that you\u2019re storing keys that all have different hash values, this means that dictionaries take constant time \u2013 O(1), in computer science notation \u2013 to retrieve a key. It also means that no sorted order of the keys is maintained, and traversing the array as the .keys() and .items() do will output the dictionary\u2019s content in some arbitrary jumbled order.\nWhy must dictionary keys be immutable? \u00b6\nThe hash table implementation of dictionaries uses a hash value calculated from the key value to find the key. If the key were a mutable object, its value could change, and thus its hash could also change. But since whoever changes the key object can\u2019t tell that it was being used as a dictionary key, it can\u2019t move the entry around in the dictionary. Then, when you try to look up the same object in the dictionary it won\u2019t be found because its hash value is different. If you tried to look up the old value it wouldn\u2019t be found either, because the value of the object found in that hash bin would be different.\nIf you want a dictionary indexed with a list, simply convert the list to a tuple first; the function tuple(L) creates a tuple with the same entries as the list L. Tuples are immutable and can therefore be used as dictionary keys.\nSome unacceptable solutions that have been proposed:\nHash lists by their address (object ID). This doesn\u2019t work because if you construct a new list with the same value it won\u2019t be found; e.g.:\nmydict = {[1, 2]: '12'} print(mydict[[1, 2]])\nwould raise a KeyError exception because the id of the [1, 2] used in the second line differs from that in the first line. In other words, dictionary keys should be compared using ==, not using is .\nMake a copy when using a list as a key. This doesn\u2019t work because the list, being a mutable object, could contain a reference to itself, and then the copying code would run into an infinite loop.\nAllow lists as keys but tell the user not to modify them. This would allow a class of hard-to-track bugs in programs when you forgot or modified a list by accident. It also invalidates an important invariant of dictionaries: every value in d.keys() is usable as a key of the dictionary.\nMark lists as read-only once they are used as a dictionary key. The problem is that it\u2019s not just the top-level object that could change its value; you could use a tuple containing a list as a key. Entering anything as a key into a dictionary would require marking all objects reachable from there as read-only \u2013 and again, self-referential objects could cause an infinite loop.\nThere is a trick to get around this if you need to, but use it at your own risk: You can wrap a mutable structure inside a class instance which has both a __eq__() and a __hash__() method. You must then make sure that the hash value for all such wrapper objects that reside in a dictionary (or other hash based structure), remain fixed while the object is in the dictionary (or other structure).\nclass ListWrapper: def __init__(self, the_list): self.the_list = the_list def __eq__(self, other): return self.the_list == other.the_list def __hash__(self): l = self.the_list result = 98767 - len(l)*555 for i, el in enumerate(l): try: result = result + (hash(el) % 9999999) * 1001 + i except Exception: result = (result % 7777777) + i * 333 return result\nNote that the hash computation is complicated by the possibility that some members of the list may be unhashable and also by the possibility of arithmetic overflow.\nFurthermore it must always be the case that if o1 == o2 (ie o1.__eq__(o2) is True) then hash(o1) == hash(o2) (ie, o1.__hash__() == o2.__hash__()), regardless of whether the object is in a dictionary or not. If you fail to meet these restrictions dictionaries and other hash based structures will misbehave.\nIn the case of ListWrapper, whenever the wrapper object is in a dictionary the wrapped list must not change to avoid anomalies. Don\u2019t do this unless you are prepared to think hard about the requirements and the consequences of not meeting them correctly. Consider yourself warned.\nWhy doesn\u2019t list.sort() return the sorted list? \u00b6\nIn situations where performance matters, making a copy of the list just to sort it would be wasteful. Therefore, list.sort() sorts the list in place. In order to remind you of that fact, it does not return the sorted list. This way, you won\u2019t be fooled into accidentally overwriting a list when you need a sorted copy but also need to keep the unsorted version around.\nIn Python 2.4 a new builtin \u2013 sorted() \u2013 has been added. This function creates a new list from a provided iterable, sorts it and returns it. For example, here\u2019s how to iterate over the keys of a dictionary in sorted order:\nfor key in sorted(mydict): ... # do whatever with mydict[key]...\nHow do you specify and enforce an interface spec in Python? \u00b6\nAn interface specification for a module as provided by languages such as C++ and Java describes the prototypes for the methods and functions of the module. Many feel that compile-time enforcement of interface specifications helps in the construction of large programs.\nPython 2.6 adds an abc module that lets you define Abstract Base Classes (ABCs). You can then use isinstance() and issubclass() to check whether an instance or a class implements a particular ABC. The collections modules defines a set of useful ABCs such as Iterable, Container, and MutableMapping.\nFor Python, many of the advantages of interface specifications can be obtained by an appropriate test discipline for components. There is also a tool, PyChecker, which can be used to find problems due to subclassing.\nA good test suite for a module can both provide a regression test and serve as a module interface specification and a set of examples. Many Python modules can be run as a script to provide a simple \u201cself test.\u201d Even modules which use complex external interfaces can often be tested in isolation using trivial \u201cstub\u201d emulations of the external interface. The doctest and unittest modules or third-party test frameworks can be used to construct exhaustive test suites that exercise every line of code in a module.\nAn appropriate testing discipline can help build large complex applications in Python as well as having interface specifications would. In fact, it can be better because an interface specification cannot test certain properties of a program. For example, the append() method is expected to add new elements to the end of some internal list; an interface specification cannot test that your append() implementation will actually do this correctly, but it\u2019s trivial to check this property in a test suite.\nWriting test suites is very helpful, and you might want to design your code with an eye to making it easily tested. One increasingly popular technique, test-directed development, calls for writing parts of the test suite first, before you write any of the actual code. Of course Python allows you to be sloppy and not write test cases at all.\nWhy are default values shared between objects? \u00b6\nThis type of bug commonly bites neophyte programmers. Consider this function:\ndef foo(mydict={}): # Danger: shared reference to one dict for all calls ... compute something ... mydict[key] = value return mydict\nThe first time you call this function, mydict contains a single item. The second time, mydict contains two items because when foo() begins executing, mydict starts out with an item already in it.\nIt is often expected that a function call creates new objects for default values. This is not what happens. Default values are created exactly once, when the function is defined. If that object is changed, like the dictionary in this example, subsequent calls to the function will refer to this changed object.\nBy definition, immutable objects such as numbers, strings, tuples, and None, are safe from change. Changes to mutable objects such as dictionaries, lists, and class instances can lead to confusion.\nBecause of this feature, it is good programming practice to not use mutable objects as default values. Instead, use None as the default value and inside the function, check if the parameter is None and create a new list/dictionary/whatever if it is. For example, don\u2019t write:\ndef foo(mydict={}): ...\nbut:\ndef foo(mydict=None): if mydict is None: mydict = {} # create a new dict for local namespace\nThis feature can be useful. When you have a function that\u2019s time-consuming to compute, a common technique is to cache the parameters and the resulting value of each call to the function, and return the cached value if the same value is requested again. This is called \u201cmemoizing\u201d, and can be implemented like this:\n# Callers will never provide a third parameter for this function. def expensive (arg1, arg2, _cache={}): if (arg1, arg2) in _cache: return _cache[(arg1, arg2)] # Calculate the value result = ... expensive computation ... _cache[(arg1, arg2)] = result # Store result in the cache return result\nYou could use a global variable containing a dictionary instead of the default value; it\u2019s a matter of taste.\nWhy is there no goto? \u00b6\nYou can use exceptions to provide a \u201cstructured goto\u201d that even works across function calls. Many feel that exceptions can conveniently emulate all reasonable uses of the \u201cgo\u201d or \u201cgoto\u201d constructs of C, Fortran, and other languages. For example:\nclass label: pass # declare a label try: ... if (condition): raise label() # goto label ... except label: # where to goto pass ...\nThis doesn\u2019t allow you to jump into the middle of a loop, but that\u2019s usually considered an abuse of goto anyway. Use sparingly.\nWhy can\u2019t raw strings (r-strings) end with a backslash? \u00b6\nMore precisely, they can\u2019t end with an odd number of backslashes: the unpaired backslash at the end escapes the closing quote character, leaving an unterminated string.\nRaw strings were designed to ease creating input for processors (chiefly regular expression engines) that want to do their own backslash escape processing. Such processors consider an unmatched trailing backslash to be an error anyway, so raw strings disallow that. In return, they allow you to pass on the string quote character by escaping it with a backslash. These rules work well when r-strings are used for their intended purpose.\nIf you\u2019re trying to build Windows pathnames, note that all Windows system calls accept forward slashes too:\nf = open(\"/mydir/file.txt\") # works fine!\nIf you\u2019re trying to build a pathname for a DOS command, try e.g. one of\ndir = r\"\\this\\is\\my\\dos\\dir\" \"\\\\\" dir = r\"\\this\\is\\my\\dos\\dir\\ \"[:-1] dir = \"\\\\this\\\\is\\\\my\\\\dos\\\\dir\\\\\"\nWhy doesn\u2019t Python have a \u201cwith\u201d statement for attribute assignments? \u00b6\nPython has a \u2018with\u2019 statement that wraps the execution of a block, calling code on the entrance and exit from the block. Some language have a construct that looks like this:\nwith obj: a = 1 # equivalent to obj.a = 1 total = total + 1 # obj.total = obj.total + 1\nIn Python, such a construct would be ambiguous.\nOther languages, such as Object Pascal, Delphi, and C++, use static types, so it\u2019s possible to know, in an unambiguous way, what member is being assigned to. This is the main point of static typing \u2013 the compiler always knows the scope of every variable at compile time.\nPython uses dynamic types. It is impossible to know in advance which attribute will be referenced at runtime. Member attributes may be added or removed from objects on the fly. This makes it impossible to know, from a simple reading, what attribute is being referenced: a local one, a global one, or a member attribute?\nFor instance, take the following incomplete snippet:\ndef foo(a): with a: print(x)\nThe snippet assumes that \u201ca\u201d must have a member attribute called \u201cx\u201d. However, there is nothing in Python that tells the interpreter this. What should happen if \u201ca\u201d is, let us say, an integer? If there is a global variable named \u201cx\u201d, will it be used inside the with block? As you see, the dynamic nature of Python makes such choices much harder.\nThe primary benefit of \u201cwith\u201d and similar language features (reduction of code volume) can, however, easily be achieved in Python by assignment. Instead of:\nfunction(args).mydict[index][index].a = 21 function(args).mydict[index][index].b = 42 function(args).mydict[index][index].c = 63\nwrite this:\nref = function(args).mydict[index][index] ref.a = 21 ref.b = 42 ref.c = 63\nThis also has the side-effect of increasing execution speed because name bindings are resolved at run-time in Python, and the second version only needs to perform the resolution once.\nWhy are colons required for the if/while/def/class statements? \u00b6\nThe colon is required primarily to enhance readability (one of the results of the experimental ABC language). Consider this:\nif a == b print(a)\nversus\nif a == b: print(a)\nNotice how the second one is slightly easier to read. Notice further how a colon sets off the example in this FAQ answer; it\u2019s a standard usage in English.\nAnother minor reason is that the colon makes it easier for editors with syntax highlighting; they can look for colons to decide when indentation needs to be increased instead of having to do a more elaborate parsing of the program text.\nWhy does Python allow commas at the end of lists and tuples? \u00b6\nPython lets you add a trailing comma at the end of lists, tuples, and dictionaries:\n[1, 2, 3,] ('a', 'b', 'c',) d = { \"A\": [1, 5], \"B\": [6, 7], # last trailing comma is optional but good style }\nThere are several reasons to allow this.\nWhen you have a literal value for a list, tuple, or dictionary spread across multiple lines, it\u2019s easier to add more elements because you don\u2019t have to remember to add a comma to the previous line. The lines can also be sorted in your editor without creating a syntax error.\nAccidentally omitting the comma can lead to errors that are hard to diagnose. For example:\nx = [ \"fee\", \"fie\" \"foo\", \"fum\" ]\nThis list looks like it has four elements, but it actually contains three: \u201cfee\u201d, \u201cfiefoo\u201d and \u201cfum\u201d. Always adding the comma avoids this source of error.\nAllowing the trailing comma may also make programmatic code generation easier.\n"}, {"score": 844.7461, "uuid": "6bf6b330-e3ff-5b80-8ba0-436e924c7049", "index": "cw12", "trec_id": "clueweb12-0011wb-57-20135", "target_hostname": "wingide.com", "target_uri": "http://wingide.com/psupport/python-manual/3.2/faq/design.html", "page_rank": 1.209888e-09, "spam_rank": 71, "title": "Design and History FAQ \u2014 <em>Python</em> v3.2.1 documentation", "snippet": "o Can <em>Python</em> be compiled to machine code, C <em>or</em> some other language? o Why are there separate tuple and list <em>data</em> types? o How do you specify and enforce an interface spec in <em>Python</em>? o Why can\u2019t raw strings (<em>r</em>-strings) end with a backslash?", "explanation": null, "document": "Design and History FAQ \u00b6\nWhy does Python use indentation for grouping of statements? \u00b6\nGuido van Rossum believes that using indentation for grouping is extremely elegant and contributes a lot to the clarity of the average Python program. Most people learn to love this feature after a while.\nSince there are no begin/end brackets there cannot be a disagreement between grouping perceived by the parser and the human reader. Occasionally C programmers will encounter a fragment of code like this:\nif (x <= y) x++; y--; z++;\nOnly the x++ statement is executed if the condition is true, but the indentation leads you to believe otherwise. Even experienced C programmers will sometimes stare at it a long time wondering why y is being decremented even for x > y.\nBecause there are no begin/end brackets, Python is much less prone to coding-style conflicts. In C there are many different ways to place the braces. If you\u2019re used to reading and writing code that uses one style, you will feel at least slightly uneasy when reading (or being required to write) another style.\nMany coding styles place begin/end brackets on a line by themselves. This makes programs considerably longer and wastes valuable screen space, making it harder to get a good overview of a program. Ideally, a function should fit on one screen (say, 20-30 lines). 20 lines of Python can do a lot more work than 20 lines of C. This is not solely due to the lack of begin/end brackets \u2013 the lack of declarations and the high-level data types are also responsible \u2013 but the indentation-based syntax certainly helps.\nWhy am I getting strange results with simple arithmetic operations? \u00b6\nSee the next question.\nWhy are floating point calculations so inaccurate? \u00b6\nPeople are often very surprised by results like this:\n>>> 1.2 - 1.0 0.199999999999999996\nand think it is a bug in Python. It\u2019s not. This has nothing to do with Python, but with how the underlying C platform handles floating point numbers, and ultimately with the inaccuracies introduced when writing down numbers as a string of a fixed number of digits.\nThe internal representation of floating point numbers uses a fixed number of binary digits to represent a decimal number. Some decimal numbers can\u2019t be represented exactly in binary, resulting in small roundoff errors.\nIn decimal math, there are many numbers that can\u2019t be represented with a fixed number of decimal digits, e.g. 1/3 = 0.3333333333.......\nIn base 2, 1/2 = 0.1, 1/4 = 0.01, 1/8 = 0.001, etc. .2 equals 2/10 equals 1/5, resulting in the binary fractional number 0.001100110011001...\nFloating point numbers only have 32 or 64 bits of precision, so the digits are cut off at some point, and the resulting number is 0.199999999999999996 in decimal, not 0.2.\nA floating point number\u2019s repr() function prints as many digits are necessary to make eval(repr(f)) == f true for any float f. The str() function prints fewer digits and this often results in the more sensible number that was probably intended:\n>>> 1.1 - 0.9 0.20000000000000007 >>> print(1.1 - 0.9) 0.2\nOne of the consequences of this is that it is error-prone to compare the result of some computation to a float with ==. Tiny inaccuracies may mean that == fails. Instead, you have to check that the difference between the two numbers is less than a certain threshold:\nepsilon = 0.0000000000001 # Tiny allowed error expected_result = 0.4 if expected_result-epsilon <= computation() <= expected_result+epsilon: ...\nPlease see the chapter on floating point arithmetic in the Python tutorial for more information.\nWhy are Python strings immutable? \u00b6\nThere are several advantages.\nOne is performance: knowing that a string is immutable means we can allocate space for it at creation time, and the storage requirements are fixed and unchanging. This is also one of the reasons for the distinction between tuples and lists.\nAnother advantage is that strings in Python are considered as \u201celemental\u201d as numbers. No amount of activity will change the value 8 to anything else, and in Python, no amount of activity will change the string \u201ceight\u201d to anything else.\nWhy must \u2018self\u2019 be used explicitly in method definitions and calls? \u00b6\nThe idea was borrowed from Modula-3. It turns out to be very useful, for a variety of reasons.\nFirst, it\u2019s more obvious that you are using a method or instance attribute instead of a local variable. Reading self.x or self.meth() makes it absolutely clear that an instance variable or method is used even if you don\u2019t know the class definition by heart. In C++, you can sort of tell by the lack of a local variable declaration (assuming globals are rare or easily recognizable) \u2013 but in Python, there are no local variable declarations, so you\u2019d have to look up the class definition to be sure. Some C++ and Java coding standards call for instance attributes to have an m_ prefix, so this explicitness is still useful in those languages, too.\nSecond, it means that no special syntax is necessary if you want to explicitly reference or call the method from a particular class. In C++, if you want to use a method from a base class which is overridden in a derived class, you have to use the :: operator \u2013 in Python you can write baseclass.methodname(self, <argument list>). This is particularly useful for __init__() methods, and in general in cases where a derived class method wants to extend the base class method of the same name and thus has to call the base class method somehow.\nFinally, for instance variables it solves a syntactic problem with assignment: since local variables in Python are (by definition!) those variables to which a value is assigned in a function body (and that aren\u2019t explicitly declared global), there has to be some way to tell the interpreter that an assignment was meant to assign to an instance variable instead of to a local variable, and it should preferably be syntactic (for efficiency reasons). C++ does this through declarations, but Python doesn\u2019t have declarations and it would be a pity having to introduce them just for this purpose. Using the explicit self.var solves this nicely. Similarly, for using instance variables, having to write self.var means that references to unqualified names inside a method don\u2019t have to search the instance\u2019s directories. To put it another way, local variables and instance variables live in two different namespaces, and you need to tell Python which namespace to use.\nWhy can\u2019t I use an assignment in an expression? \u00b6\nMany people used to C or Perl complain that they want to use this C idiom:\nwhile (line = readline(f)) { // do something with line }\nwhere in Python you\u2019re forced to write this:\nwhile True: line = f.readline() if not line: break ... # do something with line\nThe reason for not allowing assignment in Python expressions is a common, hard-to-find bug in those other languages, caused by this construct:\nif (x = 0) { // error handling } else { // code that only works for nonzero x }\nThe error is a simple typo: x = 0, which assigns 0 to the variable x, was written while the comparison x == 0 is certainly what was intended.\nMany alternatives have been proposed. Most are hacks that save some typing but use arbitrary or cryptic syntax or keywords, and fail the simple criterion for language change proposals: it should intuitively suggest the proper meaning to a human reader who has not yet been introduced to the construct.\nAn interesting phenomenon is that most experienced Python programmers recognize the while True idiom and don\u2019t seem to be missing the assignment in expression construct much; it\u2019s only newcomers who express a strong desire to add this to the language.\nThere\u2019s an alternative way of spelling this that seems attractive but is generally less robust than the \u201cwhile True\u201d solution:\nline = f.readline() while line: ... # do something with line... line = f.readline()\nThe problem with this is that if you change your mind about exactly how you get the next line (e.g. you want to change it into sys.stdin.readline()) you have to remember to change two places in your program \u2013 the second occurrence is hidden at the bottom of the loop.\nThe best approach is to use iterators, making it possible to loop through objects using the for statement. For example, file objects support the iterator protocol, so you can write simply:\nfor line in f: ... # do something with line...\nWhy does Python use methods for some functionality (e.g. list.index()) but functions for other (e.g. len(list))? \u00b6\nThe major reason is history. Functions were used for those operations that were generic for a group of types and which were intended to work even for objects that didn\u2019t have methods at all (e.g. tuples). It is also convenient to have a function that can readily be applied to an amorphous collection of objects when you use the functional features of Python (map(), apply() et al).\nIn fact, implementing len(), max(), min() as a built-in function is actually less code than implementing them as methods for each type. One can quibble about individual cases but it\u2019s a part of Python, and it\u2019s too late to make such fundamental changes now. The functions have to remain to avoid massive code breakage.\nNote\nFor string operations, Python has moved from external functions (the string module) to methods. However, len() is still a function.\nWhy is join() a string method instead of a list or tuple method? \u00b6\nStrings became much more like other standard types starting in Python 1.6, when methods were added which give the same functionality that has always been available using the functions of the string module. Most of these new methods have been widely accepted, but the one which appears to make some programmers feel uncomfortable is:\n\", \".join(['1', '2', '4', '8', '16'])\nwhich gives the result:\n\"1, 2, 4, 8, 16\"\nThere are two common arguments against this usage.\nThe first runs along the lines of: \u201cIt looks really ugly using a method of a string literal (string constant)\u201d, to which the answer is that it might, but a string literal is just a fixed value. If the methods are to be allowed on names bound to strings there is no logical reason to make them unavailable on literals.\nThe second objection is typically cast as: \u201cI am really telling a sequence to join its members together with a string constant\u201d. Sadly, you aren\u2019t. For some reason there seems to be much less difficulty with having split() as a string method, since in that case it is easy to see that\n\"1, 2, 4, 8, 16\".split(\", \")\nis an instruction to a string literal to return the substrings delimited by the given separator (or, by default, arbitrary runs of white space).\njoin() is a string method because in using it you are telling the separator string to iterate over a sequence of strings and insert itself between adjacent elements. This method can be used with any argument which obeys the rules for sequence objects, including any new classes you might define yourself. Similar methods exist for bytes and bytearray objects.\nHow fast are exceptions? \u00b6\nA try/except block is extremely efficient. Actually catching an exception is expensive. In versions of Python prior to 2.0 it was common to use this idiom:\ntry: value = mydict[key] except KeyError: mydict[key] = getvalue(key) value = mydict[key]\nThis only made sense when you expected the dict to have the key almost all the time. If that wasn\u2019t the case, you coded it like this:\nif mydict.has_key(key): value = mydict[key] else: mydict[key] = getvalue(key) value = mydict[key]\nFor this specific case, you could also use value = dict.setdefault(key, getvalue(key)), but only if the getvalue() call is cheap enough because it is evaluated in all cases.\nWhy isn\u2019t there a switch or case statement in Python? \u00b6\nYou can do this easily enough with a sequence of if... elif... elif... else. There have been some proposals for switch statement syntax, but there is no consensus (yet) on whether and how to do range tests. See PEP 275 for complete details and the current status.\nFor cases where you need to choose from a very large number of possibilities, you can create a dictionary mapping case values to functions to call. For example:\ndef function_1(...): ... functions = {'a': function_1, 'b': function_2, 'c': self.method_1, ...} func = functions[value] func()\nFor calling methods on objects, you can simplify yet further by using the getattr() built-in to retrieve methods with a particular name:\ndef visit_a(self, ...): ... ... def dispatch(self, value): method_name = 'visit_' + str(value) method = getattr(self, method_name) method()\nIt\u2019s suggested that you use a prefix for the method names, such as visit_ in this example. Without such a prefix, if values are coming from an untrusted source, an attacker would be able to call any method on your object.\nCan\u2019t you emulate threads in the interpreter instead of relying on an OS-specific thread implementation? \u00b6\nAnswer 1: Unfortunately, the interpreter pushes at least one C stack frame for each Python stack frame. Also, extensions can call back into Python at almost random moments. Therefore, a complete threads implementation requires thread support for C.\nAnswer 2: Fortunately, there is Stackless Python , which has a completely redesigned interpreter loop that avoids the C stack. It\u2019s still experimental but looks very promising. Although it is binary compatible with standard Python, it\u2019s still unclear whether Stackless will make it into the core \u2013 maybe it\u2019s just too revolutionary.\nWhy can\u2019t lambda forms contain statements? \u00b6\nPython lambda forms cannot contain statements because Python\u2019s syntactic framework can\u2019t handle statements nested inside expressions. However, in Python, this is not a serious problem. Unlike lambda forms in other languages, where they add functionality, Python lambdas are only a shorthand notation if you\u2019re too lazy to define a function.\nFunctions are already first class objects in Python, and can be declared in a local scope. Therefore the only advantage of using a lambda form instead of a locally-defined function is that you don\u2019t need to invent a name for the function \u2013 but that\u2019s just a local variable to which the function object (which is exactly the same type of object that a lambda form yields) is assigned!\nCan Python be compiled to machine code, C or some other language? \u00b6\nNot easily. Python\u2019s high level data types, dynamic typing of objects and run-time invocation of the interpreter (using eval() or exec() ) together mean that a \u201ccompiled\u201d Python program would probably consist mostly of calls into the Python run-time system, even for seemingly simple operations like x+1.\nSeveral projects described in the Python newsgroup or at past Python conferences have shown that this approach is feasible, although the speedups reached so far are only modest (e.g. 2x). Jython uses the same strategy for compiling to Java bytecode. (Jim Hugunin has demonstrated that in combination with whole-program analysis, speedups of 1000x are feasible for small demo programs. See the proceedings from the 1997 Python conference for more information.)\nInternally, Python source code is always translated into a bytecode representation, and this bytecode is then executed by the Python virtual machine. In order to avoid the overhead of repeatedly parsing and translating modules that rarely change, this byte code is written into a file whose name ends in \u201d.pyc\u201d whenever a module is parsed. When the corresponding .py file is changed, it is parsed and translated again and the .pyc file is rewritten.\nThere is no performance difference once the .pyc file has been loaded, as the bytecode read from the .pyc file is exactly the same as the bytecode created by direct translation. The only difference is that loading code from a .pyc file is faster than parsing and translating a .py file, so the presence of precompiled .pyc files improves the start-up time of Python scripts. If desired, the Lib/compileall.py module can be used to create valid .pyc files for a given set of modules.\nNote that the main script executed by Python, even if its filename ends in .py, is not compiled to a .pyc file. It is compiled to bytecode, but the bytecode is not saved to a file. Usually main scripts are quite short, so this doesn\u2019t cost much speed.\nThere are also several programs which make it easier to intermingle Python and C code in various ways to increase performance. See, for example, Cython , Pyrex and Weave .\nHow does Python manage memory? \u00b6\nThe details of Python memory management depend on the implementation. The standard C implementation of Python uses reference counting to detect inaccessible objects, and another mechanism to collect reference cycles, periodically executing a cycle detection algorithm which looks for inaccessible cycles and deletes the objects involved. The gc module provides functions to perform a garbage collection, obtain debugging statistics, and tune the collector\u2019s parameters.\nJython relies on the Java runtime so the JVM\u2019s garbage collector is used. This difference can cause some subtle porting problems if your Python code depends on the behavior of the reference counting implementation.\nIn the absence of circularities, Python programs do not need to manage memory explicitly.\nWhy doesn\u2019t Python use a more traditional garbage collection scheme? For one thing, this is not a C standard feature and hence it\u2019s not portable. (Yes, we know about the Boehm GC library. It has bits of assembler code for most common platforms, not for all of them, and although it is mostly transparent, it isn\u2019t completely transparent; patches are required to get Python to work with it.)\nTraditional GC also becomes a problem when Python is embedded into other applications. While in a standalone Python it\u2019s fine to replace the standard malloc() and free() with versions provided by the GC library, an application embedding Python may want to have its own substitute for malloc() and free(), and may not want Python\u2019s. Right now, Python works with anything that implements malloc() and free() properly.\nIn Jython, the following code (which is fine in CPython) will probably run out of file descriptors long before it runs out of memory:\nfor file in very_long_list_of_files: f = open(file) c = f.read(1)\nUsing the current reference counting and destructor scheme, each new assignment to f closes the previous file. Using GC, this is not guaranteed. If you want to write code that will work with any Python implementation, you should explicitly close the file or use the with statement; this will work regardless of GC:\nfor file in very_long_list_of_files: with open(file) as f: c = f.read(1)\nWhy isn\u2019t all memory freed when Python exits? \u00b6\nObjects referenced from the global namespaces of Python modules are not always deallocated when Python exits. This may happen if there are circular references. There are also certain bits of memory that are allocated by the C library that are impossible to free (e.g. a tool like Purify will complain about these). Python is, however, aggressive about cleaning up memory on exit and does try to destroy every single object.\nIf you want to force Python to delete certain things on deallocation use the atexit module to run a function that will force those deletions.\nWhy are there separate tuple and list data types? \u00b6\nLists and tuples, while similar in many respects, are generally used in fundamentally different ways. Tuples can be thought of as being similar to Pascal records or C structs; they\u2019re small collections of related data which may be of different types which are operated on as a group. For example, a Cartesian coordinate is appropriately represented as a tuple of two or three numbers.\nLists, on the other hand, are more like arrays in other languages. They tend to hold a varying number of objects all of which have the same type and which are operated on one-by-one. For example, os.listdir('.') returns a list of strings representing the files in the current directory. Functions which operate on this output would generally not break if you added another file or two to the directory.\nTuples are immutable, meaning that once a tuple has been created, you can\u2019t replace any of its elements with a new value. Lists are mutable, meaning that you can always change a list\u2019s elements. Only immutable elements can be used as dictionary keys, and hence only tuples and not lists can be used as keys.\nHow are lists implemented? \u00b6\nPython\u2019s lists are really variable-length arrays, not Lisp-style linked lists. The implementation uses a contiguous array of references to other objects, and keeps a pointer to this array and the array\u2019s length in a list head structure.\nThis makes indexing a list a[i] an operation whose cost is independent of the size of the list or the value of the index.\nWhen items are appended or inserted, the array of references is resized. Some cleverness is applied to improve the performance of appending items repeatedly; when the array must be grown, some extra space is allocated so the next few times don\u2019t require an actual resize.\nHow are dictionaries implemented? \u00b6\nPython\u2019s dictionaries are implemented as resizable hash tables. Compared to B-trees, this gives better performance for lookup (the most common operation by far) under most circumstances, and the implementation is simpler.\nDictionaries work by computing a hash code for each key stored in the dictionary using the hash() built-in function. The hash code varies widely depending on the key; for example, \u201cPython\u201d hashes to -539294296 while \u201cpython\u201d, a string that differs by a single bit, hashes to 1142331976. The hash code is then used to calculate a location in an internal array where the value will be stored. Assuming that you\u2019re storing keys that all have different hash values, this means that dictionaries take constant time \u2013 O(1), in computer science notation \u2013 to retrieve a key. It also means that no sorted order of the keys is maintained, and traversing the array as the .keys() and .items() do will output the dictionary\u2019s content in some arbitrary jumbled order.\nWhy must dictionary keys be immutable? \u00b6\nThe hash table implementation of dictionaries uses a hash value calculated from the key value to find the key. If the key were a mutable object, its value could change, and thus its hash could also change. But since whoever changes the key object can\u2019t tell that it was being used as a dictionary key, it can\u2019t move the entry around in the dictionary. Then, when you try to look up the same object in the dictionary it won\u2019t be found because its hash value is different. If you tried to look up the old value it wouldn\u2019t be found either, because the value of the object found in that hash bin would be different.\nIf you want a dictionary indexed with a list, simply convert the list to a tuple first; the function tuple(L) creates a tuple with the same entries as the list L. Tuples are immutable and can therefore be used as dictionary keys.\nSome unacceptable solutions that have been proposed:\nHash lists by their address (object ID). This doesn\u2019t work because if you construct a new list with the same value it won\u2019t be found; e.g.:\nmydict = {[1, 2]: '12'} print(mydict[[1, 2]])\nwould raise a KeyError exception because the id of the [1, 2] used in the second line differs from that in the first line. In other words, dictionary keys should be compared using ==, not using is .\nMake a copy when using a list as a key. This doesn\u2019t work because the list, being a mutable object, could contain a reference to itself, and then the copying code would run into an infinite loop.\nAllow lists as keys but tell the user not to modify them. This would allow a class of hard-to-track bugs in programs when you forgot or modified a list by accident. It also invalidates an important invariant of dictionaries: every value in d.keys() is usable as a key of the dictionary.\nMark lists as read-only once they are used as a dictionary key. The problem is that it\u2019s not just the top-level object that could change its value; you could use a tuple containing a list as a key. Entering anything as a key into a dictionary would require marking all objects reachable from there as read-only \u2013 and again, self-referential objects could cause an infinite loop.\nThere is a trick to get around this if you need to, but use it at your own risk: You can wrap a mutable structure inside a class instance which has both a __eq__() and a __hash__() method. You must then make sure that the hash value for all such wrapper objects that reside in a dictionary (or other hash based structure), remain fixed while the object is in the dictionary (or other structure).\nclass ListWrapper: def __init__(self, the_list): self.the_list = the_list def __eq__(self, other): return self.the_list == other.the_list def __hash__(self): l = self.the_list result = 98767 - len(l)*555 for i, el in enumerate(l): try: result = result + (hash(el) % 9999999) * 1001 + i except Exception: result = (result % 7777777) + i * 333 return result\nNote that the hash computation is complicated by the possibility that some members of the list may be unhashable and also by the possibility of arithmetic overflow.\nFurthermore it must always be the case that if o1 == o2 (ie o1.__eq__(o2) is True) then hash(o1) == hash(o2) (ie, o1.__hash__() == o2.__hash__()), regardless of whether the object is in a dictionary or not. If you fail to meet these restrictions dictionaries and other hash based structures will misbehave.\nIn the case of ListWrapper, whenever the wrapper object is in a dictionary the wrapped list must not change to avoid anomalies. Don\u2019t do this unless you are prepared to think hard about the requirements and the consequences of not meeting them correctly. Consider yourself warned.\nWhy doesn\u2019t list.sort() return the sorted list? \u00b6\nIn situations where performance matters, making a copy of the list just to sort it would be wasteful. Therefore, list.sort() sorts the list in place. In order to remind you of that fact, it does not return the sorted list. This way, you won\u2019t be fooled into accidentally overwriting a list when you need a sorted copy but also need to keep the unsorted version around.\nIn Python 2.4 a new built-in function \u2013 sorted() \u2013 has been added. This function creates a new list from a provided iterable, sorts it and returns it. For example, here\u2019s how to iterate over the keys of a dictionary in sorted order:\nfor key in sorted(mydict): ... # do whatever with mydict[key]...\nHow do you specify and enforce an interface spec in Python? \u00b6\nAn interface specification for a module as provided by languages such as C++ and Java describes the prototypes for the methods and functions of the module. Many feel that compile-time enforcement of interface specifications helps in the construction of large programs.\nPython 2.6 adds an abc module that lets you define Abstract Base Classes (ABCs). You can then use isinstance() and issubclass() to check whether an instance or a class implements a particular ABC. The collections modules defines a set of useful ABCs such as Iterable, Container, and MutableMapping.\nFor Python, many of the advantages of interface specifications can be obtained by an appropriate test discipline for components. There is also a tool, PyChecker, which can be used to find problems due to subclassing.\nA good test suite for a module can both provide a regression test and serve as a module interface specification and a set of examples. Many Python modules can be run as a script to provide a simple \u201cself test.\u201d Even modules which use complex external interfaces can often be tested in isolation using trivial \u201cstub\u201d emulations of the external interface. The doctest and unittest modules or third-party test frameworks can be used to construct exhaustive test suites that exercise every line of code in a module.\nAn appropriate testing discipline can help build large complex applications in Python as well as having interface specifications would. In fact, it can be better because an interface specification cannot test certain properties of a program. For example, the append() method is expected to add new elements to the end of some internal list; an interface specification cannot test that your append() implementation will actually do this correctly, but it\u2019s trivial to check this property in a test suite.\nWriting test suites is very helpful, and you might want to design your code with an eye to making it easily tested. One increasingly popular technique, test-directed development, calls for writing parts of the test suite first, before you write any of the actual code. Of course Python allows you to be sloppy and not write test cases at all.\nWhy are default values shared between objects? \u00b6\nThis type of bug commonly bites neophyte programmers. Consider this function:\ndef foo(mydict={}): # Danger: shared reference to one dict for all calls ... compute something ... mydict[key] = value return mydict\nThe first time you call this function, mydict contains a single item. The second time, mydict contains two items because when foo() begins executing, mydict starts out with an item already in it.\nIt is often expected that a function call creates new objects for default values. This is not what happens. Default values are created exactly once, when the function is defined. If that object is changed, like the dictionary in this example, subsequent calls to the function will refer to this changed object.\nBy definition, immutable objects such as numbers, strings, tuples, and None, are safe from change. Changes to mutable objects such as dictionaries, lists, and class instances can lead to confusion.\nBecause of this feature, it is good programming practice to not use mutable objects as default values. Instead, use None as the default value and inside the function, check if the parameter is None and create a new list/dictionary/whatever if it is. For example, don\u2019t write:\ndef foo(mydict={}): ...\nbut:\ndef foo(mydict=None): if mydict is None: mydict = {} # create a new dict for local namespace\nThis feature can be useful. When you have a function that\u2019s time-consuming to compute, a common technique is to cache the parameters and the resulting value of each call to the function, and return the cached value if the same value is requested again. This is called \u201cmemoizing\u201d, and can be implemented like this:\n# Callers will never provide a third parameter for this function. def expensive (arg1, arg2, _cache={}): if (arg1, arg2) in _cache: return _cache[(arg1, arg2)] # Calculate the value result = ... expensive computation ... _cache[(arg1, arg2)] = result # Store result in the cache return result\nYou could use a global variable containing a dictionary instead of the default value; it\u2019s a matter of taste.\nWhy is there no goto? \u00b6\nYou can use exceptions to provide a \u201cstructured goto\u201d that even works across function calls. Many feel that exceptions can conveniently emulate all reasonable uses of the \u201cgo\u201d or \u201cgoto\u201d constructs of C, Fortran, and other languages. For example:\nclass label: pass # declare a label try: ... if (condition): raise label() # goto label ... except label: # where to goto pass ...\nThis doesn\u2019t allow you to jump into the middle of a loop, but that\u2019s usually considered an abuse of goto anyway. Use sparingly.\nWhy can\u2019t raw strings (r-strings) end with a backslash? \u00b6\nMore precisely, they can\u2019t end with an odd number of backslashes: the unpaired backslash at the end escapes the closing quote character, leaving an unterminated string.\nRaw strings were designed to ease creating input for processors (chiefly regular expression engines) that want to do their own backslash escape processing. Such processors consider an unmatched trailing backslash to be an error anyway, so raw strings disallow that. In return, they allow you to pass on the string quote character by escaping it with a backslash. These rules work well when r-strings are used for their intended purpose.\nIf you\u2019re trying to build Windows pathnames, note that all Windows system calls accept forward slashes too:\nf = open(\"/mydir/file.txt\") # works fine!\nIf you\u2019re trying to build a pathname for a DOS command, try e.g. one of\ndir = r\"\\this\\is\\my\\dos\\dir\" \"\\\\\" dir = r\"\\this\\is\\my\\dos\\dir\\ \"[:-1] dir = \"\\\\this\\\\is\\\\my\\\\dos\\\\dir\\\\\"\nWhy doesn\u2019t Python have a \u201cwith\u201d statement for attribute assignments? \u00b6\nPython has a \u2018with\u2019 statement that wraps the execution of a block, calling code on the entrance and exit from the block. Some language have a construct that looks like this:\nwith obj: a = 1 # equivalent to obj.a = 1 total = total + 1 # obj.total = obj.total + 1\nIn Python, such a construct would be ambiguous.\nOther languages, such as Object Pascal, Delphi, and C++, use static types, so it\u2019s possible to know, in an unambiguous way, what member is being assigned to. This is the main point of static typing \u2013 the compiler always knows the scope of every variable at compile time.\nPython uses dynamic types. It is impossible to know in advance which attribute will be referenced at runtime. Member attributes may be added or removed from objects on the fly. This makes it impossible to know, from a simple reading, what attribute is being referenced: a local one, a global one, or a member attribute?\nFor instance, take the following incomplete snippet:\ndef foo(a): with a: print(x)\nThe snippet assumes that \u201ca\u201d must have a member attribute called \u201cx\u201d. However, there is nothing in Python that tells the interpreter this. What should happen if \u201ca\u201d is, let us say, an integer? If there is a global variable named \u201cx\u201d, will it be used inside the with block? As you see, the dynamic nature of Python makes such choices much harder.\nThe primary benefit of \u201cwith\u201d and similar language features (reduction of code volume) can, however, easily be achieved in Python by assignment. Instead of:\nfunction(args).mydict[index][index].a = 21 function(args).mydict[index][index].b = 42 function(args).mydict[index][index].c = 63\nwrite this:\nref = function(args).mydict[index][index] ref.a = 21 ref.b = 42 ref.c = 63\nThis also has the side-effect of increasing execution speed because name bindings are resolved at run-time in Python, and the second version only needs to perform the resolution once.\nWhy are colons required for the if/while/def/class statements? \u00b6\nThe colon is required primarily to enhance readability (one of the results of the experimental ABC language). Consider this:\nif a == b print(a)\nversus\nif a == b: print(a)\nNotice how the second one is slightly easier to read. Notice further how a colon sets off the example in this FAQ answer; it\u2019s a standard usage in English.\nAnother minor reason is that the colon makes it easier for editors with syntax highlighting; they can look for colons to decide when indentation needs to be increased instead of having to do a more elaborate parsing of the program text.\nWhy does Python allow commas at the end of lists and tuples? \u00b6\nPython lets you add a trailing comma at the end of lists, tuples, and dictionaries:\n[1, 2, 3,] ('a', 'b', 'c',) d = { \"A\": [1, 5], \"B\": [6, 7], # last trailing comma is optional but good style }\nThere are several reasons to allow this.\nWhen you have a literal value for a list, tuple, or dictionary spread across multiple lines, it\u2019s easier to add more elements because you don\u2019t have to remember to add a comma to the previous line. The lines can also be sorted in your editor without creating a syntax error.\nAccidentally omitting the comma can lead to errors that are hard to diagnose. For example:\nx = [ \"fee\", \"fie\" \"foo\", \"fum\" ]\nThis list looks like it has four elements, but it actually contains three: \u201cfee\u201d, \u201cfiefoo\u201d and \u201cfum\u201d. Always adding the comma avoids this source of error.\nAllowing the trailing comma may also make programmatic code generation easier.\n"}, {"score": 844.7461, "uuid": "a231334b-69f7-5ca5-8312-9cbc4e14557e", "index": "cw12", "trec_id": "clueweb12-0002wb-96-04635", "target_hostname": "archaeopteryx.com", "target_uri": "http://archaeopteryx.com/psupport/python-manual/3.2/faq/design.html", "page_rank": 1.2099172e-09, "spam_rank": 71, "title": "Design and History FAQ \u2014 <em>Python</em> v3.2.1 documentation", "snippet": "o Can <em>Python</em> be compiled to machine code, C <em>or</em> some other language? o Why are there separate tuple and list <em>data</em> types? o How do you specify and enforce an interface spec in <em>Python</em>? o Why can\u2019t raw strings (<em>r</em>-strings) end with a backslash?", "explanation": null, "document": "Design and History FAQ \u00b6\nWhy does Python use indentation for grouping of statements? \u00b6\nGuido van Rossum believes that using indentation for grouping is extremely elegant and contributes a lot to the clarity of the average Python program. Most people learn to love this feature after a while.\nSince there are no begin/end brackets there cannot be a disagreement between grouping perceived by the parser and the human reader. Occasionally C programmers will encounter a fragment of code like this:\nif (x <= y) x++; y--; z++;\nOnly the x++ statement is executed if the condition is true, but the indentation leads you to believe otherwise. Even experienced C programmers will sometimes stare at it a long time wondering why y is being decremented even for x > y.\nBecause there are no begin/end brackets, Python is much less prone to coding-style conflicts. In C there are many different ways to place the braces. If you\u2019re used to reading and writing code that uses one style, you will feel at least slightly uneasy when reading (or being required to write) another style.\nMany coding styles place begin/end brackets on a line by themselves. This makes programs considerably longer and wastes valuable screen space, making it harder to get a good overview of a program. Ideally, a function should fit on one screen (say, 20-30 lines). 20 lines of Python can do a lot more work than 20 lines of C. This is not solely due to the lack of begin/end brackets \u2013 the lack of declarations and the high-level data types are also responsible \u2013 but the indentation-based syntax certainly helps.\nWhy am I getting strange results with simple arithmetic operations? \u00b6\nSee the next question.\nWhy are floating point calculations so inaccurate? \u00b6\nPeople are often very surprised by results like this:\n>>> 1.2 - 1.0 0.199999999999999996\nand think it is a bug in Python. It\u2019s not. This has nothing to do with Python, but with how the underlying C platform handles floating point numbers, and ultimately with the inaccuracies introduced when writing down numbers as a string of a fixed number of digits.\nThe internal representation of floating point numbers uses a fixed number of binary digits to represent a decimal number. Some decimal numbers can\u2019t be represented exactly in binary, resulting in small roundoff errors.\nIn decimal math, there are many numbers that can\u2019t be represented with a fixed number of decimal digits, e.g. 1/3 = 0.3333333333.......\nIn base 2, 1/2 = 0.1, 1/4 = 0.01, 1/8 = 0.001, etc. .2 equals 2/10 equals 1/5, resulting in the binary fractional number 0.001100110011001...\nFloating point numbers only have 32 or 64 bits of precision, so the digits are cut off at some point, and the resulting number is 0.199999999999999996 in decimal, not 0.2.\nA floating point number\u2019s repr() function prints as many digits are necessary to make eval(repr(f)) == f true for any float f. The str() function prints fewer digits and this often results in the more sensible number that was probably intended:\n>>> 1.1 - 0.9 0.20000000000000007 >>> print(1.1 - 0.9) 0.2\nOne of the consequences of this is that it is error-prone to compare the result of some computation to a float with ==. Tiny inaccuracies may mean that == fails. Instead, you have to check that the difference between the two numbers is less than a certain threshold:\nepsilon = 0.0000000000001 # Tiny allowed error expected_result = 0.4 if expected_result-epsilon <= computation() <= expected_result+epsilon: ...\nPlease see the chapter on floating point arithmetic in the Python tutorial for more information.\nWhy are Python strings immutable? \u00b6\nThere are several advantages.\nOne is performance: knowing that a string is immutable means we can allocate space for it at creation time, and the storage requirements are fixed and unchanging. This is also one of the reasons for the distinction between tuples and lists.\nAnother advantage is that strings in Python are considered as \u201celemental\u201d as numbers. No amount of activity will change the value 8 to anything else, and in Python, no amount of activity will change the string \u201ceight\u201d to anything else.\nWhy must \u2018self\u2019 be used explicitly in method definitions and calls? \u00b6\nThe idea was borrowed from Modula-3. It turns out to be very useful, for a variety of reasons.\nFirst, it\u2019s more obvious that you are using a method or instance attribute instead of a local variable. Reading self.x or self.meth() makes it absolutely clear that an instance variable or method is used even if you don\u2019t know the class definition by heart. In C++, you can sort of tell by the lack of a local variable declaration (assuming globals are rare or easily recognizable) \u2013 but in Python, there are no local variable declarations, so you\u2019d have to look up the class definition to be sure. Some C++ and Java coding standards call for instance attributes to have an m_ prefix, so this explicitness is still useful in those languages, too.\nSecond, it means that no special syntax is necessary if you want to explicitly reference or call the method from a particular class. In C++, if you want to use a method from a base class which is overridden in a derived class, you have to use the :: operator \u2013 in Python you can write baseclass.methodname(self, <argument list>). This is particularly useful for __init__() methods, and in general in cases where a derived class method wants to extend the base class method of the same name and thus has to call the base class method somehow.\nFinally, for instance variables it solves a syntactic problem with assignment: since local variables in Python are (by definition!) those variables to which a value is assigned in a function body (and that aren\u2019t explicitly declared global), there has to be some way to tell the interpreter that an assignment was meant to assign to an instance variable instead of to a local variable, and it should preferably be syntactic (for efficiency reasons). C++ does this through declarations, but Python doesn\u2019t have declarations and it would be a pity having to introduce them just for this purpose. Using the explicit self.var solves this nicely. Similarly, for using instance variables, having to write self.var means that references to unqualified names inside a method don\u2019t have to search the instance\u2019s directories. To put it another way, local variables and instance variables live in two different namespaces, and you need to tell Python which namespace to use.\nWhy can\u2019t I use an assignment in an expression? \u00b6\nMany people used to C or Perl complain that they want to use this C idiom:\nwhile (line = readline(f)) { // do something with line }\nwhere in Python you\u2019re forced to write this:\nwhile True: line = f.readline() if not line: break ... # do something with line\nThe reason for not allowing assignment in Python expressions is a common, hard-to-find bug in those other languages, caused by this construct:\nif (x = 0) { // error handling } else { // code that only works for nonzero x }\nThe error is a simple typo: x = 0, which assigns 0 to the variable x, was written while the comparison x == 0 is certainly what was intended.\nMany alternatives have been proposed. Most are hacks that save some typing but use arbitrary or cryptic syntax or keywords, and fail the simple criterion for language change proposals: it should intuitively suggest the proper meaning to a human reader who has not yet been introduced to the construct.\nAn interesting phenomenon is that most experienced Python programmers recognize the while True idiom and don\u2019t seem to be missing the assignment in expression construct much; it\u2019s only newcomers who express a strong desire to add this to the language.\nThere\u2019s an alternative way of spelling this that seems attractive but is generally less robust than the \u201cwhile True\u201d solution:\nline = f.readline() while line: ... # do something with line... line = f.readline()\nThe problem with this is that if you change your mind about exactly how you get the next line (e.g. you want to change it into sys.stdin.readline()) you have to remember to change two places in your program \u2013 the second occurrence is hidden at the bottom of the loop.\nThe best approach is to use iterators, making it possible to loop through objects using the for statement. For example, file objects support the iterator protocol, so you can write simply:\nfor line in f: ... # do something with line...\nWhy does Python use methods for some functionality (e.g. list.index()) but functions for other (e.g. len(list))? \u00b6\nThe major reason is history. Functions were used for those operations that were generic for a group of types and which were intended to work even for objects that didn\u2019t have methods at all (e.g. tuples). It is also convenient to have a function that can readily be applied to an amorphous collection of objects when you use the functional features of Python (map(), apply() et al).\nIn fact, implementing len(), max(), min() as a built-in function is actually less code than implementing them as methods for each type. One can quibble about individual cases but it\u2019s a part of Python, and it\u2019s too late to make such fundamental changes now. The functions have to remain to avoid massive code breakage.\nNote\nFor string operations, Python has moved from external functions (the string module) to methods. However, len() is still a function.\nWhy is join() a string method instead of a list or tuple method? \u00b6\nStrings became much more like other standard types starting in Python 1.6, when methods were added which give the same functionality that has always been available using the functions of the string module. Most of these new methods have been widely accepted, but the one which appears to make some programmers feel uncomfortable is:\n\", \".join(['1', '2', '4', '8', '16'])\nwhich gives the result:\n\"1, 2, 4, 8, 16\"\nThere are two common arguments against this usage.\nThe first runs along the lines of: \u201cIt looks really ugly using a method of a string literal (string constant)\u201d, to which the answer is that it might, but a string literal is just a fixed value. If the methods are to be allowed on names bound to strings there is no logical reason to make them unavailable on literals.\nThe second objection is typically cast as: \u201cI am really telling a sequence to join its members together with a string constant\u201d. Sadly, you aren\u2019t. For some reason there seems to be much less difficulty with having split() as a string method, since in that case it is easy to see that\n\"1, 2, 4, 8, 16\".split(\", \")\nis an instruction to a string literal to return the substrings delimited by the given separator (or, by default, arbitrary runs of white space).\njoin() is a string method because in using it you are telling the separator string to iterate over a sequence of strings and insert itself between adjacent elements. This method can be used with any argument which obeys the rules for sequence objects, including any new classes you might define yourself. Similar methods exist for bytes and bytearray objects.\nHow fast are exceptions? \u00b6\nA try/except block is extremely efficient. Actually catching an exception is expensive. In versions of Python prior to 2.0 it was common to use this idiom:\ntry: value = mydict[key] except KeyError: mydict[key] = getvalue(key) value = mydict[key]\nThis only made sense when you expected the dict to have the key almost all the time. If that wasn\u2019t the case, you coded it like this:\nif mydict.has_key(key): value = mydict[key] else: mydict[key] = getvalue(key) value = mydict[key]\nFor this specific case, you could also use value = dict.setdefault(key, getvalue(key)), but only if the getvalue() call is cheap enough because it is evaluated in all cases.\nWhy isn\u2019t there a switch or case statement in Python? \u00b6\nYou can do this easily enough with a sequence of if... elif... elif... else. There have been some proposals for switch statement syntax, but there is no consensus (yet) on whether and how to do range tests. See PEP 275 for complete details and the current status.\nFor cases where you need to choose from a very large number of possibilities, you can create a dictionary mapping case values to functions to call. For example:\ndef function_1(...): ... functions = {'a': function_1, 'b': function_2, 'c': self.method_1, ...} func = functions[value] func()\nFor calling methods on objects, you can simplify yet further by using the getattr() built-in to retrieve methods with a particular name:\ndef visit_a(self, ...): ... ... def dispatch(self, value): method_name = 'visit_' + str(value) method = getattr(self, method_name) method()\nIt\u2019s suggested that you use a prefix for the method names, such as visit_ in this example. Without such a prefix, if values are coming from an untrusted source, an attacker would be able to call any method on your object.\nCan\u2019t you emulate threads in the interpreter instead of relying on an OS-specific thread implementation? \u00b6\nAnswer 1: Unfortunately, the interpreter pushes at least one C stack frame for each Python stack frame. Also, extensions can call back into Python at almost random moments. Therefore, a complete threads implementation requires thread support for C.\nAnswer 2: Fortunately, there is Stackless Python , which has a completely redesigned interpreter loop that avoids the C stack. It\u2019s still experimental but looks very promising. Although it is binary compatible with standard Python, it\u2019s still unclear whether Stackless will make it into the core \u2013 maybe it\u2019s just too revolutionary.\nWhy can\u2019t lambda forms contain statements? \u00b6\nPython lambda forms cannot contain statements because Python\u2019s syntactic framework can\u2019t handle statements nested inside expressions. However, in Python, this is not a serious problem. Unlike lambda forms in other languages, where they add functionality, Python lambdas are only a shorthand notation if you\u2019re too lazy to define a function.\nFunctions are already first class objects in Python, and can be declared in a local scope. Therefore the only advantage of using a lambda form instead of a locally-defined function is that you don\u2019t need to invent a name for the function \u2013 but that\u2019s just a local variable to which the function object (which is exactly the same type of object that a lambda form yields) is assigned!\nCan Python be compiled to machine code, C or some other language? \u00b6\nNot easily. Python\u2019s high level data types, dynamic typing of objects and run-time invocation of the interpreter (using eval() or exec() ) together mean that a \u201ccompiled\u201d Python program would probably consist mostly of calls into the Python run-time system, even for seemingly simple operations like x+1.\nSeveral projects described in the Python newsgroup or at past Python conferences have shown that this approach is feasible, although the speedups reached so far are only modest (e.g. 2x). Jython uses the same strategy for compiling to Java bytecode. (Jim Hugunin has demonstrated that in combination with whole-program analysis, speedups of 1000x are feasible for small demo programs. See the proceedings from the 1997 Python conference for more information.)\nInternally, Python source code is always translated into a bytecode representation, and this bytecode is then executed by the Python virtual machine. In order to avoid the overhead of repeatedly parsing and translating modules that rarely change, this byte code is written into a file whose name ends in \u201d.pyc\u201d whenever a module is parsed. When the corresponding .py file is changed, it is parsed and translated again and the .pyc file is rewritten.\nThere is no performance difference once the .pyc file has been loaded, as the bytecode read from the .pyc file is exactly the same as the bytecode created by direct translation. The only difference is that loading code from a .pyc file is faster than parsing and translating a .py file, so the presence of precompiled .pyc files improves the start-up time of Python scripts. If desired, the Lib/compileall.py module can be used to create valid .pyc files for a given set of modules.\nNote that the main script executed by Python, even if its filename ends in .py, is not compiled to a .pyc file. It is compiled to bytecode, but the bytecode is not saved to a file. Usually main scripts are quite short, so this doesn\u2019t cost much speed.\nThere are also several programs which make it easier to intermingle Python and C code in various ways to increase performance. See, for example, Cython , Pyrex and Weave .\nHow does Python manage memory? \u00b6\nThe details of Python memory management depend on the implementation. The standard C implementation of Python uses reference counting to detect inaccessible objects, and another mechanism to collect reference cycles, periodically executing a cycle detection algorithm which looks for inaccessible cycles and deletes the objects involved. The gc module provides functions to perform a garbage collection, obtain debugging statistics, and tune the collector\u2019s parameters.\nJython relies on the Java runtime so the JVM\u2019s garbage collector is used. This difference can cause some subtle porting problems if your Python code depends on the behavior of the reference counting implementation.\nIn the absence of circularities, Python programs do not need to manage memory explicitly.\nWhy doesn\u2019t Python use a more traditional garbage collection scheme? For one thing, this is not a C standard feature and hence it\u2019s not portable. (Yes, we know about the Boehm GC library. It has bits of assembler code for most common platforms, not for all of them, and although it is mostly transparent, it isn\u2019t completely transparent; patches are required to get Python to work with it.)\nTraditional GC also becomes a problem when Python is embedded into other applications. While in a standalone Python it\u2019s fine to replace the standard malloc() and free() with versions provided by the GC library, an application embedding Python may want to have its own substitute for malloc() and free(), and may not want Python\u2019s. Right now, Python works with anything that implements malloc() and free() properly.\nIn Jython, the following code (which is fine in CPython) will probably run out of file descriptors long before it runs out of memory:\nfor file in very_long_list_of_files: f = open(file) c = f.read(1)\nUsing the current reference counting and destructor scheme, each new assignment to f closes the previous file. Using GC, this is not guaranteed. If you want to write code that will work with any Python implementation, you should explicitly close the file or use the with statement; this will work regardless of GC:\nfor file in very_long_list_of_files: with open(file) as f: c = f.read(1)\nWhy isn\u2019t all memory freed when Python exits? \u00b6\nObjects referenced from the global namespaces of Python modules are not always deallocated when Python exits. This may happen if there are circular references. There are also certain bits of memory that are allocated by the C library that are impossible to free (e.g. a tool like Purify will complain about these). Python is, however, aggressive about cleaning up memory on exit and does try to destroy every single object.\nIf you want to force Python to delete certain things on deallocation use the atexit module to run a function that will force those deletions.\nWhy are there separate tuple and list data types? \u00b6\nLists and tuples, while similar in many respects, are generally used in fundamentally different ways. Tuples can be thought of as being similar to Pascal records or C structs; they\u2019re small collections of related data which may be of different types which are operated on as a group. For example, a Cartesian coordinate is appropriately represented as a tuple of two or three numbers.\nLists, on the other hand, are more like arrays in other languages. They tend to hold a varying number of objects all of which have the same type and which are operated on one-by-one. For example, os.listdir('.') returns a list of strings representing the files in the current directory. Functions which operate on this output would generally not break if you added another file or two to the directory.\nTuples are immutable, meaning that once a tuple has been created, you can\u2019t replace any of its elements with a new value. Lists are mutable, meaning that you can always change a list\u2019s elements. Only immutable elements can be used as dictionary keys, and hence only tuples and not lists can be used as keys.\nHow are lists implemented? \u00b6\nPython\u2019s lists are really variable-length arrays, not Lisp-style linked lists. The implementation uses a contiguous array of references to other objects, and keeps a pointer to this array and the array\u2019s length in a list head structure.\nThis makes indexing a list a[i] an operation whose cost is independent of the size of the list or the value of the index.\nWhen items are appended or inserted, the array of references is resized. Some cleverness is applied to improve the performance of appending items repeatedly; when the array must be grown, some extra space is allocated so the next few times don\u2019t require an actual resize.\nHow are dictionaries implemented? \u00b6\nPython\u2019s dictionaries are implemented as resizable hash tables. Compared to B-trees, this gives better performance for lookup (the most common operation by far) under most circumstances, and the implementation is simpler.\nDictionaries work by computing a hash code for each key stored in the dictionary using the hash() built-in function. The hash code varies widely depending on the key; for example, \u201cPython\u201d hashes to -539294296 while \u201cpython\u201d, a string that differs by a single bit, hashes to 1142331976. The hash code is then used to calculate a location in an internal array where the value will be stored. Assuming that you\u2019re storing keys that all have different hash values, this means that dictionaries take constant time \u2013 O(1), in computer science notation \u2013 to retrieve a key. It also means that no sorted order of the keys is maintained, and traversing the array as the .keys() and .items() do will output the dictionary\u2019s content in some arbitrary jumbled order.\nWhy must dictionary keys be immutable? \u00b6\nThe hash table implementation of dictionaries uses a hash value calculated from the key value to find the key. If the key were a mutable object, its value could change, and thus its hash could also change. But since whoever changes the key object can\u2019t tell that it was being used as a dictionary key, it can\u2019t move the entry around in the dictionary. Then, when you try to look up the same object in the dictionary it won\u2019t be found because its hash value is different. If you tried to look up the old value it wouldn\u2019t be found either, because the value of the object found in that hash bin would be different.\nIf you want a dictionary indexed with a list, simply convert the list to a tuple first; the function tuple(L) creates a tuple with the same entries as the list L. Tuples are immutable and can therefore be used as dictionary keys.\nSome unacceptable solutions that have been proposed:\nHash lists by their address (object ID). This doesn\u2019t work because if you construct a new list with the same value it won\u2019t be found; e.g.:\nmydict = {[1, 2]: '12'} print(mydict[[1, 2]])\nwould raise a KeyError exception because the id of the [1, 2] used in the second line differs from that in the first line. In other words, dictionary keys should be compared using ==, not using is .\nMake a copy when using a list as a key. This doesn\u2019t work because the list, being a mutable object, could contain a reference to itself, and then the copying code would run into an infinite loop.\nAllow lists as keys but tell the user not to modify them. This would allow a class of hard-to-track bugs in programs when you forgot or modified a list by accident. It also invalidates an important invariant of dictionaries: every value in d.keys() is usable as a key of the dictionary.\nMark lists as read-only once they are used as a dictionary key. The problem is that it\u2019s not just the top-level object that could change its value; you could use a tuple containing a list as a key. Entering anything as a key into a dictionary would require marking all objects reachable from there as read-only \u2013 and again, self-referential objects could cause an infinite loop.\nThere is a trick to get around this if you need to, but use it at your own risk: You can wrap a mutable structure inside a class instance which has both a __eq__() and a __hash__() method. You must then make sure that the hash value for all such wrapper objects that reside in a dictionary (or other hash based structure), remain fixed while the object is in the dictionary (or other structure).\nclass ListWrapper: def __init__(self, the_list): self.the_list = the_list def __eq__(self, other): return self.the_list == other.the_list def __hash__(self): l = self.the_list result = 98767 - len(l)*555 for i, el in enumerate(l): try: result = result + (hash(el) % 9999999) * 1001 + i except Exception: result = (result % 7777777) + i * 333 return result\nNote that the hash computation is complicated by the possibility that some members of the list may be unhashable and also by the possibility of arithmetic overflow.\nFurthermore it must always be the case that if o1 == o2 (ie o1.__eq__(o2) is True) then hash(o1) == hash(o2) (ie, o1.__hash__() == o2.__hash__()), regardless of whether the object is in a dictionary or not. If you fail to meet these restrictions dictionaries and other hash based structures will misbehave.\nIn the case of ListWrapper, whenever the wrapper object is in a dictionary the wrapped list must not change to avoid anomalies. Don\u2019t do this unless you are prepared to think hard about the requirements and the consequences of not meeting them correctly. Consider yourself warned.\nWhy doesn\u2019t list.sort() return the sorted list? \u00b6\nIn situations where performance matters, making a copy of the list just to sort it would be wasteful. Therefore, list.sort() sorts the list in place. In order to remind you of that fact, it does not return the sorted list. This way, you won\u2019t be fooled into accidentally overwriting a list when you need a sorted copy but also need to keep the unsorted version around.\nIn Python 2.4 a new built-in function \u2013 sorted() \u2013 has been added. This function creates a new list from a provided iterable, sorts it and returns it. For example, here\u2019s how to iterate over the keys of a dictionary in sorted order:\nfor key in sorted(mydict): ... # do whatever with mydict[key]...\nHow do you specify and enforce an interface spec in Python? \u00b6\nAn interface specification for a module as provided by languages such as C++ and Java describes the prototypes for the methods and functions of the module. Many feel that compile-time enforcement of interface specifications helps in the construction of large programs.\nPython 2.6 adds an abc module that lets you define Abstract Base Classes (ABCs). You can then use isinstance() and issubclass() to check whether an instance or a class implements a particular ABC. The collections modules defines a set of useful ABCs such as Iterable, Container, and MutableMapping.\nFor Python, many of the advantages of interface specifications can be obtained by an appropriate test discipline for components. There is also a tool, PyChecker, which can be used to find problems due to subclassing.\nA good test suite for a module can both provide a regression test and serve as a module interface specification and a set of examples. Many Python modules can be run as a script to provide a simple \u201cself test.\u201d Even modules which use complex external interfaces can often be tested in isolation using trivial \u201cstub\u201d emulations of the external interface. The doctest and unittest modules or third-party test frameworks can be used to construct exhaustive test suites that exercise every line of code in a module.\nAn appropriate testing discipline can help build large complex applications in Python as well as having interface specifications would. In fact, it can be better because an interface specification cannot test certain properties of a program. For example, the append() method is expected to add new elements to the end of some internal list; an interface specification cannot test that your append() implementation will actually do this correctly, but it\u2019s trivial to check this property in a test suite.\nWriting test suites is very helpful, and you might want to design your code with an eye to making it easily tested. One increasingly popular technique, test-directed development, calls for writing parts of the test suite first, before you write any of the actual code. Of course Python allows you to be sloppy and not write test cases at all.\nWhy are default values shared between objects? \u00b6\nThis type of bug commonly bites neophyte programmers. Consider this function:\ndef foo(mydict={}): # Danger: shared reference to one dict for all calls ... compute something ... mydict[key] = value return mydict\nThe first time you call this function, mydict contains a single item. The second time, mydict contains two items because when foo() begins executing, mydict starts out with an item already in it.\nIt is often expected that a function call creates new objects for default values. This is not what happens. Default values are created exactly once, when the function is defined. If that object is changed, like the dictionary in this example, subsequent calls to the function will refer to this changed object.\nBy definition, immutable objects such as numbers, strings, tuples, and None, are safe from change. Changes to mutable objects such as dictionaries, lists, and class instances can lead to confusion.\nBecause of this feature, it is good programming practice to not use mutable objects as default values. Instead, use None as the default value and inside the function, check if the parameter is None and create a new list/dictionary/whatever if it is. For example, don\u2019t write:\ndef foo(mydict={}): ...\nbut:\ndef foo(mydict=None): if mydict is None: mydict = {} # create a new dict for local namespace\nThis feature can be useful. When you have a function that\u2019s time-consuming to compute, a common technique is to cache the parameters and the resulting value of each call to the function, and return the cached value if the same value is requested again. This is called \u201cmemoizing\u201d, and can be implemented like this:\n# Callers will never provide a third parameter for this function. def expensive (arg1, arg2, _cache={}): if (arg1, arg2) in _cache: return _cache[(arg1, arg2)] # Calculate the value result = ... expensive computation ... _cache[(arg1, arg2)] = result # Store result in the cache return result\nYou could use a global variable containing a dictionary instead of the default value; it\u2019s a matter of taste.\nWhy is there no goto? \u00b6\nYou can use exceptions to provide a \u201cstructured goto\u201d that even works across function calls. Many feel that exceptions can conveniently emulate all reasonable uses of the \u201cgo\u201d or \u201cgoto\u201d constructs of C, Fortran, and other languages. For example:\nclass label: pass # declare a label try: ... if (condition): raise label() # goto label ... except label: # where to goto pass ...\nThis doesn\u2019t allow you to jump into the middle of a loop, but that\u2019s usually considered an abuse of goto anyway. Use sparingly.\nWhy can\u2019t raw strings (r-strings) end with a backslash? \u00b6\nMore precisely, they can\u2019t end with an odd number of backslashes: the unpaired backslash at the end escapes the closing quote character, leaving an unterminated string.\nRaw strings were designed to ease creating input for processors (chiefly regular expression engines) that want to do their own backslash escape processing. Such processors consider an unmatched trailing backslash to be an error anyway, so raw strings disallow that. In return, they allow you to pass on the string quote character by escaping it with a backslash. These rules work well when r-strings are used for their intended purpose.\nIf you\u2019re trying to build Windows pathnames, note that all Windows system calls accept forward slashes too:\nf = open(\"/mydir/file.txt\") # works fine!\nIf you\u2019re trying to build a pathname for a DOS command, try e.g. one of\ndir = r\"\\this\\is\\my\\dos\\dir\" \"\\\\\" dir = r\"\\this\\is\\my\\dos\\dir\\ \"[:-1] dir = \"\\\\this\\\\is\\\\my\\\\dos\\\\dir\\\\\"\nWhy doesn\u2019t Python have a \u201cwith\u201d statement for attribute assignments? \u00b6\nPython has a \u2018with\u2019 statement that wraps the execution of a block, calling code on the entrance and exit from the block. Some language have a construct that looks like this:\nwith obj: a = 1 # equivalent to obj.a = 1 total = total + 1 # obj.total = obj.total + 1\nIn Python, such a construct would be ambiguous.\nOther languages, such as Object Pascal, Delphi, and C++, use static types, so it\u2019s possible to know, in an unambiguous way, what member is being assigned to. This is the main point of static typing \u2013 the compiler always knows the scope of every variable at compile time.\nPython uses dynamic types. It is impossible to know in advance which attribute will be referenced at runtime. Member attributes may be added or removed from objects on the fly. This makes it impossible to know, from a simple reading, what attribute is being referenced: a local one, a global one, or a member attribute?\nFor instance, take the following incomplete snippet:\ndef foo(a): with a: print(x)\nThe snippet assumes that \u201ca\u201d must have a member attribute called \u201cx\u201d. However, there is nothing in Python that tells the interpreter this. What should happen if \u201ca\u201d is, let us say, an integer? If there is a global variable named \u201cx\u201d, will it be used inside the with block? As you see, the dynamic nature of Python makes such choices much harder.\nThe primary benefit of \u201cwith\u201d and similar language features (reduction of code volume) can, however, easily be achieved in Python by assignment. Instead of:\nfunction(args).mydict[index][index].a = 21 function(args).mydict[index][index].b = 42 function(args).mydict[index][index].c = 63\nwrite this:\nref = function(args).mydict[index][index] ref.a = 21 ref.b = 42 ref.c = 63\nThis also has the side-effect of increasing execution speed because name bindings are resolved at run-time in Python, and the second version only needs to perform the resolution once.\nWhy are colons required for the if/while/def/class statements? \u00b6\nThe colon is required primarily to enhance readability (one of the results of the experimental ABC language). Consider this:\nif a == b print(a)\nversus\nif a == b: print(a)\nNotice how the second one is slightly easier to read. Notice further how a colon sets off the example in this FAQ answer; it\u2019s a standard usage in English.\nAnother minor reason is that the colon makes it easier for editors with syntax highlighting; they can look for colons to decide when indentation needs to be increased instead of having to do a more elaborate parsing of the program text.\nWhy does Python allow commas at the end of lists and tuples? \u00b6\nPython lets you add a trailing comma at the end of lists, tuples, and dictionaries:\n[1, 2, 3,] ('a', 'b', 'c',) d = { \"A\": [1, 5], \"B\": [6, 7], # last trailing comma is optional but good style }\nThere are several reasons to allow this.\nWhen you have a literal value for a list, tuple, or dictionary spread across multiple lines, it\u2019s easier to add more elements because you don\u2019t have to remember to add a comma to the previous line. The lines can also be sorted in your editor without creating a syntax error.\nAccidentally omitting the comma can lead to errors that are hard to diagnose. For example:\nx = [ \"fee\", \"fie\" \"foo\", \"fum\" ]\nThis list looks like it has four elements, but it actually contains three: \u201cfee\u201d, \u201cfiefoo\u201d and \u201cfum\u201d. Always adding the comma avoids this source of error.\nAllowing the trailing comma may also make programmatic code generation easier.\n"}, {"score": 841.89105, "uuid": "90bb711a-bc9b-53ea-9f2a-eea19a9894cd", "index": "cw12", "trec_id": "clueweb12-0206wb-03-13936", "target_hostname": "www.ravenbrook.com", "target_uri": "http://www.ravenbrook.com/project/p4dti/doc/2000-05-30/arch-analysis/", "page_rank": 1.5659445e-09, "spam_rank": 81, "title": "<em>Analysis</em> of architectures <em>for</em> defect tracking integration", "snippet": "<em>I</em> gather this from Perforce&#x27;s own use of <em>Python</em> and discussions at the Perforce User Conference 1999.", "explanation": null, "document": "100\n0.14\nScenarios a to c represent a medium sized organization with 10 people involved in defect resolution, each making 10 changes per day to 100 active defects. In scenario a, the replication daemon polls the servers every 5 minutes for new changes to replicate, so the latency is 300 seconds. In scenario b, the replication daemon is triggered by the servers whenever a change happens, and completes the replication in 5 seconds, and in scenario c, the replication daemon completes the replication in 1 second.\nScenarios d to f represent a large organization with 100 people involved in defect resolution, each making 40 changes per day to 1000 active defects. The latencies are as described above for scenarios a to c.\nIt is clear that inconsistency will be a serious problem for large organizations if replication has a high latency.\nThe activity in the system is likely to be clustered at certain times, for example when a manager starts working on the defect tracking system and asks everyone to \"bring things up to date\" and \"where is such and such?\". Or if there are scheduled builds and everyone races to submit their changes in time for the build. This factor may make the rate of inconsistencies higher than the model suggests.\nOn the other hand, people do not work in the random fashion suggested by the model. Any sort of workflow discipline means that only one person is working on a defect at a time. It is only the manager who is outside the workflow who can create inconsistencies. This factor may make the rate of inconsistencies lower than the model suggests.\nSo I need to work out the common scenarios by which inconsistencies are introduced and make a more detailed model that incorporates these scenarios.\nIt appears from the simple calculations above that it will not be acceptable for the replication daemon to poll the servers. To make the latency small (and hence make the rate of inconsistencies small) the servers trigger the replication daemon. One consequence of this is that the Perforce server has triggers that run whenever any defect-related data changes. This is why triggers are included in the consideration of requirements 21 , 76 and 77 .\nThe predictions of the model suggest that the replication daemon needs performance tuning when it is installed. The documentation covers this.\nUsing with tracker-client architecture\nThe replication architecture is used in parallel with the tracker-client architecture ( 2.39.1 ).\nConsider the case where a developer using the defect tracker's interface simultaneously submits a change and completes a task. From the point of view of the defect tracker this is two operations: submit the change (using the Perforce client API) and complete the task (using its own API and eventually its own database). The defect tracker waits for the submit operation to complete before it can close the task. But there is a race condition: the replication daemon may replicate the closing of the task to the defect tracker's database before the defect tracker's client can close the task. So the client probably gets a \"cannot close task: task already closed\" error message. This is both annoying for the user and can lead to incorrect database state since there was probably extra information related to the closed task that was available to the defect tracker's client but not to Perforce or the replication daemon that consequently is not in the defect tracker's database.\nHowever, there is no problem if the defect tracker omits to close the task via the submit operation. In this case the replication daemon replicates the association between the changelist and the task, but the task remains open for the defect tracker's client to close.\nSo we need to instruct developers of tracker-client integrations not to close tasks via the Perforce client interface if they plan to close them using the defect tracker's interface.\nBringing things up to date\nIf one server or the other is down, the replication architecture allows defect resolution activity to continue on the other server. If the network connection is down, the replication architecture allows defect resolution to continue in parallel in both systems. In these circumstances the replication daemon fails to replicate the changes (there may be some way to set a policy option for whether it should silently fail or alert someone?). But when the other server (or the connection) comes back up, the replication daemon takes all the changes that have taken place, replicates them and notes inconsistencies for an administrator to fix.\n2.1.2\u00a0Single\nThe single-database architecture achieves this by storing each defect-related entity only once, in the defect tracker's database.\nThe defect tracker makes queries and changes only to its own database. This means that it does not use a two-phase commit protocol to ensure consistency.\nPerforce makes transactions that affect both databases, so uses a two-phase commit protocol to make sure that the databases remain consistent. This has the consequence that transactions in Perforce which affect defect-tracking data (and this includes every submit) may be slow, because Perforce locks the affected data in both databases (one possibly across a network connection) before it can start its transaction. There is also the risk of deadlock if (for example) the network connection to the defect tracker's database goes down while Perforce has some records locked.\nIf the defect tracker's database does not provide transaction support or record locking then it will not be possible for Perforce to do two-phase locking and thus maintain consistency when the defect tracker is updating its database simultaneously with Perforce.\nThis need for Perforce to implement a two-phase commit protocol to ensure consistency means that effort will be required to implement it ( 2.76.2 ).\nThe single-database architecture is used in parallel with the tracker-client architecture ( 2.39.2 ). A similar consequence to that for the replication architecture follows: we instruct developers of tracker-client integrations not to close tasks via the Perforce client interface if they plan to close them using the defect tracker's interface. (The reason is slightly different: in this case there is no race condition to worry about because Perforce always updates the defect tracker's database).\nNote that in the single-database architecture, defect resolution using Perforce cannot continue if the defect tracking database (or the network connection between them) is down.\n2.1.3\u00a0Union\nThe union architecture achieves this by storing each defect-related entity only once.\nThe database unifier provides distributed transactions as part of its unified view of the two databases, so that the databases remain consistent. By this I mean that it offers transaction support for the combined databases - when a change affects both databases it either happens in both or doesn't happen at all. And other systems accessing data in either database while the transaction is being prepared see a consistent state, not a half-complete change. This means doing two-phase locking on the resources in both databases (with the same consequences as for the single-database architecture if the defect tracking database does not support transactions).\nIn the absence of transaction support in the defect tracker's database, the database unifier could attempt to enforce consistency by serializing all transactions, but this will result in a performance impact (how much?). It also won't work when there are other systems accessing the defect-tracking data in the database (unless these other systems also use the database unifier, but that is beyond the scope of this project).\nThe union architecture is used in parallel with the tracker-client architecture (to meet requirement 39 ). A similar consequence to that for the single-database architecture follows: we instruct developers of tracker-client integrations not to close tasks via the Perforce client interface if they plan to close them using the defect tracker's interface.\nNote that in the union architecture, defect resolution using either Perforce or the defect tracking system cannot continue if the other system (or the network connection between them) is down.\n2.1.4\u00a0Tracker\nThe tracker-client architecture cannot by itself keep the databases consistent, since changes to defect-tracking entities made through Perforce's interface do not cause corresponding changes in the defect tracker's database. Recall that there is a requirement that developers can use the Perforce interface ( 2.41.4 ). For example, a developer might check in work that completes a task, but this does not record anywhere that the task is completed, so the database is inconsistent until the developer goes to the defect tracker's interface and marks the task as completed.\nThe tracker-client architecture will be used in parallel with whichever other architecture is chosen. See the discussion for the other architectures ( 2.1.1 , 2.1.2 , 2.1.3 ) for the consequences.\n2.2\nRequirement\nThe defect tracking integration must make the jobs of the developers and managers easier (i.e. make it easier for them to produce a quality product etc.).\nAnalysis\n2.3\nRequirement\nIt must be easy to discover why the product sources are the way they are, and why they have changed, in terms of the customer requirements.\n2.3.1\u00a0Repl.\nThe fixes relation in Perforce associates a change with a job. So to discover why a particular file is the way it is using the Perforce interface, a user finds the changelists that contributed to the file, queries the fixes relation to discover the jobs which caused the changes to be made, and looks at the job descriptions to discover the defect or customer requirement that motivated the changes.\nIn order for this to be reliable, developers need to work in a disciplined fashion - they need to record the reason for each change. The integration makes it easy for developers to work in this disciplined way.\nA field is added to the fixes relation to record the nature of the association between the job and the change (for example, the change solves the defect, or contributes to the solution, or is a regression test for the defect, or backs out an incorrect solution, and so on). The field takes user defined values.\n"}, {"score": 841.23425, "uuid": "90d66b4b-e285-5a73-8fc5-9cb21e74de9f", "index": "cw12", "trec_id": "clueweb12-1802wb-67-27027", "target_hostname": "michaeldhealy.com", "target_uri": "http://michaeldhealy.com/2010/12/analysis-exchange/", "page_rank": 1.253827e-09, "spam_rank": 69, "title": "<em>Analysis</em> Exchange | MichaelDHealy.com", "snippet": "If you are in the SF Bay Area, <em>or</em> willing to work with someone remotely, and could use: * Econometrics, Linguistics, <em>R</em>, <em>Python</em>, Google Analytics and more: If you just want to ask about the <em>Analysis</em> Exchange, <em>or</em> talk about the economics of crime, that would be cool too.", "explanation": null, "document": "Posted on December 2, 2010 by MichaelDHealy\nAnalysis Exchange\n\u201cThanks for all your help this summer. I\u2019ve started to implement some of the suggestions you made and there are more on the way.\u201d\nI received that update from the first client I worked with in the Analysis Exchange, which was more than anything else, the single most awesome part of participating in the Analysis Exchange. A personal note outside the scope of required end of project evaluations reminded me that the organizations are using the insights which we worked so hard to produce.\nThe projects I was able to work on as a student were just about the most fun I had at work . . . ever. As a result of the projects I was mentioned in a of web analytics blog , went to XChange in Monterey , and created some interesting charts and graphs which elucidated key points to the non-profits.\nNon-Profit Benefits\nThe Analysis Exchange , which you should by now know, is a volunteer effort to connect three groups:\n"}, {"score": 838.66504, "uuid": "ba55b340-00f0-502c-81bf-d609496753a0", "index": "cw12", "trec_id": "clueweb12-0913wb-89-09014", "target_hostname": "trizpug.org", "target_uri": "http://trizpug.org/up-to-speed", "page_rank": 1.1700305e-09, "spam_rank": 91, "title": "Get Up To Speed \u2014 Triangle Zope and <em>Python</em> Users Group", "snippet": "<em>analysis</em> package <em>for</em> chemometrics. * RPy is a <em>Python</em> binding to the <em>R</em> statistical language. * PyNGL is a <em>Python</em> interface to the NCAR command language. * PyClimate is a package <em>for</em> operations with COARDS-compliant netCDF files, EOF <em>analysis</em>, SVD and CCA <em>analysis</em> of coupled <em>data</em> sets, some linear digital", "explanation": null, "document": "2003 Plone Conference - New Orleans ( Video )\nIf you haven't seen Joel Burton's presentation Buildling a Humane CMS with Plone , then you have missed a lot.\nPlone has a built-in AJAX manager that doesn't require you to know anything about Javascript: Kinetic Style Sheets .\nUpfront Systems has an online Plone Course .\nAt your earliest opportunity, you should go to a Plone Boot Camp , the best technical training you will ever experience and ridiculously affordable.\nLearn Plone is geared more towards content creators, editors, and site managers.\nIn Small Steps is a well organized website about creating websites with Plone.\nThe free online version of Andy McKay's book about Plone 2.05, The Definitive Guide to Plone is still available.\nUML sequence diagram showing what happens in Plone between an incoming HTTP request and an outgoing HTTP response.\nHow to hotshot profile a Plone site with bstats.\nOnline manuals for training Plone 2.5 and Plone 3 content contributors and editors.\nA book for training Plone content contributors and editors, updated for Plone 3:\nContent Management with Plone\nImportant Python Projects\nA productive language like Python spawns many important software projects you should know about. You can learn a lot by joining one of these projects. This list is no way comprehensive (that would be many thousands of important projects) but is instead here to give you a flavor or where Python gets used and for what.\nSubversion is a version control system.\nBazaar is a version control system.\nMercurial is a version control system.\nTrac is lightweight through the web project management system which works well with Subversion.\nRoundup is an issue tracker.\nSCONS is a superior alternative to Make.\nSciPy is a collection of Python scientific packages.\nScientific Python is a collection of Python scientific packages.\nNumPy is a collection of Python array processring packages.\nMatplotlib is a collection of 2-D plotting packages including a complete Python replacement for Matlab.\nIPython is an enhanced Python shell for distributed and parallel computing.\nTwisted is an event-driven networking engine.\nParamiko is an implementation of the SSH2 protocol for secure connections to remote machines.\nBFG is a tested, simple, minimalist, documented, and extremely fast Python web framework which reimplements the best features of Zope.\nPylons is a lightweight web framework emphasizing flexibility and rapid development.\nDjango is a high-level web framework encouraging rapid development and clean, pragmatic design.\nTurbogears is an Ajax-powered web megaframework.\nweb2py is a WSGI-compliant zero-install MVC.\nCherryPy is a pythonic, object-oriented HTTP framework.\nmod_python embeds Python in the Apache HTTP server.\nWeb Server Gateway Interface (WSGI) is a specification for web servers and application servers to communicate with web applications.\nPaste is a system for finding, configuring, and serving WSGI applications.\nmod_wsgi is an Apache module which turns Apache into a WSGI-compliant web server.\nDeliverance is lightweight WSGI middleware which applies a theme to content according to a set of rules.\nRepoze enables deployments of Zope and Plone into a WSGI environment ando ther Python WSGI applications to use Zope technologies as middleware.\nSQLAlchemy is a SQL toolkit and Object Relational Mapper.\nSQLObject is an object relation manager providing substantial database independence for applications.\nStorm is an object-relational mapper (ORM) for Python which powers Canonical's Launchpad online project management service.\nEnthon is a Python distribution with many scientific packages pre-configured for Windows.\nMacPython is a Python distribution and set of packages pre-configured for Mac OSX.\nStackless is an enhanced version of the Python emphasizing threading.\nParallel Python is a python module which provides mechanism for parallel execution of python code on SMP (systems with multiple processors or cores) and clusters (computers connected via network).\nIronPython is an implementation of Python running on .NET.\nJython is an implementation of Python written in 100% Pure Java\nPyPy is an implementation of Python written in Python itself which somehow turns out to be faster than Python written in C.\nVPython is a module for producing real-time 3D scenes with Python.\nBlender is a 3D modeling and animation system with a huge community of users.\nPyGame is a set of Python modules designed for writing games.\nVRPlumber is a clearinghouse for Python virtual reality, ray-tracing, and 3D rendering applications of many kinds.\nBioPython is a collection of Python packages for computational molecular biology.\nPyMol is a Python molecular visualization system.\nPyDap is an implementation of OPeNDAP services.\nPAIDA is a scientific analysis package supporting AIDA (Abstract Interfaces for Data Analysis).\nMayaVi provides interactive visualization of 3D data.\nPyGlobus is a Python interface to the Globus grid computing toolkit.\nDIANE is a lightweight distributed framework for GRIDS.\nGanga is a Python frontend for job definition and management of ATLAS and Large Hadron Collider beauty experiments.\nThuban is an interactive geographic data viewer.\nPython Cartography Library (PCL) is a package of modules for rendering GIS data from a variety of backends into maps.\nVisualization ToolKit (VTK) is a system for 3D computer graphics, image processing, and visualization.\nSimPy is an process-based discrete-event simulation language based on standard Python.\nMODELLER is used for homology or comparative modeling of protein three-dimensional structures.\nPyChem is a multivariate analysis package for chemometrics.\nRPy is a Python binding to the R statistical language.\nPyNGL is a Python interface to the NCAR command language.\nPyClimate is a package for operations with COARDS-compliant netCDF files, EOF analysis, SVD and CCA analysis of coupled data sets, some linear digital filters, kernel based probability density function estimation and access to DCDFLIB.\nAstroPy is a clearinghouse for Python applications in astrophysics.\n"}]]}