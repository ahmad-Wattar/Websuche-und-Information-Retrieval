{"number": "78", "title": "Which AND algorithm AND is AND better AND quicksort AND or AND merge AND sort", "results": 341, "documents": [[{"score": 2920.5, "uuid": "00b8df2b-d8ed-55d8-b66a-0b05c8763856", "index": "cw12", "trec_id": "clueweb12-0800tw-22-06801", "target_hostname": "www.stoimen.com", "target_uri": "http://www.stoimen.com/blog/2012/03/05/computer-algorithms-merge-sort/", "page_rank": 1.3160205e-09, "spam_rank": 73, "title": "Computer <em>Algorithms</em>: <em>Merge</em> <em>Sort</em>", "snippet": "So far so good, so let\u2019s say we have a very large list of data, <em>which</em> we want to <em>sort</em>. Obviously it will be <em>better</em> if we divide the list into two sub-lists with equal length and then <em>sort</em> them.", "explanation": null, "document": "Posted on March 5, 2012 by Stoimen\nIntroduction\nBasically sorting algorithms can be divided into two main groups. Such based on comparisons and such that are not. I already posted about some of the algorithms of the first group. Insertion sort, bubble sort and Shell sort are based on the comparison model. The problem with these three algorithms is that their complexity is O(n2) so they are very slow.\nSo is it possible to sort a list of items by comparing their items faster than O(n2)? The answer is yes and here\u2019s how we can do it.\nThe nature of those three algorithms mentioned above is that we almost compared each two items from initial list.\nInsertion sort and bubble sort make too many comparisons, exactly what merge sort tries to overcome!\nThis, of course, is not the best approach and we don\u2019t need to do that. Instead we can try to divide the list into smaller lists and then sort them. After sorting the smaller lists, which is supposed to be easier than sorting the entire initial list, we can try to merge the result into one sorted list. This technique is typically known as \u201cdivide and conquer\u201d.\nNormally if a problem is too difficult to solve, we can try to break it apart into smaller sub-sets of this problem and try to solve them. Then somehow we can merge the results of the solved problems.\nIf it's too difficult to sort a large list of items, we can break it apart into smaller sub-lists and try to sort them!\nOverview\nMerge sort is a comparison model sorting algorithm based on the \u201cdivide and conquer\u201d principle. So far so good, so let\u2019s say we have a very large list of data, which we want to sort. Obviously it will be better if we divide the list into two sub-lists with equal length and then sort them. If they remain too large, we can continue breaking them down until we get to something very easy to sort as shown on the diagram bellow.\nMerge sort is a typical example of divide and conquer technique!\nThe thing is that on some step of the algorithm we have two sorted lists and the tricky part is to merge them. However this is not so difficult.\nWe can start comparing the first items of the lists and than we can pop the smaller of them both and put it into a new list containing the merged (sorted) array.\nImplementation\nThe good news is that this algorithm is fast, but not so difficult to implement and that sounds quite good from a developer\u2019s point of view. Here\u2019s the implementation in PHP. Note that every algorithm that follows the divide and conquer principles can be easily implemented in a recursive solution. However recursion can be bitter so you can go for a iterative solution. Typically recursion is \u201creplaced\u201d by additional memory space in iterative solutions. Here\u2019s a recursive version of merge sort.\n$input = array(6, 5, 3, 1, 8, 7, 2, 4); function merge_sort($arr) { if (count($arr) <= 1) { return $arr; } $left = array_slice($arr, 0, (int)(count($arr)/2)); $right = array_slice($arr, (int)(count($arr)/2)); $left = merge_sort($left); $right = merge_sort($right); $output = merge($left, $right); return $output; } function merge($left, $right) { $result = array(); while (count($left) > 0 && count($right) > 0) { if ($left[0] <= $right[0]) { array_push($result, array_shift($left)); } else { array_push($result, array_shift($right)); } } array_splice($result, count($result), 0, $left); array_splice($result, count($result), 0, $right); return $result; } // 1, 2, 3, 4, 5, 6, 7, 8 $output = merge_sort($input);\nComplexity\nIt\u2019s great that the complexity of merge sort is O(n*log(n)) even in the worst case! Note that even quicksort\u2019s complexity can be O(n2) in the worst case. So we can be sure that merge sort is very stable no matter the input.\nMerge sort complexity is O(n*log(n))\nMerge sort complexity is O(n*log(n)) even in the worst case!\nTwo reasons why merge sort is useful\n1. Fast no matter the input\nMerge sort is a great sorting algorithm mainly because it\u2019s very fast and stable. It\u2019s complexity is the same even in the worst case and it is O(n*log(n)). Note that even quicksort\u2019s complexity is O(n2) in the worst case, which for n = 20 is about 4.6 times slower!\n2. Easy implementation\nAnother cool reason is that merge sort is easy to implement. Indeed most of the developer consider something fast to be difficult to implement, but that\u2019s not the case of merge sort.\nThree reasons why merge sort is not useful\n1. Slower than non-comparison based algorithms\nMerge sort is however based on the comparison model and as such can be slower than algorithms non-based on comparisons that can sort data in linear time. Of course, this depends on the input data, so we must be careful for the input.\n2. Difficult to implement for beginners\nAlthough I don\u2019t think this can be the main reason why not to use merge sort some people say that it can be difficult to implement for beginners, especially the merge part of the algorithm.\n3. Slower than insertion and bubble sort for nearly sorted input\nAgain it is very important to know the input data. Indeed if the input is nearly sorted the insertion sort or bubble sort can be faster. Note that in the best case insertion and bubble sort complexity is O(n), while merge sort\u2019s best case is O(n*log(n)).\nAs a conclusion I can say that merge sort is practically one of the best sorting algorithms because it\u2019s easy to implement and fast, so it must be considered by every developer!\nRelated posts:\n"}, {"score": 2354.609, "uuid": "6f6ab203-ce0a-51f9-85bf-3b148f802d57", "index": "cw12", "trec_id": "clueweb12-1516wb-79-16924", "target_hostname": "www.codecodex.com", "target_uri": "http://www.codecodex.com/wiki/Merge_sort", "page_rank": 1.6139088e-09, "spam_rank": 85, "title": "<em>Merge</em> <em>sort</em>", "snippet": "On the plus side, <em>merge</em> <em>sort</em> <em>is</em> a stable <em>sort</em>, parallelizes <em>better</em>, and <em>is</em> more efficient at handling slow-to-access sequential media.", "explanation": null, "document": "[ edit ] Fortran\nsubroutine Merge(A,NA,B,NB,C,NC)\n \n   integer, intent(in)\u00a0:: NA,NB,NC        \u00a0! Normal usage: NA+NB = NC\n   integer, intent(in out)\u00a0:: A(NA)       \u00a0! B overlays C(NA+1:NC)\n   integer, intent(in)    \u00a0:: B(NB)\n   integer, intent(in out)\u00a0:: C(NC)\n \n   integer\u00a0:: I,J,K\n \n   I = 1; J = 1; K = 1;\n   do while(I <= NA .and. J <= NB)\n      if (A(I) <= B(J)) then\n         C(K) = A(I)\n         I = I+1\n      else\n         C(K) = B(J)\n         J = J+1\n      endif\n      K = K + 1\n   enddo\n   do while (I <= NA)\n      C(K) = A(I)\n      I = I + 1\n      K = K + 1\n   enddo\n   return\n \nend subroutine merge\n \nrecursive subroutine MergeSort(A,N,T)\n \n   integer, intent(in)\u00a0:: N\n   integer, dimension(N), intent(in out)\u00a0:: A\n   integer, dimension((N+1)/2), intent (out)\u00a0:: T\n \n   integer\u00a0:: NA,NB,V\n \n   if (N < 2) return\n   if (N == 2) then\n      if (A(1) > A(2)) then\n         V = A(1)\n         A(1) = A(2)\n         A(2) = V\n      endif\n      return\n   endif      \n   NA=(N+1)/2\n   NB=N-NA\n \n   call MergeSort(A,NA,T)\n   call MergeSort(A(NA+1),NB,T)\n \n   if (A(NA) > A(NA+1)) then\n      T(1:NA)=A(1:NA)\n      call Merge(T,NA,A(NA+1),NB,A,N)\n   endif\n   return\n \nend subroutine MergeSort\n \nprogram TestMergeSort\n \n   integer, parameter\u00a0:: N = 8\n   integer, dimension(N)\u00a0:: A = (/ 1, 5, 2, 7, 3, 9, 4, 6 /)\n   integer, dimension ((N+1)/2)\u00a0:: T\n   call MergeSort(A,N,T)\n   write(*,'(A,/,10I3)')'Sorted array\u00a0:',A\n \nend program TestMergeSort\n[ edit ] Groovy\ndef mergeSort(list){\n if( list.size() <= 1) return list\n  center = list.size() / 2\n  left  = list[0..center]\n  right = list[center..list.size()]\n  merge(mergeSort(left), mergeSort(right))\n}\n\ndef merge(left, right){\n  sorted = []\n  while(left.size() > 0 || right.size() > 0)\n    if(left.get(0) <= right.get(0)){\n      sorted << left\n    }else{\n      sorted << right\n    }\n  sorted = sorted + left + right\n}\n[ edit ] Haskell\nsort\u00a0:: Ord a => [a] -> [a]\n\nsort []    =  []\nsort [x]  =  [x]\nsort xs   =  merge (sort left) (sort right)\n  where\n    (left, right) = splitAt (length xs `div` 2) xs\n    merge [] xs = xs\n    merge xs [] = xs\n    merge (x:xs) (y:ys)\n      | x <= y      = x\u00a0: merge xs (y:ys)\n      | otherwise = y\u00a0: merge (x:xs) ys\n[ edit ] Java\npublic int[] mergeSort(int array[])  {\n \tif(array.length > 1)  \t{\n \t\tint elementsInA1 = array.length/2;\n \t\tint elementsInA2 = array.length - elementsInA1;\n \t\tint arr1[] = new int[elementsInA1];\n \t\tint arr2[] = new int[elementsInA2];\n \t\t\n \t\tfor(int i = 0; i < elementsInA1; i++)\n \t\t\tarr1[i] = array[i];\n\n \t\tfor(int i = elementsInA1; i < elementsInA1 + elementsInA2; i++)\n \t\t\tarr2[i - elementsInA1] = array[i];\n\n \t\tarr1 = mergeSort(arr1);\n \t\tarr2 = mergeSort(arr2);\n \t\t\n \t\tint i = 0, j = 0, k = 0;\n\n \t\twhile(arr1.length\u00a0!= j && arr2.length\u00a0!= k) {\n \t\t\tif(arr1[j] <= arr2[k]) {\n \t\t\t\tarray[i] = arr1[j];\n \t\t\t\ti++;\n \t\t\t\tj++;\n \t\t\t} else {\n \t\t\t\tarray[i] = arr2[k];\n \t\t\t\ti++;\n \t\t\t\tk++;\n \t\t\t}\n \t\t}\n\n \t\twhile(arr1.length\u00a0!= j) {\n \t\t\tarray[i] = arr1[j];\n \t\t\ti++;\n \t\t\tj++;\n \t\t}\n \t\twhile(arr2.length\u00a0!= k) {\n \t\t\tarray[i] = arr2[k];\n \t\t\ti++;\n \t\t\tk++;\n \t\t}\n \t}\n \treturn array;\n }\n[ edit ] JavaScript\nfunction merge_sort(arr) {\n    var l = arr.length, m = Math.floor(l/2);\n    if (l <= 1) return arr;\n    return merge(merge_sort(arr.slice(0, m)), merge_sort(arr.slice(m)));\n}\n\nfunction merge(left,right) {\n    var result = [];\n    var ll = left.length, rl = right.length;\n    while (ll > 0 && rl > 0) {\n        if (left[0] <= right[0]) {\n            result.push(left.shift());\n            ll--;\n        } else {\n            result.push(right.shift());\n            rl--;\n        }\n    }\n    if (ll > 0) {\n        result.push.apply(result, left);\n    } else if (rl > 0) {\n        result.push.apply(result, right);\n    }\n    return result;\n}\n[ edit ] Miranda\nsort []    = []\nsort [x]   = [x]\nsort array = merge (sort left) (sort right)\n             where\n               left  = [array!y | y <- [0..mid]]\n               right = [array!y | y <- [(mid+1)..max]]\n               max   = #array - 1\n               mid   = max div 2\n[ edit ] OCaml\nFunction to merge a pair of sorted lists:\n# let rec merge = function\n    | list, []\n    | [], list -> list\n    | h1::t1, h2::t2 ->\n        if h1 <= h2 then\n          h1\u00a0:: merge (t1, h2::t2)\n        else\n          h2\u00a0:: merge (h1::t1, t2)\n \u00a0;;\nval merge\u00a0: 'a list * 'a list -> 'a list = <fun>\nThis function is included in the OCaml stdlib as List.merge but is also included here for clarity. Function to halve a list:\n# let rec halve = function\n    | []\n    | [_] as t1 -> t1, []\n    | h::t ->\n        let t1, t2 = halve t in\n          h::t2, t1\n \u00a0;;\nval halve\u00a0: 'a list -> 'a list * 'a list = <fun>\nFunction to merge sort a list:\n# let rec merge_sort = function\n    | []\n    | [_] as list -> list\n    | list ->\n        let l1, l2 = halve list in\n          merge (merge_sort l1, merge_sort l2)\n;;\nval sort\u00a0: 'a list -> 'a list = <fun>\nFor example:\n# merge_sort [6; 7; 0; 8; 3; 2; 4; 9; 5; 1];;\n-\u00a0: int list = [0; 1; 2; 3; 4; 5; 6; 7; 8; 9]\n[ edit ] Opal\nSignature:\nSIGNATURE Merge[alpha,<]\n\nIMPORT Array[alpha] ONLY array\n       Seq   ONLY seq \n       Nat ONLY nat\n\nSORT alpha \nFUN <\u00a0: alpha ** alpha -> bool\n\nFUN mergeSort:  seq[alpha] -> seq[alpha]\nImplementation\nIMPLEMENTATION Merge[alpha,<]\n\n\nIMPORT Array[alpha] COMPLETELY\n       Seq          COMPLETELY\n       Nat          COMPLETELY\n       Bool         COMPLETELY\n\nFUN merge: seq[alpha] ** seq[alpha] -> seq[alpha]\nDEF merge((<>),B) == B\nDEF merge(A,(<>)) == A\nDEF merge(A AS (a::as),B AS (b::bs) ) == IF a < b THEN a::merge(as,B) ELSE b::merge(A,bs) FI\n\nDEF mergeSort(<>) == <>\nDEF mergeSort(A AS (a::(<>))) == A\nDEF mergeSort(lst) == LET \n                        half == #(lst)/2\n                        (a,b) == split(half,lst)\n                      IN \n                      merge(mergeSort(a),mergeSort(b))\n[ edit ] Perl\nFYI as of perl's built-in sort is merge sort by default.\n#!/usr/bin/perl\nuse strict;\nuse warnings;\n\nsub mergesort;    # to suppress prototype warnings\n\nsub mergesort(&@) {\n    my $comp = shift;\n    return @_ if @_ <= 1;\n    my @tail = splice @_, @_ >> 1;\n    merge( $comp, [ mergesort $comp, @_ ], [ mergesort $comp, @tail ] );\n}\n\nsub merge {\n    my ( $comp, $head, $tail ) = @_;\n    my @ret;\n    while ( @$head && @$tail ) {\n        push @ret,\n          (\n            $comp->( $head->[0], $tail->[0] ) < 0\n           \u00a0? shift @$head\n           \u00a0: shift @$tail\n          );\n    }\n    push @ret, @$head, @$tail;\n    @ret;\n}\n\nmy @rnd = map { int( rand 100 ) } 1 .. 20;\nprint join( \",\", @rnd ), \"\\n\";\nprint join( \",\", mergesort { $_[0] <=> $_[1] } @rnd ), \"\\n\";\nprint join( \",\", sort { $a <=> $b } @rnd ), \"\\n\";\n[ edit ] PHP\nfunction merge_sort(&$arrayToSort)\n{\n    if (sizeof($arrayToSort) <= 1)\n        return $arrayToSort;\n\n    // split our input array into two halves\n    // left...\n    $leftFrag = array_slice($arrayToSort, 0, (int)(count($arrayToSort)/2));\n    // right...\n    $rightFrag = array_slice($arrayToSort, (int)(count($arrayToSort)/2));\n\n    // RECURSION\n    // split the two halves into their respective halves...\n    $leftFrag = merge_sort($leftFrag);\n    $rightFrag = merge_sort($rightFrag);\n\n    $returnArray = merge($leftFrag, $rightFrag);\n\n    return $returnArray;\n}\n\n\nfunction merge(&$lF, &$rF)\n{\n    $result = array();\n\n    // while both arrays have something in them\n    while (count($lF)>0 && count($rF)>0) {\n        if ($lF[0] <= $rF[0]) {\n            array_push($result, array_shift($lF));\n        }\n        else {\n            array_push($result, array_shift($rF));\n        }\n    }\n\n    // did not see this in the pseudo code,\n    // but it became necessary as one of the arrays\n    // can become empty before the other\n    array_splice($result, count($result), 0, $lF);\n    array_splice($result, count($result), 0, $rF);\n\n    return $result;\n}\n[ edit ] Prolog\nThis is an ISO-Prolog compatible implementation of merge sort with the exception of the predicates append/3 and length/2 which, while not prescribed by the ISO standard, are available in virtually all Prolog implementations.\n% Merge-Sort: ms(+Source,\u00a0?Result)\n\nms(Xs, Rs)\u00a0:-\n    length(Xs, L),\n    (   L < 2 ->\n        Rs = Xs\n   \u00a0;   Split is L//2,\n        length(Front0, Split),\n        append(Front0, Back0, Xs),\n        ms(Front0, Front),\n        ms(Back0, Back),\n        merge(Front, Back, Rs)\n    ).\n\n% Merge of lists: merge(+List1, +List2, -Result)\n\nmerge([], Xs, Xs)\u00a0:-\u00a0!.\nmerge(Xs, [], Xs)\u00a0:-\u00a0!.\nmerge([X|Xs], [Y|Ys], Zs)\u00a0:-\n    (   X @=< Y ->\n        Zs = [X|Rest],\n        merge(Xs, [Y|Ys], Rest)\n   \u00a0;   Zs = [Y|Rest],\n        merge([X|Xs], Ys, Rest)\n    ).\n[ edit ] Python\ndef mergesort(arr):\n    if len(arr) == 1:\n        return arr\n    \n    m = len(arr) / 2\n    l = mergesort(arr[:m])\n    r = mergesort(arr[m:])\n\n    if not len(l) or not len(r):\n        return l or r\n        \n    result = []\n    i = j = 0\n    while (len(result) < len(r)+len(l)):        \n        if l[i] < r[j]:\n            result.append(l[i])\n            i += 1\n        else:\n            result.append(r[j])\n            j += 1            \n        if i == len(l) or j == len(r):            \n            result.extend(l[i:] or r[j:])\n            break\n        \n    return result\n[ edit ] Ruby\ndef mergesort(list)\n  return list if list.size <= 1\n  mid = list.size / 2\n  left  = list[0, mid]\n  right = list[mid, list.size]\n  merge(mergesort(left), mergesort(right))\nend\n\ndef merge(left, right)\n  sorted = []\n  until left.empty? or right.empty?\n    if left.first <= right.first\n      sorted << left.shift\n    else\n      sorted << right.shift\n    end\n  end\n  sorted.concat(left).concat(right)\nend\n[ edit ] Scheme\n(define (loe p1 p2);;implements less than or equal\n        (<= (cdr p1) (cdr p2)))\n\n\n(define (mergesort L)\n             (cond ((= (length L) 0) '())\n                   ((= (length L) 1) L); the 1 element list is sorted\n                   ((= (length L) 2) (if (< (cdar L) (cdar (cdr L)))\n                                L\n                               (list (car (cdr L)) (car L));;special case for len 2 list\n                              )\n                   )\n             (else (mergelist (mergesort (firstn L (/ (length L) 2)))\n                              (mergesort (lastn L (/ (length L) 2)))\n                   );;recursively call mergesort on both halves\n             )\n            )\n\n)\n(define (firstn L N)\n;;pre: N not bigger than size of L\n        (cond ((= N 0) '())\n              ((or (= N 1) (< N 2)) (list (car L)))\n              (else (cons (car L) (firstn (cdr L) (- N 1))))\n        )\n)\n\n(define (lastn L N)\n;;pre: N not bigger than size of L\n          (cond ((= N 0) L)\n                ((or(= N 1) (< N 2)) (cdr L))\n                (else (lastn (cdr L) (- N 1)))\n          )\n)\n\n(define (mergelist primero segundo)\n;;;pre: primero and segundo are lists sorted in increasing order\n;;;post: returns a single sorted list containing the elements of primero and segundo\n        (cond ((null? primero) segundo);;first base case\n              ((null? segundo) primero);;second base case\n              ((loe (car primero) (car segundo))\n               (cons\n                (car primero)\n                      (mergelist (cdr primero) segundo);;first main case\n                ))\n              ((> (cdar primero) (cdar segundo))\n               (cons\n                (car segundo)\n                      (mergelist primero (cdr segundo));;second main case\n                )\n              )\n        )\n)\n[ edit ] Seed7\nVersion without additional storage\nconst proc: mergeSort (inout array elemType: arr, in var integer: lo, in integer: hi) is func\n  local\n    var integer: mid is 0;\n    var elemType: help is elemType.value;\n    var integer: k is 0;\n  begin\n    if lo < hi then\n      mid\u00a0:= (lo + hi) div 2;\n      mergeSort(arr, lo, mid);\n      mergeSort(arr, succ(mid), hi);\n      incr(mid);\n      while lo < mid and mid <= hi do\n        if arr[lo] <= arr[mid] then\n          incr(lo);\n        else\n          help\u00a0:= arr[mid];\n          for k range mid downto succ(lo) do\n            arr[k]\u00a0:= arr[pred(k)];\n          end for;\n          arr[lo]\u00a0:= help;\n          incr(lo);\n          incr(mid);\n        end if;\n      end while;\n    end if;\n  end func;\n\nconst proc: mergeSort (inout array elemType: arr) is func\n  begin\n    mergeSort(arr, 1, length(arr));\n  end func;\nOriginal source: [1]\nVersion with additional storage\nconst proc: mergeSort2 (inout array elemType: arr, in integer: lo, in integer: hi, inout array elemType: scratch) is func\n  local\n    var integer: mid is 0;\n    var integer: k is 0;\n    var integer: t_lo is 0;\n    var integer: t_hi is 0;\n  begin\n    if lo < hi then\n      mid\u00a0:= (lo + hi) div 2;\n      mergeSort2(arr, lo, mid, scratch);\n      mergeSort2(arr, succ(mid), hi, scratch);\n      t_lo\u00a0:= lo;\n      t_hi\u00a0:= succ(mid);\n      for k range lo to hi do\n        if t_lo <= mid and (t_hi > hi or arr[t_lo] < arr[t_hi]) then\n          scratch[k]\u00a0:= arr[t_lo];\n          incr(t_lo);\n        else\n          scratch[k]\u00a0:= arr[t_hi];\n          incr(t_hi);\n        end if;\n      end for;\n      for k range lo to hi do\n        arr[k]\u00a0:= scratch[k];\n      end for;\n    end if;\n  end func;\n\nconst proc: mergeSort2 (inout array elemType: arr) is func\n  local\n    var array elemType: scratch is 0 times elemType.value;\n  begin\n    scratch\u00a0:= length(arr) times elemType.value;\n    mergeSort2(arr, 1, length(arr), scratch);\n  end func;\n[ edit ] Tcl\nproc mergesort list {\n set len [llength $list]\n if {$len <= 1} {return $list}\n set middle [expr {$len / 2}]\n set left [lrange $list 0 [expr {$middle - 1}]]\n set right [lrange $list $middle end]\n return [merge [mergesort $left] [mergesort $right]]\n}\n \nproc merge {left right} {\n set res {}\n while {[set lleft [llength $left]] > 0 && [set lright [llength $right]] > 0} {\n      if {[lindex $left 0] <= [lindex $right 0]} {\n        set left [lassign $left value]\n      } else {\n        set right [lassign $right value]\n      }\n      lappend res $value\n }\n if {$lleft > 0} {lappend res {*}$left}\n if {$lright > 0} {set res [concat $res $right]}\n return $res\n}\n[ edit ] Standard ML\nfun mergesort [] = []\n|   mergesort [x] = [x]\n|   mergesort lst = \n    let fun merge ([],ys) = ys (*merges two sorted lists to form a sorted list *)\n        |   merge (xs,[]) = xs\n        |   merge (x::xs,y::ys) = \n              if x<y then\n                x::merge (xs,y::ys)\n              else\n                y::merge (x::xs,ys)\n       \u00a0; \n        val half = length(lst) div 2;\n     in\n        merge (mergesort (List.take (lst, half)),mergesort (List.drop (lst, half)))\n     end\n;\n[ edit ] Analysis\nIn sorting n items, merge sort has an average and worst-case performance of O(n log n). If the running time of merge sort for a list of length n is T(n), then the recurrence T(n) = 2T(n/2) + n follows from the definition of the algorithm (apply the algorithm to two lists of half the size of the original list, and add the n steps taken to merge the resulting two lists). The closed form follows from the master theorem.\nIn the worst case, merge sort does exactly (n \u2308log n\u2309 - 2\u2308log n\u2309 + 1) comparisons, which is between (n log n - n + 1) and (n log n - 0.9139\u00b7n + 1) [logs are base 2]. Note, the worst case number given here does not agree with that given in Knuth's Art of Computer Programming, Vol 3. The discrepancy is due to Knuth analyzing a variant implementation of merge sort that is slightly sub-optimal.\nFor large n and a randomly ordered input list, merge sort's expected (average) number of comparisons approaches \u03b1\u00b7n fewer than the worst case, where \u03b1 = -1 + \u2211 1/(2k +1), k = 0 \u2192 \u221e, \u03b1 \u2248 0.2645.\nIn the worst case, merge sort does about 39% fewer comparisons than quicksort does in the average case; merge sort always makes fewer comparisons than quicksort, except in extremely rare cases, when they tie, where merge sort's worst case is found simultaneously with quicksort's best case. In terms of moves, merge sort's worst case complexity is O(n log n)\u2014the same complexity as quicksort's best case, and merge sort's best case takes about half as many iterations as the worst case.\nRecursive implementations of merge sort make 2n - 1 method calls in the worst case, compared to quicksort's n, thus has roughly twice as much recursive overhead as quicksort. However, iterative, non-recursive, implementations of merge sort, avoiding method call overhead, are not difficult to code. Merge sort's most common implementation does not sort in place, meaning memory the size of the input must be allocated for the sorted output to be stored in. Sorting in-place is possible but requires an extremely complicated implementation and hurts performance.\nMerge sort is much more efficient than quicksort if the data to be sorted can only be efficiently accessed sequentially, and is thus popular in languages such as Lisp, where sequentially accessed data structures are very common. Unlike some (inefficient) implementations of quicksort, merge sort is a stable sort as long as the merge operation is implemented properly.\n[ edit ] Optimizing Merge Sort\nThis might seem to be of historical interest only, but on modern computers, locality of reference is of paramount importance in software optimization, because multi-level memory hierarchies are used. In some sense, main RAM can be seen as a fast tape drive, level 3 cache memory as a slightly faster one, level 2 cache memory as faster still, and so on. In some circumstances, cache reloading might impose unacceptable overhead and a carefully crafted merge sort might result in a significant improvement in running time. This opportunity might change if fast memory becomes very cheap again, or if exotic architectures like the Tera MTA become commonplace.\nDesigning a merge sort to perform optimally often requires adjustment to available hardware, eg. number of tape drives, or size and speed of the relevant cache memory levels.\n[ edit ] Comparison with other Sort Algorithms\nAlthough heap sort has the same time bounds as merge sort, it requires only \u03a9(1) auxiliary space instead of merge sort's \u03a9(n), and is consequently often faster in practical implementations. Quicksort, however, is considered by many to be the fastest general-purpose sort algorithm in practice. Its average-case complexity is O(n log n), with a much smaller coefficient, in good implementations, than merge sort's, even though it is quadratic in the worst case. On the plus side, merge sort is a stable sort, parallelizes better, and is more efficient at handling slow-to-access sequential media. Merge sort is often the best choice for sorting a linked list: in this situation it is relatively easy to implement a merge sort in such a way that it does not require \u03a9(n) auxiliary space (instead only \u03a9(1)), and the slow random-access performance of a linked list makes some other algorithms (such as quick sort) perform poorly, and others (such as heapsort) completely impossible.\nAs of Perl 5.8, merge sort is its default sorting algorithm (it was quicksort in previous versions of Perl). In Java, the Arrays.sort() methods use mergesort and a tuned quicksort depending on the datatypes.\n[ edit ] Utility in Online Sorting\nMergesort's merge operation is useful in online sorting, where the list to be sorted is received a piece at a time, instead of all at the beginning. In this application, we sort each new piece that is received using any sorting algorithm, and then merge it into our sorted list so far using the merge operation. However, this approach can be expensive in time and space if the received pieces are small compared to the sorted list \u2014 a better approach in this case is to store the list in a self-balancing binary search tree and add elements to it as they are received.\n"}, {"score": 2092.7534, "uuid": "390e1500-3a88-520c-99ae-1ee1b5178535", "index": "cw12", "trec_id": "clueweb12-1509wb-68-24073", "target_hostname": "staff.ustc.edu.cn", "target_uri": "http://staff.ustc.edu.cn/~csli/graduate/algorithms/book6/chap08.htm", "page_rank": 1.1990003e-09, "spam_rank": 71, "title": "Intro to <em>Algorithms</em>: CHAPTER 8: <em>QUICKSORT</em>", "snippet": "Therefore the worstcase running time of <em>quicksort</em> <em>is</em> no <em>better</em> than that of insertion <em>sort</em>. Moreover, the (n2) running time occurs when the input array <em>is</em> already completely sorted--a common situation in <em>which</em> insertion <em>sort</em> runs in O", "explanation": null, "document": ""}, {"score": 1711.0376, "uuid": "1ab66018-66ce-53f3-a106-34f39d01379b", "index": "cw12", "trec_id": "clueweb12-1607wb-47-06054", "target_hostname": "cs.wheaton.edu", "target_uri": "http://cs.wheaton.edu/~tvandrun/cs245/proj-quicksort.html", "page_rank": 1.1768588e-09, "spam_rank": 83, "title": "Project 1: <em>QuickSort</em>", "snippet": "<em>Merge</em> <em>sort</em> <em>is</em> O(n lg n), whereas <em>quicksort</em> <em>is</em> O(n^2). (O(n lg n) <em>is</em> faster). However, these are worst cases; we might find that experimentally one of the <em>algorithms</em> may behave <em>better</em> on average.", "explanation": null, "document": "Project 1: QuickSort\nThe goal of this project is reinforce the work on sorting arrays by implementing the quicksort algorithm and to conduct a careful emperical study of the relative performace of sorting algorithms.\n1. Introduction\nRecall the difference between insertion and selection sorts. Although they share the same basic pattern (repeatedly move an element from the unsorted section to the sorted section), we have\nInsertion\nTake the next unsorted item (easy)\nput it in the right place (hard)\nSelection\nTake the smallest unsorted item (hard)\nput it in the next place (easy)\nNow, consider the basic pattern of merge sort:\nsplit\nsort the pieces recursively\nreunite\nMore specifically, merge sort splits the array simplistically (it's the \"easy\" part). The reuniting is complicated. What if there were a sorting algorithm that had the same basic pattern as merge sort, but stood in analogy to merge sort as selection stands towards insertion? In other words, could there be a recursive, divide-and-conquer, sorting algorithm for which the splitting is the hard part, but leaves an easy or trivial reuniting at the end?\nTo design such an algorithm, let's take another look at what merge sort does. Since the splitting is trivial, the interesting parts are the recursive sorting and the reuniting. The recursive call sorts the subarrays internally, that is, each subarray is turned into a sorted subarray. The reuniting sorts the subarrays with respect to each other, that is, it moves elements between subarrays, so the entire range is sorted.\nTo rephrase our earlier question, the analogue to mergesort would be an algorithm that sorts the subarrays with respect to each other before sorting each subarray. Then reuniting them is trivial. The algorithm is called \"quicksort,\" and as its name suggests, it is a very good sorting algorithm.\nHere's a big picture view of it. Suppose we have an array, as pictured below. We pick one element, x, and delegate it as the \"pivot.\" In many formulations of array-based quicksort, the last element is used as the pivot (though it doesn't need to be the last).\nThen we separate the other elements based on whether they are greater than or less than the pivot, placing the pivot in the middle.\nIn a closer look, while we do this partitioning of the array, we maintain three portions of the array: the (processed) elements that are less than the pivot, the (processed) elements that are greater than the pivot, and the unprocessed elements. (In a way, the pivot itself constitutes a fourth portion.) The indices i and j mark the boundaries between portions: i is the last position of the portion less than the pivot and j is the first position in the unprocessed portion. Initially, the \"less than\" and \"greater than\" portions are empty.\nDuring each step in the partitioning, the element at position j is examined. It is either brought into the \"greater than\" portion (simply by incrementing j) or brought into the \"less than\" portion (by doing a swap and incrementing both i and j).\n(Think about that very carefully; it's the core and most difficult part of this project.)\nAt the end, the unprocessed portion is empty.\nAll that's left is to move the pivot in between the two portions, using a swap. We're then set to do the recursive calls on each portion.\n2. Setup\nMake a directory for this class (if you haven't already). Make a new directory for this project. Then copy into it some starter code, which will be similar to the code you started with in lab.\ncd 245\nmkdir proj1\ncd proj1\ncp /homes/tvandrun/Public/cs245/proj1/* .\n3. Implementing quicksort\nNotice there are two functions with quickSort in their name. The second function corresponds to the prototype for quick sort found in sorts.h and used in the driver sDriver.c It is finished. It starts the recursive process by calling quickSortR(), which is the one you need to finish.\nNotice that quickSortR() takes not only an array but also a starting point and a stopping point in the array. It is only sorting a given range in the array. Both functions return an int to be consistent with the other functions, all of which return the number of comparisons.\n4. Experiments\nAs we have been talking about in class, merge sort and quick sort are in different \"complexity classes\" in terms of the their worst case performance. Merge sort is O(n lg n), whereas quicksort is O(n^2). (O(n lg n) is faster). However, these are worst cases; we might find that experimentally one of the algorithms may behave better on average.\nWrite a program or programs to automate experiments to compare the running time and number of comparisons for quicksort, mergesort, shell sort, and one of the other sorts (selection, insertion, or one of the bubbles)\nThis time, instead of counting comparisons, we'll take actual time measurements. arrayUtil has a function getTimeMillis(). It returns the number of milliseconds that have elapsed since midnight, Jan 1, 1970---the standard way to do time keeping on many computer platforms. By calling this function before and after sorting and taking the difference, we can compute how long it takes to sort.\nUse several different arrays of several sizes, and compare the results for the different algorithms.\n6. To turn in\nWrite a few paragraphs explaining how you did your experiment and what your findings are. Present your data using a table and one or more graphs.\nTurn in your code, results, and write-up by copying them to a turnin directory I have prepared for you. Please turn in an entire directory. For example, if you make a directory called proj1-turnin containing all the files you want to turn in, then copy this using\ncp -r proj1-turnin /cslab.all/ubuntu/cs245/turnin/(your user id)\nDUE: Monday, Jan 30, 5:00 pm.\n"}, {"score": 1670.4525, "uuid": "b4fcbf0d-fdc2-57d6-a5fb-a3ef95bab4d9", "index": "cw12", "trec_id": "clueweb12-0500wb-77-18730", "target_hostname": "www.cs.sunysb.edu", "target_uri": "http://www.cs.sunysb.edu/~skiena/214/lectures/lect17/lect17.html", "page_rank": 1.2114754e-09, "spam_rank": 75, "title": "Mergesort and <em>Quicksort</em> Lecture 17", "snippet": "However, because the worst case <em>is</em> no longer a natural order it <em>is</em> much more difficult to occur. Since Mergesort <em>is</em> and selection <em>sort</em> <em>is</em> , there <em>is</em> no debate about <em>which</em> will be <em>better</em> for decent-sized files. But how can we compare two <em>algorithms</em> to see <em>which</em> <em>is</em> faster?", "explanation": null, "document": ", it is somewhat inconvienient to implementate using arrays, since we need space to merge.\nIn practice, the fastest sorting algorithm is Quicksort, which uses partitioning as its main idea.\nExample: Pivot about 10.\n17 12 6 19 23 8 5 10 - before\n6 8 5 10 23 19 12 17 - after\nPartitioning places all the elements less than the pivot in the left part of the array, and all elements greater than the pivot in the right part of the array. The pivot fits in the slot between them.\nNote that the pivot element ends up in the correct place in the total order!\nPartitioning the elements\nOnce we have selected a pivot element, we can partition the array in one linear scan, by maintaining three sections of the array: < pivot, > pivot, and unexplored.\nExample: pivot about 10\n| 17   12   6   19   23    8   5  | 10\n|  5   12   6   19   23    8 | 17 \n   5 | 12   6   19   23    8 | 17   \n   5 | 8    6   19   23 | 12   17\n   5   8  | 6   19   23 | 12   17\n   5   8    6 | 19   23 | 12   17   \n   5   8    6 | 23 | 19   12   17\n   5   8    6 ||23   19   12   17   \n   5   8    6   10   19   12   17   23\nAs we scan from left to right, we move the left bound to the right when the element is less than the pivot, otherwise we swap it with the rightmost unexplored element and move the right bound one step closer to the left.\nSince the partitioning step consists of at most n swaps, takes time linear in the number of keys. But what does it buy us?\nThe pivot element ends up in the position it retains in the final sorted order.\nAfter a partitioning, no element flops to the other side of the pivot in the final sorted order.\nThus we can sort the elements to the left of the pivot and the right of the pivot independently!\nThis gives us a recursive sorting algorithm, since we can use the partitioning approach to sort each subproblem.\nQuicksort Implementation\nMODULE Quicksort EXPORTS Main;            (*18.07.94. LB*)\n(* Read in an array of integers, sort it using the Quicksort algorithm,\n   and output the array.\n\n   See Chapter 14 for the explanation of the file handling and Chapter\n   15 for exception handling, which is used in this example.\n*)\n\n  IMPORT SIO, SF;\n\n  VAR \n    out: SIO.Writer;\n\n  TYPE \n    ElemType = INTEGER;\n  VAR \n    array: ARRAY [1 .. 10] OF ElemType;\n    \n  PROCEDURE InArray(VAR a: ARRAY OF ElemType) RAISES {SIO.Error} =\n  (*Reads a sequence of numbers. Passes SIO.Error for bad file format.*)\n  VAR \n    in:= SF.OpenRead(\"vector\");           (*open input file*)\n  BEGIN\n    FOR i:= FIRST(a) TO LAST(a) DO a[i]:= SIO.GetInt(in) END;\n  END InArray;\n\n  PROCEDURE OutArray(READONLY a: ARRAY OF ElemType) =\n  (*Outputs an array of numbers*)\n  BEGIN\n    FOR i:= FIRST(a) TO LAST(a) DO SIO.PutInt(a[i], 4, out) END;\n    SIO.Nl(out);\n  END OutArray;\n\n  PROCEDURE Quicksort(VAR a: ARRAY OF ElemType; left, right: CARDINAL) =\n  VAR \n    i, j: INTEGER; \n    x, w: ElemType;\n  BEGIN\n     \n   (*Partitioning:*)\n    i:= left;                             (*i iterates upwards from left*)\n    j:= right;                            (*j iterates down from right*)\n    x:= a[(left + right) DIV 2];          (*x is the middle element*)\n    REPEAT\n      WHILE a[i] < x DO INC(i) END;       (*skip elements < x in left part*)\n      WHILE a[j] > x DO DEC(j) END;       (*skip elements > x in right part*)\n      IF i <= j THEN\n        w:= a[i]; a[i]:= a[j]; a[j]:= w;  (*swap a[i] and a[j]*)\n        INC(i); \n        DEC(j);\n      END; (*IF i <= j*)\n    UNTIL i > j;\n    \n    (*recursive application of partitioning to subarrays:*)\n    \n    IF left < j  THEN Quicksort(a, left, j) END;\n    IF i < right THEN Quicksort(a, i, right) END;\n    \n  END Quicksort;\n\nBEGIN\n  TRY                                         (*grasps bad file format*)\n    InArray(array);                           (*read an array in*)\n    out:= SF.OpenWrite();                     (*create output file*)\n    OutArray(array);                          (*output the array*)\n    Quicksort(array, 0, NUMBER(array) - 1);   (*sort the array*)\n    OutArray(array);                          (*display the array*)\n    SF.CloseWrite(out);                       (*close output file to make it permanent*)\n  EXCEPT\n    SIO.Error =>  SIO.PutLine(\"bad file format\");\n  END; (*TRY*)\nEND Quicksort.\nBest Case for Quicksort\nSince each element ultimately ends up in the correct position, the algorithm correctly sorts. But how long does it take?\nThe best case for divide-and-conquer algorithms comes when we split the input as evenly as possible. Thus in the best case, each subproblem is of size n/2.\nThe partition step on each subproblem is linear in its size. Thus the total effort in partitioning the\nproblems of size\nThe recursion tree for the best case looks like this:\nThe total partitioning on each level is O(n), and it take\nlevels of perfect partitions to get to single element subproblems. When we are down to single elements, the problems are sorted. Thus the total time in the best case is\n.\nWorst Case for Quicksort\nSuppose instead our pivot element splits the array as unequally as possible. Thus instead of n/2 elements in the smaller half, we get zero, meaning that the pivot element is the biggest or smallest element in the array.\nNow we have n-1 levels, instead of\n, for a worst case time of\n, since the first n/2 levels each have\nelements to partition.\nThus the worst case time for Quicksort is worse than Heapsort or Mergesort.\nTo justify its name, Quicksort had better be good in the average case. Showing this requires some fairly intricate analysis.\nThe divide and conquer principle applies to real life. If you will break a job into pieces, it is best to make the pieces of equal size!\nIntuition: The Average Case for Quicksort\nThe book contains a rigorous proof that quicksort is\nin the average case. I will instead give an intuitive, less formal explanation of why this is so.\nSuppose we pick the pivot element at random in an array of n keys.\nHalf the time, the pivot element will be from the center half of the sorted array.\nWhenever the pivot element is from positions n/4 to 3n/4, the larger remaining subarray contains at most 3n/4 elements.\nIf we assume that the pivot element is always in this range, what is the maximum number of partitions we need to get from n elements down to 1 element?\ngood partitions suffice.\nAt most\nlevels of decent partitions suffices to sort an array of n elements.\nBut how often when we pick an arbitrary element as pivot will it generate a decent partition?\nSince any number ranked between n/4 and 3n/4 would make a decent pivot, we get one half the time on average.\nIf we need\nlevels of decent partitions to finish the job, and half of random partitions are decent, then on average the recursion tree to quicksort the array has\nlevels.\nSince O(n) work is done partitioning on each level, the average time is\n.\nMore careful analysis shows that the expected number of comparisons is\n.\nWhat is the Worst Case?\nThe worst case for Quicksort depends upon how we select our partition or pivot element. If we always select either the first or last element of the subarray, the worst-case occurs when the input is already sorted!\nA B D F H J K\n            B D F H J K\n              D F H J K\n                F H J K\n                  H J K\n                    J K\n                      K\nHaving the worst case occur when they are sorted or almost sorted is very bad, since that is likely to be the case in certain applications.\nTo eliminate this problem, pick a better pivot:\nUse the middle element of the subarray as pivot.\nUse a random element of the array as the pivot.\nPerhaps best of all, take the median of three elements (first, last, middle) as the pivot. Why should we use median instead of the mean?\nWhichever of these three rules we use, the worst case remains\n. However, because the worst case is no longer a natural order it is much more difficult to occur.\nIs Quicksort really faster than Mergesort?\nSince Mergesort is\n, there is no debate about which will be better for decent-sized files.\nBut how can we compare two\nalgorithms to see which is faster? Using the RAM model and the big Oh notation, we can't!\nWhen Quicksort is implemented well, it is typically 2-3 times faster than mergesort or heapsort. The primary reason is that the operations in the innermost loop are simpler. The best way to see this is to implement both and experiment with different inputs.\nSince the difference between the two programs will be limited to a multiplicative constant factor, the details of how you program each algorithm will make a big difference.\nIf you don't want to believe me when I say Quicksort is faster, I won't argue with you. It is a question whose solution lies outside the tools we are using. The best way to tell is to implement them and experiment.\nCombining Quicksort and Insertion Sort\nWhen we compare the expected number of comparisons for Quicksort + Insertion sort, a funny thing happens for small n:\nWhy not take advantage of this, and switch over to insertion sort when the size of the subarray falls below a certain threshhold?\nWhy not indeed? But how do we find the right switch point to optimize performance? Experiments are more useful than analysis here.\nRandomization\nSuppose you are writing a sorting program, to run on data given to you by your worst enemy. Quicksort is good on average, but bad on certain worst-case instances.\nIf you used Quicksort, what kind of data would your enemy give you to run it on? Exactly the worst-case instance, to make you look bad.\nBut instead of picking the median of three or the first element as pivot, suppose you picked the pivot element at random.\nNow your enemy cannot design a worst-case instance to give to you, because no matter which data they give you, you would have the same probability of picking a good pivot!\nRandomization is a very important and useful idea. By either picking a random pivot or scrambling the permutation before sorting it, we can say:\n``With high probability, randomized quicksort runs in\ntime.''\nWhere before, all we could say is:\n``If you give me random input data, quicksort runs in expected\ntime.''\nSince the time bound how does not depend upon your input distribution, this means that unless we are extremely unlucky (as opposed to ill prepared or unpopular) we will certainly get good performance.\nRandomization is a general tool to improve algorithms with bad worst-case but good average-case complexity.\nThe worst-case is still there, but we almost certainly won't see it.\n"}, {"score": 1652.8116, "uuid": "0ba84daa-b768-5847-b1a5-2b332031ce4d", "index": "cw12", "trec_id": "clueweb12-1211wb-01-21381", "target_hostname": "java.dzone.com", "target_uri": "http://java.dzone.com/articles/algorithm-week-bubble-sort?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+javalobby%2Ffrontpage+%28Javalobby+%2F+Java+Zone%29", "page_rank": 1.1700305e-09, "spam_rank": 89, "title": "<em>Algorithm</em> of the Week: Bubble <em>Sort</em> | Javalobby", "snippet": "Bubble <em>sort</em> compared to <em>quicksort</em>, <em>merge</em> <em>sort</em> and heapsort in the average case Even for small values of n, the number of comparisons and swaps can be tremendous.", "explanation": null, "document": "| More\nThis section is brought to you in partnership with:\nIt\u2019s weird that bubble sort is the most famous sorting algorithm in practice since it is one of the worst approaches for data sorting. Why is bubble sort so famous? Perhaps because of its exotic name or because it is so easy to implement. First let\u2019s take a look on its nature.\nBubble sort consists of comparing each pair of adjacent items. Then one of those two items is considered smaller (lighter) and if the lighter element is on the right side of its neighbour, they swap places. Thus the lightest element bubbles to the surface and at the end of each iteration it appears on the top. I\u2019ll try to explain this simple principle with some pictures.\n1. Each two adjacent elements are compared\nIn bubble sort we've to compare each two adjacent elements\nHere \u201c2\u2033 appears to be less than \u201c4\u2033, so it is considered lighter and it continues to bubble to the surface (the front of the array).\n2. Swap with heavier elements\nIf heavier elements appear on the way we should swap them\nOn his way to the surface the currently lightest item meets a heavier element. Then they swap places.\n3. Move forward and swap with each heavier item\nSwapping is slow and that is the main reason not to use bubble sort\nThe problem with bubble sort is that you may have to swap a lot of elements.\n4. If there is a lighter element, then this item begins to bubble to the surface\nWe can be sure that on each step the algorithm bubbles the lightest element so far\nIf the currently lightest element meets another item that is lighter, then the newest currently lightest element starts to bubble to the top.\n5. Finally the lightest element is on its place\nFinally the list begins to look sorted\nAt the end of each iteration we can be sure that the lightest element is on the right place \u2013 at the beginning of the list.\nThe problem is that this algorithm needs a tremendous number of comaprisons and as we know already this can be slow.\nWe can easily see how ineffective bubble sort is. Now the question remains \u2013 why is it so famous? Maybe indeed the answer lies in the simplicity of its implementation. Let\u2019s see how to implement bubble sort.\nImplementation\nImplementing bubble sort is easy. The question is how easy? Well, obviously after understanding the principles of this algorithm every developer, even a beginner, can implement it. Here\u2019s a PHP implementation of bubble sort.\n$input = array(6, 5, 3, 1, 8, 7, 2, 4);\n \nfunction bubble_sort($arr)\n{\n\t$length = count($arr);\n \n\tfor ($i = 0; $i < $length; $i++) {\n\t\tfor ($j = $length-1; $j > $i; $j--) {\n\t\t\tif ($arr[$j] < $arr[$j-1]) {\n\t\t\t\t$t = $arr[$j];\n\t\t\t\t$arr[$j] = $arr[$j-1];\n\t\t\t\t$arr[$j-1] = $t;\n\t\t\t}\n\t\t}\n\t}\n \n\treturn $arr;\n}\n \n// 1, 2, 3, 4, 5, 6, 7, 8\n$output = bubble_sort($input);\nClearly the implementation consists of few lines of code and two nested loops.\nComplexity: Where\u2019s Bubble Sort Compared to Other Sorting Algorithms\nCompared to other sorting algorithm, bubble sort is really slow. Indeed the complexity of this algoritm is O(n2) which can\u2019t be worse. It\u2019s weird that the most well known sorting algorithm is the slowest one.\nBubble sort compared to quicksort, merge sort and heapsort in the average case\nEven for small values of n, the number of comparisons and swaps can be tremendous.\nBubble sort is three times slower than quicksort even for n = 100, but it's easier to impelemnt\nAnother problem is that most of the languages (libraries) have built-in sorting functions, that they don\u2019t make use of bubble sort and are faster for sure. So why a developer should implement bubble sort at all?\nApplication: 3 Cool Reasons To Use Bubble Sort\nWe saw that bubble sort is slow and ineffective, yet it is used in practice. Why? Is there any reason to use this slow, ineffective algorithm with weird name? Yes and here are some of them that might be helpful for any developer.\n1. It is easy to implement\nDefinitely bubble sort is easier to implement than other \u201ccomplex\u201d sorting algorithms as quicksort. Bubble sort is easy to remember and easy to code and that\u2019s great instead of learning and remembering tons of code.\n2. Because the library can\u2019t help\nLet\u2019s say you work with JavaScript. Great, there you get array.sort() which can help for this: [3, 1, 2].sort(). But what would happen if you\u2019d rather like to sort more \u201ccomplex\u201d structures like \u2026 some DOM nodes. Here we have three LI nodes and we want to sort them in some order. Obviously you can\u2019t compare them with the \u201c<\" operator, so we've to come up with some custom solution.\n<a href=\"#\">Click here to sort the list</a>\n \n<li>node 3</li>\n<li>node 1</li>\n<li>node 2</li>\nHere sort() can\u2019t help us and we\u2019ve to code our own function. However we have only few elements (three in our case). Why not using bubble sort?\n3. The list is almost sorted\nOne of the problems with bubble sort is that it consists of too much swapping, but what if we know that the list is almost sorted?\n// almost sorted\n[1, 2, 4, 3, 5]\nWe have to swap only 3 with 4.\nNote that in the best case bubble sort\u2019s complexity is O(n) \u2013 faster than quicksort\u2019s best case!\n"}, {"score": 1543.2021, "uuid": "c0eb35c3-8110-55ed-9484-bd402b72cdfb", "index": "cw12", "trec_id": "clueweb12-1211wb-46-04504", "target_hostname": "java.dzone.com", "target_uri": "http://java.dzone.com/articles/algorithm-week-shell-sort?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+zones%2Fphp+%28PHP+Zone%29", "page_rank": 1.1700305e-09, "spam_rank": 84, "title": "<em>Algorithm</em> of the Week: Shell <em>Sort</em> | Javalobby", "snippet": "Well, as insertion <em>sort</em> and bubble <em>sort</em>, Shell <em>sort</em> <em>is</em> not very effective compared to <em>quicksort</em> <em>or</em> <em>merge</em> <em>sort</em>. The good thing <em>is</em> that it <em>is</em> quite easy to implement (not easier than insertion <em>sort</em>), but in general it should be avoided for large data sets.", "explanation": null, "document": "| More\nThis section is brought to you in partnership with:\nInsertion sort is a great algorithm, because it\u2019s very intuitive and it is easy to implement, but the problem is that it makes many exchanges for each \u201clight\u201d element in order to put it on the right place. Thus \u201clight\u201d elements at the end of the list may slow down the performance of insertion sort a lot. That is why in 1959 Donald Shell proposed an algorithm that tries to overcome this problem by comparing items of the list that lie far apart.\nInsertion sort compares every single item with all the rest elements of the list in order to find its place, while Shell sort compares items that lie far apart. This means light elements move faster to the front of the list.\nOn the other hand, it is obvious that by comparing items that lie apart the list can\u2019t be sorted in one pass like insertion sort. That is why on each pass we should use a fixed gap between the items, then decrease the value on every consecutive iteration.\nWe start to compare items with a fixed gap, that becomes lesser on each iteration until it gets to 1.\nHowever it is intuitively clear that Shell sort may need even more comparisons than insertion sort. Then why should we use it?\nThe thing is that insertion sort is not an effective sorting algorithm at all, but in some cases, when the list is almost sorted it can be quite useful. With Shell sort, once the list is sorted for gap = i, it is sorted for every gap = j, where j < i, and this is its main advantage.\nShell sort can make less exchanges than insertion sort.\nHow to choose gap size\nA \"not-so-cool\" thing about Shell sort is that we have to choose \u201cthe perfect\u201d gap sequence for our list. This is not an easy task, because it is very dependent on he input data. The good news is that there are some gap sequences proven to work well in general cases.\nShell Sequence\nDonald Shell proposes a sequence that follows the formula FLOOR(N/2k), then for N = 1000, we get the following sequence: [500, 250, 125, 62, 31, 15, 7, 3, 1]\nShell sequence for N=1000: (500, 250, 125, 62, 31, 15, 7, 3, 1)\nPratt Sequence\nPratt proposes another sequence that\u2019s growing with a slower pace than the Shell\u2019s sequence. He proposes successive numbers of the form 2p3q or [1, 2, 3, 4, 6, 8, 9, 12, ...].\nPratt sequence: (1, 2, 3, 4, 6, 8, 9, 12, ...)\nKnuth Sequence\nKnuth on other hand proposes his own sequence following the formula (3k \u2013 1) / 2 or [1, 4, 14, 40, 121, ...]\nKnuth sequence: (1, 4, 13, 40, 121, ...)\nOf course there are many other gap sequences, proposed by various developers and researchers, but the problem is that the effectiveness of the algorithm strongly depends on the input data. But before taking a look to the complexity of Shell sort, let\u2019s see first its implementation.\nImplementation\nHere\u2019s a Shell sort implementation on PHP using the Pratt gap sequence. The thing is that for this data set other gap sequences may appear to be better solutions.\n$input = array(6, 5, 3, 1, 8, 7, 2, 4);\n \nfunction shell_sort($arr)\n{\n\t$gaps = array(1, 2, 3, 4, 6);\n\t$gap  = array_pop($gaps);\n\t$len  = count($arr);\n \n\twhile($gap > 0)\n\t{\n\t\tfor($i = $gap; $i < $len; $i++) {\n \n\t\t\t$temp = $arr[$i];\n\t\t\t$j = $i;\n \n\t\t\twhile($j >= $gap && $arr[$j - $gap] > $temp) {\n\t\t\t\t$arr[$j] = $arr[$j - $gap];\n\t\t\t\t$j -= $gap;\n\t\t\t}\n \n\t\t\t$arr[$j] = $temp;\n\t\t}\n \n\t\t$gap = array_pop($gaps);\n\t}\n \n\treturn $arr;\n}\n \n// 1, 2, 3, 4, 5, 6, 7, 8\nshell_sort($input);\nIt\u2019s easy to change this code in order to work with Shell sequence.\n$input = array(6, 5, 3, 1, 8, 7, 2, 4);\n \nfunction shell_sort($arr)\n{\n        $len  = count($arr);\n\t$gap  = floor($len/2);\n \n\twhile($gap > 0)\n\t{\n\t\tfor($i = $gap; $i < $len; $i++) {\n \n\t\t\t$temp = $arr[$i];\n\t\t\t$j = $i;\n \n\t\t\twhile($j >= $gap && $arr[$j - $gap] > $temp) {\n\t\t\t\t$arr[$j] = $arr[$j - $gap];\n\t\t\t\t$j -= $gap;\n\t\t\t}\n \n\t\t\t$arr[$j] = $temp;\n\t\t}\n \n\t\t$gap = floor($gap/2);\n\t}\n \n\treturn $arr;\n}\n \n// 1, 2, 3, 4, 5, 6, 7, 8\nshell_sort($input);\nComplexity\nYet again we can\u2019t determine the exact complexity of this algorithm, because it depends on the gap sequence. However we may say what is the complexity of Shell sort with the sequences of Knuth, Pratt and Donald Shell. For the Shell\u2019s sequence the complexity is O(n2), while for the Pratt\u2019s sequence it is O(n*log2(n)). The best approach is the Knuth sequence where the complexity is O(n3/2), as you can see on the diagram bellow.\nComplexity of Shell sort with different gap sequences.\nApplication\nWell, as insertion sort and bubble sort, Shell sort is not very effective compared to quicksort or merge sort. The good thing is that it is quite easy to implement (not easier than insertion sort), but in general it should be avoided for large data sets. Perhaps the main advantage of Shell sort is that the list can be sorted for a gap greater than 1 and thus making less exchanges than insertion sort.\nRelated posts:\n"}, {"score": 1522.7202, "uuid": "640667ff-f6e5-5352-a09b-fa7c7d8fda01", "index": "cw12", "trec_id": "clueweb12-1613wb-83-27256", "target_hostname": "python.dzone.com", "target_uri": "http://python.dzone.com/articles/algorithm-week-radix-sort", "page_rank": 1.2215032e-09, "spam_rank": 93, "title": "<em>Algorithm</em> of the Week: Radix <em>Sort</em> | Python Zone", "snippet": "Because the input <em>is</em> so important for an algorithm&#x27;s efficiency, we may ask if there are any sorting <em>algorithms</em> that are faster than O(n.log(n)), <em>which</em> <em>is</em> the average-case complexity for <em>merge</em> <em>sort</em> and <em>quicksort</em>.", "explanation": null, "document": "| More\nAlgorithms always depend on the input. We saw that general purpose sorting algorithms like insertion sort, bubble sort and quicksort can be very efficient in some cases and inefficient in others. Indeed, insertion and bubble sort are considered slow, with a best-case complexity of O(n2), but they are quite effective when the input is fairly sorted. So, when you have a sorted array and you add some \u201cnew\u201d values to the array you can sort it quite effectively with insertion sort. On the other hand, quicksort is considered one of the best general purpose sorting algorithms, but while it\u2019s a great algorithm when the data is randomized, it\u2019s practically as slow as bubble sort when the input is almost or fully sorted.\nNow we see that the effectiveness of algorithms depends greatly on the input. For input that is almost sorted, insertion sort may be preferred instead of quicksort, which is generally a faster algorithm.\nBecause the input is so important for an algorithm's efficiency, we may ask if there are any sorting algorithms that are faster than O(n.log(n)), which is the average-case complexity for merge sort and quicksort. And the answer is yes there are faster, linear complexity algorithms, that can sort data faster than quicksort, merge sort and heapsort. But there are some constraints!\nEverything sounds great but we can\u2019t sort any particular data with linear complexity, so the question is what rules must the input follow in order to be sorted in linear time?\nSuch an algorithm that is capable of sorting data in linear O(n) time is radix sort and the domain of the input is restricted \u2013 it must consist only of integers.\nOverview\nLet\u2019s say we have an array of integers which is not sorted. Because it consists only of integers and because array keys are integers in programming languages we can implement radix sort.\nFirst for each value of the input array we put the value of \u201c1\u201d on the key-th place of the temporary array as explained on the following diagram.\nIf there are repeating values in the input array, we increment the corresponding value in the temporary array. After \u201cinitializing\u201d the temporary array with one pass (with linear complexity) we can sort the input.\nImplementation\nImplementing radix sort is in fact very easy, which is great. The thing is that old-school programming languages weren\u2019t very flexible and we needed to initialize the entire temporary array. That leads to another problem \u2013 we must know the interval of values from the input. Fortunately, modern programming languages and libraries are more flexible so we can initialize our temporary array even if we don\u2019t know the interval of input values, as in the example bellow. Indeed, PHP is flexible enough to build-up arrays in the memory without knowing their size in advance.\n$list = array(4, 3, 5, 9, 7, 2, 4, 1, 6, 5);\n \nfunction radix_sort($input)\n{\n    $temp = $output = array();\n\t$len = count($input);\n \n    for ($i = 0; $i < $len; $i++) {\n\t\t$temp[$input[$i]] = ($temp[$input[$i]] > 0) \n\t\t\t? ++$temp[$input[$i]]\n\t\t\t: 1;\n    }\n \n    ksort($temp);\n \n    foreach ($temp as $key => $val) {\n\t\tif ($val == 1) {\n\t\t\t$output[] = $key; \n\t\t} else {\n\t\t\twhile ($val--) {\n\t\t\t\t$output[] = $key;\n\t\t\t}\n        }\n    }\n \n    return $output;\n}\n \n// 1, 2, 3, 4, 4, 5, 5, 6, 7, 9\nprint_r(radix_sort($list));\nThe problem is that PHP needs ksort \u2013 which is completely foolish as we\u2019re trying to sort an array using \u201canother\u201d sorting method, but to overcome this you must know the interval of values in advance and initialize a temporary array with 0s, as in the example bellow.\ndefine(MIN, 1);\ndefine(MAX, 9);\n$list = array(4, 3, 5, 9, 7, 2, 4, 1, 6, 5);\n \nfunction radix_sort(&$input)\n{\n    $temp = array();\n\t$len = count($input);\n \n\t// initialize with 0s\n    $temp = array_fill(MIN, MAX-MIN+1, 0);\n \n    foreach ($input as $key => $val) {\n    \t$temp[$val]++;\n    }\n \n    $input = array();\n    foreach ($temp as $key => $val) {\n\tif ($val == 1) {\n\t\t$input[] = $key;\n\t} else {\n\t\twhile ($val--) {\n\t\t\t$input[] = $key;\n\t\t}\n\t}\n    }\n}\n \n// 4, 3, 5, 9, 7, 2, 4, 1, 6, 5\nvar_dump($list);\n \nradix_sort(&$list);\n \n// 1, 2, 3, 4, 5, 5, 6, 7, 8, 9\nvar_dump($list);\nHere the input is modified during the sorting process and it\u2019s used as a result.\nComplexity\nThe complexity of radix sort is linear, which in terms of omega means O(n). That is a great benefit in performance compared to O(n.log(n)) or even worse with O(n2) as we can see in the following chart.\nWhy Use Radix Sort\n1. It\u2019s fast\nRadix sort is very fast compared to other sorting algorithms as we saw on the diagram above. This algorithm is very useful in practice because in practice we often sort sets of integers.\n2. It\u2019s easy to understand and implement\nEven a beginner can understand and implement radix sort, which is great. You need no more than a few loops to implement it.\nWhy NOT using radix sort\n1. Works only with integers\nIf you\u2019re not sure about the input, you're better off not using radix sort. We may think that our input consists only of integers and we can go for radix sort, but what if in the future someone passes floats or strings to our routine.\n2. Requires additional space\nRadix sort needs additional space \u2013 at least as much as the input.\nFinal Words\nRadix sort is restricted by the input\u2019s domain, but I must say that in practice there are tons of cases where only integers are sorted. This is when we get some data from the db based on primary keys \u2013 typically primary in database tables are integers as well. So practically there are lots of cases of sorting integers, so radix sort may be one very, very useful algorithm and it is so cool that it is also easy to implement.\nTags:\n"}, {"score": 1521.9666, "uuid": "602a7339-3227-52e4-9702-9297a78e3498", "index": "cw12", "trec_id": "clueweb12-1613wb-61-18874", "target_hostname": "php.dzone.com", "target_uri": "http://php.dzone.com/articles/algorithm-week-radix-sort", "page_rank": 1.2176898e-09, "spam_rank": 93, "title": "<em>Algorithm</em> of the Week: Radix <em>Sort</em> | PHP Zone", "snippet": "Because the input <em>is</em> so important for an algorithm&#x27;s efficiency, we may ask if there are any sorting <em>algorithms</em> that are faster than O(n.log(n)), <em>which</em> <em>is</em> the average-case complexity for <em>merge</em> <em>sort</em> and <em>quicksort</em>.", "explanation": null, "document": "| More\nAlgorithms always depend on the input. We saw that general purpose sorting algorithms like insertion sort, bubble sort and quicksort can be very efficient in some cases and inefficient in others. Indeed, insertion and bubble sort are considered slow, with a best-case complexity of O(n2), but they are quite effective when the input is fairly sorted. So, when you have a sorted array and you add some \u201cnew\u201d values to the array you can sort it quite effectively with insertion sort. On the other hand, quicksort is considered one of the best general purpose sorting algorithms, but while it\u2019s a great algorithm when the data is randomized, it\u2019s practically as slow as bubble sort when the input is almost or fully sorted.\nNow we see that the effectiveness of algorithms depends greatly on the input. For input that is almost sorted, insertion sort may be preferred instead of quicksort, which is generally a faster algorithm.\nBecause the input is so important for an algorithm's efficiency, we may ask if there are any sorting algorithms that are faster than O(n.log(n)), which is the average-case complexity for merge sort and quicksort. And the answer is yes there are faster, linear complexity algorithms, that can sort data faster than quicksort, merge sort and heapsort. But there are some constraints!\nEverything sounds great but we can\u2019t sort any particular data with linear complexity, so the question is what rules must the input follow in order to be sorted in linear time?\nSuch an algorithm that is capable of sorting data in linear O(n) time is radix sort and the domain of the input is restricted \u2013 it must consist only of integers.\nOverview\nLet\u2019s say we have an array of integers which is not sorted. Because it consists only of integers and because array keys are integers in programming languages we can implement radix sort.\nFirst for each value of the input array we put the value of \u201c1\u201d on the key-th place of the temporary array as explained on the following diagram.\nIf there are repeating values in the input array, we increment the corresponding value in the temporary array. After \u201cinitializing\u201d the temporary array with one pass (with linear complexity) we can sort the input.\nImplementation\nImplementing radix sort is in fact very easy, which is great. The thing is that old-school programming languages weren\u2019t very flexible and we needed to initialize the entire temporary array. That leads to another problem \u2013 we must know the interval of values from the input. Fortunately, modern programming languages and libraries are more flexible so we can initialize our temporary array even if we don\u2019t know the interval of input values, as in the example bellow. Indeed, PHP is flexible enough to build-up arrays in the memory without knowing their size in advance.\n$list = array(4, 3, 5, 9, 7, 2, 4, 1, 6, 5);\n \nfunction radix_sort($input)\n{\n    $temp = $output = array();\n\t$len = count($input);\n \n    for ($i = 0; $i < $len; $i++) {\n\t\t$temp[$input[$i]] = ($temp[$input[$i]] > 0) \n\t\t\t? ++$temp[$input[$i]]\n\t\t\t: 1;\n    }\n \n    ksort($temp);\n \n    foreach ($temp as $key => $val) {\n\t\tif ($val == 1) {\n\t\t\t$output[] = $key; \n\t\t} else {\n\t\t\twhile ($val--) {\n\t\t\t\t$output[] = $key;\n\t\t\t}\n        }\n    }\n \n    return $output;\n}\n \n// 1, 2, 3, 4, 4, 5, 5, 6, 7, 9\nprint_r(radix_sort($list));\nThe problem is that PHP needs ksort \u2013 which is completely foolish as we\u2019re trying to sort an array using \u201canother\u201d sorting method, but to overcome this you must know the interval of values in advance and initialize a temporary array with 0s, as in the example bellow.\ndefine(MIN, 1);\ndefine(MAX, 9);\n$list = array(4, 3, 5, 9, 7, 2, 4, 1, 6, 5);\n \nfunction radix_sort(&$input)\n{\n    $temp = array();\n\t$len = count($input);\n \n\t// initialize with 0s\n    $temp = array_fill(MIN, MAX-MIN+1, 0);\n \n    foreach ($input as $key => $val) {\n    \t$temp[$val]++;\n    }\n \n    $input = array();\n    foreach ($temp as $key => $val) {\n\tif ($val == 1) {\n\t\t$input[] = $key;\n\t} else {\n\t\twhile ($val--) {\n\t\t\t$input[] = $key;\n\t\t}\n\t}\n    }\n}\n \n// 4, 3, 5, 9, 7, 2, 4, 1, 6, 5\nvar_dump($list);\n \nradix_sort(&$list);\n \n// 1, 2, 3, 4, 5, 5, 6, 7, 8, 9\nvar_dump($list);\nHere the input is modified during the sorting process and it\u2019s used as a result.\nComplexity\nThe complexity of radix sort is linear, which in terms of omega means O(n). That is a great benefit in performance compared to O(n.log(n)) or even worse with O(n2) as we can see in the following chart.\nWhy Use Radix Sort\n1. It\u2019s fast\nRadix sort is very fast compared to other sorting algorithms as we saw on the diagram above. This algorithm is very useful in practice because in practice we often sort sets of integers.\n2. It\u2019s easy to understand and implement\nEven a beginner can understand and implement radix sort, which is great. You need no more than a few loops to implement it.\nWhy NOT using radix sort\n1. Works only with integers\nIf you\u2019re not sure about the input, you're better off not using radix sort. We may think that our input consists only of integers and we can go for radix sort, but what if in the future someone passes floats or strings to our routine.\n2. Requires additional space\nRadix sort needs additional space \u2013 at least as much as the input.\nFinal Words\nRadix sort is restricted by the input\u2019s domain, but I must say that in practice there are tons of cases where only integers are sorted. This is when we get some data from the db based on primary keys \u2013 typically primary in database tables are integers as well. So practically there are lots of cases of sorting integers, so radix sort may be one very, very useful algorithm and it is so cool that it is also easy to implement.\nTags:\n"}, {"score": 1474.1934, "uuid": "e33f98f1-a7f2-593a-b3a8-6766fd4c5b20", "index": "cw12", "trec_id": "clueweb12-1700wb-32-23418", "target_hostname": "ruby.dzone.com", "target_uri": "http://ruby.dzone.com/articles/algorithm-week-bubble-sort", "page_rank": 1.1789956e-09, "spam_rank": 84, "title": "<em>Algorithm</em> of the Week: Bubble <em>Sort</em> | Ruby Zone", "snippet": "Bubble <em>sort</em> compared to <em>quicksort</em>, <em>merge</em> <em>sort</em> and heapsort in the average case Even for small values of n, the number of comparisons and swaps can be tremendous.", "explanation": null, "document": "| More\nIt\u2019s weird that bubble sort is the most famous sorting algorithm in practice since it is one of the worst approaches for data sorting. Why is bubble sort so famous? Perhaps because of its exotic name or because it is so easy to implement. First let\u2019s take a look on its nature.\nBubble sort consists of comparing each pair of adjacent items. Then one of those two items is considered smaller (lighter) and if the lighter element is on the right side of its neighbour, they swap places. Thus the lightest element bubbles to the surface and at the end of each iteration it appears on the top. I\u2019ll try to explain this simple principle with some pictures.\n1. Each two adjacent elements are compared\nIn bubble sort we've to compare each two adjacent elements\nHere \u201c2\u2033 appears to be less than \u201c4\u2033, so it is considered lighter and it continues to bubble to the surface (the front of the array).\n2. Swap with heavier elements\nIf heavier elements appear on the way we should swap them\nOn his way to the surface the currently lightest item meets a heavier element. Then they swap places.\n3. Move forward and swap with each heavier item\nSwapping is slow and that is the main reason not to use bubble sort\nThe problem with bubble sort is that you may have to swap a lot of elements.\n4. If there is a lighter element, then this item begins to bubble to the surface\nWe can be sure that on each step the algorithm bubbles the lightest element so far\nIf the currently lightest element meets another item that is lighter, then the newest currently lightest element starts to bubble to the top.\n5. Finally the lightest element is on its place\nFinally the list begins to look sorted\nAt the end of each iteration we can be sure that the lightest element is on the right place \u2013 at the beginning of the list.\nThe problem is that this algorithm needs a tremendous number of comaprisons and as we know already this can be slow.\nWe can easily see how ineffective bubble sort is. Now the question remains \u2013 why is it so famous? Maybe indeed the answer lies in the simplicity of its implementation. Let\u2019s see how to implement bubble sort.\nImplementation\nImplementing bubble sort is easy. The question is how easy? Well, obviously after understanding the principles of this algorithm every developer, even a beginner, can implement it. Here\u2019s a PHP implementation of bubble sort.\n$input = array(6, 5, 3, 1, 8, 7, 2, 4);\n \nfunction bubble_sort($arr)\n{\n\t$length = count($arr);\n \n\tfor ($i = 0; $i < $length; $i++) {\n\t\tfor ($j = $length-1; $j > $i; $j--) {\n\t\t\tif ($arr[$j] < $arr[$j-1]) {\n\t\t\t\t$t = $arr[$j];\n\t\t\t\t$arr[$j] = $arr[$j-1];\n\t\t\t\t$arr[$j-1] = $t;\n\t\t\t}\n\t\t}\n\t}\n \n\treturn $arr;\n}\n \n// 1, 2, 3, 4, 5, 6, 7, 8\n$output = bubble_sort($input);\nClearly the implementation consists of few lines of code and two nested loops.\nComplexity: Where\u2019s Bubble Sort Compared to Other Sorting Algorithms\nCompared to other sorting algorithm, bubble sort is really slow. Indeed the complexity of this algoritm is O(n2) which can\u2019t be worse. It\u2019s weird that the most well known sorting algorithm is the slowest one.\nBubble sort compared to quicksort, merge sort and heapsort in the average case\nEven for small values of n, the number of comparisons and swaps can be tremendous.\nBubble sort is three times slower than quicksort even for n = 100, but it's easier to impelemnt\nAnother problem is that most of the languages (libraries) have built-in sorting functions, that they don\u2019t make use of bubble sort and are faster for sure. So why a developer should implement bubble sort at all?\nApplication: 3 Cool Reasons To Use Bubble Sort\nWe saw that bubble sort is slow and ineffective, yet it is used in practice. Why? Is there any reason to use this slow, ineffective algorithm with weird name? Yes and here are some of them that might be helpful for any developer.\n1. It is easy to implement\nDefinitely bubble sort is easier to implement than other \u201ccomplex\u201d sorting algorithms as quicksort. Bubble sort is easy to remember and easy to code and that\u2019s great instead of learning and remembering tons of code.\n2. Because the library can\u2019t help\nLet\u2019s say you work with JavaScript. Great, there you get array.sort() which can help for this: [3, 1, 2].sort(). But what would happen if you\u2019d rather like to sort more \u201ccomplex\u201d structures like \u2026 some DOM nodes. Here we have three LI nodes and we want to sort them in some order. Obviously you can\u2019t compare them with the \u201c<\" operator, so we've to come up with some custom solution.\n<a href=\"#\">Click here to sort the list</a>\n \n<li>node 3</li>\n<li>node 1</li>\n<li>node 2</li>\nHere sort() can\u2019t help us and we\u2019ve to code our own function. However we have only few elements (three in our case). Why not using bubble sort?\n3. The list is almost sorted\nOne of the problems with bubble sort is that it consists of too much swapping, but what if we know that the list is almost sorted?\n// almost sorted\n[1, 2, 4, 3, 5]\nWe have to swap only 3 with 4.\nNote that in the best case bubble sort\u2019s complexity is O(n) \u2013 faster than quicksort\u2019s best case!\n"}], [{"score": 1466.087, "uuid": "2685a060-58c6-50d2-b642-477be9cb4535", "index": "cw12", "trec_id": "clueweb12-1613wb-95-18022", "target_hostname": "python.dzone.com", "target_uri": "http://python.dzone.com/articles/algorithm-week-bubble-sort", "page_rank": 1.2193554e-09, "spam_rank": 85, "title": "<em>Algorithm</em> of the Week: Bubble <em>Sort</em> | Python Zone", "snippet": "Bubble <em>sort</em> compared to <em>quicksort</em>, <em>merge</em> <em>sort</em> and heapsort in the average case Even for small values of n, the number of comparisons and swaps can be tremendous.", "explanation": null, "document": "| More\nIt\u2019s weird that bubble sort is the most famous sorting algorithm in practice since it is one of the worst approaches for data sorting. Why is bubble sort so famous? Perhaps because of its exotic name or because it is so easy to implement. First let\u2019s take a look on its nature.\nBubble sort consists of comparing each pair of adjacent items. Then one of those two items is considered smaller (lighter) and if the lighter element is on the right side of its neighbour, they swap places. Thus the lightest element bubbles to the surface and at the end of each iteration it appears on the top. I\u2019ll try to explain this simple principle with some pictures.\n1. Each two adjacent elements are compared\nIn bubble sort we've to compare each two adjacent elements\nHere \u201c2\u2033 appears to be less than \u201c4\u2033, so it is considered lighter and it continues to bubble to the surface (the front of the array).\n2. Swap with heavier elements\nIf heavier elements appear on the way we should swap them\nOn his way to the surface the currently lightest item meets a heavier element. Then they swap places.\n3. Move forward and swap with each heavier item\nSwapping is slow and that is the main reason not to use bubble sort\nThe problem with bubble sort is that you may have to swap a lot of elements.\n4. If there is a lighter element, then this item begins to bubble to the surface\nWe can be sure that on each step the algorithm bubbles the lightest element so far\nIf the currently lightest element meets another item that is lighter, then the newest currently lightest element starts to bubble to the top.\n5. Finally the lightest element is on its place\nFinally the list begins to look sorted\nAt the end of each iteration we can be sure that the lightest element is on the right place \u2013 at the beginning of the list.\nThe problem is that this algorithm needs a tremendous number of comaprisons and as we know already this can be slow.\nWe can easily see how ineffective bubble sort is. Now the question remains \u2013 why is it so famous? Maybe indeed the answer lies in the simplicity of its implementation. Let\u2019s see how to implement bubble sort.\nImplementation\nImplementing bubble sort is easy. The question is how easy? Well, obviously after understanding the principles of this algorithm every developer, even a beginner, can implement it. Here\u2019s a PHP implementation of bubble sort.\n$input = array(6, 5, 3, 1, 8, 7, 2, 4);\n \nfunction bubble_sort($arr)\n{\n\t$length = count($arr);\n \n\tfor ($i = 0; $i < $length; $i++) {\n\t\tfor ($j = $length-1; $j > $i; $j--) {\n\t\t\tif ($arr[$j] < $arr[$j-1]) {\n\t\t\t\t$t = $arr[$j];\n\t\t\t\t$arr[$j] = $arr[$j-1];\n\t\t\t\t$arr[$j-1] = $t;\n\t\t\t}\n\t\t}\n\t}\n \n\treturn $arr;\n}\n \n// 1, 2, 3, 4, 5, 6, 7, 8\n$output = bubble_sort($input);\nClearly the implementation consists of few lines of code and two nested loops.\nComplexity: Where\u2019s Bubble Sort Compared to Other Sorting Algorithms\nCompared to other sorting algorithm, bubble sort is really slow. Indeed the complexity of this algoritm is O(n2) which can\u2019t be worse. It\u2019s weird that the most well known sorting algorithm is the slowest one.\nBubble sort compared to quicksort, merge sort and heapsort in the average case\nEven for small values of n, the number of comparisons and swaps can be tremendous.\nBubble sort is three times slower than quicksort even for n = 100, but it's easier to impelemnt\nAnother problem is that most of the languages (libraries) have built-in sorting functions, that they don\u2019t make use of bubble sort and are faster for sure. So why a developer should implement bubble sort at all?\nApplication: 3 Cool Reasons To Use Bubble Sort\nWe saw that bubble sort is slow and ineffective, yet it is used in practice. Why? Is there any reason to use this slow, ineffective algorithm with weird name? Yes and here are some of them that might be helpful for any developer.\n1. It is easy to implement\nDefinitely bubble sort is easier to implement than other \u201ccomplex\u201d sorting algorithms as quicksort. Bubble sort is easy to remember and easy to code and that\u2019s great instead of learning and remembering tons of code.\n2. Because the library can\u2019t help\nLet\u2019s say you work with JavaScript. Great, there you get array.sort() which can help for this: [3, 1, 2].sort(). But what would happen if you\u2019d rather like to sort more \u201ccomplex\u201d structures like \u2026 some DOM nodes. Here we have three LI nodes and we want to sort them in some order. Obviously you can\u2019t compare them with the \u201c<\" operator, so we've to come up with some custom solution.\n<a href=\"#\">Click here to sort the list</a>\n \n<li>node 3</li>\n<li>node 1</li>\n<li>node 2</li>\nHere sort() can\u2019t help us and we\u2019ve to code our own function. However we have only few elements (three in our case). Why not using bubble sort?\n3. The list is almost sorted\nOne of the problems with bubble sort is that it consists of too much swapping, but what if we know that the list is almost sorted?\n// almost sorted\n[1, 2, 4, 3, 5]\nWe have to swap only 3 with 4.\nNote that in the best case bubble sort\u2019s complexity is O(n) \u2013 faster than quicksort\u2019s best case!\n"}, {"score": 1460.7213, "uuid": "8aa08986-abd6-5610-a2ea-370b5772037b", "index": "cw12", "trec_id": "clueweb12-0809wb-99-09741", "target_hostname": "groovy.dzone.com", "target_uri": "http://groovy.dzone.com/articles/algorithm-week-bubble-sort", "page_rank": 2.2083955e-09, "spam_rank": 90, "title": "<em>Algorithm</em> of the Week: Bubble <em>Sort</em> | Groovy Zone", "snippet": "Bubble <em>sort</em> compared to <em>quicksort</em>, <em>merge</em> <em>sort</em> and heapsort in the average case Even for small values of n, the number of comparisons and swaps can be tremendous.", "explanation": null, "document": "| More\nThis section is brought to you in partnership with:\nIt\u2019s weird that bubble sort is the most famous sorting algorithm in practice since it is one of the worst approaches for data sorting. Why is bubble sort so famous? Perhaps because of its exotic name or because it is so easy to implement. First let\u2019s take a look on its nature.\nBubble sort consists of comparing each pair of adjacent items. Then one of those two items is considered smaller (lighter) and if the lighter element is on the right side of its neighbour, they swap places. Thus the lightest element bubbles to the surface and at the end of each iteration it appears on the top. I\u2019ll try to explain this simple principle with some pictures.\n1. Each two adjacent elements are compared\nIn bubble sort we've to compare each two adjacent elements\nHere \u201c2\u2033 appears to be less than \u201c4\u2033, so it is considered lighter and it continues to bubble to the surface (the front of the array).\n2. Swap with heavier elements\nIf heavier elements appear on the way we should swap them\nOn his way to the surface the currently lightest item meets a heavier element. Then they swap places.\n3. Move forward and swap with each heavier item\nSwapping is slow and that is the main reason not to use bubble sort\nThe problem with bubble sort is that you may have to swap a lot of elements.\n4. If there is a lighter element, then this item begins to bubble to the surface\nWe can be sure that on each step the algorithm bubbles the lightest element so far\nIf the currently lightest element meets another item that is lighter, then the newest currently lightest element starts to bubble to the top.\n5. Finally the lightest element is on its place\nFinally the list begins to look sorted\nAt the end of each iteration we can be sure that the lightest element is on the right place \u2013 at the beginning of the list.\nThe problem is that this algorithm needs a tremendous number of comaprisons and as we know already this can be slow.\nWe can easily see how ineffective bubble sort is. Now the question remains \u2013 why is it so famous? Maybe indeed the answer lies in the simplicity of its implementation. Let\u2019s see how to implement bubble sort.\nImplementation\nImplementing bubble sort is easy. The question is how easy? Well, obviously after understanding the principles of this algorithm every developer, even a beginner, can implement it. Here\u2019s a PHP implementation of bubble sort.\n$input = array(6, 5, 3, 1, 8, 7, 2, 4);\n \nfunction bubble_sort($arr)\n{\n\t$length = count($arr);\n \n\tfor ($i = 0; $i < $length; $i++) {\n\t\tfor ($j = $length-1; $j > $i; $j--) {\n\t\t\tif ($arr[$j] < $arr[$j-1]) {\n\t\t\t\t$t = $arr[$j];\n\t\t\t\t$arr[$j] = $arr[$j-1];\n\t\t\t\t$arr[$j-1] = $t;\n\t\t\t}\n\t\t}\n\t}\n \n\treturn $arr;\n}\n \n// 1, 2, 3, 4, 5, 6, 7, 8\n$output = bubble_sort($input);\nClearly the implementation consists of few lines of code and two nested loops.\nComplexity: Where\u2019s Bubble Sort Compared to Other Sorting Algorithms\nCompared to other sorting algorithm, bubble sort is really slow. Indeed the complexity of this algoritm is O(n2) which can\u2019t be worse. It\u2019s weird that the most well known sorting algorithm is the slowest one.\nBubble sort compared to quicksort, merge sort and heapsort in the average case\nEven for small values of n, the number of comparisons and swaps can be tremendous.\nBubble sort is three times slower than quicksort even for n = 100, but it's easier to impelemnt\nAnother problem is that most of the languages (libraries) have built-in sorting functions, that they don\u2019t make use of bubble sort and are faster for sure. So why a developer should implement bubble sort at all?\nApplication: 3 Cool Reasons To Use Bubble Sort\nWe saw that bubble sort is slow and ineffective, yet it is used in practice. Why? Is there any reason to use this slow, ineffective algorithm with weird name? Yes and here are some of them that might be helpful for any developer.\n1. It is easy to implement\nDefinitely bubble sort is easier to implement than other \u201ccomplex\u201d sorting algorithms as quicksort. Bubble sort is easy to remember and easy to code and that\u2019s great instead of learning and remembering tons of code.\n2. Because the library can\u2019t help\nLet\u2019s say you work with JavaScript. Great, there you get array.sort() which can help for this: [3, 1, 2].sort(). But what would happen if you\u2019d rather like to sort more \u201ccomplex\u201d structures like \u2026 some DOM nodes. Here we have three LI nodes and we want to sort them in some order. Obviously you can\u2019t compare them with the \u201c<\" operator, so we've to come up with some custom solution.\n<a href=\"#\">Click here to sort the list</a>\n \n<li>node 3</li>\n<li>node 1</li>\n<li>node 2</li>\nHere sort() can\u2019t help us and we\u2019ve to code our own function. However we have only few elements (three in our case). Why not using bubble sort?\n3. The list is almost sorted\nOne of the problems with bubble sort is that it consists of too much swapping, but what if we know that the list is almost sorted?\n// almost sorted\n[1, 2, 4, 3, 5]\nWe have to swap only 3 with 4.\nNote that in the best case bubble sort\u2019s complexity is O(n) \u2013 faster than quicksort\u2019s best case!\n"}, {"score": 1357.0703, "uuid": "0387c6c1-652c-5821-abc0-655d16bbfb62", "index": "cw12", "trec_id": "clueweb12-0809wb-99-09742", "target_hostname": "groovy.dzone.com", "target_uri": "http://groovy.dzone.com/articles/algorithm-week-shell-sort", "page_rank": 2.2342743e-09, "spam_rank": 84, "title": "<em>Algorithm</em> of the Week: Shell <em>Sort</em> | Groovy Zone", "snippet": "Well, as insertion <em>sort</em> and bubble <em>sort</em>, Shell <em>sort</em> <em>is</em> not very effective compared to <em>quicksort</em> <em>or</em> <em>merge</em> <em>sort</em>. The good thing <em>is</em> that it <em>is</em> quite easy to implement (not easier than insertion <em>sort</em>), but in general it should be avoided for large data sets.", "explanation": null, "document": "| More\nThis section is brought to you in partnership with:\nInsertion sort is a great algorithm, because it\u2019s very intuitive and it is easy to implement, but the problem is that it makes many exchanges for each \u201clight\u201d element in order to put it on the right place. Thus \u201clight\u201d elements at the end of the list may slow down the performance of insertion sort a lot. That is why in 1959 Donald Shell proposed an algorithm that tries to overcome this problem by comparing items of the list that lie far apart.\nInsertion sort compares every single item with all the rest elements of the list in order to find its place, while Shell sort compares items that lie far apart. This means light elements move faster to the front of the list.\nOn the other hand, it is obvious that by comparing items that lie apart the list can\u2019t be sorted in one pass like insertion sort. That is why on each pass we should use a fixed gap between the items, then decrease the value on every consecutive iteration.\nWe start to compare items with a fixed gap, that becomes lesser on each iteration until it gets to 1.\nHowever it is intuitively clear that Shell sort may need even more comparisons than insertion sort. Then why should we use it?\nThe thing is that insertion sort is not an effective sorting algorithm at all, but in some cases, when the list is almost sorted it can be quite useful. With Shell sort, once the list is sorted for gap = i, it is sorted for every gap = j, where j < i, and this is its main advantage.\nShell sort can make less exchanges than insertion sort.\nHow to choose gap size\nA \"not-so-cool\" thing about Shell sort is that we have to choose \u201cthe perfect\u201d gap sequence for our list. This is not an easy task, because it is very dependent on he input data. The good news is that there are some gap sequences proven to work well in general cases.\nShell Sequence\nDonald Shell proposes a sequence that follows the formula FLOOR(N/2k), then for N = 1000, we get the following sequence: [500, 250, 125, 62, 31, 15, 7, 3, 1]\nShell sequence for N=1000: (500, 250, 125, 62, 31, 15, 7, 3, 1)\nPratt Sequence\nPratt proposes another sequence that\u2019s growing with a slower pace than the Shell\u2019s sequence. He proposes successive numbers of the form 2p3q or [1, 2, 3, 4, 6, 8, 9, 12, ...].\nPratt sequence: (1, 2, 3, 4, 6, 8, 9, 12, ...)\nKnuth Sequence\nKnuth on other hand proposes his own sequence following the formula (3k \u2013 1) / 2 or [1, 4, 14, 40, 121, ...]\nKnuth sequence: (1, 4, 13, 40, 121, ...)\nOf course there are many other gap sequences, proposed by various developers and researchers, but the problem is that the effectiveness of the algorithm strongly depends on the input data. But before taking a look to the complexity of Shell sort, let\u2019s see first its implementation.\nImplementation\nHere\u2019s a Shell sort implementation on PHP using the Pratt gap sequence. The thing is that for this data set other gap sequences may appear to be better solutions.\n$input = array(6, 5, 3, 1, 8, 7, 2, 4);\n \nfunction shell_sort($arr)\n{\n\t$gaps = array(1, 2, 3, 4, 6);\n\t$gap  = array_pop($gaps);\n\t$len  = count($arr);\n \n\twhile($gap > 0)\n\t{\n\t\tfor($i = $gap; $i < $len; $i++) {\n \n\t\t\t$temp = $arr[$i];\n\t\t\t$j = $i;\n \n\t\t\twhile($j >= $gap && $arr[$j - $gap] > $temp) {\n\t\t\t\t$arr[$j] = $arr[$j - $gap];\n\t\t\t\t$j -= $gap;\n\t\t\t}\n \n\t\t\t$arr[$j] = $temp;\n\t\t}\n \n\t\t$gap = array_pop($gaps);\n\t}\n \n\treturn $arr;\n}\n \n// 1, 2, 3, 4, 5, 6, 7, 8\nshell_sort($input);\nIt\u2019s easy to change this code in order to work with Shell sequence.\n$input = array(6, 5, 3, 1, 8, 7, 2, 4);\n \nfunction shell_sort($arr)\n{\n        $len  = count($arr);\n\t$gap  = floor($len/2);\n \n\twhile($gap > 0)\n\t{\n\t\tfor($i = $gap; $i < $len; $i++) {\n \n\t\t\t$temp = $arr[$i];\n\t\t\t$j = $i;\n \n\t\t\twhile($j >= $gap && $arr[$j - $gap] > $temp) {\n\t\t\t\t$arr[$j] = $arr[$j - $gap];\n\t\t\t\t$j -= $gap;\n\t\t\t}\n \n\t\t\t$arr[$j] = $temp;\n\t\t}\n \n\t\t$gap = floor($gap/2);\n\t}\n \n\treturn $arr;\n}\n \n// 1, 2, 3, 4, 5, 6, 7, 8\nshell_sort($input);\nComplexity\nYet again we can\u2019t determine the exact complexity of this algorithm, because it depends on the gap sequence. However we may say what is the complexity of Shell sort with the sequences of Knuth, Pratt and Donald Shell. For the Shell\u2019s sequence the complexity is O(n2), while for the Pratt\u2019s sequence it is O(n*log2(n)). The best approach is the Knuth sequence where the complexity is O(n3/2), as you can see on the diagram bellow.\nComplexity of Shell sort with different gap sequences.\nApplication\nWell, as insertion sort and bubble sort, Shell sort is not very effective compared to quicksort or merge sort. The good thing is that it is quite easy to implement (not easier than insertion sort), but in general it should be avoided for large data sets. Perhaps the main advantage of Shell sort is that the list can be sorted for a gap greater than 1 and thus making less exchanges than insertion sort.\nRelated posts:\n"}, {"score": 1369.5259, "uuid": "035db6b6-fd58-5928-87e7-2c469af450b7", "index": "cw12", "trec_id": "clueweb12-0211wb-75-22420", "target_hostname": "www.math.grin.edu", "target_uri": "http://www.math.grin.edu/~rebelsky/Courses/CS152/98S/Labs/lab.09.html", "page_rank": 1.1963608e-09, "spam_rank": 93, "title": "CSC-152 98S : Lab 9: <em>Quicksort</em>", "snippet": "Purpose: In this lab, you will gain greater knowledge of the <em>quicksort</em> <em>algorithm</em> and begin to develop some ideas on <em>algorithm</em> animation. I&#x27;ve prepared a simple implementation of the <em>quicksort</em> <em>algorithm</em> appropriate for Comparable objects. Make a copy of that class and my test class.", "explanation": null, "document": "The test program is intended to sort strings while illustrating what is happening during the sorting. You may want to read the underlying code to make sure it all looks correct.\nYou can test the quicksort algorithm by running\n% ji Quicksort str1 str2 str3 ... strn\n(filling in the strings of your choice). The output will illustrate the steps used in quicksort, including pivoting and recursive sorting of subvectors. If you need help interpreting the output, please ask me.\nAt present, the testing routine is rather primitive. It might be appropriate to test with larger inputs (that we'd prefer not to generate by hand) and it doesn't really support automatic testing.\nWrite a testing routine that uses java.util.Random and rebelsky.util.ComparableInteger to test the quickSort routine on larger arrays.\nNote that I make no guarantee that any of my code is correct.\nQuicksortableVector.java\nimport java.util.Vector;\t\t// These are extended vectors\nimport rebelsky.util.Comparable;\t// We need to compare elements\nimport rebelsky.util.IncomparableException;\n\t\t\t\t\t// Some things can't be compared\nimport rebelsky.util.StringUtils;\t// For the \"animated\" version\nimport rebelsky.io.SimpleOutput;\t// For the \"animated\" version\n\n/**\n * An extension of java.util.Vector that supports an\n * additional quicksort method that sorts the current\n * vector.  Written as part of a laboratory for CS152.\n * Copyright (c) 1998 Samuel A. Rebelsky and YOUR NAME HERE. \n * All rights reserved.\n *\n * @author Samuel A. Rebelsky\n * @author YOUR NAME HERE\n * @version 1.0 of March 1998\n */\npublic class QuicksortableVector\n  extends Vector\n{\n\n  // +-----------+---------------------------------------------------------\n  // | Variables |\n  // +-----------+\n\n  /**\n   * The number of steps in the last call to quicksort.\n   */\n  protected int steps;\n\n  /**\n   * Where does output go?  If this is not set to null, produces\n   * an \"animation\" of sorting to the output channel.  Otherwise,\n   * does not produce any output when sorting.\n   */\n  SimpleOutput out;\n\n  /**\n   * The width of the widest entry in the vector.  Used only when\n   * printing the animation.\n   */\n  int width;\n\n\n  // +--------------+------------------------------------------------------\n  // | Constructors |\n  // +--------------+\n\n  /**\n   * Create a new empty vector.\n   */\n  public QuicksortableVector()\n  {\n    super();\n    out = null;\n    width = -1;\n  } // QuicksortableVector()\n\n  /**\n   * Construct an empty vector with the specified initial capacity.\n   */\n  public QuicksortableVector(int initial_capacity)\n  {\n    super(initial_capacity);\n    out = null;\n    width = -1;\n  } // QuicksortableVector(int)\n\n  /**\n   * Construct a new vector with specified initial capacity and\n   * capacity increment. \n   */\n  public QuicksortableVector(int initial_capacity, int increment)\n  {\n    super(initial_capacity, increment);\n    out = null;\n    width = -1;\n  } // QuicksortableVector\n\n\n  // +------------------+--------------------------------------------------\n  // | Exported Methods |\n  // +------------------+\n\n  /**\n   * Sort the current vector.\n   * pre: All the elements of the current vector are Comparable\n   *          and can be compared to each other.\n   * post: The elements are in order.\n   *\n   * @exception IncomparableException\n   *   when two elements can't be compared.\n   */\n  public void quickSort()\n    throws IncomparableException\n  {\n    // Indicate that we've spent 0 steps on this process.\n    steps = 0;\n    // No \"animation\"\n    this.out = null;\n    // Quicksort all the elements of the vector\n    quickSort(0, size()-1);\n  } // quickSort()\n\n  /**\n   * Sort the current vector.  Describe the steps as we go to the\n   * given output.  See the other version for pre and postconditions.\n   */\n  public void quickSort(SimpleOutput out)\n    throws IncomparableException\n  {\n    // Indicate that we've spent 0 steps on this process.\n    steps = 0;\n    // Set up output\n    this.out = out;\n    // Determine the maximum width of any cell\n    for (int i = 0; i < size(); ++i) {\n      int new_width = elementAt(i).toString().length();\n      if (width < new_width) {\n        width = new_width;\n      }\n    } // for\n    width = width + 2;\n    // Quicksort all the elements of the vector\n    quickSort(0, size()-1);\n    // Print the sorted vector\n    printSubvector(0, size()-1);\n    out.println();\n    // And restore the animation stuff\n    out = null;\n    width = -1;\n  } // QuicksortableVector\n\n\n  // +---------------+-----------------------------------------------------\n  // | Local Methods |\n  // +---------------+\n\n  /**\n   * Partition a subvector vector based on a pivot (which must be\n   * in the subvector.  While determining that the pivot is at\n   * position P, this method ensures that all the elements smaller\n   * than or equal to the pivot precede the pivot, and all elements\n   * larger than the pivot follow the pivot.\n   * pre: 0 <= lb <= ub < size()\n   * post: for all i, lb <= i <= P, for all j, P < j <= ub,\n   *    elementAt(i) <= elementAt(j)\n   * @exception IncomparableException\n   *   When two elements can't be compared.\n   */\n  protected int partition(Comparable pivot, int lb, int ub)\n    throws IncomparableException\n  {\n    /*\n     * We'll use two indices into the subvector, one to the\n     * \"larger\" half, one to the \"smaller\" half.  Both begin\n     * at the far end of their presumed section.  We advance\n     * each toward the center over correctly placed elements\n     * until they cross or hit incorrectly placed elements.\n     * When both are over incorrectly placed elements, we swap.\n     */\n    // The index of the pivot\n    int pivot_loc = -1;\n    // The index into the larger elements.\n    int larger = ub;\n    // The index into the smaller elements.\n    int smaller = lb;\n    // Keep going until we cross.  Throw an exception if we ever hit\n    // an invalid type.\n    try {\n      while (smaller < larger) {\n        // Move the larger index towards the middle as much as is possible.\n        // The casting of the element to Comparable is caught outside the\n        // while loop.\n        while ( (larger >= lb) &&\n                (pivot.lessThan(((Comparable) elementAt(larger)))) ) {\n          larger = larger - 1;\n        } // while ( pivot < elementAt(larger) )\n        // Move the smaller index towards the middle as much as possible.\n        while ( (smaller <= ub) &&\n                (!pivot.lessThan(((Comparable) elementAt(smaller)))) ) {\n          if (pivot.equals(elementAt(smaller))) {\n            pivot_loc = smaller;\n          }\n          smaller = smaller + 1;\n        } // while ( pivot >= elementAt(smaller) )\n        // Swap if necessary\n        if (smaller < larger) {\n          swap(smaller,larger);\n        } // if\n      } // while (smaller < larger)\n    } // try\n    catch (ClassCastException e) {\n      throw new IncomparableException();\n    } // catch\n\n    // At this point, we know that smaller points to the beginning of\n    // the large element and larger points to the end of the small\n    // elements.  We want to put the pivot at that position and then\n    // return that position.\n    swap(pivot_loc, larger);\n    return larger;\n  } // partition(Comparable, int, int)\n \n  /**\n   * Pick a pivot.  Analyses I've seen suggest that the second\n   * element works better than the first.  The use of a separate\n   * method lets us experiment.\n   * pre: 0 <= lb < ub < size()\n   * post: some element in the range [lb .. ub] is returned\n   *\n   * @exception IncomparableException\n   *   when the selected pivots is not Comparable\n   */\n  protected Comparable pickPivot(int lb, int ub)\n    throws IncomparableException\n  {\n    try {\n      return (Comparable) elementAt(lb+1);\n    }\n    catch (Exception e) {\n      throw new IncomparableException();\n    }\n  } // pickPivot(int, int)\n\n  /**\n   * Print a subvector.  Does not indent by the appropriate amount.\n   */\n  protected void printSubvector(int lb, int ub) {\n    // If there are no cells, stop\n    if (ub < lb) return;\n    // Print an intitial separator\n    out.print(\"|\");\n    // Print all the cells followed by a separator\n    for (int i = lb; i <= ub; ++i) {\n      out.print(StringUtils.center(elementAt(i).toString(), width) + \"|\");\n    }\n  } // printSubvector\n\n  /**\n   * Sort a subrange of the current vector (all elements between\n   * lb and ub inclusive).\n   * pre: All the elements in the subrange are Comparable\n   *          and can be compared to each other.\n   * pre: 0 <= lb, ub < size()\n   * post: The elements in the range [lb .. ub] are in order.\n   *\n   * @exception IncomparableException\n   *   when two elements can't be compared.\n   */\n  protected void quickSort(int lb, int ub)\n    throws IncomparableException\n  {\n    // Observe that we've spent an additional step\n    steps = steps + 1;\n    // If we're animating, print the vector\n    if ((out != null) && (lb <= ub)) {\n      out.print(StringUtils.spaces((width+1)*lb));\n      printSubvector(lb, ub);\n      out.println();\n    }\n    // Base case: subrange of length 0 or 1.\n    if (ub <= lb) return;\n    // Identify a pivot.  This pivot must be in the vector.\n    Comparable pivot = pickPivot(lb,ub);\n    // Partition the subvector based on that pivot.  This pivot must\n    // be in the vector.  The function returns the position of the\n    // pivot.\n    int pivot_loc = partition(pivot, lb, ub);\n    // If we're animating, print the partitioned vector\n    if (out != null) {\n      out.print(StringUtils.spaces((width+1)*pivot_loc+1));\n      out.print(\"*\");\n      out.print(StringUtils.center(elementAt(pivot_loc).toString(), width-2));\n      out.print(\"*\");\n      out.println();\n    } // if we're animating\n    // Sort the two subvectors.  It's important that we be able to\n    // shrink at least one of the subvectors by at least one element,\n    // otherwise we may never stop (e.g., when all the elements are\n    // the same).\n    quickSort(lb, pivot_loc-1);\n    quickSort(pivot_loc+1,ub);\n    // No need to merge, as we've been doing everything in place\n  } // quickSort(int,int)\n\n  /**\n   * Swap two elements of the vector.\n   * pre: 0 <= alpha, beta < size()\n   * post: the element at alpha and the element at beta have\n   *     swapped positions.\n   */\n  protected void swap(int alpha, int beta)\n  {\n    Object tmp = elementAt(alpha);\n    setElementAt(elementAt(beta), alpha);\n    setElementAt(tmp, beta);\n  } // swap(int, int)\n\n} // QuicksortableVector\nQuicksort.java\nimport rebelsky.io.SimpleOutput;\nimport rebelsky.util.ComparableString;\n\n/**\n * A simple test of the animated quicksort routines provided by\n * QuicksortableVector.java.\n * Copyright (c) 1998 Samuel A. Rebelsky.  All rights reserved.\n *\n * @author Samuel A. Rebelsky\n * @verstion 1.0 of March 1998\n */\npublic class Quicksort\n{\n  public static void main(String[] args)\n    throws Exception\n  {\n    // Oh boy!  Output!\n    SimpleOutput out = new SimpleOutput();\n    // The sortable vector.\n    QuicksortableVector stuff = new QuicksortableVector();\n    // Verify the number of parameters\n    if (args.length == 0) {\n      out.println(\"Usage: ji Quicksort string1 string2 ... stringn\");\n      System.exit(1);\n    } // if\n    // Append all the parameters\n    for (int i = 0; i < args.length; ++i) {\n      stuff.addElement(new ComparableString(args[i]));\n    } // for\n    // And sort!\n    stuff.quickSort(out);\n  } // main\n} // Quicksort\nDisclaimer Often, these pages were created \"on the fly\" with little, if any, proofreading. Any or all of the information on the pages may be incorrect. Please contact me if you notice errors.\nSource text last modified Thu Mar 5 21:55:42 1998.\nThis page generated on Thu Mar 5 22:06:12 1998 by SiteWeaver.\nContact our webmaster at rebelsky@math.grin.edu\n"}, {"score": 1365.4595, "uuid": "27422836-04c2-5d3e-a1bb-8b3793534d8a", "index": "cw12", "trec_id": "clueweb12-1700wb-09-27334", "target_hostname": "ruby.dzone.com", "target_uri": "http://ruby.dzone.com/articles/algorithm-week-shell-sort", "page_rank": 1.1869941e-09, "spam_rank": 82, "title": "<em>Algorithm</em> of the Week: Shell <em>Sort</em> | Ruby Zone", "snippet": "Well, as insertion <em>sort</em> and bubble <em>sort</em>, Shell <em>sort</em> <em>is</em> not very effective compared to <em>quicksort</em> <em>or</em> <em>merge</em> <em>sort</em>. The good thing <em>is</em> that it <em>is</em> quite easy to implement (not easier than insertion <em>sort</em>), but in general it should be avoided for large data sets.", "explanation": null, "document": "| More\nInsertion sort is a great algorithm, because it\u2019s very intuitive and it is easy to implement, but the problem is that it makes many exchanges for each \u201clight\u201d element in order to put it on the right place. Thus \u201clight\u201d elements at the end of the list may slow down the performance of insertion sort a lot. That is why in 1959 Donald Shell proposed an algorithm that tries to overcome this problem by comparing items of the list that lie far apart.\nInsertion sort compares every single item with all the rest elements of the list in order to find its place, while Shell sort compares items that lie far apart. This means light elements move faster to the front of the list.\nOn the other hand, it is obvious that by comparing items that lie apart the list can\u2019t be sorted in one pass like insertion sort. That is why on each pass we should use a fixed gap between the items, then decrease the value on every consecutive iteration.\nWe start to compare items with a fixed gap, that becomes lesser on each iteration until it gets to 1.\nHowever it is intuitively clear that Shell sort may need even more comparisons than insertion sort. Then why should we use it?\nThe thing is that insertion sort is not an effective sorting algorithm at all, but in some cases, when the list is almost sorted it can be quite useful. With Shell sort, once the list is sorted for gap = i, it is sorted for every gap = j, where j < i, and this is its main advantage.\nShell sort can make less exchanges than insertion sort.\nHow to choose gap size\nA \"not-so-cool\" thing about Shell sort is that we have to choose \u201cthe perfect\u201d gap sequence for our list. This is not an easy task, because it is very dependent on he input data. The good news is that there are some gap sequences proven to work well in general cases.\nShell Sequence\nDonald Shell proposes a sequence that follows the formula FLOOR(N/2k), then for N = 1000, we get the following sequence: [500, 250, 125, 62, 31, 15, 7, 3, 1]\nShell sequence for N=1000: (500, 250, 125, 62, 31, 15, 7, 3, 1)\nPratt Sequence\nPratt proposes another sequence that\u2019s growing with a slower pace than the Shell\u2019s sequence. He proposes successive numbers of the form 2p3q or [1, 2, 3, 4, 6, 8, 9, 12, ...].\nPratt sequence: (1, 2, 3, 4, 6, 8, 9, 12, ...)\nKnuth Sequence\nKnuth on other hand proposes his own sequence following the formula (3k \u2013 1) / 2 or [1, 4, 14, 40, 121, ...]\nKnuth sequence: (1, 4, 13, 40, 121, ...)\nOf course there are many other gap sequences, proposed by various developers and researchers, but the problem is that the effectiveness of the algorithm strongly depends on the input data. But before taking a look to the complexity of Shell sort, let\u2019s see first its implementation.\nImplementation\nHere\u2019s a Shell sort implementation on PHP using the Pratt gap sequence. The thing is that for this data set other gap sequences may appear to be better solutions.\n$input = array(6, 5, 3, 1, 8, 7, 2, 4);\n \nfunction shell_sort($arr)\n{\n\t$gaps = array(1, 2, 3, 4, 6);\n\t$gap  = array_pop($gaps);\n\t$len  = count($arr);\n \n\twhile($gap > 0)\n\t{\n\t\tfor($i = $gap; $i < $len; $i++) {\n \n\t\t\t$temp = $arr[$i];\n\t\t\t$j = $i;\n \n\t\t\twhile($j >= $gap && $arr[$j - $gap] > $temp) {\n\t\t\t\t$arr[$j] = $arr[$j - $gap];\n\t\t\t\t$j -= $gap;\n\t\t\t}\n \n\t\t\t$arr[$j] = $temp;\n\t\t}\n \n\t\t$gap = array_pop($gaps);\n\t}\n \n\treturn $arr;\n}\n \n// 1, 2, 3, 4, 5, 6, 7, 8\nshell_sort($input);\nIt\u2019s easy to change this code in order to work with Shell sequence.\n$input = array(6, 5, 3, 1, 8, 7, 2, 4);\n \nfunction shell_sort($arr)\n{\n        $len  = count($arr);\n\t$gap  = floor($len/2);\n \n\twhile($gap > 0)\n\t{\n\t\tfor($i = $gap; $i < $len; $i++) {\n \n\t\t\t$temp = $arr[$i];\n\t\t\t$j = $i;\n \n\t\t\twhile($j >= $gap && $arr[$j - $gap] > $temp) {\n\t\t\t\t$arr[$j] = $arr[$j - $gap];\n\t\t\t\t$j -= $gap;\n\t\t\t}\n \n\t\t\t$arr[$j] = $temp;\n\t\t}\n \n\t\t$gap = floor($gap/2);\n\t}\n \n\treturn $arr;\n}\n \n// 1, 2, 3, 4, 5, 6, 7, 8\nshell_sort($input);\nComplexity\nYet again we can\u2019t determine the exact complexity of this algorithm, because it depends on the gap sequence. However we may say what is the complexity of Shell sort with the sequences of Knuth, Pratt and Donald Shell. For the Shell\u2019s sequence the complexity is O(n2), while for the Pratt\u2019s sequence it is O(n*log2(n)). The best approach is the Knuth sequence where the complexity is O(n3/2), as you can see on the diagram bellow.\nComplexity of Shell sort with different gap sequences.\nApplication\nWell, as insertion sort and bubble sort, Shell sort is not very effective compared to quicksort or merge sort. The good thing is that it is quite easy to implement (not easier than insertion sort), but in general it should be avoided for large data sets. Perhaps the main advantage of Shell sort is that the list can be sorted for a gap greater than 1 and thus making less exchanges than insertion sort.\nRelated posts:\n"}, {"score": 1311.9238, "uuid": "b09b13ac-cb7a-5b1d-b6e0-91f20d4bb90a", "index": "cw12", "trec_id": "clueweb12-0007wb-33-10664", "target_hostname": "perldoc.perl.org", "target_uri": "http://perldoc.perl.org/sort.html", "page_rank": 1.1867041e-09, "spam_rank": 72, "title": "<em>sort</em> - perldoc.perl.org", "snippet": "In Perl versions 5.6 and earlier the <em>quicksort</em> <em>algorithm</em> was used to implement <em>sort</em>(), but in Perl 5.8 a mergesort <em>algorithm</em> was also made available, mainly to guarantee worst case O(N log N) behaviour: the worst case of <em>quicksort</em> <em>is</em> O", "explanation": null, "document": "$current = sort::current(); # identify prevailing algorithm\nDESCRIPTION\nWith the sort pragma you can control the behaviour of the builtin sort() function.\nIn Perl versions 5.6 and earlier the quicksort algorithm was used to implement sort() , but in Perl 5.8 a mergesort algorithm was also made available, mainly to guarantee worst case O(N log N) behaviour: the worst case of quicksort is O(N**2). In Perl 5.8 and later, quicksort defends against quadratic behaviour by shuffling large arrays before sorting.\nA stable sort means that for records that compare equal, the original input ordering is preserved. Mergesort is stable, quicksort is not. Stability will matter only if elements that compare equal can be distinguished in some other way. That means that simple numerical and lexical sorts do not profit from stability, since equal elements are indistinguishable. However, with a comparison such as\n{ substr ($a, 0, 3) cmp substr ($b, 0, 3) }\nstability might matter because elements that compare equal on the first 3 characters may be distinguished based on subsequent characters. In Perl 5.8 and later, quicksort can be stabilized, but doing so will add overhead, so it should only be done if it matters.\nThe best algorithm depends on many things. On average, mergesort does fewer comparisons than quicksort, so it may be better when complicated comparison routines are used. Mergesort also takes advantage of pre-existing order, so it would be favored for using sort() to merge several sorted arrays. On the other hand, quicksort is often faster for small arrays, and on arrays of a few distinct values, repeated many times. You can force the choice of algorithm with this pragma, but this feels heavy-handed, so the subpragmas beginning with a _ may not persist beyond Perl 5.8. The default algorithm is mergesort, which will be stable even if you do not explicitly demand it. But the stability of the default sort is a side-effect that could change in later versions. If stability is important, be sure to say so with a\n"}, {"score": 1310.4835, "uuid": "d44d0d49-61c2-5b50-a684-da9b1d654b05", "index": "cw12", "trec_id": "clueweb12-0009wb-59-12682", "target_hostname": "perldoc.perl.org", "target_uri": "http://perldoc.perl.org/5.8.8/sort.html", "page_rank": 1.1822198e-09, "spam_rank": 77, "title": "<em>sort</em> - perldoc.perl.org", "snippet": "In Perl versions 5.6 and earlier the <em>quicksort</em> <em>algorithm</em> was used to implement <em>sort</em>(), but in Perl 5.8 a mergesort <em>algorithm</em> was also made available, mainly to guarantee worst case O(N log N) behaviour: the worst case of <em>quicksort</em> <em>is</em> O", "explanation": null, "document": "sort - perl pragma to control sort() behaviour\nSYNOPSIS\nuse sort 'stable'; # guarantee stability\nuse sort '_quicksort'; # use a quicksort algorithm\nuse sort '_mergesort'; # use a mergesort algorithm\nuse sort 'defaults'; # revert to default behavior\nuse sort '_qsort'; # alias for quicksort\nmy $current = sort::current(); # identify prevailing algorithm\nDESCRIPTION\nWith the sort pragma you can control the behaviour of the builtin sort() function.\nIn Perl versions 5.6 and earlier the quicksort algorithm was used to implement sort() , but in Perl 5.8 a mergesort algorithm was also made available, mainly to guarantee worst case O(N log N) behaviour: the worst case of quicksort is O(N**2). In Perl 5.8 and later, quicksort defends against quadratic behaviour by shuffling large arrays before sorting.\nA stable sort means that for records that compare equal, the original input ordering is preserved. Mergesort is stable, quicksort is not. Stability will matter only if elements that compare equal can be distinguished in some other way. That means that simple numerical and lexical sorts do not profit from stability, since equal elements are indistinguishable. However, with a comparison such as\n{ substr ($a, 0, 3) cmp substr ($b, 0, 3) }\nstability might matter because elements that compare equal on the first 3 characters may be distinguished based on subsequent characters. In Perl 5.8 and later, quicksort can be stabilized, but doing so will add overhead, so it should only be done if it matters.\nThe best algorithm depends on many things. On average, mergesort does fewer comparisons than quicksort, so it may be better when complicated comparison routines are used. Mergesort also takes advantage of pre-existing order, so it would be favored for using sort() to merge several sorted arrays. On the other hand, quicksort is often faster for small arrays, and on arrays of a few distinct values, repeated many times. You can force the choice of algorithm with this pragma, but this feels heavy-handed, so the subpragmas beginning with a _ may not persist beyond Perl 5.8. The default algorithm is mergesort, which will be stable even if you do not explicitly demand it. But the stability of the default sort is a side-effect that could change in later versions. If stability is important, be sure to say so with a\n"}, {"score": 1309.4789, "uuid": "6b7c5c70-f490-59db-b571-b3dcec5c5ba2", "index": "cw12", "trec_id": "clueweb12-1508wb-15-26005", "target_hostname": "perldoc.perl.org", "target_uri": "http://perldoc.perl.org/5.10.0/sort.html", "page_rank": 1.1835981e-09, "spam_rank": 80, "title": "<em>sort</em> - perldoc.perl.org", "snippet": "In Perl versions 5.6 and earlier the <em>quicksort</em> <em>algorithm</em> was used to implement <em>sort</em>(), but in Perl 5.8 a mergesort <em>algorithm</em> was also made available, mainly to guarantee worst case O(N log N) behaviour: the worst case of <em>quicksort</em> <em>is</em> O", "explanation": null, "document": "$current = sort::current(); # identify prevailing algorithm\nDESCRIPTION\nWith the sort pragma you can control the behaviour of the builtin sort() function.\nIn Perl versions 5.6 and earlier the quicksort algorithm was used to implement sort() , but in Perl 5.8 a mergesort algorithm was also made available, mainly to guarantee worst case O(N log N) behaviour: the worst case of quicksort is O(N**2). In Perl 5.8 and later, quicksort defends against quadratic behaviour by shuffling large arrays before sorting.\nA stable sort means that for records that compare equal, the original input ordering is preserved. Mergesort is stable, quicksort is not. Stability will matter only if elements that compare equal can be distinguished in some other way. That means that simple numerical and lexical sorts do not profit from stability, since equal elements are indistinguishable. However, with a comparison such as\n{ substr ($a, 0, 3) cmp substr ($b, 0, 3) }\nstability might matter because elements that compare equal on the first 3 characters may be distinguished based on subsequent characters. In Perl 5.8 and later, quicksort can be stabilized, but doing so will add overhead, so it should only be done if it matters.\nThe best algorithm depends on many things. On average, mergesort does fewer comparisons than quicksort, so it may be better when complicated comparison routines are used. Mergesort also takes advantage of pre-existing order, so it would be favored for using sort() to merge several sorted arrays. On the other hand, quicksort is often faster for small arrays, and on arrays of a few distinct values, repeated many times. You can force the choice of algorithm with this pragma, but this feels heavy-handed, so the subpragmas beginning with a _ may not persist beyond Perl 5.8. The default algorithm is mergesort, which will be stable even if you do not explicitly demand it. But the stability of the default sort is a side-effect that could change in later versions. If stability is important, be sure to say so with a\n"}, {"score": 1307.3405, "uuid": "0c9eb94d-0a84-5d4b-ad37-0ecab0e61b62", "index": "cw12", "trec_id": "clueweb12-1703wb-00-25359", "target_hostname": "www.andrewnoske.com", "target_uri": "http://www.andrewnoske.com/wiki/index.php?title=Sorting_algorithms", "page_rank": 1.1974252e-09, "spam_rank": 86, "title": "Sorting <em>algorithms</em> - NoskeWiki", "snippet": "<em>Quicksort</em> <em>is</em> a divide and conquer <em>algorithm</em> where a &quot;pivot&quot; value <em>is</em> selected and all values arranged above <em>or</em> below. It&#x27;s one of the fastest sorting <em>algorithms</em> and the steps are: Pick an element, called a pivot, from the list.", "explanation": null, "document": "[ edit ] About\nA sorting algorithm is an algorithm that used to rearrange elements in a list in a certain order - for example taking a series of numbers and rearranging them in ascending order. Rearranging objects like this is not as easy as it sounds, and so there are several different types of well known sorting algorithms, some more efficient than others. Below is a brief account of some of the most well known sorting algorithms in computer science.\n[ edit ] Bubble Sort\nBuuble sort is a simple sorting algorithm that works by repeatedly stepping through the list to be sorted, comparing each pair of adjacent items and swapping them if they are in the wrong order. Larger elements thus \"bubble\" to the top of the list. Bubble sort is simple to code but not recommended for long lists.\nPerformance: (O(n^2)) average and worst case.\nint nUnsort     = arr.size();      // number of unsorted elements left\nwhile (nUnsort > 1)\n{\n  int highSwap  = 0;               // highest swapped value (could also use boolean \"swapped\")\n  for(int i=1;i<nUnsort\u00a0;i++)         // for each unsorted value:\n  {\n    if( arr[i-1] > arr[i] )      // if its more than next value: swap these values\n    {\n      int temp = arr[i-1];\n      arr[i-1] = arr[i];\n      arr[i]   = temp;\n      highSwap = i;\n    }\n  }\n  nUnsort = highSwap;\n}\nMerge sort is an O(n log n) divide and conquer algorithm sorting algorithm. The main steps are:\nDivide the unsorted list into n sublists, each containing 1 element.\nRepeatedly merge sublists to produce new sublists until there is only 1 sublist remaining... the merge involves starting at the front of two sublists then removing the smallest values into a new list until both sublists are empty.\nPerformance: (O(n log n)) average case... (O(n log n)) worst case... con: requires extra memory\n[ edit ] Quick Sort\nQuicksort is a divide and conquer algorithm where a \"pivot\" value is selected and all values arranged above or below. It's one of the fastest sorting algorithms and the steps are:\nPick an element, called a pivot, from the list.\nReorder the list so that all elements with values less than the pivot come before the pivot, while all elements with values greater than the pivot come after it (equal values can go either way). After this partitioning, the pivot is in its final position. This is called the partition operation.\nRecursively sort the sub-list of lesser elements and the sub-list of greater elements.\nPerformance: (O(n log n)) average case... (O(n^2)) worst case (rare).\nIn pseudocode :\nfunction quicksort('array')\n  if length('array') \u2264 1               // base case\n    return 'array'                      // an array of zero or one elements is already sorted\n  select and remove a pivot value 'pivot' from 'array'\n  create empty lists 'less' and 'greater'\n  for each 'x' in 'array'\n    if 'x' \u2264 'pivot' then append 'x' to 'less'\n    else append 'x' to 'greater'\n  return concatenate(quicksort('less'), 'pivot', quicksort('greater'))    // two recursive calls\n[ edit ] Radix Sort\nRadix sort is a non-comparative integer sorting algorithm which groups keys using their individual digits starting from least to most significant (from right to left) and placing them into buckets each round. Since a computer stores numbers as bits it's most convenient to consider each bit as a digit, but in the example below I have used base 10 notation and show several three digit numbers, hence there are three rounds.\nINPUT ....\n"}, {"score": 1293.0045, "uuid": "25ef185f-d55d-59a2-8d7c-71685c4f05a1", "index": "cw12", "trec_id": "clueweb12-1808wb-80-10339", "target_hostname": "jorgetavares.com", "target_uri": "http://jorgetavares.com/tag/sorting-algorithms/", "page_rank": 2.0671798e-09, "spam_rank": 76, "title": "Sorting <em>Algorithms</em> \u00ab Jorge Tavares weblog", "snippet": "Right now the implemented <em>algorithms</em> are: insertion <em>sort</em>, <em>quicksort</em>, randomized <em>quicksort</em>, <em>merge</em> <em>sort</em>, heapsort and counting <em>sort</em>. The plan <em>is</em> to add more <em>algorithms</em>, for example, bucket <em>sort</em> and timsort.", "explanation": null, "document": "merge sort (lists) / heapsort\nmerge sort\nIn terms of the implementation of sort, quicksort is the most used algorithm, followed by heapsort . The choice for these algorithms is expected. Both have an average-case performance of O(nlgn) and heapsort guarantees a worst-case performace of O(nlgn) too. Quicksort has a worst-case performance of O(n2) but it can be optimized in several ways so that it also gives an expected worst-case performance of O(nlgn). However, it seems that the quicksort implementations are not completely optimized. In ECL (and ABCL) quicksort implements a partition scheme which deals better with duplicate elements (although is not the three-way partitioning) but it always picks as pivot the first element. CCL chooses the pivot with a median-of-3 method and always sorts the smaller partition to ensure a worst-case stack depth of O(lgn).\nAs for CLISP, I think it uses a tree sort but I am not entirely sure. The only source file I could find with a sort implementation was sort.d and it looks like it contains an implementation of tree sort with a self-balanced binary tree, which also gives this algorithm an average and worst-case performance of O(nlgn).\nAs expected, most of the implementations use merge sort to implement stable-sort since it is a stable sort with average and worst-case performance of O(nlgn). Apparently, all implementations are bottom-up merge sorts with the exception of CCL and ECL. Another interesting thing is that merge sort is also used for lists in sort, in most of the implementations. However, I found it surprising to find quicksort in the stable-sort column because it is not a stable algorithm. Since it is only used for strings and bit vectors, it is not really an issue. While reading the source code of the implementations, I realized that ABCL was using quicksort in stable-sort for all non-list sequences. This is a problem that exists in the current 1.0.1 release but I\u2019ve sent a bug report with a quick fix to the maintainers. The next release should have stable-sort fixed.\nThis exploration of the sorting algorithms used in the open source implementations was very educational and interesting to me. I\u2019ve learned what algorithms are actually used and enjoyed seing how they were implemented. Just spotting the issue in ABCL stable-sort made this review worthwhile. I think there is still room for improvement in some implementations but knowing now the strengths and weaknesses of the sorts in CL is already good enough. On a final note, I just wonder what are the algorithms used in ACL and LW .\nWritten by Jorge Tavares\n"}], [{"score": 1177.9724, "uuid": "047fc617-6ae6-5de7-a4bf-604b7506dd5c", "index": "cw12", "trec_id": "clueweb12-1015wb-14-22230", "target_hostname": "www.cs.utsa.edu", "target_uri": "http://www.cs.utsa.edu/~dj/cs3343/lecture9.html", "page_rank": 1.1754517e-09, "spam_rank": 68, "title": "Analysis of <em>Algorithms</em>: Lecture 9", "snippet": "., <em>merge</em> <em>sort</em>, heap <em>sort</em>, <em>Quicksort</em>, and some other O(n ln n) <em>algorithms</em> seem to all run up against the same lower bound, i.e. (n ln n). Can we do any <em>better</em> than this? <em>Is</em> it just a weird coincidence that all of these &quot;efficient&quot; <em>sorts</em> have the same lower bound asymptotic performance?", "explanation": null, "document": "Lecture 9\nThe Limits of Sorting Algorithms\nSo far, we have seen two classes of sorting algorithms: those that take O( n 2) time and those that take O( n ln n) time. We discard the O( n 2) algorithms as inefficient in all but the simplest cases. Those that are left, e.g., merge sort, heap sort, Quicksort, and some other O( n ln n) algorithms seem to all run up against the same lower bound, i.e.\n( n ln n). Can we do any better than this? Is it just a weird coincidence that all of these \"efficient\" sorts have the same lower bound asymptotic performance?\nComparison Sorts and General Purpose Sorts\nA comparison sort is a sorting algorithm where the final order the items end up in is determined only by comparisons between individual items of input. All of the sorts we have seen so far are comparison sorts:\nIn selection sort, the minimum element is found by comparing the current known minimum to every other element in the array, one at a time.\nIn heap sort, the Heapify procedure determines where to place items based on their comparisons with adjacent elements of the tree.\nIn merge sort, the merge procedure chooses an item from one of two arrays after comparing the next item from both arrays.\nIn Quicksort, the Partition procedure compares each item of the subarray, one by one, to the pivot element to determine whether or not to swap it with another element.\nIf we ignore the procedural aspects of these algorithms and look only at the data being sorted, we see that each comparison results in at most one change in the order of the array, e.g., maybe two elements may be swapped, or maybe nothing will happen at any one step. Without loss of generality, let's assume that each array element is different. This makes the analysis easier and is often not too far an assumption from the truth. We can think of this process as search through a binary search tree where each node is a permutation (a particular order) of the array. The root of this tree is the order of the array as the algorithm initially encounters it. What we're searching for is the node where the permutation of elements is sorted. The right and left children of a node are the two resulting permutations when the comparison is \"less than\" and \"greater than,\" respectively. It is up to the algorithm which two elements to compare. For example, the following decision tree shows the movement of data in the bubble sort algorithm performed on three items (the tree is not complete; it is large):\n{ a b c }\n                       /                 \\\n                      /                   \\\n                     /                     \\\n            a < b   /                       \\   a > b\n                   /                         \\\n               { a b c }                   { b a c }\n        b < c  /       \\ b > c      a < c  /       \\ a > c\n              /         \\                 /         \\\n         { a b c }   { a c b }        { b a c }   { b c a }\n                       /   \\            /   \\       /   \\\nA general purpose sort is a sorting algorithm that works on any kind of ordered data. You provide the algorithm with an ordering on the data, and the algorithm sorts them for you. It is thought that a general purpose sort and a comparison sort are the same thing. You provide the comparison sort with a way to compare two items of data and the algorithms sorts them for you. The standard C function qsort is a good example of a general sort:\n#include <stdlib.h>\nvoid qsort(void *base, size_t nel, size_t width,\n     int (*compar) (const void *, const void *));\nbase is a pointer to the first element of the array to sort.\nnel is the number of elements in the array.\nwidth is the size of an individual element of the array, for example, in an array of doubles, you would write sizeof (double) for width.\ncompar is a pointer to a function that compares any two items from the array through pointers to the elements. You have to write this comparison function yourself. It should return a positive integer if the first element is greater than the second, a negative integer of it is less than, and 0 if they are equal. For an array of pointers to character strings, the strcmp function works fine. qsort is a randomized version of Quicksort with very good performance.\nSo sorting is like a search from the initial permutation (root) to the sorted permutation (some node in the tree). In the worst case, the sorted permutation may be a leaf node, requiring a number of comparisons proportional to the height of the tree. So a worst case lower bound on comparison sorting is the height of this decision tree. If our algorithm is clever, its decision tree will be an almost-complete binary tree. The height of a decision tree with m nodes is\n(ln m).\nHow many nodes are there in the decision tree for an array of size n? Since there is a node for every permutation of the array, there are n! nodes (i.e., n-factorial, n * (n-1) * (n-2) * (n-3) * ... * 1 nodes). So the height of the decision tree is\n(ln (n!)). In Chapter 2.12, we see that a lower bound on the factorial function is:\n(2\nn) 1/2 ( n/ e) n <= n!\nfor all n. If we take logarithms on both sides and use the properties that log ab = log a + log b and log a/ b = log a - log b, and some asymptotic notation to hide constants, we get:\n(1) + ln n + n ln n -\n( n) <= ln ( n!)\nwhich works out to simply\nln ( n!) =\n( n ln n)\nSo the height of the decision tree has a lower bound of\n( n ln n). In the worst case, the sorting algorithm will have to \"search\" all the way down to a leaf node, so\n( n ln n) comparisons is the best a comparison sort can be expected to do. Since the number of comparisons is at least the number of array accesses or other operations, this is the lower bound on the worst case time-complexity of any comparison sort.\nLinear-Time Sorting Algorithms\nAny sorting algorithm at all, comparison or not, has a trivial\n( n) lower bound time complexity; it has to at least examine all n elements of the array before it can guarantee they are sorted. So this is definitely \"the best we can do.\" Are there any sorts that realize this optimistic time complexity? As we have just seen, comparison sorts, which correspond to the notion of a general purpose sort, must take at least\n( n ln n) time in the worst case. But there are sorts that work on specialized data that work even faster.\nCounting Sort\nLet's first consider a very simple problem: given an array A [1.. n] of bits (0's and 1's), sort them according to the order 0 < 1. We could use Quicksort or merge sort, but these are really overkill. A very easy method is to just count the number m of 0's, then fill A[1.. m] with 0's and A[ m+1.. n] with 1's. Counting the 0's takes\n( n) time, and filling the array takes another\n( n), so the whole time to sort is simply\n( n). We can generalize this notion to sort an array where the elements come from a set of small integers. This is the idea behind counting sort (note that this is different than the version in the book).\n// A is the array to sort.\n// The array elements may be in the set of integers [0..k].\n// C is an array from [0..k]; C[i] will tell how many times i occurs in A\n\nCounting-Sort (A, k)\n        for i in 0 to k do\n                C[i] = 0                // all counts are initially 0\n        end for\n        for j = 1 to length(A) do\n                C[A[j]]++               // count each element\n        end for\n                                        // C[i] is now the # of times\n                                        // i occurs in A\n        i = 1                           // i is the index in A[1..length(A)]\n        j = 0                           // j is the index in C[0..k]\n        while j <= k do                 // while we have more elements...\n                if C[j] != 0 then       // if there are more j's in A\n                        A[i++] = j      // place a copy of j into A\n                        C[j]--          // one less j\n                else\n                        j++             // next item in order\n                end if\n        end while\nThis sort takes\n( k+ n) time: the times to process C and A. If k is a small constant, particularly small compared to the values of n we expect to see (i.e., k = O( n)), then this sort takes\n( n) time. We require only \"constant\" storage and time to store and process the array C. This sort is very sensitive to the kinds of data to be stored; they must be integral (like integers and characters) and they must be in a very small range. Sorting even moderate sized integers, like 32-bit integers in the range -2e9..2e9, is just impossible because the array C would have to contain four billion elements. Of course, we can forget about sorting floats altogether; what is C[3.14159]? But if we're sorting, say, the ages (in years) of people at UTSA, where k is around 100 and n is in the several thousands, counting sort would be much faster than any of the\n( n ln n) sorts. It turns out we can use a stable version of counting sort as the basis for another sort called radix sort that can sort a much wider range of data, like character strings and numbers with small decimal representations.\n"}, {"score": 1170.1031, "uuid": "d3cef363-9cb8-5185-b2c1-b5db4049d7d0", "index": "cw12", "trec_id": "clueweb12-1511wb-58-16474", "target_hostname": "perl.org.br", "target_uri": "http://perl.org.br/Perldoc/V500808/Sort", "page_rank": 1.1711565e-09, "spam_rank": 82, "title": "Perl Brasil - Perldoc v5.8.8 - <em>Sort</em>", "snippet": "In Perl versions 5.6 and earlier the <em>quicksort</em> <em>algorithm</em> was used to implement <em>sort</em>(), but in Perl 5.8 a mergesort <em>algorithm</em> was also made available, mainly to guarantee worst case O(N log N) behaviour: the worst case of <em>quicksort</em> <em>is</em> O", "explanation": null, "document": "use sort 'stable';          # guarantee stability\nuse sort '_quicksort';      # use a quicksort algorithm\nuse sort '_mergesort';      # use a mergesort algorithm\nuse sort 'defaults';        # revert to default behavior\nno  sort 'stable';          # stability not important\nuse sort '_qsort';          # alias for quicksort\nmy $current = sort::current();      # identify prevailing algorithm\nDESCRIPTION\nWith the sort pragma you can control the behaviour of the builtin sort() function. In Perl versions 5.6 and earlier the quicksort algorithm was used to implement sort(), but in Perl 5.8 a mergesort algorithm was also made available, mainly to guarantee worst case O(N log N) behaviour: the worst case of quicksort is O(N**2). In Perl 5.8 and later, quicksort defends against quadratic behaviour by shuffling large arrays before sorting. A stable sort means that for records that compare equal, the original input ordering is preserved. Mergesort is stable, quicksort is not. Stability will matter only if elements that compare equal can be distinguished in some other way. That means that simple numerical and lexical sorts do not profit from stability, since equal elements are indistinguishable. However, with a comparison such as\n{ substr($a, 0, 3) cmp substr($b, 0, 3) }\nstability might matter because elements that compare equal on the first 3 characters may be distinguished based on subsequent characters. In Perl 5.8 and later, quicksort can be stabilized, but doing so will add overhead, so it should only be done if it matters. The best algorithm depends on many things. On average, mergesort does fewer comparisons than quicksort, so it may be better when complicated comparison routines are used. Mergesort also takes advantage of pre-existing order, so it would be favored for using sort() to merge several sorted arrays. On the other hand, quicksort is often faster for small arrays, and on arrays of a few distinct values, repeated many times. You can force the choice of algorithm with this pragma, but this feels heavy-handed, so the subpragmas beginning with a _ may not persist beyond Perl 5.8. The default algorithm is mergesort, which will be stable even if you do not explicitly demand it. But the stability of the default sort is a side-effect that could change in later versions. If stability is important, be sure to say so with a\nuse sort 'stable';\nThe no sort pragma doesn't forbid what follows, it just leaves the choice open. Thus, after\nno sort qw(_mergesort stable);\na mergesort, which happens to be stable, will be employed anyway. Note that\nno sort \"_quicksort\";\n"}, {"score": 1135.858, "uuid": "e800848d-de22-5075-83f6-dd65c3b441a4", "index": "cw12", "trec_id": "clueweb12-1305wb-97-12943", "target_hostname": "manoftoday.wordpress.com", "target_uri": "http://manoftoday.wordpress.com/2007/11/12/algorithm/", "page_rank": 2.2634126e-09, "spam_rank": 74, "title": "<em>Algorithm</em> \u00ab life ideas", "snippet": "While at the beginning the sorted part consists of element a0 only, at the end it consists of all elements a0, \u2026, an-1. nsertion <em>sort</em> <em>is</em> an elementary sorting <em>algorithm</em>. It has a time complexity of \u0398(n2), thus being slower than heapsort, <em>merge</em> <em>sort</em> and also shellsort.", "explanation": null, "document": "process(v);\nmark v as visited;\nfor all children i of node v // if it is graph, all vertices i adjacent to v\npush i into Stack S;\ndfs(v) //recursive\nif v.isVisitedBefore()\nreturn;\nprocess(v)\n    mark v as visited\n    for all children i of v //if it is graph, all vertices i adjacent to v dfs(i)\nsorting\nhttp://cs.fit.edu/~wds/classes/algorithms/Sort/sort/sort.html\nQuick sort\nFirst, the sequence to be sorted a is partitioned into two parts, such that all elements of the first part b are less than or equal to all elements of the second part c (divide). Then the two parts are sorted separately by recursive application of the same procedure (conquer).\nvoid quicksort (int[] a, int lo, int hi)\n{\n//  lo is the lower index, hi is the upper index\n//  of the region of array a that is to be sorted\n    int i=lo, j=hi, h;\n    int x=a[(lo+hi)/2];\n\n    //  partition\n    do\n    {\n        while (a[i]<x) i++;\n        while (a[j]>x) j--;\n        if (i<=j)\n        {\n            swap(i, j);\n            i++; j--;\n        }\n    } while (i<=j);\n\n    //  recursion\n    if (lo<j) quicksort(a, lo, j);\n    if (i<hi) quicksort(a, i, hi);\n}\nAverage: O(nlogn), Best: O(logn), Worst: O(n2)\nIt requires log n space for recursion.\nMerge sort\nSimilar to Quicksort , the Mergesort algorithm is based on a divide and conquer strategy\n. First, the sequence to be sorted is decomposed into two halves (Divide). Each half is sorted independently (Conquer). Then the two sorted halves are merged to a sorted sequence (Combine)\nvoid mergesort(int lo, int hi)\n{\n    if (lo<hi)\n    {\n        int m=(lo+hi)/2;\n        mergesort(lo, m);\n        mergesort(m+1, hi);\n        merge(lo, m, hi);\n    }\n}\n// Straightforward variant\nvoid merge(int lo, int m, int hi)\n{\n    int i, j, k;\n\n    // copy both halves of a to auxiliary array b\n    for (i=lo; i<=hi; i++)\n        b[i]=a[i];\n\n    i=lo; j=m+1; k=lo;\n// copy back next-greatest element at each time\n    while (i<=m && j<=hi)\n        if (b[i]<=b[j])\n            a[k++]=b[i++];\n        else\n            a[k++]=b[j++];\n\n    // copy back remaining elements of first half (if any)\n    while (i<=m)\n        a[k++]=b[i++];\n}\nSimilar to QuickSort, but requires O(n) extra space.\nExchange sort:\n[[Bubble sort]]=\npublic void  bubbleSort (Record[] record) {\n  for (int i = record.length; i > 1; i--) {\n    for (int j = 1; j < i; j++) {\n      if (record[j-1].key > record[j].key) {\n        record.swap(j-1, j);\n      }\n    }\n  }\n}\nAverage: O(n2), Best: O(1), Worst: O(n2)\nInsertion Sort\nLet a0,\u00a0\u2026, an-1 be the sequence to be sorted. At the beginning and after each iteration of the algorithm the sequence consists of two parts: the first part a0,\u00a0\u2026, ai-1 is already sorted, the second part ai,\u00a0\u2026, an-1 is still unsorted (i\n0,\u00a0\u2026, n).\nIn order to insert element ai into the sorted part, it is compared with ai-1, ai-2 etc. When an element aj with aj\nai is found, ai is inserted behind it. If no such element is found, then ai is inserted at the beginning of the sequence.\nAfter inserting element ai the length of the sorted part has increased by one. In the next iteration, ai+1 is inserted into the sorted part etc. While at the beginning the sorted part consists of element a0 only, at the end it consists of all elements a0,\u00a0\u2026, an-1.\nnsertion sort is an elementary sorting algorithm. It has a time complexity of \u0398(n2), thus being slower than heapsort , merge sort and also shellsort . Insertion sort is well suited for sorting small data sets or for the insertion of new elements into a sorted sequence.\nprivate void insertionsort()\n    {\n        int i, j, t;\n        for (i=1; i<n; i++)\n        {\n            j=i-1;\n            value=A[i];\n            while (j>0 && A[j]>value)\n            {\n                A[j+1]=A[j]; //shift right side for one more position\n                j--;\n            }\n            A[j]=value;\n        }\n    }\nsorting by selection\nthe smallest (largest) item is found and placed first (last), then the next smallest (largest) is selected, and so on.\nStraight selection sort\nFind the smallest key and transfer it to output (or the first location in the file). Repeat this step with the next smallest key, and continue.\nNotice after i steps, the records from location 0 to i - 1 will be sorted.\n[[Straight selection sort]]=\npublic void selectionSort(Record[] record) {\n  for (int i = 0; i < record.length; i++) {\n    int min = i;\n    for (int j = i+1; j < record.length; j++) {\n      if (record[j].key < record[min].key)  min = j;   //find the min value's location\n    }\n    Swap (record[min], record[i]);\n  }\n}\nAverage: O(n2)\nHeapSort\nThat is, a heap is a binary tree which is completely filled at all levels except possibly the last, which is filled from\nleft to right and the key in each node is larger than (or equal to) the keys of its children.\n[Build a heap]=\n  buildHeap(Record[] record) {\n  for (int k = Math.floor ( record.length/2 ); i > 0; i--) {\n    heapify(record, k);\n  }\n}\n[[Heapify from Node k ]]=\npublic void  heapify (Record[] records, int k) {\n  int j = k;\n  int left = 2*k;\n  int right = 2*k+1;\n  if (left <= records.length-1 && records[left].key > records[k].key) {\n    j = left;\n  }\n  if (right <= records.length-1 && records[right].key > records[largest].key) {\n    j = right;\n  }\n  if (largest != k) {\n    records.swap(k, j);\n    heapify(records, j);\n  }\n}\nBuilding a heap is O(n).\n[[Heapsort]]=\npublic void  heapSort(Record[] record) {\n  int n = record.length;\n  buildHeap(record);\n  for (int k = n-1; k > 1; k--) {\n    record.swap(1, k);  //Exchange the root of the tree with the last element of the tree;\n    --record.length;    //Decrement the heap size by one;\n    heapify(record, 1); //Heapify from the root of the tree;\n  }\n  record.length = n;\n}\nSwapping the root and the last element and decrementing heap size are\n(1). Each time we heapify from the root of the tree it will take\ntime  O(logK), thus total time is O(nlogK)\nShell Sort\nThe idea of Shellsort is the following:\narrange the data sequence in a two-dimensional array\nsort the columns of the array\nThe effect is that the data sequence is partially sorted. The process above is repeated, but each time with a narrower array, i.e. with a smaller number of columns. In the last step, the array consists of only one column.\n"}, {"score": 1118.2758, "uuid": "60b2d754-9eea-5fbd-bb0b-47bf8477eebf", "index": "cw12", "trec_id": "clueweb12-1608wb-31-27624", "target_hostname": "www.advancedqtp.com", "target_uri": "http://www.advancedqtp.com/2008/04/sorting-arrays-quick-sort/", "page_rank": 1.1847906e-09, "spam_rank": 83, "title": "Sorting Arrays \u2013 Quick <em>Sort</em> | AdvancedQTP", "snippet": "The <em>quicksort</em> <em>algorithm</em> was developed by Hoare in 1960 while working for the small British scientific computer manufacturer Elliott Brothers. The quick <em>sort</em> <em>is</em> by far the fastest of the common sorting <em>algorithms</em>.", "explanation": null, "document": "The resource hub for Quicktest Professional\nLast Updated: April 17, 2012\nSearch for:\nAdvancedQTP \u00bb Blog \u00bb VB Script \u00bb Scripting Techniques \u00bb Arrays \u00bb Sorting Arrays \u2013 Quick Sort\nSorting Arrays \u2013 Quick Sort\nLast Updated on :\nJul 16, 2011\nQuicksort in action on a list of random numbers. The horizontal lines are pivot values.Quicksort is a well-known sorting algorithm developed by C. A. R. Hoare\nthat, on average, makes (big O notation) comparisons to sort n items.\nHowever, in the worst case, it makes L(n2) comparisons. Typically, quicksort is significantly faster in practice than other algorithms, because its inner loop can be efficiently implemented on most architectures, and in most real-world data it is possible to make design choices which minimize the possibility of requiring quadratic time. The quicksort algorithm was developed by Hoare in 1960 while working for the small British scientific computer manufacturer Elliott Brothers.\nThe quick sort is by far the fastest of the common sorting algorithms. It\u2019s possible to write a special-purpose sorting algorithm that can beat the quick sort for some data sets, but for general-case sorting there isn\u2019t anything faster.\nAs soon as students figure this out, their immediate impulse is to use the quick sort for everything \u2013 after all, faster is better, right? It\u2019s important to resist this urge \u2013 the quick sort isn\u2019t always the best choice. As mentioned earlier, it\u2019s massively recursive (which means that for very large sorts, you can run the system out of stack space pretty easily). It\u2019s also a complex algorithm \u2013 a little too complex to make it practical for a one-time sort of 25 items, for example.\nWith that said, in most cases the quick sort is the best choice if speed is important (and it almost always is). Use it for repetitive sorting, sorting of medium to large lists, and as a default choice when you\u2019re not really sure which sorting algorithm to use. Ironically, the quick sort has horrible efficiency when operating on lists that are mostly sorted in either forward or reverse order \u2013 avoid it in those situations.\nPublic Sub QuickSortArray( ByRef arr(), ByVal nLow, ByVal nHigh )\nDim pivot, tmpSwap, tmpLow, tmpHigh\ntmpLow = nLow\npivot = arr( ( nLow + nHigh ) / 2 )\nDo While tmpLow <= tmpHigh\nDo While( arr( tmpLow ) < pivot And tmpLow < nHigh )\ntmpLow = tmpLow + 1\nDo While( pivot < arr( tmpHigh ) And tmpHigh > nLow )\ntmpHigh = tmpHigh \u2013 1\n"}, {"score": 1102.4119, "uuid": "4ecc855e-b040-54e2-b943-3083d69adb2c", "index": "cw12", "trec_id": "clueweb12-1103wb-16-11218", "target_hostname": "hpccsystems.com", "target_uri": "http://hpccsystems.com/community/docs/ecl-language-reference/html/sort", "page_rank": 1.2480149e-09, "spam_rank": 93, "title": "<em>SORT</em> | HPCC Systems", "snippet": "However, where the ideal <em>sort</em> <em>algorithm</em> <em>is</em> only available in a stable version, it may often be <em>better</em> than the unstable version of a different <em>algorithm</em>.", "explanation": null, "document": "SORT(recordset,value [, JOINED( joinedset )][, SKEW( limit [,target] )] [, THRESHOLD( size )][, LOCAL] [,FEW] [, STABLE [ ( algorithm )] | UNSTABLE [ ( algorithm )] ] )\nrecordset\nThe set of records to process. This may be the name of a dataset or a record set derived from some filter condition, or any expression that results in a derived record set.\nvalue\nA comma-delimited list of expressions or key fields in the recordset on which to sort, with the leftmost being the most significant sort criteria. A leading minus sign (-) indicates a descending-order sort on that element. You may have multiple value parameters to indicate sorts within sorts. You may use the keyword RECORD to indicate an ascending sort on all fields, and/or you may use the keyword EXCEPT to list non-sort fields in the recordset.\nJOINED\nOptional. Indicates this sort will use the same radix-points as already used by the joinedset so that matching records between the recordset and joinedset end up on the same supercomputer nodes. Used to optimize supercomputer joins where the joinedset is very large and the recordset is small.\njoinedset\nA set of records that has been previously sorted by the same value parameters as the recordset.\nSKEW\nOptional. Indicates that you know this join will not be spread evenly across nodes (will be skewed) and you choose to override the default by specifying your own limit value to allow the job to continue despite the skewing.\nlimit\nA value between zero (0) and one (1.0 = 100%) indicating the maximum percentage of skew to allow before the job fails (the default is 0.1 = 10%).\ntarget\nOptional. A value between zero (0) and one (1.0 = 100%) indicating the desired maximum percentage of skew to allow (the default is 0.1 = 10%).\nTHRESHOLD\nOptional. Indicates the minimum size for a single part of the recordset before the SKEW limit is enforced.\nsize\nAn integer value indicating the minimum number of Gigabytes for a single part (the default is 1 = 1Gb).\nLOCAL\nOptional. Specifies the operation is performed on each supercomputer node independently, without requiring interaction with all other nodes to acquire data; the operation maintains the distribution of any previous DISTRIBUTE. An error occurs if the recordset has been GROUPed.\nFEW\nOptional. Specifies that few records will be sorted. This prevents spilling the SORT to disk if another resource-intensive activity is executing concurrently.\nSTABLE\nOptional. Specifies a stable sort\u2014duplicates output in the same order they were in the input. This is the default if neither STABLE nor UNSTABLE sorting is specified. Ignored if not supported by the target platform.\nalgorithm\nOptional. A string constant that specifies the sorting algorithm to use (see the list of valid values below). If omitted, the default algorithm depends on which platform is targeted by the query.\nUNSTABLE\nOptional. Specifies an unstable sort\u2014duplicates may output in any order. Ignored if not supported by the target platform.\nReturn:\nSORT returns a set of records.\nThe SORT function sorts the recordset according to the values specified. SORT is usually used to produce the record sets operated on by the DEDUP, GROUP, and ROLLUP functions, so that those functions may operate optimally. Sorting final output is, of course, another common use.\nSorting Algorithms\nThere are three sort algorithms available: quicksort, insertionsort, and heapsort. They are not all available on all platforms. Specifying an invalid algorithm for the targeted platform will be ignored and the default algorithm for that platform will be implemented.\nThor\nSupports stable and unstable quicksort\u2014the sort will spill to disk, if necessary. Parallel sorting happens automatically on clusters with multiple-CPU or multi-CPU-core nodes.\nhthor\nSupports stable and unstable quicksort, stable and unstable insertionsort, and stable heapsort\u2014the sort will spill to disk, if necessary. Stable heapsort is the default if both STABLE and UNSTABLE are omitted or if STABLE is present without an algorithm parameter.\nUnstable quicksort is the default if UNSTABLE is present without an algorithm parameter.\nRoxie\nSupports unstable quicksort, stable insertionsort, and stable heapsort\u2014the sort does not spill to disk.\nStable heapsort is the default if both STABLE and UNSTABLE are omitted or if STABLE is present without an algorithm parameter. The insertionsort implements blocking and heapmerging when there are more than 1024 rows.\nQuick Sort\nA quick sort does nothing until it receives the last row of its input, and it produces no output until the sort is complete, so the time required to perform the sort cannot overlap with either the time to process its input or to produce its output. Under normal circumstances, this type of sort is expected to take the least CPU time. There are rare exceptional cases where it can perform badly (the famous \"median-of-three killer\" is an example) but you are very unlikely to hit these by chance.\nOn a Thor cluster where each node has multiple CPUs or CPU cores, it is possible to split up the quick sort problem and run sections of the work in parallel. This happens automatically if the hardware supports it. Doing this does not improve the amount of actual CPU time used (in fact, it fractionally increases it because of the overhead of splitting the task) but the overall time required to perform the sort operation is significantly reduced. On a cluster with dual CPU/core nodes it should only take about half the time, only about a quarter of the time on a cluster with quad-processor nodes, etc.\nInsertion Sort\nAn insertion sort does all its work while it is receiving its input. Note that the algorithm used performs a binary search for insertion (unlike the classic insertion sort). Under normal circumstances, this sort is expected to produce the worst CPU time. In the case where the input source is slow but not CPU-bound (for example, a slow remote data read or input from a slow SOAPCALL), the time required to perform the sort is entirely overlapped with the input.\nHeap Sort\nA heap sort does about half its work while receiving input, and the other half while producing output. Under normal circumstances, it is expected to take more CPU time than a quick sort, but less than an insertion sort. Therefore, in queries where the input source is slow but not CPU-bound, half of the time taken to perform the sort is overlapped with the input. Similarly, in queries where the output processing is slow but not CPU-bound, the other half of the time taken to perform the sort is overlapped with the output. Also, if the sort processing terminates without consuming all of its input, then some of the work can be avoided entirely (about half in the limiting case where no output is consumed), saving both CPU and total time.\nIn some cases, such as when a SORT is quickly followed by a CHOOSEN, the compiler will be able to spot that only a part of the sort's output will be required and replace it with a more efficient implementation. This will not be true in the general case.\nStable vs. Unstable\nA stable sort is required when the input might contain duplicates (that is, records that have the same values for all the sort fields) and you need the duplicates to appear in the result in the same order as they appeared in the input. When the input contains no duplicates, or when you do not mind what order the duplicates appear in the result, an unstable sort will do.\nAn unstable sort will normally be slightly faster than the stable version of the same algorithm. However, where the ideal sort algorithm is only available in a stable version, it may often be better than the unstable version of a different algorithm.\nPerformance Considerations\nThe following discussion applies principally to local sorts, since Thor is the only platform that performs global sorts, and Thor does not provide a choice of algorithms.\nCPU time vs. Total time\nIn some situations a query might take the least CPU time using a quick sort, but it might take the most total time because the sort time cannot be overlapped with the time taken by an I/O-heavy task before or after it. On a system where only one subgraph or query is being run at once (Thor or hthor), this might make quick sort a poor choice since the extra time is simply wasted. On a system where many subgraphs or queries are running concurrently (such as a busy Roxie) there is a trade-off, because minimizing total time will minimize the latency for the particular query, but minimizing CPU time will maximize the throughput of the whole system.\nWhen considering the parallel quick sort, we can see that it should significantly reduce the latency for this query; but that if the other CPUs/cores were in use for other jobs (such as when dual Thors are running on the same dual CPU/core machines) it will not increase (and will slightly decrease) the throughput for the machines.\nSpilling to disk\nNormally, records are sorted in memory. When there is not enough memory, spilling to disk may occur. This means that blocks of records are sorted in memory and written to disk, and the sorted blocks are then merged from disk on completion. This significantly slows the sort. It also means that the processing time for the heap sort will be longer, as it is no longer able to overlap with its output.\nWhen there is not enough memory to hold all the records and spilling to disk is not available (like on the Roxie platform), the query will fail.\nHow sorting affects JOINs\nA normal JOIN operation requires that both its inputs be sorted by the fields used in the equality portion of the match condition. The supercomputer automatically performs these sorts \"under the covers\" unless it knows that an input is already sorted correctly. Therefore, some of the considerations that apply to the consideration of the algorithm for a SORT can also apply to a JOIN. To take advantage of these alternate sorting algorithms in a JOIN context you need to SORT the input dataset(s) the way you want, then specify the NOSORT option on the JOIN.\nNote well that no sorting is required for JOIN operations using the KEYED (or half-keyed), LOOKUP, or ALL options. Under some circumstances (usually in Roxie queries or in those cases where the optimizer thinks there are few records in the right input dataset) the supercomputer's optimizer will automatically perform a LOOKUP or ALL join instead of a regular join. This means that, if you have done your own SORT and specified the NOSORT option on the JOIN, that you will be defeating this possible optimization.\nExample:\nMySet1 := SORT(Person,-last_name, first_name);\r\n// descending last name, ascending first name\r\n\r\nMySet2 := SORT(Person,RECORD,EXCEPT per_sex,per_marital_status);\r\n// sort by all fields except sex and marital status\r\n\r\nMySet3 := SORT(Person,last_name, first_name,STABLE('quicksort'));\r\n// stable quick sort, not supported by Roxie\r\n\r\nMySet4 := SORT(Person,last_name, first_name,UNSTABLE('heapsort'));\r\n// unstable heap sort,\r\n// not supported by any platform,\r\n// therefore ignored\r\n\r\nMySet5 := SORT(Person,last_name,first_name,STABLE('insertionsort'));\r\n// stable insertion sort, not supported by Thor\nSee Also: SORTED, RANK, RANKED, EXCEPT\n"}, {"score": 1092.049, "uuid": "4e5c332a-af33-5c70-b448-6b159fa756ea", "index": "cw12", "trec_id": "clueweb12-0915wb-72-21778", "target_hostname": "philcrissman.com", "target_uri": "http://philcrissman.com/2010/07/18/how-not-to-write-sorting-algorithms-in-ruby/", "page_rank": 1.2090596e-09, "spam_rank": 72, "title": "How Not To Write Sorting <em>Algorithms</em> in Ruby - Automagical Thinking", "snippet": "Well, the main reason <em>is</em> that both of these methods are significantly slower than good old Array::<em>sort</em>, <em>which</em> <em>is</em> built in to Ruby (and <em>which</em>, as far as I know, <em>is</em> implemented using the <em>quicksort</em> <em>algorithm</em>).", "explanation": null, "document": "How Not to Write Sorting Algorithms in Ruby\nJul 18th, 2010\nThe second chapter of Introduction To Algorithms by Cormen, Leiserson, et al., describes the algorithms for Insertion Sort and Merge Sort in pseudocode, and compares their relative efficiency (or lack thereof). I decided to write them out in Ruby and run some simple benchmarks, just for fun.\nInsertion Sort\nThere are a variety of ways one could write the insertion sort in Ruby; this is more or less an literal translation of the Intro to Algorithms pseudocode to ruby \u2013 it could perhaps be made a little more ruby-ish, but here\u2019s my version:\ndef insertion_sort(sequence)\n  sequence[1..sequence.size].each_with_index do |j, index|\n    i = index\n    sequence[0..i].reverse.each do |k|\n      break unless k > j\n      sequence[i + 1] = k\n      i = i - 1\n    end\n    sequence[i+1] = j\n  end\nend\nIn a nutshell:\nStart with the second element (index 1).\nSave the index in a local variable, because we\u2019re going to use it to figure out which array element to swap the current element into.\nTake the sub-array 0..i (where i is the current index being looked at), and go backwards until the element in your inner loop is less than the current element from the outer loop, also decrementing i each iteration.\nCopy the element in index [i] to index [i + 1]\nOutside the inner loop, we\u2019ll put the current element at [i + 1].\nIn case this is a little unclear, it would go something like this:\nStarting array: [5,2,3,1,4,0]\nFourth pass: [1,2,3,4,5,0]\nFifth pass: [0,1,2,3,4,5]\n(Bold elements are the ones which were the \u201ccurrent\u201d element moved back, italic elements are the ones that had to be shifted forward in this step.)\nMerge Sort\nThe merge sort is split into two parts; a merge method that acts as a helper, and the actual merge_sort which calls itself recursively, also calling merge when needed.\nHere\u2019s a version modeled after the pseudocode in Intro to Algorithms:\ndef merge_sort(sequence, first, last)\n  if first < last\n    mid = (first + last) / 2\n    merge_sort(sequence, first, mid)\n    merge_sort(sequence, mid + 1, last)\n    merge(sequence, first, mid + 1, last)\n  end\nend\n\ndef merge(sequence, first, mid, last)\n  left = sequence[first..mid - 1]\n  right = sequence[mid..last]\n  left.push(Float::MAX)\n  right.push(Float::MAX)\n  i = 0\n  j = 0\n  (first..last).each do |n|\n    if left[i] <= right[j]\n      sequence[n] = left[i]\n      i = i + 1\n    else\n      sequence[n] = right[j]\n      j = j + 1\n    end\n  end\nend\nA few notes: You\u2019ll notice that in the merge method, when the sequence is split into two parts, the value Float::MAX is pushed onto each as the final element. Intro to Algorithms uses the idea that \u201cinfinity\u201d is set to the last element. Hopefully I can explain why in the description of the algorithm.\nFirst, the merge method. The merge method assumes that the left half and the right half of the section of the array it\u2019s looking at are already sorted. Having made this assumption, it loops through the range it\u2019s considering (first..last), and puts either the first element of the left side, or the first element of the right side, whichever it least. It increments each subarray as it goes; when it gets past the last element of either array, Float::MAX will always be higher (or so we assume, for the sake of this implementation) than whatever is in the other array. So merge finishes up, basically (ahem) merging the two halves together in place, with the whole first..last range sorted.\nNow, the actual merge_sort method. It first checks to see if first < last; if this were false, then the range would be an array of size 1, and an array of size 1 is already sorted. So assuming the range first..last is > 1, it calculates a midpoint; integer division works fine for this, using (first + last) / 2. Then it calls itself recursively on each subarray; these in turn will continue splitting each subarray into two parts until it gets to the point where it\u2019s just two subarrays of one element each; which are then fed to merge and sorted. As the method returns from each level of recursion, each \u201chalf\u201d of the subarray under consideration has been sorted, until it gets all the way back to the first call to merge_sort, and executes merge on each (sorted) half of the full array.\nPhew. I\u2019m going to attempt to show another example, in case that\u2019s helpful. Let\u2019s say we started with the same starting array as the insertion_sort example. This time, to show what the recursion is actually doing, I\u2019ll write out what each nested call to merge_sort( and/or merge) would actually look like:\nStarting array: [5,2,3,1,4,0]\nmerge_sort([5,2,3,1,4,0], 0, 5):\n  mid <- 2\n  merge_sort([5,2,3], 0, 2):\n    mid <- 1\n    merge_sort([5,2], 0, 1):\n      mid <- 0\n      merge_sort([5], 0, 0):\n      merge_sort([2], 1, 1):\n      merge([5,2],0, 1, 1):\n        left <- [5,\u221e]\n        right <- [2,\u221e]\n        loop(0..1):\n          [2,5] # just showing the result in this step; the two elements are sorted.\n    merge_sort([3], 2, 2):\n    merge([2,5,3], 0, 2, 2):\n      left <- [2,5,\u221e]\n      right <- [3,\u221e]\n      loop(0..2):\n        [2,3,5]\n  merge_sort([1,4,0], 3, 5):\n    mid <- 4\n    merge_sort([1,4], 3, 4):\n      mid <- 3\n      merge_sort([1], 3, 3):\n      merge_sort([4], 4, 4):\n      merge([1,4], 3, 4, 4):\n        left <- [1,\u221e]\n        right <- [4,\u221e]\n        loop(3..4):\n          [1,4]\n    merge_sort([0], 5, 5):\n    merge([1,4,0], 3, 5, 5):\n      left <- [1,4,\u221e]\n      right <- [0,\u221e]\n      loop(3..5):\n        [0,1,4]\n  merge([2,3,5,0,1,4], 0, 3, 5):\n    left <- [2,3,5,\u221e]\n    right <- [0,1,4,\u221e]\n    loop(0..5):\n      [0,1,2,3,4,5]\nUhm, hopefully that\u2019s clear. If you\u2019re bothering to read this and it still isn\u2019t clear, just run the sort locally and add some print or puts statements liberally so you can see what\u2019s going on. Note: I did take a liberty in the above listing of what\u2019s happening; in actuality the entire array is passed each time; I made it look as though just the subarray were being passed in each case, because that is the only part that is being looked at in each nested section. Again, if you\u2019re interested enough in the topic that you\u2019re still working out this algorithm, fire up irb and just try it out, it will make more sense as you do so.\nWait, why is this called \u201chow not to write sorting algorithms\u2026?\u201d\nAha. I thought you\u2019d never ask. Well, the main reason is that both of these methods are significantly slower than good old Array::sort, which is built in to Ruby (and which, as far as I know, is implemented using the quicksort algorithm).\nSo, just for fun, I wrote a little script to make some benchmarks of these various methods on arrays of various sizes. Here were the results:\nuser     system      total        real\n\n small_array has 100 elements in it...\nInsertion Sort (small_array): \n  0.000000   0.000000   0.000000 (  0.002122)\nMerge sort (small_array): \n  0.000000   0.000000   0.000000 (  0.001499)\nRuby Array::sort (small_array): \n  0.000000   0.000000   0.000000 (  0.000016)\n\n bigger_array has 1000 elements in it...\nInsertion sort (bigger_array): \n  0.240000   0.000000   0.240000 (  0.238994)\nMerge sort (bigger array): \n  0.030000   0.010000   0.040000 (  0.027746)\nRuby Array::sort (bigger_array): \n  0.000000   0.000000   0.000000 (  0.000161)\n\n medium_array has 10,000 elements in it...\nInsertion sort (medium_array): \n 23.960000   0.010000  23.970000 ( 24.096455)\nMerge sort (medium_array): \n  0.720000   0.020000   0.740000 (  0.726312)\nRuby Array::sort (medium_array): \n  0.000000   0.000000   0.000000 (  0.001637)\n\n huge_array has 100,000 elements in it...\nSorry, the insertion_sort was taking too long! Skipped it.\nMerge sort (huge_array): \n 61.710000  11.990000  73.700000 ( 74.332743)\nruby Array::sort (huge_array): \n  0.020000   0.000000   0.020000 (  0.020800)\nThe interesting column is the last one: real, as in \u201creal time\u201d (more or less, taking into the account that we\u2019re just benchmarking).\nThe small array is pretty fast for all three methods, but even still, we can see that merge_sort is a little faster, and the built in .sort! (using quicksort) is way faster: only 0.000016 seconds.\nThe next bigger, bigger_array, the differences stand out a little more. Insertion takes 0.24 seconds, merge takes 0.028, and sort! (again), way faster, only 0.00016 seconds.\nA ten thousand element array starts to show the difference even more dramatically: insertion_sort now takes 24.1 seconds, close to half a minute, while the merge_sort still runs in under a second. sort!, once again, is blowing them away at only 0.0016 seconds.\nFinally, a hundred thousand element array; sorry for the lack of stats, but I was too impatient to let insertion_sort finish. Let\u2019s just say it would take awhile. Now even the merge_sort shows its limits, taking over a minute, about 74.3 seconds. And finally, good old sort! clocking in at only 0.02 seconds.\nSo the reason this is how not to write sorting algorithms in ruby is simply that, you really probably don\u2019t need to, unless you need something faster than a quicksort written in C. The reason, hopefully, to be playing around writing algorithms in Ruby or in whatever your favorite language happens to be, is just to understand them better.\nAnd it\u2019s fun.\n"}, {"score": 1024.8451, "uuid": "dcf4d92b-e319-5f20-8d67-ded005e39e10", "index": "cw12", "trec_id": "clueweb12-1510wb-72-29647", "target_hostname": "staff.ustc.edu.cn", "target_uri": "http://staff.ustc.edu.cn/~csli/graduate/algorithms/book6/partii.htm", "page_rank": 1.2937607e-09, "spam_rank": 71, "title": "Intro to <em>Algorithms</em>: PART II: Sorting and Order Statistics", "snippet": "In particular, the average-case analyses of <em>quicksort</em>, bucket <em>sort</em>, and the order-statistic <em>algorithm</em> use probability, <em>which</em> <em>is</em> reviewed in Chapter 6.", "explanation": null, "document": ""}, {"score": 1023.41766, "uuid": "e11e248d-f0b7-543d-ad0d-d9eb3617510f", "index": "cw12", "trec_id": "clueweb12-0002wb-05-31622", "target_hostname": "imej.wfu.edu", "target_uri": "http://imej.wfu.edu/articles/2002/2/02/index.asp", "page_rank": 1.4267333e-09, "spam_rank": 94, "title": "IMEJ Article - Animating Recursive <em>Algorithms</em>", "snippet": "Recursive Manipulation of Data Items within an Existing Linear Data Structure In recursive sorting <em>algorithms</em>, such as <em>quicksort</em> and <em>merge</em> <em>sort</em>, the starting point <em>is</em> an array <em>or</em> linked list containing the data items to be sorted.", "explanation": null, "document": "Lee Naish , The University of Melbourne\nAbstract\nDesigning visual representations for recursive algorithms has been addressed within a pedagogically-oriented framework for animating algorithms. We present a classification for choosing the kind of visual representation that is most helpful to students. The classification is based on the way the algorithm navigates through a data structure and manipulates data items within a data structure, and suggest strategies for visual representation that work within the categories of this classification. Further opportunities for tailoring representation derive from the shape of the data structure and particular forms of recursion, such as tail recursion. While there may be no single, general way to represent recursive algorithms, our classification is a\nuseful guide to picking an appropriate strategy for use when animating recursive algorithms for teaching purposes.\n1. Introduction\nThe Algorithms in Action (AIA) project [14] uses multimedia animations of algorithms as a pedagogical tool to help students learn. Our target group of students is studying a second-year core curriculum subject on data structures and algorithms, where the concentration is on searching and sorting algorithms. The students are familiar with the concept of recursion, but have not yet been exposed to any but the simplest of algorithms.\nAlthough several algorithm animations are available, especially for the simpler algorithms [4], few satisfied our need for pedagogically oriented animation. Notable exceptions are Gloor's CD companion [7] to a text on algorithms [5] and the collaborative active textbooks of Marc Brown [2, 3], neither of which fit well with our syllabus and choice of textbook [11].\nOur aim in developing AIA was to find a general approach to presenting and animating algorithms and to implement a framework that could be used as the basis for presenting specific algorithms. The expectation was that interactive visual presentation would help students develop both an overall conceptual idea of how each algorithm works and a more detailed procedural understanding.\n2. The Algorithms in Action Framework\nThe AIA framework consists of animation, pseudocode, and textual explanation, all coordinated, as previously reported [15]. A cursor traces the execution of the algorithm through pseudocode, while the animation displays a conceptual representation of each step. Students control the level of detail being displayed by expanding or contracting lines of pseudocode. At higher levels of abstraction the animation helps students to grasp the overall concept of an algorithm, while the more detailed views help them understand the workings of the algorithm at a procedural level. The level of detail in the animation, in the explanation, and in the pseudocode expand and contract synchronously.\nAbout the authors...\n3. Recursion\nRepresenting recursive algorithms in ways that will be helpful to students has proven to be more challenging than designing representations for iterative algorithms. While initially we sought a general approach to the problem of displaying recursive algorithm, in practice, we found that the choice of most suitable representation depended on the particular algorithm being displayed.\nThe most appropriate visual representation for recursive algorithms was correlated with a classification based on how the algorithm navigates through and manipulates the data items within the data structure [13]. In this classification, one category of algorithms consists of those that traverse a static data structure (e.g. search), another category contains algorithms that manipulate items within an existing data structure (e.g. sort), and a third category contains algorithms that build a data structure (e.g. insert). Within these categories, it was sometimes useful to further subdivide based on additional criteria, such as tail recursiveness and data structure shape.\n4. Code Tracing of Recursive Calls\nIn AIA, only one copy of the recursive function is shown in the pseudocode. As a recursive call is made, the pseudocode tracing restarts at the top of the function. Visual clues assist with clarity here, and most of our second-year students understand that when the pseudocode cursor jumps from the recursive call within the function to the top of the same function, this represents a new function call. This approach might not be suitable for beginning programmers, where it may be useful to pop up a new window for each recursive call [16].\n5. Recursive Manipulation of Data Items within an Existing Linear Data Structure\nIn recursive sorting algorithms, such as quicksort and merge sort, the starting point is an array or linked list containing the data items to be sorted. The data structures are linear, and the algorithms use the classical divide-and-conquer approach, where the algorithm makes a number of recursive function calls to smaller and smaller parts of the data structure. During each recursive call, some data items are rearranged. For example, in quicksort, each function call partitions the data such that all items before a designated \"pivot\" element are smaller than (or equal to) the pivot, and all items after are larger (or equal). Two recursive calls are then made, one to the part of the array with the smaller elements, and another to the part of the array with the larger elements. The student's task is to learn what happens during each recursive call and to understand how the progression of recursive calls moves the sorting process through the data structure.\nFor these algorithms, where data items within an existing linear data structure are manipulated without the data structure changing its size or shape, we represent each recursive call by a horizontal line under the sequence of data items referred to in the recursive call (Figure 1). The value of each data item is represented by a vertical bar, similar to the representation used in the seminal algorithm animation video, \"Sorting out Sorting\" [1]. Our use of horizontal bars to underline the segment of the data structure being manipulated during the recursive call bears a similarity to the design used by Dershem and Brummond [6]. Additionally, different shades of color distinguish between the currently active call, calls on the function stack, and completed calls.\nFigure 1. Quicksort animation, showing recursive calls. The current recursive call is sorting the elements 25, 35, 30, with 30 as the pivot (shown by the box). Elements up to 20 have already been sorted, while elements from 40 onwards are waiting for a call on the stack to be done.\nQuicksort animation applet.\n6. Recursive Navigation through a Linear Data Structure\nSearching through a dictionary abstract data type entails navigating through a data structure, searching for a data item with a particular key. Initially nothing is known about the location of the key. As the search progresses the subset of possible locations is narrowed down.\nWhere the data structure is linear, navigation can be represented by horizontal lines under the data items referred to in the recursive call. This is similar to the representation used for quicksort, except that the search narrows in scope as it progresses (Figure 2), since there is only one recursive call, instead of the two recursive calls made by each invocation of quicksort.\nThis representation can also be useful for an equivalent iterative coding as it gives the history of the search at a single glance, but is less natural.\nFigure 2. Binary search, showing 3 recursive calls. The horizontal lines under the data structure show subarrays for successive recursive calls.\n7. Navigation and Data Manipulation in Non-linear Data Structures\nData structures for search often have a nonlinear tree structure, which makes visualizing the current subset of the data more challenging. For a tree structure, there are several different ways in which the current subtree can be shown. Given some constraints on the tree layout, a horizontal bar similar to the horizontal bars used in quicksort can underline the leaves of the current subtree (Figure 3a). Alternatively, a modified approach can enclose subtrees involved in recursive calls in polygons (Figure 3b). Less abstractly, a pointer to the root of the subtree can be shown (Figure 3c).\nThe choice of representation is influenced by the students' background. First year students might find it easiest to understand the algorithm where the entire subtree is outlined. Later year students would generally be able to understand the significance of a pointer to the root of the subtree. The pointer representation has the advantage of being closest to the recursive code, and students might find this representation assists them in following the pseudocode closely. The pointer representation, alone or in combination with other representations, may help students make the link between the algorithm code and progression through the data structure, conceptualizing the idea that the subtree whose root is being pointed to consists of all the data still under consideration. Horizontal bars under the data structure might be useful in cases where the data structure is complicated and an objective is to reduce clutter on the screen, and can highlight the similarities between different algorithms, such as searching a (balanced) binary search tree (Figure 3a) and binary search in an array containing the inorder traversal of the same tree (Figure 2).\n(a)\n(b)\n(c)\nFigure 3. Recursive binary search tree, showing three different ways to display the part of the data involved in the current recursive call:\n(a) The leaves, or outer extent, of the current tree are underlined;\n(b) Polygons are used to outline the subtree under investigation, with larger triangles showing the history of previous recursive calls;\n(c) A pointer to the root of the subtree corresponding to the current recursive call is shown (solid line arrow); optionally, a history of recursive calls can be kept (dotted line arrows).\n8. Navigation Using Non-Tail Recursive Algorithms\nWhere code is tail recursive, the currently active function call narrows the search one step, while the rest of the searching is done in successive recursive calls. The tail recursive algorithm is very similar to an iterative while loop. Visual representation for both recursive and iterative codings must indicate which part of the data structure is under investigation. For a recursive implementation, it is natural for the display to indicate multiple subsets of the data, getting smaller as the search progresses through multiple recursive calls.\nWhen the code is not tail recursive, there is work to be done upon returning from the recursive call. The location of the recursive call within the code is important because it determines what work is to be done upon return from the call. The pedagogical challenge here is to help students keep track of the point in the calling function where the recursive call is made, as well as how the navigation is progressing through the data structure.\nFor example, when navigating through a splay tree [11] \"rotations\" are performed after the return from the recursive call. Rotations are local operations on a small portion of the tree, intended to improve the overall balance of the tree. Because the place of the rotations within the algorithm may not be immediately obvious to a student just learning the algorithm, we supplement the main animation display with an abbreviated function stack (Figure 4). The aim of showing the function stack is to track the progress through the calling function, and to show the work still to be done after return from the recursive call. We are, in effect, combining two types of animation: animating the result of the program, in the tree, and simultaneously animating the state of the program in the function stack [8], to help students make the mental link between the two processes.\n(a)\n(b)\nFigure 4. Splay tree search showing stack.\nIn (a) the stack contains three function calls. The third (top) call has been called from the first line in the second call (highlighted).\nIn (b), the third call has returned, and the second call is performing the rotation specified in its second line (now highlighted).\nSplay tree applet.\n9. Recursive Insertion into a Data Structure\nUnlike navigation, which works on an existing data structure, insertion of items into a data structure involves major changes in the size and possibly the shape of the data structure. Accurate visual representation of recursive insertion requires depicting changes in the data structure both during the growth phase, when the correct insertion point is sought and recursive calls are added to the function stack, and during the shrinking phase, when the recursive calls return, the function stack shrinks, and the appropriate links within the data structure are made.\nFor relatively uncomplicated algorithms, representation can be straightforward. Taking the radix search trie as an example, insertion involves creation of one new external node and possibly one or more internal nodes, which must all be linked into the trie. In animating insertion here, we depict the growth stage of the algorithm, where decisions about the path and nodes are made, by tracing the path to the insertion point, leaving gaps for the links, which are still to be made. (Figure 5). During the shrinking phase of the insertion algorithm, the animation shows the links forming as the recursive calls return. Since the eventual shape of the subtree after insertion of the new data is not known at the time of the growth phase, the pointer representation is convenient.\n(a)\n(b)\n(c)\nFigure 5. Insertion into radix trie. New nodes are formed as recursive calls are being made, but are not linked to the rest of the trie. The links will be made as the recursive calls return.\nFor more complicated algorithms, we supplement the main part of the animation with a thumbnail sketch that abstracts the data structure and indicates the part of the data structure referred to in the recursive call. For example, the main animation in our representation of a Patricia trie [11] shows all the details necessary to follow the workings of the algorithms, such as keys and upward links. Thumbnail sketches capture the essence, showing only nodes and links in the subtree currently being searched, without detail. The region to be searched by future recursive call(s) is represented by an large blank space (Figure 6). Thumbnail sketches are used in both the growth and shrinking phases of the algorithm, and are visually linked to the main animation. Over the course of the algorithm the thumbnail sketches allow us to build up a history of recursive calls, which helps students keep track of the overall progress of the recursive navigation.\n(a)\n"}, {"score": 994.86273, "uuid": "0bdbb187-9a64-5361-9ac1-5d1b420d65e7", "index": "cw12", "trec_id": "clueweb12-0714wb-57-02175", "target_hostname": "developers.slashdot.org", "target_uri": "http://developers.slashdot.org/story/10/08/20/1656250/Sorting-Algorithms-mdash-Boring-Until-You-Add-Sound", "page_rank": 1.1700305e-09, "spam_rank": 81, "title": "Sorting <em>Algorithms</em> \u2014 Boring Until You Add Sound - Slashdot", "snippet": "If you bother to learn Heapsort well, you don&#x27;t really *need* any other sorting <em>algorithm</em>. Yep, I&#x27;ve seen optimized implementations of <em>quicksort</em> <em>which</em> drop down to bubble <em>sort</em> when there&#x27;s only three <em>or</em> four elements in the current subdivision.", "explanation": null, "document": "on Friday August 20 2010, @12:18PM\nfrom the bloop-bleep-bloop dept.\nAn anonymous reader writes \"Anyone who's ever taken a programming course or tried to learn how to code out of a book will have come across sorting algorithms. Bubble, heap, merge \u2014 there's a long list of methods for sorting data. The subject matter is fairly dry. Thankfully, someone has found a way to not only make sorting more interesting , but easier to remember and understand, too.\"\n"}, {"score": 986.7415, "uuid": "758315c3-af14-5b31-93a7-0109a8c034a2", "index": "cw12", "trec_id": "clueweb12-0501wb-57-34732", "target_hostname": "www.math.grin.edu", "target_uri": "http://www.math.grin.edu/~rebelsky/Courses/CS152/2000S/Outlines/outline.27.html", "page_rank": 1.2083576e-09, "spam_rank": 95, "title": "CSC-152 2000S : Class 27: More Efficient Sorting <em>Algorithms</em>", "snippet": "One of the most straightforward <em>is</em> <em>merge</em> <em>sort</em>. * In <em>merge</em> <em>sort</em>, you split the list, array, collection <em>or</em> ... into two parts, <em>sort</em> each part, and then <em>merge</em> them together. * Unlike the previous <em>algorithms</em>, <em>merge</em> <em>sort</em> requires extra space for the sorted arrays <em>or</em> subarrays. * We&#x27;ll write this as a non-inplace", "explanation": null, "document": "Sorting with Divide and Conquer\nIn the previous class, we identified a number of interesting sorting algorithms which took O(n2) time.\nCan we do better? Well, sometimes using divide-and-conquer helps speed up algorithms (in our experience from O(n) to O(log2n)).\nWe'll look at two different ways of ``splitting'' the array.\nMerge Sort\nWe can develop a number of sorting techniques based on the divide and conquer technique. One of the most straightforward is merge sort.\nIn merge sort, you split the list, array, collection or ... into two parts, sort each part, and then merge them together.\nUnlike the previous algorithms, merge sort requires extra space for the sorted arrays or subarrays.\nWe'll write this as a non-inplace routine which returns a new, sorted array, rather than sorting the existing array.\nIn approximate Java code,\nimport SimpleOutput; /**\n * A collection of techniques for sorting an input array.\n *\n * @author Samuel A. Rebelsky\n * @version 1.1 of March 2000\n */ public class MergeSorter\n{ // +--------+-------------------------------------------------- // | Fields | // +--------+ /** The current indent level.  Used when logging steps. */ String indent = \"\"; // +----------------+------------------------------------------ // | Public Methods | // +----------------+ /**\n   * Sort an array, creating a new sorted version of the array.\n   * If the SimpleOutput object is non-null, prints a simple log\n   * of what's happening.\n   * Pre: The elements in the array can be compared to each other.\n   * Pre: There is sufficient memory to complete the creation of the\n   *   new array (and the other steps of the algorithm).\n   * Post: Returns a sorted version of the array (where sorted is\n   *   defined carefully elsewhere).\n   * Post: Does not affect the original array.\n   */ public Object[] sort(Object[] stuff, \n                       Comparator compare, \n                       SimpleOutput observer) throws IncomparableException\n  { return mergeSort(stuff, 0, stuff.length-1, compare, observer);\n  } // sort(Object[]) // +----------------+------------------------------------------ // | Helper Methods | // +----------------+ /**\n   * Sort part of an array, creating a new sorted version of the\n   * part of the array.\n   * Pre: The elements in the array can be compared to each other.\n   * Pre: There is sufficient memory to complete the creation of the\n   *   new array (and the other steps of the algorithm).\n   * Post: Returns a sorted version of the array (where sorted is\n   *   defined carefully elsewhere).\n   * Post: Does not affect the original array.\n   */ protected Object[] mergeSort(Object[] stuff, int lb, int ub, \n                               Comparator compare,\n                               SimpleOutput observer) throws IncomparableException\n  {\n    Object[] sorted; // The sorted version int middle; // Index of middle element // Print some basic information. if (observer != null) {\n      observer.print(indent + \"Sorting: \");\n      printSubArray(stuff, lb, ub, observer);\n      indent = indent + \" \";\n    } // Base case: vector of size 0 or 1.  Make a fresh copy so that // it's safe to modify (and is the appropriate size. if (ub <= lb) {\n      sorted = copySubArray(stuff, lb, ub);\n    } // base case // Recursive case: split and merge else { // Find the middle of the subarray. middle = (lb + ub) / 2; // Sort the two halves. Object[] left = mergeSort(stuff, lb, middle, compare, observer);\n      Object[] right = mergeSort(stuff, middle+1, ub, compare, observer);\n      sorted = merge(left, right, compare);\n    } // recursive case // Print information, if appropriate if (observer != null) {\n      indent = indent.substring(2);\n      observer.print(indent + \"Sorted: \");\n      printSubArray(sorted, 0, sorted.length-1, observer);\n    } // That's it. return sorted;\n  } // mergeSort(Object[], int, int, Comparator) /**\n   * Merge two sorted arrays into a new single sorted array.\n   * Pre: Both vectors are sorted.\n   * Pre: Elements in both vectors may be compared to each other.\n   * Pre: There is sufficient memory to allocate the new array.\n   * Post: The returned array is sorted, and contains all the\n   *   elements of the two arrays (no more, no less).\n   * Post: The two arguments are not changed\n   */ public Object[] merge(Object[] left, Object[] right, Comparator compare) throws IncomparableException\n  { // Create a new array of the appropriate size. Object[] result = new Object[left.length + right.length]; // Create indices into the three arrays. int leftIndex=0; // Index into left array. int rightIndex=0; // Index into right array. int index=0; // Index into result array. // As long both vectors have elements, copy the smaller one. while ((leftIndex < left.length) && (rightIndex < right.length)) { if(compare.precedes(left[leftIndex],right[rightIndex])) {\n        result[index++] = left[leftIndex++];\n      } // first element in left subvector is smaller else {\n        result[index++] = right[rightIndex++];\n      } // first element in right subvector is smaller\n    } // while both vectors have elements // Copy any remaining parts of each vector. while(leftIndex < left.length) {\n      result[index++] = left[leftIndex++];\n    } // while the left vector has elements while(rightIndex < right.length) {\n      result[index++] = right[rightIndex++];\n    } // while the right vector has elements // That's it return result;\n  } // merge /**\n   * Copy a subarray (so that we can return it without affecting it).\n   * Pre: 0 <= lb <= ub < stuff.length\n   * Post: Does not affect stuff.\n   * Post: Returns a new array containing only stuff[lb] .. stuff[ub].\n   */ protected Object[] copySubArray(Object[] stuff, int lb, int ub) { // Create the new array. Object[] result = new Object[ub-lb+1]; for (int i = lb; i <= ub; i++) {\n      result[i-lb] = stuff[i];\n    } return result;\n  } // copySubArray /**\n   * Print a subarray.\n   * Pre: 0 <= lb <= ub < stuff.length\n   * Post: Does not affect stuff.\n   */ protected void printSubArray(Object[] stuff, int lb, int ub,\n                               SimpleOutput out) { // Print all but the last element followed by a comma for (int i = lb; i < ub; ++i) {\n      out.print(stuff[i].toString() + \",\");\n    } // Print the last element out.println(stuff[ub]);\n  } // printSubArray\n} // MergeSorter\nHere's the corresponding test class.\nimport MergeSorter; import SimpleOutput; import StringComparator; /**\n * A simple test of selection sort.\n *\n * @author Samuel A. Rebelsky\n * @version 1.0 of September 1999\n */ public class TestMergeSorter { public static void main(String[] args) throws Exception\n  {\n    SimpleOutput out = new SimpleOutput();\n    MergeSorter sorter = new MergeSorter();\n    Object[] sorted = sorter.sort(args, new StringComparator(), out); for (int i = 0; i < sorted.length; ++i) {\n      out.println(i + \": \" + sorted[i]);\n    } // for\n  } // main(String[])\n} // class TestMergeSorter\nWhat is the running time?\nWe can use recurrence relations:\nLet f(n) be the running time of merge sort on input of size n.\nf(1) = 1\nLet's run this for a few steps\nf(n)\n"}], [{"score": 973.7742, "uuid": "b510c152-2da7-530c-a096-4d46126f1809", "index": "cw12", "trec_id": "clueweb12-1908wb-19-02045", "target_hostname": "algs4.cs.princeton.edu", "target_uri": "http://algs4.cs.princeton.edu/14analysis/index.php", "page_rank": 1.1700305e-09, "spam_rank": 95, "title": "Analysis of <em>Algorithms</em>", "snippet": "Instead, you can call get(i, k) <em>which</em> returns the kth bit of a[i] <em>or</em> you can call swap(i, j) <em>which</em> swaps the ith and jth elements of a[]. Design an O(N) <em>algorithm</em> to find the missing integer. For simplicity, assume N <em>is</em> a power of 2. Longest row of 0s.", "explanation": null, "document": "Programming Assignments\n1.4 \u00a0 Analysis of Algorithms\nAs people gain experience using computers, they use them to solve difficult problems or to process large amounts of data and are invariably led to questions like these:\nHow long will my program take?\nWhy does my program run out of memory?\nScientific method.\nThe very same approach that scientists use to understand the natural world is effective for studying the running time of programs:\nObserve some feature of the natural world, generally with precise measurements.\nHypothesize a model that is consistent with the observations.\nPredict events using the hypothesis.\nVerify the predictions by making further observations.\nValidate by repeating until the hypothesis and observations agree.\nThe experiments we design must be reproducible and the hypotheses that we formulate must be falsifiable.\nObservations.\nOur first challenge is to determine how to make quantitative measurements of the running time of our programs. Stopwatch.java is a data type that measures the elapsed running time of a program.\nThreeSum.java counts the number of triples in a file of N integers that sums to 0 (ignoring integer overflow). DoublingTest.java generates a sequence of random input arrays, doubling the array size at each step, and prints the running times of ThreeSum.count() for each input size.\nMathematical models.\nThe total running time of a program is determined by two primary factors: the cost of executing each statement and the frequency of execution of each statement.\nTilde approximations. We use tilde approximations, where we throw away low-order terms that complicate formulas. We write ~ f(N) to represent any function that when divided by f(N) approaches 1 as N grows. We write g(N) ~ f(N) to indicate that g(N) / f(N) approaches 1 as N grows.\nOrder-of-growth classifications. Most often, we work with tilde approximations of the form g(N) ~ a f(N) where f(N) = N^b log^c N and refer to f(N) as the The order of growth of g(N). We use just a few structural primitives (statements, conditionals, loops, nesting, and method calls) to implement algorithms, so very often the order of growth of the cost is one of just a few functions of the problem size N.\nCost model. We focus attention on properties of algorithms by articulating a cost model that defines the basic operations. For example, an appropriate cost model for the 3-sum problem is the number of times we access an array entry, for read or write.\nProperty. The order of growth of the running time of ThreeSum.java is N^3.\nProposition. The brute-force 3-sum algorithm uses ~ N^3 / 2 array accesses to compute the number of triples that sum to 0 among N numbers.\nDesigning faster algorithms.\nOne of the primary reasons to study the order of growth of a program is to help design a faster algorithm to solve the same problem. Using mergesort and binary search, we develop faster algorithms for the 2-sum and 3-sum problems.\n2-sum. The brute-force solution TwoSum.java takes time proportional to N^2. TwoSumFast.java solves the 2-sum problem in time proportional to N log N time.\n3-sum. ThreeSumFast.java solves the 3-sum problem in time proportional to N^2 log N time.\nCoping with dependence on inputs.\nFor many problems, the running time can vary widely depending on the input.\nInput models. We can carefully model the kind of input to be processed. This approach is challenging because the model may be unrealistic.\nWorst-case performance guarantees. Running time of a program is less than a certain bound (as a function of the input size), no matter what the input. Such a conservative approach might be appropriate for the software that runs a nuclear reactor or a pacemaker or the brakes in your car.\nRandomized algorithms. One way to provide a performance guarantee is to introduce randomness, e.g., quicksort and hashing. Every time you run the algorithm, it will take a different amount of time. These guarantees are not absolute, but the chance that they are invalid is less than the chance your computer will be struck by lightning. Thus, such guarantees are as useful in practice as worst-case guarantees.\nAmortized analysis. For many applications, the algorithm input might be not just data, but the sequence of operations performed by the client. Amortized analysis provides a worst-case performance guarantee on a sequence of operations.\nProposition. In the linked-list implementation of Bag, Stack, and Queue, all operations take constant time in the worst case.\nProposition. In the resizing-array implementation of Bag, Stack, and Queue, starting from an empty data structure, any sequence of N operations takes time proportional to N in the worst case (amortized constant time per operation).\nMemory usage.\nTo estimate how much memory our program uses, we can count up the number of variables and weight them by the number of bytes according to their type. For a typical 64-bit machine,\nPrimitive types. the following table gives the memory requirements for primitive types.\nObjects. To determine the memory usage of an object, we add the amount of memory used by each instance variable to the overhead associated with each object, typically 16 bytes. Moreover, the memory usage is typically padded to be a multiple of 8 bytes (on a 64-bit machine).\nReferences. A reference to an object typically is a memory address and thus uses 8 bytes of memory (on a 64-bit machine).\nLinked lists. A nested non-static (inner) class such as our Node class requires an extra 8 bytes of overhead (for a reference to the enclosing instance).\nArrays. Arrays in Java are implemented as objects, typically with extra overhead for the length. An array of primitive-type values typically requires 24 bytes of header information (16 bytes of object overhead, 4 bytes for the length, and 4 bytes of padding) plus the memory needed to store the values.\nStrings. A String of length N typically uses 40 bytes (for the String object) plus 24 + 2N bytes (for the array that contains the characters) for a total of 64 + 2N bytes.\nQ + A\nQ. How do I increasing the amount of memory and stack space that Java allocates?\nA. You can increase the amount of memory allotted to Java by executing with java -Xmx200m Hello where 200m means 200 megabytes. The default setting is typically 64MB. You can increase the amount of stack space allotted to Java by executing with java -Xss200k Hello where 200k means 200 kilobytes. The default setting is typically 128KB. It's possible to increase both the amount of memory and stack space by executing with executing with java -Xmx200m -Xss200k Hello.\nExercises\nGive the order of growth (as a function of N) of the running times of each of the following code fragments:\nint sum = 0;\nfor (int n = N; n > 0; n /= 2)\n   for (int i = 0; i < n; i++) \n      sum++;\nint sum = 0;\nfor (int i = 1; i < N; i *= 2)\n   for(int j = 0; j < i; j++)\n      sum++;\nint sum = 0;\nfor (int i = 1; i < N; i *= 2)\n   for (int j = 0; j < N; j++)\n      sum++;\nAnswer: linear (N + N/2 + N/4 + ...); linear (1 + 2 + 4 + 8 + ...); linearithmic (the outer loop loops lg N times).\nCreative Problems\n4-sum. Develop a brute-force solution FourSum.java to the 4-sum problem.\nLocal minimum of an array. Write a program that, given an array a[] of N distinct integers, finds a local minimum: an index i such that a[i] < a[i-1] and a[i] < a[i+1]. Your program should use ~2 lg N compares in the worst case.\nAnswer: Examine the middle value a[N/2] and its two neighbors a[N/2 - 1] and a[N/2 + 1]. If a[N/2] is a local minimum, stop; otherwise search in the half with the smaller neighbor.\nLocal minimum of a matrix. Given an N-by-N array a[] of N2 distinct integers, design an algorithm that runs in time proportional to N to find a local minimum: an pair of indices i and j such that a[i][j] < a[i+1][j], a[i][j] < a[i][j+1], a[i][j] < a[i-1][j], and a[i][j] < a[i][j-1].\nHint: find the minimum in row N/2, check neighbors p and q in column, if p or q is smaller then recur in that half.\nBitonic search. An array is bitonic if it is comprised of an increasing sequence of integers followed immediately by a decreasing sequence of integers. Write a program that, given a bitonic array of N distinct int values, determines whether a given integer is in the array. Your program should use ~ 3 lg N compares in the worst case.\nAnswer: Use a version of binary search, as in BitonicMax.java , to find the maximum (in ~ 1 lg N compares); then use binary search to search in each piece (in ~ 1 lg N compares per piece).\nBinary search with only addition and subtraction. [ Mihai Patrascu ] Write a program that, given an array of N distinct int values in ascending order, determines whether a given integer is in the array. You may use only additions and subtractions and a constant amount of extra memory. The running time of your program should be proportional to log N in the worst case.\nAnswer: Instead of searching based on powers of two (binary search), use Fibonacci numbers (which also grow exponentially). Maintain the current search range to be [i, i + F(k)] and keep F(k), F(k-1) in two variables. At each step compute F(k-2) via subtraction, check element i + F(k-2), and update the range to either [i, i + F(k-2)] or [i + F(k-2), i + F(k-2) + F(k-1)].\nBinary search for a fraction. Devise a method that uses a logarithmic number of queries of the form Is the number less than x? to find a rational number p / q such that 0 < p < q < N.\nHint: Two fractions with denominator less than N cannot differ by more than than 1/N^2.\nThrowing eggs from a building. Suppose that you have an N-story building and plenty of eggs. Suppose also that an egg is broken if it is thrown off floor F or higher, and unbroken otherwise. First, devise a strategy to determine the value of F such that the number of broken eggs is ~ lg N when using ~ lg N throws, then find a way to reduce the cost to ~ 2 lg F.\nHint: binary search; repeated doubling and binary search.\nThrowing two eggs from a building. Consider the previous question, but now suppose you only have two eggs, and your cost model is the number of throws. Devise a strategy to determine F such that the number of throws is at most 2 sqrt(\u221a N), then find a way to reduce the cost to ~c \u221a F.\nHot or cold. Your goal is the guess a secret integer between 1 and N. You repeatedly guess integers between 1 and N. After each guess you learn if it equals the secret integer (and the game stops); otherwise (starting with the second guess), you learn if the guess is hotter (closer to) or colder (farther from) the secret number than your previous guess. Design an algorithm that finds the secret number in ~ 2 lg N guesses. Then, design an algorithm that finds the secret number in ~ 1 lg N guesses.\nHint: use binary search for the first part. For the second part, first design an algorithm that solves the problem in ~1 lg N guesses assuming you are permitted to guess integers in the range -N to 2N.\nWeb Exercises\nLet f be a monotonically increasing function with f(0) < 0 and f(N) > 0. Find the smallest integer i such that f(i) > 0. Devise an algorithm that makes O(log N) calls to f().\nFloor and ceiling. Given a set of comparable elements, the ceiling of x is the smallest element in the set greater than or equal to x, and the floor is the largest element less than or equal to x. Suppose you have an array of N items in ascending order. Give an O(log N) algorithm to find the floor and ceiling of x.\nBinary search with duplicates. Modify binary search so that it always returns the smallest (largest) index of a key of an item matching the search key.\nRank with lg N two-way compares. Implement rank() so that it uses ~ 1 lg N two-way compares (instead of ~ 1 lg N 3-way compares).\nIdentity. Given an array a of N distinct integers (positive or negative) in ascending order. Devise an algorithm to find an index i such that a[i] = i if such an index exists. Hint: binary search.\nMajority. Given an array of N strings. An element is a majority if it appears more than N/2 times. Devise an algorithm to identify the majority if it exists. Your algorithm should run in linearithmic time.\nMajority. Repeat the previous exercise, but this time your algorithm should run in linear time, and only use a constant amount of extra space. Moreover, you may only compare elements for equality, not for lexicographic order.\nAnswer: if a and b are two elements and a != b, then remove both of them; majority still remains. Use N-1 compares to find candidate for majority; use N-1 comparisons to check if candidate really is a majority.\nSecond smallest. Give an algorithm to find the smallest and second smallest elements from a list of N items using the minimum number of comparisons. Answer: you can do it in ceil(N + lg(N) - 2) comparisons by building a tournament tree where each parent is the minimum of its two children. The minimum ends up at the root; the second minimum is on the path from the root to the minimum.\nFind a duplicate. Given an array of N elements in which each element is an integer between 1 and N, write an algorithm to determine if there are any duplicates. Your algorithm should run in linear time and use O(1) extra space. Hint: you may destroy the array.\nFind a duplicate. Given an array of N+1 elements in which each element is an integer between 1 and N, write an algorithm to find a duplicate. Your algorithm should run in linear time, use O(1) extra space, and may not modify the original array. Hint: pointer doubling.\nFinding common elements. Given two arrays of N 64-bit integers, design an algorithm to print out all elements that appear in both lists. The output should be in sorted order. Your algorithm should run in N log N. Hint: mergesort, mergesort, merge. Remark: not possible to do better than N log N in comparison based model.\nFinding common elements. Repeat the above exercise but assume the first array has M integers and the second has N integers where M is much less than N. Give an algorithm that runs in N log M time. Hint: sort and binary search.\nAnagrams. Design a O(N log N) algorithm to read in a list of words and print out all anagrams. For example, the strings \"comedian\" and \"demoniac\" are anagrams of each other. Assume there are N words and each word contains at most 20 letters. Designing a O(N^2) algorithms should not be too difficult, but getting it down to O(N log N) requires some cleverness.\nSearch in a sorted, rotated list. Given a sorted list of N integers that has been rotated an unknown number of positions, e.g., 15 36 1 7 12 13 14, design an O(log N) algorithm to determine if a given integer is in the list.\nFind the missing integer. An array a[] contains all of the integers from 0 to N, except 1. However, you cannot access an element with a single operation. Instead, you can call get(i, k) which returns the kth bit of a[i] or you can call swap(i, j) which swaps the ith and jth elements of a[]. Design an O(N) algorithm to find the missing integer. For simplicity, assume N is a power of 2.\nLongest row of 0s. Given an N-by-N matrix of 0s and 1s such that in each row no 0 comes before a 1, find the row with the most 0s in O(N) time.\nMonotone 2d array. Give an n-by-n array of elements such that each row is in ascending order and each column is in ascending order, devise an O(n) algorithm to determine if a given element x in the array. You may assume all elements in the n-by-n array are distinct.\nYou are in the middle of a road, but there is a duststorm obscuring your view and orientation. There is a shelter in only one direction, but you cannot see anything until you are right in front of it. Devise an algorithm that is guaranteed to find the shelter. Your goal is to minimize the amount you have to walk. Hint: some kind of doubling back-and-forth strategy.\nImprove the following code fragment by as big a constant factor as you can for large N. Profile it to determine where is the bottleneck.\ndouble[] a = new double[N];\nfor (int i = 0; i < N; i++)\n   for (int j = 0; j < N; j++)\n       a[j] += Math.exp(-0.5 * (Math.pow(b[i] - b[j], 2));\nInplace permutation. Write a program Permutation.java that includes functions that take an array and a permutation (or inverse permutation) and rearranges the elements in the array according to the permutation (or inverse permutation). Do it in-place: use only a constant amount of extra memory.\nSumOfThree. Given three sets A, B, and C of at most N integers each, determine whether this is a triple a in A, b in B, and c in C such that a + b + c = 0.\nAnswer: Sort B in increasing order; sort C in decreasing order; for each a in A, scan B and C for a pair that sums to -a (when the sum is too small, advance in B, when the sum is too large, advance in C).\nSumOfTwo. Given two sets A and B of at most N integers each, determine whether the sum of any two distinct integers in A equals an integer in B.\nContiguous sum. Given a list of real numbers and a target value V, find a contiguous block (of any length) whose sum is as close to K as possible.\nBrute force: compute the sum of each contiguous block by brute force. This takes O(N^3) time.\nPartial sums: compute all partial sums s[i] = a[0] + a[1] + ... + a[i] so that contiguous blocks have a sum of the form s[j] - s[i]. This takes O(N^2) time.\nSort and binary search: form the partial sums as above and then sort them in ascending order. For each i, binary search for the s[j] that is as close to s[i] as possible. This takes O(N log N) time.\nLinear equation with 3 variables. For some fixed linear equation in 3 variables (say with integer coefficients), given N numbers, do any 3 of them satisfy the equation? Design a quadratic algorithm for the problem. Hint: see quadratic algorithm for 3-sum.\nFind a majority item. Given a arbitrarily long sequence of items from standard input such that one item appears a strict majority of the time, identify the majority item. Use only a constant amount of memory.\nSolution. Maintain one integer counter and one variable to store the current champion item. Read in the next item and (i) if the item equals the champion item, increment the counter by one. (ii) else decrement the counter by one and if the counter reaches 0 replace the champion value with the current item. Upon termination, the champion value will be the majority item.\nMemory of arrays. MemoryOfArrays.java .\nMemory of strings and substrings. MemoryOfStrings.java .\nAnalysis of Euclid's algorithm. Prove that Euclid's algorithm takes at most time proportional to N, where N is the number of bits in the larger input.\nAnswer: First we assume that p > q. If not, then the first recursive call effectively swaps p and q. Now, we argue that p decreases by a factor of 2 after at most 2 recursive calls. To see this, there are two cases to consider. If q \u2264 p / 2, then the next recursive call will have p' = q \u2264 p / 2 so p decreases by at least a factor of 2 after only one recursive call. Otherwise, if p / 2 < q < p, then q' = p % q = p - q < p / 2 so p'' = q' < p / 2 and p will decrease by a factor of 2 or more after two iterations. Thus if p has N bits, then after at most 2N recursive calls, Euclid's algorithm will reach the base case. Therefore, the total number of steps is proportional to N.\nFind the duplicate. Given a sorted array of N+2 integers between 0 and N with exactly one duplicate, design a logarithmic time algorithm to find the duplicate.\nHint binary search.\nGiven an array a[] of N real numbers, design a linear-time algorithm to find the maximum value of a[j] - a[i] where j \u2265 i.\nSolution:\ndouble best = 0.0;\ndouble min = a[0];\nfor (int i = 0; i < N; i++) {\n    min  = Math.min(a[i], min);\n    best = Math.max(a[i] - min, best);\n}\nLast modified on April 03, 2012.\n"}, {"score": 970.26685, "uuid": "041ed652-1782-5c80-b01b-37ad2e1efbbd", "index": "cw12", "trec_id": "clueweb12-1905wb-99-13349", "target_hostname": "algs4.cs.princeton.edu", "target_uri": "http://algs4.cs.princeton.edu/14analysis/", "page_rank": 1.1700305e-09, "spam_rank": 95, "title": "Analysis of <em>Algorithms</em>", "snippet": "Instead, you can call get(i, k) <em>which</em> returns the kth bit of a[i] <em>or</em> you can call swap(i, j) <em>which</em> swaps the ith and jth elements of a[]. Design an O(N) <em>algorithm</em> to find the missing integer. For simplicity, assume N <em>is</em> a power of 2. Longest row of 0s.", "explanation": null, "document": "Programming Assignments\n1.4 \u00a0 Analysis of Algorithms\nAs people gain experience using computers, they use them to solve difficult problems or to process large amounts of data and are invariably led to questions like these:\nHow long will my program take?\nWhy does my program run out of memory?\nScientific method.\nThe very same approach that scientists use to understand the natural world is effective for studying the running time of programs:\nObserve some feature of the natural world, generally with precise measurements.\nHypothesize a model that is consistent with the observations.\nPredict events using the hypothesis.\nVerify the predictions by making further observations.\nValidate by repeating until the hypothesis and observations agree.\nThe experiments we design must be reproducible and the hypotheses that we formulate must be falsifiable.\nObservations.\nOur first challenge is to determine how to make quantitative measurements of the running time of our programs. Stopwatch.java is a data type that measures the elapsed running time of a program.\nThreeSum.java counts the number of triples in a file of N integers that sums to 0 (ignoring integer overflow). DoublingTest.java generates a sequence of random input arrays, doubling the array size at each step, and prints the running times of ThreeSum.count() for each input size.\nMathematical models.\nThe total running time of a program is determined by two primary factors: the cost of executing each statement and the frequency of execution of each statement.\nTilde approximations. We use tilde approximations, where we throw away low-order terms that complicate formulas. We write ~ f(N) to represent any function that when divided by f(N) approaches 1 as N grows. We write g(N) ~ f(N) to indicate that g(N) / f(N) approaches 1 as N grows.\nOrder-of-growth classifications. Most often, we work with tilde approximations of the form g(N) ~ a f(N) where f(N) = N^b log^c N and refer to f(N) as the The order of growth of g(N). We use just a few structural primitives (statements, conditionals, loops, nesting, and method calls) to implement algorithms, so very often the order of growth of the cost is one of just a few functions of the problem size N.\nCost model. We focus attention on properties of algorithms by articulating a cost model that defines the basic operations. For example, an appropriate cost model for the 3-sum problem is the number of times we access an array entry, for read or write.\nProperty. The order of growth of the running time of ThreeSum.java is N^3.\nProposition. The brute-force 3-sum algorithm uses ~ N^3 / 2 array accesses to compute the number of triples that sum to 0 among N numbers.\nDesigning faster algorithms.\nOne of the primary reasons to study the order of growth of a program is to help design a faster algorithm to solve the same problem. Using mergesort and binary search, we develop faster algorithms for the 2-sum and 3-sum problems.\n2-sum. The brute-force solution TwoSum.java takes time proportional to N^2. TwoSumFast.java solves the 2-sum problem in time proportional to N log N time.\n3-sum. ThreeSumFast.java solves the 3-sum problem in time proportional to N^2 log N time.\nCoping with dependence on inputs.\nFor many problems, the running time can vary widely depending on the input.\nInput models. We can carefully model the kind of input to be processed. This approach is challenging because the model may be unrealistic.\nWorst-case performance guarantees. Running time of a program is less than a certain bound (as a function of the input size), no matter what the input. Such a conservative approach might be appropriate for the software that runs a nuclear reactor or a pacemaker or the brakes in your car.\nRandomized algorithms. One way to provide a performance guarantee is to introduce randomness, e.g., quicksort and hashing. Every time you run the algorithm, it will take a different amount of time. These guarantees are not absolute, but the chance that they are invalid is less than the chance your computer will be struck by lightning. Thus, such guarantees are as useful in practice as worst-case guarantees.\nAmortized analysis. For many applications, the algorithm input might be not just data, but the sequence of operations performed by the client. Amortized analysis provides a worst-case performance guarantee on a sequence of operations.\nProposition. In the linked-list implementation of Bag, Stack, and Queue, all operations take constant time in the worst case.\nProposition. In the resizing-array implementation of Bag, Stack, and Queue, starting from an empty data structure, any sequence of N operations takes time proportional to N in the worst case (amortized constant time per operation).\nMemory usage.\nTo estimate how much memory our program uses, we can count up the number of variables and weight them by the number of bytes according to their type. For a typical 64-bit machine,\nPrimitive types. the following table gives the memory requirements for primitive types.\nObjects. To determine the memory usage of an object, we add the amount of memory used by each instance variable to the overhead associated with each object, typically 16 bytes. Moreover, the memory usage is typically padded to be a multiple of 8 bytes (on a 64-bit machine).\nReferences. A reference to an object typically is a memory address and thus uses 8 bytes of memory (on a 64-bit machine).\nLinked lists. A nested non-static (inner) class such as our Node class requires an extra 8 bytes of overhead (for a reference to the enclosing instance).\nArrays. Arrays in Java are implemented as objects, typically with extra overhead for the length. An array of primitive-type values typically requires 24 bytes of header information (16 bytes of object overhead, 4 bytes for the length, and 4 bytes of padding) plus the memory needed to store the values.\nStrings. A String of length N typically uses 40 bytes (for the String object) plus 24 + 2N bytes (for the array that contains the characters) for a total of 64 + 2N bytes.\nQ + A\nQ. How do I increasing the amount of memory and stack space that Java allocates?\nA. You can increase the amount of memory allotted to Java by executing with java -Xmx200m Hello where 200m means 200 megabytes. The default setting is typically 64MB. You can increase the amount of stack space allotted to Java by executing with java -Xss200k Hello where 200k means 200 kilobytes. The default setting is typically 128KB. It's possible to increase both the amount of memory and stack space by executing with executing with java -Xmx200m -Xss200k Hello.\nExercises\nGive the order of growth (as a function of N) of the running times of each of the following code fragments:\nint sum = 0;\nfor (int n = N; n > 0; n /= 2)\n   for (int i = 0; i < n; i++) \n      sum++;\nint sum = 0;\nfor (int i = 1; i < N; i *= 2)\n   for(int j = 0; j < i; j++)\n      sum++;\nint sum = 0;\nfor (int i = 1; i < N; i *= 2)\n   for (int j = 0; j < N; j++)\n      sum++;\nAnswer: linear (N + N/2 + N/4 + ...); linear (1 + 2 + 4 + 8 + ...); linearithmic (the outer loop loops lg N times).\nCreative Problems\n4-sum. Develop a brute-force solution FourSum.java to the 4-sum problem.\nLocal minimum of an array. Write a program that, given an array a[] of N distinct integers, finds a local minimum: an index i such that a[i] < a[i-1] and a[i] < a[i+1]. Your program should use ~2 lg N compares in the worst case.\nAnswer: Examine the middle value a[N/2] and its two neighbors a[N/2 - 1] and a[N/2 + 1]. If a[N/2] is a local minimum, stop; otherwise search in the half with the smaller neighbor.\nLocal minimum of a matrix. Given an N-by-N array a[] of N2 distinct integers, design an algorithm that runs in time proportional to N to find a local minimum: an pair of indices i and j such that a[i][j] < a[i+1][j], a[i][j] < a[i][j+1], a[i][j] < a[i-1][j], and a[i][j] < a[i][j-1].\nHint: find the minimum in row N/2, check neighbors p and q in column, if p or q is smaller then recur in that half.\nBitonic search. An array is bitonic if it is comprised of an increasing sequence of integers followed immediately by a decreasing sequence of integers. Write a program that, given a bitonic array of N distinct int values, determines whether a given integer is in the array. Your program should use ~ 3 lg N compares in the worst case.\nAnswer: Use a version of binary search, as in BitonicMax.java , to find the maximum (in ~ 1 lg N compares); then use binary search to search in each piece (in ~ 1 lg N compares per piece).\nBinary search with only addition and subtraction. [ Mihai Patrascu ] Write a program that, given an array of N distinct int values in ascending order, determines whether a given integer is in the array. You may use only additions and subtractions and a constant amount of extra memory. The running time of your program should be proportional to log N in the worst case.\nAnswer: Instead of searching based on powers of two (binary search), use Fibonacci numbers (which also grow exponentially). Maintain the current search range to be [i, i + F(k)] and keep F(k), F(k-1) in two variables. At each step compute F(k-2) via subtraction, check element i + F(k-2), and update the range to either [i, i + F(k-2)] or [i + F(k-2), i + F(k-2) + F(k-1)].\nBinary search for a fraction. Devise a method that uses a logarithmic number of queries of the form Is the number less than x? to find a rational number p / q such that 0 < p < q < N.\nHint: Two fractions with denominator less than N cannot differ by more than than 1/N^2.\nThrowing eggs from a building. Suppose that you have an N-story building and plenty of eggs. Suppose also that an egg is broken if it is thrown off floor F or higher, and unbroken otherwise. First, devise a strategy to determine the value of F such that the number of broken eggs is ~ lg N when using ~ lg N throws, then find a way to reduce the cost to ~ 2 lg F.\nHint: binary search; repeated doubling and binary search.\nThrowing two eggs from a building. Consider the previous question, but now suppose you only have two eggs, and your cost model is the number of throws. Devise a strategy to determine F such that the number of throws is at most 2 sqrt(\u221a N), then find a way to reduce the cost to ~c \u221a F.\nHot or cold. Your goal is the guess a secret integer between 1 and N. You repeatedly guess integers between 1 and N. After each guess you learn if it equals the secret integer (and the game stops); otherwise (starting with the second guess), you learn if the guess is hotter (closer to) or colder (farther from) the secret number than your previous guess. Design an algorithm that finds the secret number in ~ 2 lg N guesses. Then, design an algorithm that finds the secret number in ~ 1 lg N guesses.\nHint: use binary search for the first part. For the second part, first design an algorithm that solves the problem in ~1 lg N guesses assuming you are permitted to guess integers in the range -N to 2N.\nWeb Exercises\nLet f be a monotonically increasing function with f(0) < 0 and f(N) > 0. Find the smallest integer i such that f(i) > 0. Devise an algorithm that makes O(log N) calls to f().\nFloor and ceiling. Given a set of comparable elements, the ceiling of x is the smallest element in the set greater than or equal to x, and the floor is the largest element less than or equal to x. Suppose you have an array of N items in ascending order. Give an O(log N) algorithm to find the floor and ceiling of x.\nBinary search with duplicates. Modify binary search so that it always returns the smallest (largest) index of a key of an item matching the search key.\nRank with lg N two-way compares. Implement rank() so that it uses ~ 1 lg N two-way compares (instead of ~ 1 lg N 3-way compares).\nIdentity. Given an array a of N distinct integers (positive or negative) in ascending order. Devise an algorithm to find an index i such that a[i] = i if such an index exists. Hint: binary search.\nMajority. Given an array of N strings. An element is a majority if it appears more than N/2 times. Devise an algorithm to identify the majority if it exists. Your algorithm should run in linearithmic time.\nMajority. Repeat the previous exercise, but this time your algorithm should run in linear time, and only use a constant amount of extra space. Moreover, you may only compare elements for equality, not for lexicographic order.\nAnswer: if a and b are two elements and a != b, then remove both of them; majority still remains. Use N-1 compares to find candidate for majority; use N-1 comparisons to check if candidate really is a majority.\nSecond smallest. Give an algorithm to find the smallest and second smallest elements from a list of N items using the minimum number of comparisons. Answer: you can do it in ceil(N + lg(N) - 2) comparisons by building a tournament tree where each parent is the minimum of its two children. The minimum ends up at the root; the second minimum is on the path from the root to the minimum.\nFind a duplicate. Given an array of N elements in which each element is an integer between 1 and N, write an algorithm to determine if there are any duplicates. Your algorithm should run in linear time and use O(1) extra space. Hint: you may destroy the array.\nFind a duplicate. Given an array of N+1 elements in which each element is an integer between 1 and N, write an algorithm to find a duplicate. Your algorithm should run in linear time, use O(1) extra space, and may not modify the original array. Hint: pointer doubling.\nFinding common elements. Given two arrays of N 64-bit integers, design an algorithm to print out all elements that appear in both lists. The output should be in sorted order. Your algorithm should run in N log N. Hint: mergesort, mergesort, merge. Remark: not possible to do better than N log N in comparison based model.\nFinding common elements. Repeat the above exercise but assume the first array has M integers and the second has N integers where M is much less than N. Give an algorithm that runs in N log M time. Hint: sort and binary search.\nAnagrams. Design a O(N log N) algorithm to read in a list of words and print out all anagrams. For example, the strings \"comedian\" and \"demoniac\" are anagrams of each other. Assume there are N words and each word contains at most 20 letters. Designing a O(N^2) algorithms should not be too difficult, but getting it down to O(N log N) requires some cleverness.\nSearch in a sorted, rotated list. Given a sorted list of N integers that has been rotated an unknown number of positions, e.g., 15 36 1 7 12 13 14, design an O(log N) algorithm to determine if a given integer is in the list.\nFind the missing integer. An array a[] contains all of the integers from 0 to N, except 1. However, you cannot access an element with a single operation. Instead, you can call get(i, k) which returns the kth bit of a[i] or you can call swap(i, j) which swaps the ith and jth elements of a[]. Design an O(N) algorithm to find the missing integer. For simplicity, assume N is a power of 2.\nLongest row of 0s. Given an N-by-N matrix of 0s and 1s such that in each row no 0 comes before a 1, find the row with the most 0s in O(N) time.\nMonotone 2d array. Give an n-by-n array of elements such that each row is in ascending order and each column is in ascending order, devise an O(n) algorithm to determine if a given element x in the array. You may assume all elements in the n-by-n array are distinct.\nYou are in the middle of a road, but there is a duststorm obscuring your view and orientation. There is a shelter in only one direction, but you cannot see anything until you are right in front of it. Devise an algorithm that is guaranteed to find the shelter. Your goal is to minimize the amount you have to walk. Hint: some kind of doubling back-and-forth strategy.\nImprove the following code fragment by as big a constant factor as you can for large N. Profile it to determine where is the bottleneck.\ndouble[] a = new double[N];\nfor (int i = 0; i < N; i++)\n   for (int j = 0; j < N; j++)\n       a[j] += Math.exp(-0.5 * (Math.pow(b[i] - b[j], 2));\nInplace permutation. Write a program Permutation.java that includes functions that take an array and a permutation (or inverse permutation) and rearranges the elements in the array according to the permutation (or inverse permutation). Do it in-place: use only a constant amount of extra memory.\nSumOfThree. Given three sets A, B, and C of at most N integers each, determine whether this is a triple a in A, b in B, and c in C such that a + b + c = 0.\nAnswer: Sort B in increasing order; sort C in decreasing order; for each a in A, scan B and C for a pair that sums to -a (when the sum is too small, advance in B, when the sum is too large, advance in C).\nSumOfTwo. Given two sets A and B of at most N integers each, determine whether the sum of any two distinct integers in A equals an integer in B.\nContiguous sum. Given a list of real numbers and a target value V, find a contiguous block (of any length) whose sum is as close to K as possible.\nBrute force: compute the sum of each contiguous block by brute force. This takes O(N^3) time.\nPartial sums: compute all partial sums s[i] = a[0] + a[1] + ... + a[i] so that contiguous blocks have a sum of the form s[j] - s[i]. This takes O(N^2) time.\nSort and binary search: form the partial sums as above and then sort them in ascending order. For each i, binary search for the s[j] that is as close to s[i] as possible. This takes O(N log N) time.\nLinear equation with 3 variables. For some fixed linear equation in 3 variables (say with integer coefficients), given N numbers, do any 3 of them satisfy the equation? Design a quadratic algorithm for the problem. Hint: see quadratic algorithm for 3-sum.\nFind a majority item. Given a arbitrarily long sequence of items from standard input such that one item appears a strict majority of the time, identify the majority item. Use only a constant amount of memory.\nSolution. Maintain one integer counter and one variable to store the current champion item. Read in the next item and (i) if the item equals the champion item, increment the counter by one. (ii) else decrement the counter by one and if the counter reaches 0 replace the champion value with the current item. Upon termination, the champion value will be the majority item.\nMemory of arrays. MemoryOfArrays.java .\nMemory of strings and substrings. MemoryOfStrings.java .\nAnalysis of Euclid's algorithm. Prove that Euclid's algorithm takes at most time proportional to N, where N is the number of bits in the larger input.\nAnswer: First we assume that p > q. If not, then the first recursive call effectively swaps p and q. Now, we argue that p decreases by a factor of 2 after at most 2 recursive calls. To see this, there are two cases to consider. If q \u2264 p / 2, then the next recursive call will have p' = q \u2264 p / 2 so p decreases by at least a factor of 2 after only one recursive call. Otherwise, if p / 2 < q < p, then q' = p % q = p - q < p / 2 so p'' = q' < p / 2 and p will decrease by a factor of 2 or more after two iterations. Thus if p has N bits, then after at most 2N recursive calls, Euclid's algorithm will reach the base case. Therefore, the total number of steps is proportional to N.\nFind the duplicate. Given a sorted array of N+2 integers between 0 and N with exactly one duplicate, design a logarithmic time algorithm to find the duplicate.\nHint binary search.\nGiven an array a[] of N real numbers, design a linear-time algorithm to find the maximum value of a[j] - a[i] where j \u2265 i.\nSolution:\ndouble best = 0.0;\ndouble min = a[0];\nfor (int i = 0; i < N; i++) {\n    min  = Math.min(a[i], min);\n    best = Math.max(a[i] - min, best);\n}\nLast modified on April 03, 2012.\n"}, {"score": 972.7741, "uuid": "3829839c-c503-5e33-896f-84585d907142", "index": "cw12", "trec_id": "clueweb12-0810wb-44-35368", "target_hostname": "www.paultaylor.eu", "target_uri": "http://www.paultaylor.eu/algorithms/info.html", "page_rank": 1.2806213e-09, "spam_rank": 69, "title": "Introduction to <em>Algorithms</em>:\\Course Information Sheet", "snippet": ". * Binary search: there are at least three published <em>algorithms</em>, differing by \u00b11 in various important places. * Insertion <em>sort</em>: search and shift are used as subroutines in this quadratic <em>algorithm</em>. * Selection <em>sort</em>: another quadratic sorting <em>algorithm</em>. * <em>Merge</em> <em>sort</em>: this O(nlogn) <em>algorithm</em> <em>is</em> your first", "explanation": null, "document": "Lecture cancelled because I was at a\nImplement heapification, heap sort and priority queues.\nHash tables for fast access to dictionaries.\n26/27 March (Week 11)\nAdditional coursework options or finish regular coursework.\n2/3 April (Week 12)\nRecursion- and data- (class-) invariants (non-examinable)\nAdditional coursework options or finish regular coursework.\nRevision lecture.\nFriday 6 April (End of term): Coursework deadline\nThursday 9 May\nexam at 10am in Stratford Old Town Hall.\n9 Monitoring student work\nIf you are a student registered for this course, you should find in your personal filespace a directory (folder) called DCS127.\nAll of your work for this course must be done in this directory.\n(Think of it as the electronic equivalent of a school exercise book for a particular subject.)\nAs the various pieces of lab work for the course are set, template files will be copied into this directory for you to work on. They contain the structure of the methods and classes that you have to implement, instructions for writing and testing your code, and questions for you to answer about the behaviour of the code.\nYour answers to these questions, as well as your J AVA code, should be written in these files. Then all of the things that you are supposed to learn about each algorithm will be collected together in one place for you to learn and revise.\nThe directory also contains a short-cut ( pt) to the teaching materials for the course. It is also the only place in which you can do the run, trace and check commands that invoke the automatic tester .\nEverything that you do in this directory will be copied overnight to a place where your lecturer, teaching assistants, adviser and tutor can read it.\nOnly the contents of this directory will be copied and made available to these people. Other parts of your personal filespace (such as your email records) will not be copied. Other students will not be able to see the copy; in fact the access permissions on this directory itself are automatically reset so that only you, and not other students, can read its contents.\nOnly plain text files ( J AVA source files and notes that you make using xemacs in plain ASCII) will be copied. Compiled J AVA classes and files that appear to be mail or Microsoft Word format are not copied. Nor are directory trees called private, not_mine or RCS, and some other types of file 1 are also excluded.\nWhen files are copied, a \"receipt\" for them is recorded in the file COPY_LOG. You cannot delete or modify this file yourself.\nModification histories of the files are also maintained (in the place where the copies are made) using the Unix RCS tool. It may be possible to recover old versions of the file from this.\nThe purpose of this is to ensure that you do the lab work that is set each week.\nTo submit your end of term coursework , you simply leave it in this directory.\nThe TESTER will be run on the copied files, but they will be printed out and looked at individually before being marked.\nFiles that you put in this directory must be entirely your own work.\nSince some of the things that you put here will be marked as coursework, contributing to your final grade for this course and ultimately to your degree, they must be treated in the same way as exam scripts. The College examination regulations therefore apply.\nIf you wish to keep materials in this directory that are relevant to your study of this course, but are not your own work (for example, code that you have downloaded from the Web), you must put it in the subdirectory not_mine. It will then not be treated as coursework.\n10 Assessment of coursework\nThe assessment for this course consists of 55% for the May exam and 15% for each of three items of term-time work, two of these being tests.\nAll lab work is collected automatically from the directory DCS127 in your filespace, every night. All work for this course should be done in this directory, as a matter of routine. In particular, you \" submit\" coursework simply by leaving it there.\nThere are two alternative ways of gaining the third 15% of marks:\nOn the basis of your weekly coursework, or\nBy doing one of the options listed below in the last three weeks of term.\nYou will be awarded the better of the two marks. You do not need to tell me which you want to count.\nPlease note:\nTo gain marks for each week's work (approximately 1% of the final credit per week), it must be done on time, not at the end of term.\nNormally, the deadline is the end of the day of the lab, but it will be set and changed each week depending on how difficult the class as a whole has found the exercise. No extensions of deadlines will be given for colds or single absences (and you will only cause annoyance by asking for them), but of course allowance will be made for medical problems that last several weeks. The intention behind this method of assessment is to persuade you to do the work that is set in each week's lab.\nFormally, the assessment will be for the term's work collectively, not for individual items. Assessment will include a elements for programming style, etc., in which the items that have been submitted will be examined qualitatively, not quantitatively.\nDo not think that you can miss labs and make up the marks by submitting coursework at the end of term instead. Such work will be examined critically and suspiciously for correctness and evidence of copying. But lab work is there to help you learn, so it is rather unlikely that you will be able to do the (more difficult) end of term coursework well unless you have done the earlier stuff properly.\nThe optional topics are intended for the best students. Mediocre work will get far fewer marks than it did last year.\n11 The optional topics\nHash tables for creating, accessing and updating dictionaries.\nFinding a path between two nodes in a network was covered in 2000-1 for the first time.\nExternal sorting of a list of names (such as the US population) that fits on disk but not in RAM. You have to do your own reading on this option - there will be no lectures.\nHeap sort and priority queues were an option in 1998-9 and 1999-2000, but became a core part of the course (with structured lab work and exam question) in 2000-1.\nMinimax and alpha-beta pruning for playing games such as chess or draughts. This was an option in 1998-9 and 1999-2000, prompted by the Software Engineering projects about implementing games, but not very many students took it, so it was droppped in 2000-1.\nBlack box testing of an alleged sorting method was an option in 1999-2000, but only one student (Daniel Dor-Chay) took it. He discovered the \"sawtooth\" array that makes quicksort quadratic, but otherwise this option was not a very good idea.\nYou can get credit for the options listed above twice:\n15% for assessed lab-work. You have to implement the algorithm, and hand in your code on 6 April, along with some of the results of testing that the code works.\nApproximately 14% more for an optional essay in the exam.\nDo not write an essay for the coursework. Postpone further reading and experimentation (beyond what is necessary to get your program working) until after you have handed in your program.\n12 The essay question in the exam\nYou do not have to choose the same topic for the coursework and the exam, but it would obviously be more effective in gaining marks to do so. However, the coursework and exam test different aspects of the topic.\nFor the exam you are not expected to remember and reproduce 200 lines of code, but you will be expected to give the code for the core algorithms. Your essay should also summarise what you have learned about the algorithm from running and modifying your own program or my compiled model answer, my lectures, Richard Bornat's lecture notes, Weiss's book, and any other sources you can find.\nThe essay question was compulsory in May 1999, but in 2000 and 2001 students had to choose four questions out of eight, one of them being the essay; the 14% figure above is based on this.\nWriting an \"essay\" in an exam is not a piece of English composition (although I would like you to know how to spell \"pigeonhole\", and the difference between \"its\" and \"it's\"). At school you were probably told to write \"a beginning, a middle and an end\", but any \"introduction\" or \"conclusion\" you write for this essay is unlikely to (contain material which will) gain you any marks. An exam essay is like one of those TV game shows where you have to remember as many items as possible that have gone past on a conveyor belt (\"TV, teddy bear, toaster, ...\"). How many facts can you remember about this topic?\nThe exam rubric for the essay question [in May 2000] read as follows:\nWrite an essay about one of the [above] topics.\nYour answer should include a selection of the most important parts of the code: do not include J AVA declarations unless they illustrate some aspect of the algorithm. Explain the main points in proving correctness. State the main facts about complexity.\nYou are unlikely to gain any marks at all for this question unless you have implemented the algorithm in the lab sessions and thought carefully about its behaviour. Remember that this is an exam about algorithms, not about English composition, and that your answer will be marked according to the information that it contains about algorithms: wordy answers to questions like this gain very few marks.\nThe marking of both the coursework and essay question will be open-ended: the mark-scheme will give full marks for the basic issues, but generous bonus marks will be given for anything else you discover by experimentation with your program. Unfortunately, it is not within my power to give total marks above 100% for the course, or to give any prizes for supererogation.\n13 Warning about plagiarism\nIt is the College's policy to punish plagiarism severely.\nThis is part of your degree assessment, and is intended to be individual work.\nCopying from other students' work (with or without their consent) is plagiarism.\nIf you work in pairs or larger groups, each member of the group must submit identical work, which must state the names of all of the members of the group and the agreed proportions of the work that they contributed. If you have had the help of others to do the work you submit then you will of course get fewer marks, to compensate.\nTo copy code out from Richard Bornat's notes, Weiss's book or web pages, or from any other sources without identifying the source precisely is also plagiarism,\nIf you use a book, you must give the author, title, publisher and page numbers.\nIf you use the web you must give the full URL of the relevant page, not just the name of the site.\nChanging what you copy from books, web pages or other students counts as deliberate attempted deception. It is also insulting to expect someone (a lecturer or a colleague) to give individual attention to something that does not in fact have any original content.\nGiving precise references is a basic professional habit, and is useful. For example if you've filched someone else's code like this, and adapted it to your own needs, and then it doesn't work, you (or your colleagues) can go directly back to the original source.\nYou don't have to cite your sources in the exam (in my course).\nFootnotes:\n1 Emacs backup files ( file\u00a0\u223c and #file#); \"dot\" files ( .login); your \"receipt\" COPY_LOG; files whose names contain spaces, quotes or other strange characters; symbolic links; PostScript; PDF; executables; images ( picture.gif); archives ( tester.jar, folder.zip, folder.tar.gz).\nThis is www.PaulTaylor.EU/algorithms/info.html and it was derived from algorithms/info.tex which was last modified on 31 December 2007.\n"}, {"score": 966.416, "uuid": "d5eae22f-5eb7-5afd-83d8-920797204b5b", "index": "cw12", "trec_id": "clueweb12-1809wb-42-17819", "target_hostname": "jorgetavares.com", "target_uri": "http://jorgetavares.com/2012/04/22/zsort-portable-sorting-algorithms-in-common-lisp/?like=1&_wpnonce=eea138f48a&replytocom=276", "page_rank": 1.1700305e-09, "spam_rank": 67, "title": "zsort: portable sorting <em>algorithms</em> in Common Lisp \u00ab Jorge Tavares weblog", "snippet": "Right now the implemented <em>algorithms</em> are: insertion <em>sort</em>, <em>quicksort</em>, randomized <em>quicksort</em>, <em>merge</em> <em>sort</em>, heapsort and counting <em>sort</em>. The plan <em>is</em> to add more <em>algorithms</em>, for example, bucket <em>sort</em> and timsort.", "explanation": null, "document": "with 6 comments\nzsort is a library that I started working on as a simple hobby project. More or less around the same time I decided to check which algorithms the different Common Lisp implementations use . It is now part of Quicklisp so it can be easily used (thanks Zack !).\nThe main goal of zsort is to be a collection of portable sorting algorithms. If they can be fast, even better. Common lisp provides the sort and stable-sort functions but these can have different algorithms implemented according to each implementation, which can make an application unportable if you rely on a specific type of sorting. Also, the standard functions might not be the best for a certain situation and as such you might need a specialized sort. Even if for most situations the standard functions are more than enough, the zsort library could be a useful complement.\nRight now the implemented algorithms are: insertion sort , quicksort , randomized quicksort , merge sort , heapsort and counting sort . The plan is to add more algorithms, for example, bucket sort and timsort . However, the main thing on the todo list is adding the possibility of external sorting (to handle large amounts of data) and parallel versions of some sorting algorithms. I am considering using lparallel for this but I am still undecided.\nThere is still a lot of work to be done, but I think the library as it is can already be a little useful. And of course, all kind of suggestions and improvements are welcome!\nLike this:\nBe the first to like this post.\nWritten by Jorge Tavares\n"}, {"score": 964.5747, "uuid": "3ada2793-da12-5a78-9e52-0300f3091bd9", "index": "cw12", "trec_id": "clueweb12-1809wb-34-01629", "target_hostname": "jorgetavares.com", "target_uri": "http://jorgetavares.com/2012/04/22/zsort-portable-sorting-algorithms-in-common-lisp/?like=1&_wpnonce=eea138f48a", "page_rank": 1.1700305e-09, "spam_rank": 68, "title": "zsort: portable sorting <em>algorithms</em> in Common Lisp \u00ab Jorge Tavares weblog", "snippet": "Right now the implemented <em>algorithms</em> are: insertion <em>sort</em>, <em>quicksort</em>, randomized <em>quicksort</em>, <em>merge</em> <em>sort</em>, heapsort and counting <em>sort</em>. The plan <em>is</em> to add more <em>algorithms</em>, for example, bucket <em>sort</em> and timsort.", "explanation": null, "document": "with 6 comments\nzsort is a library that I started working on as a simple hobby project. More or less around the same time I decided to check which algorithms the different Common Lisp implementations use . It is now part of Quicklisp so it can be easily used (thanks Zack !).\nThe main goal of zsort is to be a collection of portable sorting algorithms. If they can be fast, even better. Common lisp provides the sort and stable-sort functions but these can have different algorithms implemented according to each implementation, which can make an application unportable if you rely on a specific type of sorting. Also, the standard functions might not be the best for a certain situation and as such you might need a specialized sort. Even if for most situations the standard functions are more than enough, the zsort library could be a useful complement.\nRight now the implemented algorithms are: insertion sort , quicksort , randomized quicksort , merge sort , heapsort and counting sort . The plan is to add more algorithms, for example, bucket sort and timsort . However, the main thing on the todo list is adding the possibility of external sorting (to handle large amounts of data) and parallel versions of some sorting algorithms. I am considering using lparallel for this but I am still undecided.\nThere is still a lot of work to be done, but I think the library as it is can already be a little useful. And of course, all kind of suggestions and improvements are welcome!\nLike this:\nBe the first to like this post.\nWritten by Jorge Tavares\n"}, {"score": 961.7629, "uuid": "892f7412-ccc4-501b-b5ae-789a9ff099a9", "index": "cw12", "trec_id": "clueweb12-1808wb-99-24349", "target_hostname": "jorgetavares.com", "target_uri": "http://jorgetavares.com/2012/04/22/zsort-portable-sorting-algorithms-in-common-lisp/", "page_rank": 2.0451723e-09, "spam_rank": 66, "title": "zsort: portable sorting <em>algorithms</em> in Common Lisp \u00ab Jorge Tavares weblog", "snippet": "Right now the implemented <em>algorithms</em> are: insertion <em>sort</em>, <em>quicksort</em>, randomized <em>quicksort</em>, <em>merge</em> <em>sort</em>, heapsort and counting <em>sort</em>. The plan <em>is</em> to add more <em>algorithms</em>, for example, bucket <em>sort</em> and timsort.", "explanation": null, "document": "with 6 comments\nzsort is a library that I started working on as a simple hobby project. More or less around the same time I decided to check which algorithms the different Common Lisp implementations use . It is now part of Quicklisp so it can be easily used (thanks Zack !).\nThe main goal of zsort is to be a collection of portable sorting algorithms. If they can be fast, even better. Common lisp provides the sort and stable-sort functions but these can have different algorithms implemented according to each implementation, which can make an application unportable if you rely on a specific type of sorting. Also, the standard functions might not be the best for a certain situation and as such you might need a specialized sort. Even if for most situations the standard functions are more than enough, the zsort library could be a useful complement.\nRight now the implemented algorithms are: insertion sort , quicksort , randomized quicksort , merge sort , heapsort and counting sort . The plan is to add more algorithms, for example, bucket sort and timsort . However, the main thing on the todo list is adding the possibility of external sorting (to handle large amounts of data) and parallel versions of some sorting algorithms. I am considering using lparallel for this but I am still undecided.\nThere is still a lot of work to be done, but I think the library as it is can already be a little useful. And of course, all kind of suggestions and improvements are welcome!\nLike this:\nBe the first to like this post.\nWritten by Jorge Tavares\n"}, {"score": 959.0733, "uuid": "f03f6f6e-75be-5859-80f0-921031f4f0bb", "index": "cw12", "trec_id": "clueweb12-0506wb-57-03857", "target_hostname": "www.math.grin.edu", "target_uri": "http://www.math.grin.edu/~rebelsky/Courses/CS152/99F/Outlines/outline.26.html", "page_rank": 1.2112296e-09, "spam_rank": 94, "title": "CSC-152 99F : Class 26: More Efficient Sorting <em>Algorithms</em>", "snippet": "One of the most straightforward <em>is</em> <em>merge</em> <em>sort</em>. * In <em>merge</em> <em>sort</em>, you split the list, array, collection <em>or</em> ... into two parts, <em>sort</em> each part, and then <em>merge</em> them together. * Unlike the previous <em>algorithms</em>, <em>merge</em> <em>sort</em> requires extra space for the sorted arrays <em>or</em> subarrays. * We&#x27;ll write this as a non-inplace", "explanation": null, "document": "Sorting with Divide and Conquer\nIn the previous class, we identified a number of interesting sorting algorithms which took O(n2) time.\nCan we do better? Well, sometimes using divide-and-conquer helps speed up algorithms (in our experience from O(n) to O(log2n)).\nWe'll look at two different ways of ``splitting'' the array.\nMerge Sort\nWe can develop a number of sorting techniques based on the divide and conquer technique. One of the most straightforward is merge sort.\nIn merge sort, you split the list, array, collection or ... into two parts, sort each part, and then merge them together.\nUnlike the previous algorithms, merge sort requires extra space for the sorted arrays or subarrays.\nWe'll write this as a non-inplace routine which returns a new, sorted array, rather than sorting the existing array.\nIn approximate Java code,\nimport SimpleOutput; /**\n * A collection of techniques for sorting an input array.\n *\n * @author Samuel A. Rebelsky\n * @version 1.0 of October 1999\n */ public class MergeSorter\n{ // +--------+-------------------------------------------------- // | Fields | // +--------+ /** The current indent level.  Used when logging steps. */ String indent = \"\"; // +----------------+------------------------------------------ // | Public Methods | // +----------------+ /**\n   * Sort an array, creating a new sorted version of the array.\n   * If the SimpleOutput object is non-null, prints a simple log\n   * of what's happening.\n   * Pre: The elements in the array can be compared to each other.\n   * Pre: There is sufficient memory to complete the creation of the\n   *   new array (and the other steps of the algorithm).\n   * Post: Returns a sorted version of the array (where sorted is\n   *   defined carefully elsewhere).\n   * Post: Does not affect the original array.\n   */ public Object[] sort(Object[] stuff, \n                       Comparator compare, \n                       SimpleOutput observer) throws IncomparableException\n  { return mergeSort(stuff, 0, stuff.length-1, compare, observer);\n  } // sort(Object[]) // +----------------+------------------------------------------ // | Helper Methods | // +----------------+ /**\n   * Sort part of an array, creating a new sorted version of the\n   * part of the array.\n   * Pre: The elements in the array can be compared to each other.\n   * Pre: There is sufficient memory to complete the creation of the\n   *   new array (and the other steps of the algorithm).\n   * Post: Returns a sorted version of the array (where sorted is\n   *   defined carefully elsewhere).\n   * Post: Does not affect the original array.\n   */ protected Object[] mergeSort(Object[] stuff, int lb, int ub, \n                               Comparator compare,\n                               SimpleOutput observer) throws IncomparableException\n  {\n    Object[] sorted; // The sorted version int middle; // Index of middle element // Print some basic information. if (observer != null) {\n      observer.print(indent + \"Sorting: \");\n      printSubArray(stuff, lb, ub, observer);\n      indent = indent + \" \";\n    } // Base case: vector of size 0 or 1.  Make a fresh copy so that // it's safe to modify (and is the appropriate size. if (ub <= lb) {\n      sorted = copySubArray(stuff, lb, ub);\n    } // base case // Recursive case: split and merge else { // Find the middle of the subarray. middle = (lb + ub) / 2; // Sort the two halves. Object[] left = mergeSort(stuff, lb, middle, compare, observer);\n      Object[] right = mergeSort(stuff, middle+1, ub, compare, observer);\n      sorted = merge(left, right, compare);\n    } // recursive case // Print information, if appropriate if (observer != null) {\n      indent = indent.substring(2);\n      observer.print(indent + \"Sorted: \");\n      printSubArray(sorted, 0, sorted.length-1, observer);\n    } // That's it. return sorted;\n  } // mergeSort(Object[], int, int, Comparator) /**\n   * Merge two sorted arrays into a new single sorted array.\n   * Pre: Both vectors are sorted.\n   * Pre: Elements in both vectors may be compared to each other.\n   * Pre: There is sufficient memory to allocate the new array.\n   * Post: The returned array is sorted, and contains all the\n   *   elements of the two arrays (no more, no less).\n   * Post: The two arguments are not changed\n   */ public Object[] merge(Object[] left, Object[] right, Comparator compare) throws IncomparableException\n  { // Create a new array of the appropriate size. Object[] result = new Object[left.length + right.length]; // Create indices into the three arrays. int leftIndex=0; // Index into left array. int rightIndex=0; // Index into right array. int index=0; // Index into result array. // As long both vectors have elements, copy the smaller one. while ((leftIndex < left.length) && (rightIndex < right.length)) { if(compare.lessThan(left[leftIndex],right[rightIndex])) {\n        result[index++] = left[leftIndex++];\n      } // first element in left subvector is smaller else {\n        result[index++] = right[rightIndex++];\n      } // first element in right subvector is smaller\n    } // while both vectors have elements // Copy any remaining parts of each vector. while(leftIndex < left.length) {\n      result[index++] = left[leftIndex++];\n    } // while the left vector has elements while(rightIndex < right.length) {\n      result[index++] = right[rightIndex++];\n    } // while the right vector has elements // That's it return result;\n  } // merge /**\n   * Copy a subarray (so that we can return it without affecting it).\n   * Pre: 0 <= lb <= ub < stuff.length\n   * Post: Does not affect stuff.\n   * Post: Returns a new array containing only stuff[lb] .. stuff[ub].\n   */ protected Object[] copySubArray(Object[] stuff, int lb, int ub) { // Create the new array. Object[] result = new Object[ub-lb+1]; for (int i = lb; i <= ub; i++) {\n      result[i-lb] = stuff[i];\n    } return result;\n  } // copySubArray /**\n   * Print a subarray.\n   * Pre: 0 <= lb <= ub < stuff.length\n   * Post: Does not affect stuff.\n   */ protected void printSubArray(Object[] stuff, int lb, int ub,\n                               SimpleOutput out) { // Print all but the last element followed by a comma for (int i = lb; i < ub; ++i) {\n      out.print(stuff[i].toString() + \",\");\n    } // Print the last element out.println(stuff[ub]);\n  } // printSubArray\n} // MergeSorter\nHere's the corresponding test class.\nimport MergeSorter; import SimpleOutput; import StringComparator; /**\n * A simple test of selection sort.\n *\n * @author Samuel A. Rebelsky\n * @version 1.0 of September 1999\n */ public class TestMergeSorter { public static void main(String[] args) throws Exception\n  {\n    SimpleOutput out = new SimpleOutput();\n    MergeSorter sorter = new MergeSorter();\n    Object[] sorted = sorter.sort(args, new StringComparator(), out); for (int i = 0; i < sorted.length; ++i) {\n      out.println(i + \": \" + sorted[i]);\n    } // for\n  } // main(String[])\n} // class TestMergeSorter\nWhat is the running time?\nWe can use recurrence relations:\nLet f(n) be the running time of merge sort on input of size n.\nf(1) = 1\nLet's run this for a few steps\nf(n)\n"}, {"score": 957.00824, "uuid": "e4ff210c-7597-5914-964e-01227945ed5f", "index": "cw12", "trec_id": "clueweb12-0807wb-82-26346", "target_hostname": "cz.php.net", "target_uri": "http://cz.php.net/manual/en/function.sort.php", "page_rank": 1.3090513e-09, "spam_rank": 81, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 955.1535, "uuid": "739649d9-0ca6-5093-971e-9451ac29c487", "index": "cw12", "trec_id": "clueweb12-0001wb-67-32870", "target_hostname": "fr.php.net", "target_uri": "http://fr.php.net/manual/en/function.sort.php", "page_rank": 1.2505954e-09, "spam_rank": 70, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 03:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 11:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 11:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 09:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 954.9155, "uuid": "b7bed6cc-ffb1-5b92-a9fa-5bc5d0d9f44e", "index": "cw12", "trec_id": "clueweb12-0308wb-62-29805", "target_hostname": "ar2.php.net", "target_uri": "http://ar2.php.net/manual/en/function.sort.php", "page_rank": 1.2494632e-09, "spam_rank": 70, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 11:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 06:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 06:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 04:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}], [{"score": 954.87726, "uuid": "dd90679e-1164-5a39-9bbb-c6a179fe6d37", "index": "cw12", "trec_id": "clueweb12-0012wb-46-20424", "target_hostname": "php.he.net", "target_uri": "http://php.he.net/manual/en/function.sort.php", "page_rank": 1.3064952e-09, "spam_rank": 70, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 954.86414, "uuid": "b7efff48-12ee-5331-9e53-d9fee87c7dd5", "index": "cw12", "trec_id": "clueweb12-0807wb-27-34024", "target_hostname": "th.php.net", "target_uri": "http://th.php.net/manual/en/function.sort.php", "page_rank": 1.337135e-09, "spam_rank": 78, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 09:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 21-Oct-2011 04:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 04:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 02:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 954.6303, "uuid": "fa693819-81d9-5c95-96d5-80b852608518", "index": "cw12", "trec_id": "clueweb12-0105wb-02-23015", "target_hostname": "mx2.php.net", "target_uri": "http://mx2.php.net/manual/ro/function.sort.php", "page_rank": 1.2183315e-09, "spam_rank": 67, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 954.6303, "uuid": "da92d570-1c26-5dd2-9a51-e2109ddb3367", "index": "cw12", "trec_id": "clueweb12-0308wb-65-11317", "target_hostname": "de2.php.net", "target_uri": "http://de2.php.net/manual/ro/function.sort.php", "page_rank": 1.2537192e-09, "spam_rank": 70, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 954.1778, "uuid": "f7633403-9858-567e-80a6-d68ae72ca762", "index": "cw12", "trec_id": "clueweb12-0007wb-57-27684", "target_hostname": "de2.php.net", "target_uri": "http://de2.php.net/manual/en/function.sort.php", "page_rank": 1.2719348e-09, "spam_rank": 69, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 954.4159, "uuid": "4e1b6882-38ee-50f1-966a-d5b4df3547bd", "index": "cw12", "trec_id": "clueweb12-0308wb-82-08533", "target_hostname": "docs.php.net", "target_uri": "http://docs.php.net/manual/en/function.sort.php", "page_rank": 1.2167357e-09, "spam_rank": 73, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 954.2648, "uuid": "816eba44-457d-5640-be67-a1381af86f5f", "index": "cw12", "trec_id": "clueweb12-0300wb-43-08984", "target_hostname": "www.php.net", "target_uri": "http://www.php.net/manual/en/function.sort.php", "page_rank": 1.7960776e-09, "spam_rank": 67, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 954.259, "uuid": "49b9e5f5-01d1-5c4f-8143-531a02c5c5b3", "index": "cw12", "trec_id": "clueweb12-0003wb-72-16546", "target_hostname": "de3.php.net", "target_uri": "http://de3.php.net/manual/en/function.sort.php", "page_rank": 1.2924624e-09, "spam_rank": 69, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 954.1975, "uuid": "d5001557-1ed5-5579-a9f9-fedeec1b2967", "index": "cw12", "trec_id": "clueweb12-0011wb-51-08351", "target_hostname": "uk3.php.net", "target_uri": "http://uk3.php.net/manual/en/function.sort.php", "page_rank": 1.3405712e-09, "spam_rank": 67, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 10:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 10:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 08:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 954.1778, "uuid": "ed67d325-9075-5616-b180-fd023c674453", "index": "cw12", "trec_id": "clueweb12-0306wb-49-03260", "target_hostname": "ca3.php.net", "target_uri": "http://ca3.php.net/manual/en/function.sort.php", "page_rank": 1.3881795e-09, "spam_rank": 70, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 09:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 05:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 05:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 03:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}], [{"score": 954.16034, "uuid": "96528198-36fb-56e8-8073-883404bfaaf5", "index": "cw12", "trec_id": "clueweb12-0009wb-02-34247", "target_hostname": "php.med.harvard.edu", "target_uri": "http://php.med.harvard.edu/manual/en/function.sort.php", "page_rank": 1.2896697e-09, "spam_rank": 68, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 09:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 05:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 05:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 03:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 954.0228, "uuid": "20cb8887-d4b8-5761-a728-7a3a92a0f002", "index": "cw12", "trec_id": "clueweb12-0310wb-99-20440", "target_hostname": "php.ca", "target_uri": "http://php.ca/manual/en/function.sort.php", "page_rank": 1.2535487e-09, "spam_rank": 71, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 09:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 05:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 05:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 03:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 954.0097, "uuid": "43aa84f1-7744-518f-9a9c-6f5cc4ce13a1", "index": "cw12", "trec_id": "clueweb12-0307wb-08-01183", "target_hostname": "be.php.net", "target_uri": "http://be.php.net/manual/en/function.sort.php", "page_rank": 1.2584764e-09, "spam_rank": 66, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 953.54877, "uuid": "cb0d7a32-a725-5688-97c8-297b39b6735d", "index": "cw12", "trec_id": "clueweb12-0600tw-05-11486", "target_hostname": "us3.php.net", "target_uri": "http://us3.php.net/manual/en/function.sort.php", "page_rank": 1.3357476e-09, "spam_rank": 76, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 07:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 03:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 03:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 01:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 953.15204, "uuid": "6a589dec-3eca-5e6c-826a-d1123039a1e8", "index": "cw12", "trec_id": "clueweb12-0717wb-15-12468", "target_hostname": "my.php.net", "target_uri": "http://my.php.net/manual/en/function.sort.php", "page_rank": 1.3447933e-09, "spam_rank": 81, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 952.70166, "uuid": "30b69b6f-da19-544e-b753-97413cfe148e", "index": "cw12", "trec_id": "clueweb12-0805wb-70-21594", "target_hostname": "be2.php.net", "target_uri": "http://be2.php.net/manual/en/function.sort.php", "page_rank": 1.3735914e-09, "spam_rank": 77, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 03:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 11:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 11:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 09:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 952.408, "uuid": "5ae752e9-db57-5042-a345-928c14a8dd55", "index": "cw12", "trec_id": "clueweb12-0004wb-41-23103", "target_hostname": "cn.php.net", "target_uri": "http://cn.php.net/manual/en/function.sort.php", "page_rank": 1.3459726e-09, "spam_rank": 70, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 952.07886, "uuid": "ebd6d7e6-cc97-5bd4-b9d5-cb59d4d44d3d", "index": "cw12", "trec_id": "clueweb12-0010wb-68-28363", "target_hostname": "php.linux.hr", "target_uri": "http://php.linux.hr/manual/en/function.sort.php", "page_rank": 1.2566793e-09, "spam_rank": 69, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 951.7534, "uuid": "8e4aa0f5-6ee2-55f0-8f00-a746e3a5faa0", "index": "cw12", "trec_id": "clueweb12-0008wb-80-20510", "target_hostname": "it.php.net", "target_uri": "http://it.php.net/manual/en/function.sort.php", "page_rank": 1.3436086e-09, "spam_rank": 68, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 951.7534, "uuid": "bab141d4-eebf-554d-8ca7-5aebe9566e24", "index": "cw12", "trec_id": "clueweb12-0308wb-27-21918", "target_hostname": "ca2.php.net", "target_uri": "http://ca2.php.net/manual/en/function.sort.php", "page_rank": 1.2604643e-09, "spam_rank": 67, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 09:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 05:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 05:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 03:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}], [{"score": 951.66626, "uuid": "c5913414-6609-5186-85ea-88f034372cc7", "index": "cw12", "trec_id": "clueweb12-0009wb-74-20907", "target_hostname": "mx2.php.net", "target_uri": "http://mx2.php.net/manual/en/function.sort.php", "page_rank": 1.349192e-09, "spam_rank": 68, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 951.6568, "uuid": "749dc67a-da7f-546b-b61c-ccb6adfa15b1", "index": "cw12", "trec_id": "clueweb12-0400wb-79-16742", "target_hostname": "jp2.php.net", "target_uri": "http://jp2.php.net/manual/ro/function.sort.php", "page_rank": 1.2596455e-09, "spam_rank": 67, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 11:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 21-Oct-2011 06:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 06:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 04:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 949.0717, "uuid": "3d3bd972-555c-5377-bdd6-44da11317f82", "index": "cw12", "trec_id": "clueweb12-0011wb-65-21605", "target_hostname": "se.php.net", "target_uri": "http://se.php.net/manual/en/function.sort.php", "page_rank": 1.2734517e-09, "spam_rank": 68, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 03:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 11:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 11:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 09:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 938.25806, "uuid": "41b90c04-c1a0-593a-860c-52bf53a043e6", "index": "cw12", "trec_id": "clueweb12-1204wb-18-31905", "target_hostname": "cn2.php.net", "target_uri": "http://cn2.php.net/manual/en/function.sort.php", "page_rank": 1.2532829e-09, "spam_rank": 68, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 934.5363, "uuid": "85109d19-6835-5237-86c1-63f1d9c9cc87", "index": "cw12", "trec_id": "clueweb12-1214wb-92-35252", "target_hostname": "pt.php.net", "target_uri": "http://pt.php.net/manual/en/function.sort.php", "page_rank": 1.3122846e-09, "spam_rank": 68, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 932.423, "uuid": "9e9a284f-e70e-5c17-9ef4-1580fad07790", "index": "cw12", "trec_id": "clueweb12-1203wb-38-05048", "target_hostname": "ca.php.net", "target_uri": "http://ca.php.net/manual/en/function.sort.php", "page_rank": 1.3636321e-09, "spam_rank": 67, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 928.14233, "uuid": "31a67d9c-3f28-5ec1-868d-9a5a88b6a5d6", "index": "cw12", "trec_id": "clueweb12-1202wb-44-25931", "target_hostname": "ca.php.net", "target_uri": "http://ca.php.net/manual/fa/function.sort.php", "page_rank": 1.3435911e-09, "spam_rank": 67, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 932.3385, "uuid": "a238e649-ea71-5f9e-83f8-68a6dc97f200", "index": "cw12", "trec_id": "clueweb12-1210wb-33-02373", "target_hostname": "il2.php.net", "target_uri": "http://il2.php.net/manual/en/function.sort.php", "page_rank": 1.3437573e-09, "spam_rank": 69, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 02:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 09:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 09:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 07:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 929.9454, "uuid": "f03ea4f1-63f7-572d-a098-b4fa50f157bd", "index": "cw12", "trec_id": "clueweb12-1301wb-53-26025", "target_hostname": "us3.php.net", "target_uri": "http://us3.php.net/sort/", "page_rank": 1.1700305e-09, "spam_rank": 66, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 07:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 03:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 03:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 01:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}, {"score": 926.7276, "uuid": "8068c791-5fe3-56b3-8b5d-3b4c97d2367a", "index": "cw12", "trec_id": "clueweb12-0306wb-47-10306", "target_hostname": "br.php.net", "target_uri": "http://br.php.net/manual/fa/function.sort.php", "page_rank": 1.3219859e-09, "spam_rank": 74, "title": "PHP: <em>sort</em> - Manual", "snippet": "here <em>is</em> little script <em>which</em> will <em>merge</em> arrays, remove duplicates and <em>sort</em> it by alphabetical order: &#x2F;&#x2F; $<em>sort</em> used as variable function--can be natcasesort, for example It appears to me that there are only two <em>algorithms</em> being proposed here (several times each): 1) copy into temp, pass temp to <em>sort</em> function", "explanation": null, "document": "Walter Tross 16-Dec-2011 12:38\nunless you specify the second argument, \"regular\" comparisons will be used. I quote from the page on comparison operators:\n\"If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.\"\nWhat this means is that \"10\" < \"1a\", and \"1a\" < \"2\", but \"10\" > \"2\". In other words, regular PHP string comparisons are not transitive.\nThis implies that the output of sort() can in rare cases depend on the order of the input array:\n<?php\necho \"{$a[0]} {$a[1]} {$a[2]}\";\nsort($a);\necho \" => {$a[0]} {$a[1]} {$a[2]}\\n\";\n// on PHP 5.2.6:\necho_sorted(array( \"10\", \"1a\", \"2\")); // => 10 1a 2\necho_sorted(array( \"10\", \"2\", \"1a\")); // => 1a 2 10\necho_sorted(array( \"1a\", \"10\", \"2\")); // => 2 10 1a\necho_sorted(array( \"1a\", \"2\", \"10\")); // => 1a 2 10\necho_sorted(array( \"2\", \"10\", \"1a\")); // => 2 10 1a\necho_sorted(array( \"2\", \"1a\", \"10\")); // => 10 1a 2\n?>\najanata at gmail dot com 20-Oct-2011 07:13\nThis took me longer than it should have to figure out, but if you want the behavior of sort($array, SORT_STRING) (that is, re-indexing the array unlike natcasesort) in a case-insensitive manner, it is a simple matter of doing usort($array, strcasecmp).\nholdoffhunger at gmail dot com 18-Jul-2011 07:14\nI have discovered an interesting trick with the sort function.\u00a0 Many coders are using it to sort options for a user, because a computer will only need sorted data in limited cases.\u00a0 However, every coder wants their sorted data to be usable.\u00a0 It's very common to find any sorted data online, and to find options, \"Sort by Date,\" \"Sort Alphabetically,\" etc., etc., whether you're looking at an online catalogue or some bulletin board.\nThe trick is to use a combination of html commenting and str_pad's to do definitive \"sort by\" functions.\u00a0 Let me demonstrate with an example: consider two arrays, Sorted_By_Date and Sorted_By_Name.\u00a0 You want every element to be \"<option value=something>Option Name</option\", so that you can do a simple foreach statement that prints every option value.\u00a0 But, sorting these arrays will technically sort just sort whatever you put into \"value\".\u00a0 So, here's a neat trick.\u00a0 Make every entry \"<!-- Sort_Value --><option value=something>Option Name</option.\"\u00a0 That way, when you sort, it'll have to sort technically by whatever is the Sort_Value.\nThe greatness of this trick is that you can make as many \"sort this data by X\" fields as you want!\u00a0 Just create multiple arrays, each with the same option values and names, but with a different value put in \"Sort_Value\", which could be a date, a name, a price, etc..\u00a0 With numbers, you may need to use the str_pad function, to push all of the zeros out to the left, for a proper sort.\u00a0 For instance, in my code, that looks like...\n<?php\n$number_of_versions_sort_value = str_pad($number_of_versions, 20, \"0\", STR_PAD_LEFT);\n?>\nalex dot hristov dot 88 at gmail dot com 16-Jun-2011 05:26\nAs some people have mentioned before sorting a multidimentional array can be a bit tricky. it took me quite a while to get it going but it works as a charm:\n<?php\n//$order has to be either asc or desc\nfunction sortmulti ($array, $index, $order, $natsort=FALSE, $case_sensitive=FALSE) {\nif(is_array($array) && count($array)>0) {\n$temp[$key]=$array[$key][$index];\nif(!$natsort) {\n"}], [{"score": 901.5074, "uuid": "103d9125-b663-5a8a-b3df-882fe28ba0f9", "index": "cw12", "trec_id": "clueweb12-1312wb-61-04251", "target_hostname": "www.saylor.org", "target_uri": "http://www.saylor.org/courses/cs303/", "page_rank": 1.186733e-09, "spam_rank": 87, "title": "CS303: <em>Algorithms</em> \u00ab The Saylor Foundation", "snippet": "Some of <em>algorithms</em> that we will study include Quick <em>Sort</em>, Insertion <em>Sort</em>, Bubble <em>Sort</em>, and <em>Merge</em> <em>Sort</em>. o Lecture: YouTube: Knight School (2009)\u2019s \u201cLec 3: Algorithmic Thinking\u201d and Google CEO\u2019s interview of 2008 Presidential Candidate Barack Obama in \u201cBarack Obama \u2013 Computer Science Question\u201d Links: YouTube", "explanation": null, "document": "Mp4\nInstructions: Please click the link above, and view the video fully to gain an understanding of the basics of the algorithm. \u00a0You can skip the first 17 minutes of the video as they talk about MIT class related logistics for the course.\nTerms of use: \u00a0Erik\u00a0Demaine and Charles\u00a0Leiserson\u00a0 6.046J/18.410J Introduction to Algorithms, Fall 2005. (Massachusetts Institute of Technology: MIT OpenCourseWare), http://ocw.mit.edu (Accessed September 21, 2011). License: Creative Commons BY-NC-SA 3.0. Original version can be seen here .\nSee a broken link? Please let us know!\n1.2 Introduction to Framework for Algorithm Analysis\nLecture: IIT Bombay Course on Introduction to Algorithms: Dr. Abhiram Ranade\u2019s \u201cLec 2: Framework for Algorithm Analysis\u201d\nLink: IIT Bombay Course on Introduction to Algorithms: Dr. Abhiram Ranade\u2019s \u201c Lecture 2: Framework for Algorithm Analysis \u201d (YouTube)\nInstructions: Please click the link above, and view the video in its entirety (56:22 minutes) to gain an understanding of the basics of the algorithm analysis and associated framework.\nTerms of Use: Please respect the copyright and terms of use displayed on the webpage above.\nSee a broken link? Please let us know!\n1.3 The Importance of Algorithms\nLink: Topcoder: Ibackstrom\u2019s \u201c Importance of Algorithms \u201d (HTML)\nInstructions: Please read the \u201cImportance of Algorithms\u201d webpage in its entirety for an overview of importance of algorithms as well as a listing of some of the key algorithm areas.\nTerms of Use: Please respect the copyright and terms of use displayed on the webpage above.\nSee a broken link? Please let us know!\n1.4 Control Instructions\nLink: Wikibooks: \u201c Algorithms/Introduction \u201d (PDF)\nInstructions: Please read all of the sections on this webpage to get an introduction to algorithms.\nTerms of Use: The article above is released under a Creative Commons Attribution-Share-Alike License 3.0 (HTML).\u00a0 You can find the original Wikibooks version of this article here (HTML).\nSee a broken link? Please let us know!\nUnit 2: Introduction to Analysis of Algorithms\nIn this unit, we explore how we can express an algorithm\u2019s efficiency as a function of its input size. \u00a0The order of growth of running time of an algorithm gives a simple characterization of algorithm\u2019s efficiency and allows us to relate performance of alternative algorithms. \u00a0Asymptotic analysis is based on the idea that as the problem size grows, the complexity will eventually settle down to a simple proportionality to some known function. \u00a0This idea is incorporated in the ``Big Oh,'' ``Big Omega,'' and ``Big Theta'' notations for asymptotic performance. \u00a0These notations are useful for expressing the complexity of an algorithm without getting lost in unnecessary detail.\nUnit 2 Learning Outcomes show close\n2.1 Introduction to Algorithms\nLecture: YouTube: MIT Course on Introduction to Algorithms: Dr. Erik Demaine\u2019s \u201cLec 2: Introduction to Algorithms\u201d\nLink: YouTube: MIT Course on Introduction to Algorithms: Dr. Erik Demaine\u2019s \u201c Lec 2: Introduction to Algorithms \u201d (YouTube)\nInstructions: Please click the link above, and view the video in its entirety (about 1 hour and 10 minutes) to gain an understanding of the basics of the algorithm. \u00a0Please note that this lecture also covers the topic outlined in subunit 2.4 of this unit.\nTerms of use: \u00a0Erik\u00a0Demaine and Charles\u00a0Leiserson\u00a0 6.046J/18.410J Introduction to Algorithms, Fall 2005. (Massachusetts Institute of Technology: MIT OpenCourseWare), http://ocw.mit.edu (Accessed September 21, 2011). License: Creative Commons BY-NC-SA 3.0. Original version can be seen here .\nSee a broken link? Please let us know!\n2.2 Asymptotic Analysis\nLecture: YouTube: University of California, Berkeley\u2019s Course on Data Structures: Dr. Jonathan Shewchuk\u2019s \u201cAsymptotic Analysis\u201d\nLink: YouTube: University of California, Berkeley\u2019s Course on Data Structures: Dr.\u00a0 Jonathan Shewchuk\u2019s \u201c Asymptotic Analysis \u201d (YouTube)\nInstructions: Please click the link above, and view the video in its entirety (about 49 minutes) to gain an understanding of the ideas behind the use of asymptotic analysis in algorithms.\nTerms of Use: Please respect the copyright and terms of use displayed on the webpage above.\nSee a broken link? Please let us know!\n2.3 Introduction to Analysis of Algorithms\nReading: University of California, Berkeley: S. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\u2019s Algorithms: \u201cChapter 0: Prologue\u201d\nLink: University of California, Berkeley: S. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\u2019s Algorithms: \u201c Chapter 0: Prologue \u201d (PDF)\nInstructions: Please click on the hyperlink titled \u201cPrologue\u201d to download the PDF filed of Chapter 0 of S. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\u2019s book, Algorithms.\u00a0 Please read the entire text (9 pages).\nTerms of Use: Please respect the copyright and terms of use displayed on the webpage above.\nSee a broken link? Please let us know!\n2.4 Master Theorem\nLink: Wikipedia: \u201c Master Theorem \u201d (PDF)\nInstructions:\u00a0 Please read the article titled \u201cMaster Theorem\u201d to get an understanding of the use of Master Theorem in analyzing recursive problems.\nTerms of Use: The article above is released under a Creative Commons Attribution-Share-Alike License 3.0 (HTML).\u00a0 You can find the original Wikipedia version of this article here (HTML).\nSee a broken link? Please let us know!\nUnit 3: Divide and Conquer Method\nIn this unit, we will examine a popular technique called divide-and-conquer that is used in solving computer science problems. \u00a0This technique solves the problem by breaking up the problem into smaller problems of same type and then recursively solving these smaller problems and combining their answers. \u00a0We will also look into analysis of these algorithms through the use of recursion techniques.\nUnit 3 Learning Outcomes show close\n3.1 Introduction to Divide and Conquer Algorithms\nReading: University of California, Berkeley: S. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\u2019s Algorithms: \u201cChapter 2: Divide-and-conquer Algorithms\u201d\nLink: University of California, Berkeley: S. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\u2019s Algorithms: \u201c Chapter 2: Divide-and-conquer Algorithms \u201d (PDF)\nInstructions: Please click on the hyperlink titled \u201cDivide-and-conquer algorithms\u201d to download the PDF file for Chapter 2 of S. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\u2019s book, Algorithms. Please read the entire text (36 pages).\nTerms of Use: Please respect the copyright and terms of use displayed on the webpage above.\nSee a broken link? Please let us know!\n3.2 Recurrences in Algorithms\nLecture: YouTube: MIT Course on Introduction to Algorithms: Dr. Erik Demaine\u2019s \u201cLec 3: Introduction to Algorithms\u201d\nLink: YouTube: MIT Course on Introduction to Algorithms: Dr. Erik Demaine\u2019s \u201c Lec 3: Introduction to Algorithms \u201d (YouTube)\nInstructions: Please click the link above, and view the video in its entirety (approximately 1 hour and 8 minutes) to gain an understanding of the basics of the algorithm.\nTerms of use: \u00a0Erik\u00a0Demaine and Charles\u00a0Leiserson\u00a0 6.046J/18.410J Introduction to Algorithms, Fall 2005. (Massachusetts Institute of Technology: MIT OpenCourseWare), http://ocw.mit.edu (Accessed September 21, 2011). License: Creative Commons BY-NC-SA 3.0. Original version can be seen here .\nSee a broken link? Please let us know!\n3.3 Recursion\nReading: Wikipedia: \u201cRecursion\u201d\nLink: Wikipedia: \u201c Recursion \u201d (PDF)\nInstructions:\u00a0 Please read the article titled \u201cRecursion\u201d to get an understanding of the mathematical concept behind the idea of recursion.\nTerms of Use: The article above is released under a Creative Commons Attribution-Share-Alike License 3.0 (HTML).\u00a0 You can find the original Wikipedia version of this article here (HTML).\nSee a broken link? Please let us know!\nUnit 4: Sorting Algorithms\nThis unit introduces algorithms that sort real numbers. \u00a0Algorithms often use sorting as a key subroutine. \u00a0There is a wide variety of sorting algorithms, and they use a rich set of techniques. \u00a0These algorithms have different runtime complexities and work better in certain conditions. \u00a0Some of algorithms that we will study include Quick Sort, Insertion Sort, Bubble Sort, and Merge Sort.\nUnit 4 Learning Outcomes show close\n4.1 Introduction to Sorting Algorithms\nLecture: YouTube: Knight School (2009)\u2019s \u201cLec 3: Algorithmic Thinking\u201d and Google CEO\u2019s interview of 2008 Presidential Candidate Barack Obama in \u201cBarack Obama \u2013 Computer Science Question\u201d\nLinks: YouTube: Knight School (2009)\u2019s \u201c Lec 3: Algorithmic Thinking \u201d and Google CEO\u2019s interview of 2008 Presidential Candidate Barack Obama in \u201c Barack Obama \u2013 Computer Science Question \u201d (YouTube)\nInstructions: Please click on the links above, and view each video (about 10 minutes total) fully to gain an introduction to sorting algorithms.\nTerms of Use: Please respect the copyright and terms of use displayed on the webpage above.\nSee a broken link? Please let us know!\n4.2 Sorting Algorithms \u2013 Part I\nLink: YouTube: MIT Course on Introduction to Algorithms: Dr. Charles E. Leiserson\u2019s \u201c Lec 4: Quicksort, Randomized Algorithms \u201d (YouTube)\nAlso available in:\n"}, {"score": 900.2175, "uuid": "199d853a-f5e2-597e-a741-82946b068125", "index": "cw12", "trec_id": "clueweb12-0103wb-72-26012", "target_hostname": "www.cs.wustl.edu", "target_uri": "http://www.cs.wustl.edu/%7Epless/506/l3.html", "page_rank": 1.1700305e-09, "spam_rank": 84, "title": "Lecture 3: More Convex Hull <em>Algorithms</em> Reading: Today&#x27;s material <em>is</em> not", "snippet": "QuickHull: If the divide and conquer <em>algorithm</em> can be viewed as a <em>sort</em> of generalization of <em>Merge</em> <em>Sort</em>, one might ask whether there <em>is</em> corresponding generalization of other sorting <em>algorithm</em> for computing In particular, the next <em>algorithm</em> that we will consider can be thought of as a generalization of", "explanation": null, "document": "Lecture 3: More Convex Hull Algorithms\n\nReading: Today's material is not covered in the 4M's book. \nIt is covered in O'Rourke's book on Computational Geometry.\n\nConvex Hull by Divide and Conquer: Recall the convex hull problem,\nwhich we introduced last time. We presented Graham's scan, which is an\nO(n log n) time algorithm for this problem.  Next we will consider\nanother O(n log n) algorithm, which is based on the divide and conquer\ndesign technique. It can be viewed as a generalization of the\nMergeSort sorting algorithm (see Cormen, Leiserson, and Rivest). Here\nis an outline of the algorithm. It begins by sorting the points by\ntheir x coordinate, in O(n log n) time.\n\nDivide and Conquer Convex Hull \n\nHull(S) : \n\n(1) If |S| <= 3, then compute the convex hull by brute force in O(1)\ntime and return.\n\n(2) Otherwise, partition the point set S into two sets A and B, where\nA consists of half the points with the lowest x coordinates and B\nconsists of half of the points with the highest x coordinates.\n\n(3) Recursively compute HA = Hull(A) and HB = Hull(B).\n\n(4) Merge the two hulls into a common convex hull, H, by computing the\nupper and lower tangents for HA and HB and discarding all the points\nlying between these two tangents.\n\n(See the figure below.)\nFigure 10: Computing the lower tangent.\n\nThe asymptotic running time of the algorithm can be expressed by a\nrecurrence. Given an input of size n, consider the time needed to\nperform all the parts of the procedure, ignoring the recursive\ncalls. This includes the time to partition the point set, compute the\ntwo tangents, and return the final result. Clearly the first and third\nof these steps can be performed in O(n) time, assuming a linked list\nrepresentation of the hull vertices. Below we will show that the\ntangents can be computed in O(n) time. Thus, ignoring constant\nfactors, we can describe the running time by the following recurrence.\n\nT(n) =\n\n1 if n <= 3\nn + 2T (n/2) otherwise.\n\nThis is the same recurrence that arises in Mergesort. It is easy to\nshow that it solves to T (n) 2 O(n log n) (see CLR).\n\nAll that remains is showing how to compute the two tangents.  One\nthing that simplifies the process of computing the tangents is that\nthe two point sets A and B are separated from each other by a vertical\nline (assuming no duplicate x coordinates).  Let's concentrate on the\nlower tangent, since the upper tangent is symmetric. The algorithm\noperates by a simple ``walking'' procedure. We initialize a to be the\nrightmost point of HA and b is the leftmost point of HB . (These can\nbe found in linear time.) Lower tangency is a condition that can be\ntested locally by an orientation test of the two vertices and\nneighboring vertices on the hull. (This is a simple exercise.) We\niterate the following two loops, which march a and b down, until they\nreach the points lower tangency.\n\nFinding the Lower Tangent\nLowerTangent(HA ; HB ) :\n(1) Let a be the rightmost point of HA .\n(2) Let b be the leftmost point of HB .\n(3) While ab is not a lower tangent for HA and HB do\n(a) While ab is not a lower tangent to HA do a = a - 1 (move a clockwise).\n(b) While ab is not a lower tangent to HB do b = b + 1 (move b counterclockwise).\n(4) Return ab.\n\nProving the correctness of this procedure is a little tricky, but not\ntoo bad. Check O'Rourke's book out for a careful proof. The important\nthing is that each vertex on each hull can be visited at most once by\nthe search, and hence its running time is O(m), where \n\nm = |HA| + |HB| <= |A| + |B|.\n\nThis is exactly what we needed to get the overall O(n log n) running time.\n\nQuickHull: If the divide and conquer algorithm can be viewed as a sort\nof generalization of Merge Sort, one might ask whether there is\ncorresponding generalization of other sorting algorithm for computing\nconvex hulls. In particular, the next algorithm that we will consider\ncan be thought of as a generalization of the QuickSort sorting\nprocedure. The resulting algorithm is called QuickHull.  Like\nQuickSort, this algorithm runs in O(n log n) time for favorable inputs\nbut can take as long as O(n^2) time for unfavorable inputs. However,\nunlike QuickSort, there is no obvious way to convert it into a\nrandomized algorithm with O(n log n) expected running\ntime. Nonetheless, QuickHull tends to perform very well in practice.\n\n\nThe intuition is that in many applications most of the points lie in\nthe interior of the hull. For example, if the points are uniformly\ndistributed in a unit square, then it can be shown that the expected\nnumber of points on the convex hull is O(log n).  The idea behind\nQuickHull is to discard points that are not on the hull as quickly as\npos sible. QuickHull begins by computing the points with the maximum\nand minimum, x and y coordinates. Clearly these points must be on the\nhull. Horizontal and vertical lines passing through these points are\nsupport lines for the hull, and so define a bounding rectangle, within\nwhich the hull is contained. Furthermore, the convex quadrilateral\ndefined by these four points lies within the convex hull, so the\npoints lying within this quadrilateral can be eliminated from further\nconsideration. All of this can be done in O(n) time.\nFigure 11: QuickHull's initial quadrilateral.  Points inside are discarded.\n\n\nTo continue the algorithm, we classify the remaining points into the\nfour corner triangles that remain. In general, as this algorithm\nexecutes, we will have an inner convex polygon, and associated with\neach edge we have a set of points that lie ``outside'' of that\nedge. (More formally, these points are witnesses to the fact that this\nedge is not on the convex hull, because they lie outside the half\nplane defined by this edge.) When this set of points is empty, the\nedge is a final edge of the hull. Consider some edge ab. Assume that\nthe points that lie ``outside'' of this hull edge have been placed in\na bucket that is associated with ab. Our job is to find a point c\namong these points that lies on the hull, discard the points in the\ntriangle abc, and split the remaining points into two subsets, those\nthat lie outside ac and those than lie outside of cb. We can classify\neach point by making two orientation tests.\nFigure 12: QuickHull elimination procedure.\n\nHow should c be selected? There are a number of possible selection\ncriteria that one might think of. The method that is most often\nproposed is to let c be the point that maximizes the perpendicular\ndistance from the line ab. (For example, another possible choice might\nbe the point that maximizes the angle cba or cab. It turns out that\nthese can be are very poor choices because they tend to produce\nimbalanced partitions of the remaining points.) We replace the edge ab\nwith the two edges ac and cb, and classify the points as lying in one\nof three groups: those that lie in the triangle abc, which are\ndiscarded, those that lie outside of ac, and those that lie outside of\ncb. We put these points in buckets for these edges, and recurse. (We\nclaim that it is not hard to classify each point p, by computing the\norientations of the triples acp and cbp.)\n\nThe running time of Quickhull, as with QuickSort, depends on how\nevenly the points are split at each stage. Let T (n) denote the\nrunning time on the algorithm assuming that n points remain outside of\nsome edge. In O(n) time we can select a candidate splitting point c\nand classify the points in the bucket in O(n) time. Let n1 and n2\ndenote the number of remaining points, where n1 + n2 <= n. Then the\nrunning time is given by the recurrence:\n\nT(n) =\n\n1\t\t\t\t\t  if n = 1\nT(n1) + T (n2) where n1 + n2 <= n.        otherwise\n\nIn order to solve this recurrence, it would be necessary to determine\nthe ``reasonable'' values for n1 and n2 . If we assume that the\npoints are ``evenly'' distributed, in the sense that \nmax(n1 ; n2 ) <= a * n for some constant a < 1, \n\nthen by applying the same analysis as that used in QuickSort (see\nCormen, Leiserson, Rivest) the running time will solve to O(n log n),\nwhere the constant factor depends on a. On the other hand, if the\nsplits are not balanced, then the running time can easily increase to\nO(n^2).  \n\nDoes QuickHull outperform Graham's scan? This depends to a great\nextent on the distribution of the point set. There are variations of\nQuickHull that are designed for specific point distributions\n(e.g. points uniformly distributed in a square) and their authors\nclaim that they manage to eliminate almost all of the points in a\nmatter of only a few iterations.\n\n\nGift Wrapping and Jarvis's March: The next algorithm that we will\nconsider is a variant on an O(n^2 ) sorting algorithm called\nSelectionSort. For sorting, this algorithm repeatedly finds the next\nelement to add to the sorted order from the remaining items. The\ncorresponding convex hull algorithm is called Jarvis's march. which\nbuilds the hull in O(nh) time by a process called ``gift\nwrapping''. The algorithm operates by considering any one point that\nis on the hull, say, the lowest point. We then find the ``next'' edge\non the hull in counterclockwise order.  Assuming that p(k) and p(k-1)\nwere the last two points added to the hull, compute the point q that\nmaximizes the angle  [ p(k-1) p(k) q ]. Thus, we can find the point q in\nO(n) time. After repeating this h times, we will return back to the\nstarting point and we are done. Thus, the overall running time is\nO(nh). Note that if h is o(log n) (asymptotically smaller than log n)\nthen this is a better method than Graham's algorithm.\n\n\nOne technical detail is how we to find an edge from which to\nstart. One easy way to do this is to let p(1) be the point with the\nlowest y coordinate, and let p(0) be the point (Inf, 0), which is\ninfinitely far to the right. The point p(0) is only used for computing\nthe initial angles, after which it is discarded.  \n\n\n\nAn Output Sensitive Algorithm for Convex Hulls\n\nReading: Today's material is not covered in our text. Chan's\nalgorithm can be found in T.  Chan, ``Optimal output sensitive convex\nhull algorithms in two and three dimensions'', Discrete and\nComputational Geometry, 16, 1996, 361--368.\nFigure 13: Jarvis's march.\n\nThe Omega(n log h) lower bound on convex hulls is given in the paper,\nD. G. Kirkpatrick and R. Seidel, ``The ultimate planar convex hull\nalgorithm,'' SIAM J. Comput., 15, 1986, 287--299.  The O(logn) bound\non the complexity of the convex hull is folklore.\n\nOutput Sensitive Convex Hull Algorithms: It turns out that in the\nworst case, convex hulls cannot be computed faster than in O( n log n)\ntime. One way to see this intuitively is to observe that the convex\nhull itself is sorted along its boundary, and hence if every point\nlies on the hull, then computing the hull requires sorting of some\nform. Yao proved the much harder result that determining which points\nare on the hull (without sorting them along the boundary) still\nrequiresO( n log n) time. However both of these results rely on the\nfact that all (or at least a constant fraction) of the points lie on\nthe convex hull. This is often not true in practice.\n\n\nThe QuickHull and Jarvis's March algorithms that we saw last time\nsuggest the question of how fast can convex hulls be computed if we\nallow the running time to be described in terms of both the input size\nn and the output size h. Many geometric algorithms have the property\nthat the output size can be a widely varying function of the input\nsize, and worst case output size may not be a good indicator of what\nhappens typically. An algorithm which attempts to be more efficient\nfor small output sizes is called an output sensitive algorithm, and\nrunning time is described as a asymptotic function of both input size\nand output size.\n\n\nChan's Algorithm: Given than any convex hull algorithm must take at\nleast O(n) time, and given that ``log n'' factor arises from the fact\nthat you need to sort the at most n points on the hull, if you were\ntold that there are only h points on the hull, then a reasonable\ntarget running time is O(n log h). (Below we will see that this is\noptimal.) Kirkpatrick and Seidel discovered a relatively complicated\nO(n log h) time algorithm, based on a clever pruning method in 1986.\nThe problem was considered closed until around 10 years later when\nTimothy Chan came up with a much simpler algorithm with the same\nrunning time. One of the interesting aspects of Chan's algorithm is\nthat it involves combining two slower algorithms (Graham's scan and\nJarvis's March) together to form an algorithm that is faster than\neither one.\n\nThe problem with Graham's scan is that it sorts all the points, and\nhence is doomed to having an Omega( n log n) running time, irrespective of\nthe size of the hull. On the other hand, Jarvis's march can perform\nbetter if you have few vertices on the hull, but it takes O( n) time\nfor each hull vertex.\n\nChan's idea was to partition the points into groups of equal\nsize. There are m points in each group, and so the number of groups is\nr = dn=me. For each group we compute its hull using Graham's scan,\nwhich takes O(m log m) time per group, for a total time of O(rm log m)\n= O(n log m). Next, we run Jarvis's march on the groups. Here we take\nadvantage of the fact that you can compute the tangent between a point\nand a convex m gon in O(logm) time.  (We will leave this as an\nexercise.) So, as before there are h steps of Jarvis's march, but\nbecause we are applying it to r convex hulls, each step takes only \nO(r log m) time, for a total of O(hr log m) = ((hn/m) log m)\ntime. Combining these two parts, we get a total of\n\nO((n + (hn/m)) log m) time. \n\nObserve that if we set m = h then the total running time will be O(n\nlog h), as desired.  There is only one small problem here. We do not\nknow what h is in advance, and therefore we do not know what m should\nbe when running the algorithm. We will see how to remedy this\nlater. For now, let's imagine that someone tells us the value of\nm. The following algorithm works correctly as long as m \u0096 h. If we\nare wrong, it returns a special error status.\n\n\nChan's Partial Convex Hull Algorithm (Given m)\n\nPartialHull(P; m) :\n(1) Let r = ceil(n/m). Partition P into disjoint subsets P(1),P(2),... P(r),\n\t\t       each of size at most m.\n(2) For i = 1 to r do:\n\n(a) Compute Hull(P(i)) using Graham's scan and store the vertices in\n    an ordered array.\n(3) Let p0 = (-Inf; 0) and let p1 be the bottommost point of P .\n(4) For k = 1 to m do:\n(a) For i = 1 to r do:\n        Compute point q in P(i) that maximizes the angle\n                  p(k-1)  p(k)  q\n(b) Let p(k+1) be the point q in q(1),q(2),...q(r) than maximizes the angle\n                  p(k-1)  p(k)  q\n(c) If p(k+1) = p(1) then return {p(1), p(2), ... p(k)}.\n(5) Return ``m was too small, try again.''\nFigure 14: Chan's Convex Hull Algorithm.\n\nWe assume that we store the convex hulls from step (2a) in an ordered\narray so that the step inside the for loop of step (4a) can be solved\nin O(logm) time using binary search. Otherwise, the analysis follows\ndirectly from the comments made earlier.\n\n\nThe only question remaining is how do we know what value to give to m?\nThe last trick, is a common trick used in algorithms to guess the\nvalue of a parameter that affects the running time. We could try m =\n1, 2, 3, ... , until we luck out and have m >= h, but this would take\ntoo long. Binary search would be a more efficient option, but if we\nguess to large a value for m (e.g. m = n=2) then we are immediately\nstuck with O(n log n) time, and this is too slow.  Instead, the trick\nis to start with a small value of m and increase it rapidly. Since the\ndependence on m is only in the log term, as long as our value of m is\nwithin a polynomial of h, that is, m = h^c for some constant c, then\nthe running time will still be O(n log h). So, our approach will be to\nguess successively larger values of m, each time squaring the previous\nvalue, until the algorithm returns a successful result. This trick is\noften called doubling search (because the unknown parameter is\nsuccessively doubled), but in this case we will be squaring rather\nthan doubling.\n\n\nChan's Complete Convex Hull Algorithm\n\nHull(P ) :\n(1) For t = 1; 2; : : : do:\n(a) Let m = min(2^(2^t),n)\n(b) Invoke PartialHull(P, m), returning the result in L.\n(c) If L != ``try again'' then return L.\n\n\nNote that 2^2^t has the effect of squaring the previous value of\nm. How long does this take? The t-th iteration takes \n\nO(n log 2^2^t ) = O(n* 2^t ) time.\n\nWe know that it will stop as soon as 2^2^t >= h, that is if \nt = ceil(lg lg n). (We will use lg to denote logarithm base 2.)\nSo, the total running time (ignoring the constant factors) is:\n\nSum(t = 1 ... lg lg h) n*2^t =\nn * Sum(t = 1 ... lg lg h) 2^t <= \nn * 2^(1+lg lg h) = \n2n lg h = \nO(n log h).\n\nwhich is just what we want.\n\nLower Bound: Next we will show that Chan's result is asymptotically\noptimal in the sense that any algorithm for computing the convex hull\nof n points with h points on the hull requires O( n log h) time. The\nproof is a generalization of the proof that sorting a set of n numbers\nrequires O( n log n) comparisons.  If you recall the proof that\nsorting takes at least O( n log n) comparisons, it is based on the\nidea that any sorting algorithm can be described in terms of a\ndecision tree. Each comparison has at most 3 outcomes (!, =, or\n?). Each such comparison corresponds to an internal node in the\ntree. The execution of an algorithm can be viewed as a traversal along\na path in the resulting 3-ary tree. The height of the tree is a lower\nbound on the worst case running time of the algorithm. There are at\nleast n! different possible inputs, each of which must be reordered\ndifferently, and so you have a 3-ary tree with at least n leaves. Any\nsuch tree must have\n O(log(base 3) n!) height. Using Stirling's approximation which says \n(n! is approximately sqrt(2*PI*n)*(n/e)^n), \nthis solves to O( n log n) height.\n\nWe will give an O( n log h) lower bound for the convex hull\nproblem. In fact, we will give an O( n log h) lower bound on the\nfollowing simpler decision problem, whose output is either yes or no.\n\nConvex Hull Size Verification Problem (CHSV): \n\nGiven a point set P and integer h, does the convex hull of P have h\ndistinct vertices?\n\nClearly if this takes O( n log h) time, then computing the hull must\ntake at least as long. As with sorting, we will assume that the\ncomputation is described in the form of a decision tree.  Assuming\nthat the algorithm uses only comparisons is too restrictive for\ncomputing convex hulls, so we will generalize the model of computation\nto allow for more complex functions. We assume that we are allowed to\ncompute any algebraic function of the point coordinates, and test the\nsign of the resulting function. The result is called a algebraic\ndecision tree.  The input to the CHSV problem is a sequence of 2n = N\nreal numbers. We can think of these numbers as forming a vector \n(z(1), z(2), ... z(N) = vector(z) in R^N, (a vector z in N-dimensional space),\n which we will call a configuration.\n\n\nEach node of the decision tree is associated with a multivariate\nalgebraic formula of degree at most d. for example:\n\nf(z) = z(1)*z(4) -  2z(3)z(6) + 5(z(6)^2).\n\nwould be an algebraic function of degree 2. The node branches in one\nof three ways, depending on whether the result is negative, zero, or\npositive. (Computing orientations and dot products both fall into this\ncategory.) Each leaf of the resulting tree corresponds to a possible\nanswer that the algorithm might give.\n\nFor each input vector z to the CHSV problem, the answer is either\n``yes'' or ``no''. The set of all ``yes'' points is just a subset of\npoints Y in R^N , that is a region in this space. Given an arbitrary\ninput z the purpose of the decision tree is to tell us whether this\npoint is in Y or not. This is done by walking down the tree,\nevaluating the functions on z and following the appropriate branches\nuntil arriving at a leaf, which is either labeled ``yes'' (meaning z\nin Y) or ``no''. An abstract example (not for the convex hull problem)\nof a region of configuration space and a possible algebraic decision\ntree (of degree 1) is shown in the following figure. (We have\nsimplified it by making it a binary tree.) In this case the input is\njust a pair of real numbers.\nFigure 15: The geometric interpretation of an algebraic decision tree.\n\nWe say that two points u, v in  Y are in the same connected component\nof Y if there is a path in R^N from u to v such that all the points\nalong the path are in the set Y . (There are two connected components\nin the figure.) We will make use of the following fundamental result\non algebraic decision trees, due to Ben Or. Intuitively, it states\nthat if your set has M connected components, then there must be at\nleast M leaves in any decision tree for the set, and the tree must\nhave height at least the logarithm of the number of leaves.\n\n\nTheorem: Let Y in R^N be any set and let T be any d th order algebraic\ndecision tree that determines membership in W . If W has M disjoint\nconnected components, then T must have height at least Omega(log M - N).\n\n\nWe will begin our proof with a simpler problem.  \n\nMultiset Size Verification Problem (MSV):\n\nGiven a multiset of n real numbers and an integer k, confirm that the\nmultiset has exactly k distinct elements.\n\nLemma: The MSV problem requires O( n log k) steps in the worst case in\nthe d th order algebraic decision tree\n\nProof: In terms of points in R^n , the set of points for which the\nanswer is ``yes'' is \n\nY = {(z(1), z(2),...  z(n) ) in R^n such that |{z(1), z(2), ... ,\nz(n)}| = k}\n\nIt suffices to show that there are at least k!*k^(n-k) different\nconnected components in this set, because by Ben Or's result it would\nfollow that the time to test membership in Y would be\n Omega(log(k!*k^( n-k )) - n) =  \n Omega(k log k + (n - k) log k - n) = \n Omega( n log k)\n\nConsider the all the tuples (z(1) ... z(n) ) with z(1), ..., z(k) set\nto the distinct integers from 1 to k, and z(k+1) ... z(n) each set to\nan arbitrary integer in the same range. Clearly there are k! ways to\nselect the first k elements and k^(n-k) ways to select the\nremaining elements.  Each such tuple has exactly k distinct items, but\nit is not hard to see that if we attempt to continuously modify one of\nthese tuples to equal another one, we must change the number of\ndistinct elements, implying that each of these tuples is in a\ndifferent connected component of Y .\n\nTo finish the lower bound proof, we argue that any instance of MSV can\nbe reduced to the convex hull size verification problem (CHSV). Thus\nany lower bound for MSV problem applies to CHSV as well.\n\nTheorem: The CHSV problem requires O( n log h) time to solve.  Proof:\nLet Z = (z(1), ..., z(n) ) and k be an instance of the MSV problem. We\ncreate a point set p(1), ...,p(n) in the plane where p(i) = (z(i),\nz(i)^), and set h = k.\n\n(Observe that the points lie on a parabola, so that all the points are\non the convex hull.) Now, if the multiset Z has exactly k distinct\nelements, then there are exactly h = k points in the point set (since\nthe others are all duplicates of these) and so there are exactly h\npoints on the hull.  Conversely, if there are h points on the convex\nhull, then there were exactly h = k distinct numbers in the multiset\nto begin with in Z.\n\nThus, we cannot solve CHSV any faster than O( n log h) time, for\notherwise we could solve MSV in the same time.\n\nThe proof is rather unsatisfying, because it relies on the fact that\nthere are many duplicate points. You might wonder, does the lower\nbound still hold if there are no duplicates? Kirkpatric and Seidel\nactually prove a stronger (but harder) result that the O( n log h)\nlower bound holds even you assume that the points are distinct.\n\n\nExpected Number of Hull Points \n\nFor different distributions a different number of points is ex pected\nto lie on the hull. One of the simplest cases to analyze is that of\npoints uniformly distributed in a unit square.\n\nTheorem: If n points are sampled from a uniform distribution in a unit\nsquare, then the expected number of points on the convex hull is O(log\nn).  Proof: First, we will break the hull into four parts, the upper\nleft, upper right, lower left, and lower right hulls. This is done by\nbreaking the hull at its leftmost, rightmost, topmost and bottommost\npoints. We will show that each has O(log n) points. By symmetry, we\nmay consider one, say the upper right hull.\n\nRather than bounding the number of points on the hull, we will bound a\nlarger quantity.  A point p(i) = (x(i), y(i) ) is said to be dominated\nby another point p(j) if x(j) >= x(i) and y(j) >= y(i) .  The maxima\nof a point set P are the points that are not dominated by any other\npoint in P . Observe that each vertex of the convex hull is a maxima\n(but not vice versa), and therefore there are at least as many maxima\npoints as convex hull vertices. So it suffices to prove that the\nexpected number of maxima is O(log n).  \n\nSuppose that the points are sorted in decreasing order of x\ncoordinate. We will assume that all points have distinct coordinates\nto simplify things. Clearly p(i) is in the maxima if and only if p(i)\nhas the largest y coordinate among the subset {p(1), p(2),\n... p(i)}. What is the probability that this is true? The y\ncoordinates of these points are independent and identically\ndistributed. Therefore each of these points is equally likely to be\nthe maximum, and so the probability that p(i) has the largest y\ncoordinate is just 1/i.  Thus, each point p(i) is a maxima with\nprobability 1/i.\nFigure 16: Upper right hull and maxima.\n\nThe expected number of maxima is the sum of these expectations: \n\nsum(i = 1..n) 1/i\n\nThe sum is the harmonic series and it is well known that it is very\nclose to ln n (see Cormen, Leiserson, and Rivest). From the comments\nmade earlier it follows that the expected number of points on all the\nhulls is at most four times this quantity, which is O(log n).\nThis course is modeled on the Computational Geometry Course taught by\nDave Mount, at the University of Maryland.  These notes are\nmodifications of his Lecture Notes, which are copyrighted as follows:\n\n\nCopyright, David M. Mount, 2000, Dept. of Computer Science,\nUniversity of Maryland, College Park, MD, 20742. These lecture notes\nwere prepared by David Mount for the course CMSC 754, Computational\nGeometry, at the University of Maryland, College Park. Permission to\nuse, copy, modify, and distribute these notes for educational purposes\nand without fee is hereby granted, provided that this copyright notice\nappear in all copies.\n"}, {"score": 897.46405, "uuid": "b914aae2-cf19-5212-89fb-be5d31c8cc63", "index": "cw12", "trec_id": "clueweb12-0104wb-24-29273", "target_hostname": "www.cs.wustl.edu", "target_uri": "http://www.cs.wustl.edu/~pless/506/l3.html", "page_rank": 1.2148998e-09, "spam_rank": 84, "title": "Lecture 3: More Convex Hull <em>Algorithms</em> Reading: Today&#x27;s material <em>is</em> not", "snippet": "QuickHull: If the divide and conquer <em>algorithm</em> can be viewed as a <em>sort</em> of generalization of <em>Merge</em> <em>Sort</em>, one might ask whether there <em>is</em> corresponding generalization of other sorting <em>algorithm</em> for computing In particular, the next <em>algorithm</em> that we will consider can be thought of as a generalization of", "explanation": null, "document": "Lecture 3: More Convex Hull Algorithms\n\nReading: Today's material is not covered in the 4M's book. \nIt is covered in O'Rourke's book on Computational Geometry.\n\nConvex Hull by Divide and Conquer: Recall the convex hull problem,\nwhich we introduced last time. We presented Graham's scan, which is an\nO(n log n) time algorithm for this problem.  Next we will consider\nanother O(n log n) algorithm, which is based on the divide and conquer\ndesign technique. It can be viewed as a generalization of the\nMergeSort sorting algorithm (see Cormen, Leiserson, and Rivest). Here\nis an outline of the algorithm. It begins by sorting the points by\ntheir x coordinate, in O(n log n) time.\n\nDivide and Conquer Convex Hull \n\nHull(S) : \n\n(1) If |S| <= 3, then compute the convex hull by brute force in O(1)\ntime and return.\n\n(2) Otherwise, partition the point set S into two sets A and B, where\nA consists of half the points with the lowest x coordinates and B\nconsists of half of the points with the highest x coordinates.\n\n(3) Recursively compute HA = Hull(A) and HB = Hull(B).\n\n(4) Merge the two hulls into a common convex hull, H, by computing the\nupper and lower tangents for HA and HB and discarding all the points\nlying between these two tangents.\n\n(See the figure below.)\nFigure 10: Computing the lower tangent.\n\nThe asymptotic running time of the algorithm can be expressed by a\nrecurrence. Given an input of size n, consider the time needed to\nperform all the parts of the procedure, ignoring the recursive\ncalls. This includes the time to partition the point set, compute the\ntwo tangents, and return the final result. Clearly the first and third\nof these steps can be performed in O(n) time, assuming a linked list\nrepresentation of the hull vertices. Below we will show that the\ntangents can be computed in O(n) time. Thus, ignoring constant\nfactors, we can describe the running time by the following recurrence.\n\nT(n) =\n\n1 if n <= 3\nn + 2T (n/2) otherwise.\n\nThis is the same recurrence that arises in Mergesort. It is easy to\nshow that it solves to T (n) 2 O(n log n) (see CLR).\n\nAll that remains is showing how to compute the two tangents.  One\nthing that simplifies the process of computing the tangents is that\nthe two point sets A and B are separated from each other by a vertical\nline (assuming no duplicate x coordinates).  Let's concentrate on the\nlower tangent, since the upper tangent is symmetric. The algorithm\noperates by a simple ``walking'' procedure. We initialize a to be the\nrightmost point of HA and b is the leftmost point of HB . (These can\nbe found in linear time.) Lower tangency is a condition that can be\ntested locally by an orientation test of the two vertices and\nneighboring vertices on the hull. (This is a simple exercise.) We\niterate the following two loops, which march a and b down, until they\nreach the points lower tangency.\n\nFinding the Lower Tangent\nLowerTangent(HA ; HB ) :\n(1) Let a be the rightmost point of HA .\n(2) Let b be the leftmost point of HB .\n(3) While ab is not a lower tangent for HA and HB do\n(a) While ab is not a lower tangent to HA do a = a - 1 (move a clockwise).\n(b) While ab is not a lower tangent to HB do b = b + 1 (move b counterclockwise).\n(4) Return ab.\n\nProving the correctness of this procedure is a little tricky, but not\ntoo bad. Check O'Rourke's book out for a careful proof. The important\nthing is that each vertex on each hull can be visited at most once by\nthe search, and hence its running time is O(m), where \n\nm = |HA| + |HB| <= |A| + |B|.\n\nThis is exactly what we needed to get the overall O(n log n) running time.\n\nQuickHull: If the divide and conquer algorithm can be viewed as a sort\nof generalization of Merge Sort, one might ask whether there is\ncorresponding generalization of other sorting algorithm for computing\nconvex hulls. In particular, the next algorithm that we will consider\ncan be thought of as a generalization of the QuickSort sorting\nprocedure. The resulting algorithm is called QuickHull.  Like\nQuickSort, this algorithm runs in O(n log n) time for favorable inputs\nbut can take as long as O(n^2) time for unfavorable inputs. However,\nunlike QuickSort, there is no obvious way to convert it into a\nrandomized algorithm with O(n log n) expected running\ntime. Nonetheless, QuickHull tends to perform very well in practice.\n\n\nThe intuition is that in many applications most of the points lie in\nthe interior of the hull. For example, if the points are uniformly\ndistributed in a unit square, then it can be shown that the expected\nnumber of points on the convex hull is O(log n).  The idea behind\nQuickHull is to discard points that are not on the hull as quickly as\npos sible. QuickHull begins by computing the points with the maximum\nand minimum, x and y coordinates. Clearly these points must be on the\nhull. Horizontal and vertical lines passing through these points are\nsupport lines for the hull, and so define a bounding rectangle, within\nwhich the hull is contained. Furthermore, the convex quadrilateral\ndefined by these four points lies within the convex hull, so the\npoints lying within this quadrilateral can be eliminated from further\nconsideration. All of this can be done in O(n) time.\nFigure 11: QuickHull's initial quadrilateral.  Points inside are discarded.\n\n\nTo continue the algorithm, we classify the remaining points into the\nfour corner triangles that remain. In general, as this algorithm\nexecutes, we will have an inner convex polygon, and associated with\neach edge we have a set of points that lie ``outside'' of that\nedge. (More formally, these points are witnesses to the fact that this\nedge is not on the convex hull, because they lie outside the half\nplane defined by this edge.) When this set of points is empty, the\nedge is a final edge of the hull. Consider some edge ab. Assume that\nthe points that lie ``outside'' of this hull edge have been placed in\na bucket that is associated with ab. Our job is to find a point c\namong these points that lies on the hull, discard the points in the\ntriangle abc, and split the remaining points into two subsets, those\nthat lie outside ac and those than lie outside of cb. We can classify\neach point by making two orientation tests.\nFigure 12: QuickHull elimination procedure.\n\nHow should c be selected? There are a number of possible selection\ncriteria that one might think of. The method that is most often\nproposed is to let c be the point that maximizes the perpendicular\ndistance from the line ab. (For example, another possible choice might\nbe the point that maximizes the angle cba or cab. It turns out that\nthese can be are very poor choices because they tend to produce\nimbalanced partitions of the remaining points.) We replace the edge ab\nwith the two edges ac and cb, and classify the points as lying in one\nof three groups: those that lie in the triangle abc, which are\ndiscarded, those that lie outside of ac, and those that lie outside of\ncb. We put these points in buckets for these edges, and recurse. (We\nclaim that it is not hard to classify each point p, by computing the\norientations of the triples acp and cbp.)\n\nThe running time of Quickhull, as with QuickSort, depends on how\nevenly the points are split at each stage. Let T (n) denote the\nrunning time on the algorithm assuming that n points remain outside of\nsome edge. In O(n) time we can select a candidate splitting point c\nand classify the points in the bucket in O(n) time. Let n1 and n2\ndenote the number of remaining points, where n1 + n2 <= n. Then the\nrunning time is given by the recurrence:\n\nT(n) =\n\n1\t\t\t\t\t  if n = 1\nT(n1) + T (n2) where n1 + n2 <= n.        otherwise\n\nIn order to solve this recurrence, it would be necessary to determine\nthe ``reasonable'' values for n1 and n2 . If we assume that the\npoints are ``evenly'' distributed, in the sense that \nmax(n1 ; n2 ) <= a * n for some constant a < 1, \n\nthen by applying the same analysis as that used in QuickSort (see\nCormen, Leiserson, Rivest) the running time will solve to O(n log n),\nwhere the constant factor depends on a. On the other hand, if the\nsplits are not balanced, then the running time can easily increase to\nO(n^2).  \n\nDoes QuickHull outperform Graham's scan? This depends to a great\nextent on the distribution of the point set. There are variations of\nQuickHull that are designed for specific point distributions\n(e.g. points uniformly distributed in a square) and their authors\nclaim that they manage to eliminate almost all of the points in a\nmatter of only a few iterations.\n\n\nGift Wrapping and Jarvis's March: The next algorithm that we will\nconsider is a variant on an O(n^2 ) sorting algorithm called\nSelectionSort. For sorting, this algorithm repeatedly finds the next\nelement to add to the sorted order from the remaining items. The\ncorresponding convex hull algorithm is called Jarvis's march. which\nbuilds the hull in O(nh) time by a process called ``gift\nwrapping''. The algorithm operates by considering any one point that\nis on the hull, say, the lowest point. We then find the ``next'' edge\non the hull in counterclockwise order.  Assuming that p(k) and p(k-1)\nwere the last two points added to the hull, compute the point q that\nmaximizes the angle  [ p(k-1) p(k) q ]. Thus, we can find the point q in\nO(n) time. After repeating this h times, we will return back to the\nstarting point and we are done. Thus, the overall running time is\nO(nh). Note that if h is o(log n) (asymptotically smaller than log n)\nthen this is a better method than Graham's algorithm.\n\n\nOne technical detail is how we to find an edge from which to\nstart. One easy way to do this is to let p(1) be the point with the\nlowest y coordinate, and let p(0) be the point (Inf, 0), which is\ninfinitely far to the right. The point p(0) is only used for computing\nthe initial angles, after which it is discarded.  \n\n\n\nAn Output Sensitive Algorithm for Convex Hulls\n\nReading: Today's material is not covered in our text. Chan's\nalgorithm can be found in T.  Chan, ``Optimal output sensitive convex\nhull algorithms in two and three dimensions'', Discrete and\nComputational Geometry, 16, 1996, 361--368.\nFigure 13: Jarvis's march.\n\nThe Omega(n log h) lower bound on convex hulls is given in the paper,\nD. G. Kirkpatrick and R. Seidel, ``The ultimate planar convex hull\nalgorithm,'' SIAM J. Comput., 15, 1986, 287--299.  The O(logn) bound\non the complexity of the convex hull is folklore.\n\nOutput Sensitive Convex Hull Algorithms: It turns out that in the\nworst case, convex hulls cannot be computed faster than in O( n log n)\ntime. One way to see this intuitively is to observe that the convex\nhull itself is sorted along its boundary, and hence if every point\nlies on the hull, then computing the hull requires sorting of some\nform. Yao proved the much harder result that determining which points\nare on the hull (without sorting them along the boundary) still\nrequiresO( n log n) time. However both of these results rely on the\nfact that all (or at least a constant fraction) of the points lie on\nthe convex hull. This is often not true in practice.\n\n\nThe QuickHull and Jarvis's March algorithms that we saw last time\nsuggest the question of how fast can convex hulls be computed if we\nallow the running time to be described in terms of both the input size\nn and the output size h. Many geometric algorithms have the property\nthat the output size can be a widely varying function of the input\nsize, and worst case output size may not be a good indicator of what\nhappens typically. An algorithm which attempts to be more efficient\nfor small output sizes is called an output sensitive algorithm, and\nrunning time is described as a asymptotic function of both input size\nand output size.\n\n\nChan's Algorithm: Given than any convex hull algorithm must take at\nleast O(n) time, and given that ``log n'' factor arises from the fact\nthat you need to sort the at most n points on the hull, if you were\ntold that there are only h points on the hull, then a reasonable\ntarget running time is O(n log h). (Below we will see that this is\noptimal.) Kirkpatrick and Seidel discovered a relatively complicated\nO(n log h) time algorithm, based on a clever pruning method in 1986.\nThe problem was considered closed until around 10 years later when\nTimothy Chan came up with a much simpler algorithm with the same\nrunning time. One of the interesting aspects of Chan's algorithm is\nthat it involves combining two slower algorithms (Graham's scan and\nJarvis's March) together to form an algorithm that is faster than\neither one.\n\nThe problem with Graham's scan is that it sorts all the points, and\nhence is doomed to having an Omega( n log n) running time, irrespective of\nthe size of the hull. On the other hand, Jarvis's march can perform\nbetter if you have few vertices on the hull, but it takes O( n) time\nfor each hull vertex.\n\nChan's idea was to partition the points into groups of equal\nsize. There are m points in each group, and so the number of groups is\nr = dn=me. For each group we compute its hull using Graham's scan,\nwhich takes O(m log m) time per group, for a total time of O(rm log m)\n= O(n log m). Next, we run Jarvis's march on the groups. Here we take\nadvantage of the fact that you can compute the tangent between a point\nand a convex m gon in O(logm) time.  (We will leave this as an\nexercise.) So, as before there are h steps of Jarvis's march, but\nbecause we are applying it to r convex hulls, each step takes only \nO(r log m) time, for a total of O(hr log m) = ((hn/m) log m)\ntime. Combining these two parts, we get a total of\n\nO((n + (hn/m)) log m) time. \n\nObserve that if we set m = h then the total running time will be O(n\nlog h), as desired.  There is only one small problem here. We do not\nknow what h is in advance, and therefore we do not know what m should\nbe when running the algorithm. We will see how to remedy this\nlater. For now, let's imagine that someone tells us the value of\nm. The following algorithm works correctly as long as m \u0096 h. If we\nare wrong, it returns a special error status.\n\n\nChan's Partial Convex Hull Algorithm (Given m)\n\nPartialHull(P; m) :\n(1) Let r = ceil(n/m). Partition P into disjoint subsets P(1),P(2),... P(r),\n\t\t       each of size at most m.\n(2) For i = 1 to r do:\n\n(a) Compute Hull(P(i)) using Graham's scan and store the vertices in\n    an ordered array.\n(3) Let p0 = (-Inf; 0) and let p1 be the bottommost point of P .\n(4) For k = 1 to m do:\n(a) For i = 1 to r do:\n        Compute point q in P(i) that maximizes the angle\n                  p(k-1)  p(k)  q\n(b) Let p(k+1) be the point q in q(1),q(2),...q(r) than maximizes the angle\n                  p(k-1)  p(k)  q\n(c) If p(k+1) = p(1) then return {p(1), p(2), ... p(k)}.\n(5) Return ``m was too small, try again.''\nFigure 14: Chan's Convex Hull Algorithm.\n\nWe assume that we store the convex hulls from step (2a) in an ordered\narray so that the step inside the for loop of step (4a) can be solved\nin O(logm) time using binary search. Otherwise, the analysis follows\ndirectly from the comments made earlier.\n\n\nThe only question remaining is how do we know what value to give to m?\nThe last trick, is a common trick used in algorithms to guess the\nvalue of a parameter that affects the running time. We could try m =\n1, 2, 3, ... , until we luck out and have m >= h, but this would take\ntoo long. Binary search would be a more efficient option, but if we\nguess to large a value for m (e.g. m = n=2) then we are immediately\nstuck with O(n log n) time, and this is too slow.  Instead, the trick\nis to start with a small value of m and increase it rapidly. Since the\ndependence on m is only in the log term, as long as our value of m is\nwithin a polynomial of h, that is, m = h^c for some constant c, then\nthe running time will still be O(n log h). So, our approach will be to\nguess successively larger values of m, each time squaring the previous\nvalue, until the algorithm returns a successful result. This trick is\noften called doubling search (because the unknown parameter is\nsuccessively doubled), but in this case we will be squaring rather\nthan doubling.\n\n\nChan's Complete Convex Hull Algorithm\n\nHull(P ) :\n(1) For t = 1; 2; : : : do:\n(a) Let m = min(2^(2^t),n)\n(b) Invoke PartialHull(P, m), returning the result in L.\n(c) If L != ``try again'' then return L.\n\n\nNote that 2^2^t has the effect of squaring the previous value of\nm. How long does this take? The t-th iteration takes \n\nO(n log 2^2^t ) = O(n* 2^t ) time.\n\nWe know that it will stop as soon as 2^2^t >= h, that is if \nt = ceil(lg lg n). (We will use lg to denote logarithm base 2.)\nSo, the total running time (ignoring the constant factors) is:\n\nSum(t = 1 ... lg lg h) n*2^t =\nn * Sum(t = 1 ... lg lg h) 2^t <= \nn * 2^(1+lg lg h) = \n2n lg h = \nO(n log h).\n\nwhich is just what we want.\n\nLower Bound: Next we will show that Chan's result is asymptotically\noptimal in the sense that any algorithm for computing the convex hull\nof n points with h points on the hull requires O( n log h) time. The\nproof is a generalization of the proof that sorting a set of n numbers\nrequires O( n log n) comparisons.  If you recall the proof that\nsorting takes at least O( n log n) comparisons, it is based on the\nidea that any sorting algorithm can be described in terms of a\ndecision tree. Each comparison has at most 3 outcomes (!, =, or\n?). Each such comparison corresponds to an internal node in the\ntree. The execution of an algorithm can be viewed as a traversal along\na path in the resulting 3-ary tree. The height of the tree is a lower\nbound on the worst case running time of the algorithm. There are at\nleast n! different possible inputs, each of which must be reordered\ndifferently, and so you have a 3-ary tree with at least n leaves. Any\nsuch tree must have\n O(log(base 3) n!) height. Using Stirling's approximation which says \n(n! is approximately sqrt(2*PI*n)*(n/e)^n), \nthis solves to O( n log n) height.\n\nWe will give an O( n log h) lower bound for the convex hull\nproblem. In fact, we will give an O( n log h) lower bound on the\nfollowing simpler decision problem, whose output is either yes or no.\n\nConvex Hull Size Verification Problem (CHSV): \n\nGiven a point set P and integer h, does the convex hull of P have h\ndistinct vertices?\n\nClearly if this takes O( n log h) time, then computing the hull must\ntake at least as long. As with sorting, we will assume that the\ncomputation is described in the form of a decision tree.  Assuming\nthat the algorithm uses only comparisons is too restrictive for\ncomputing convex hulls, so we will generalize the model of computation\nto allow for more complex functions. We assume that we are allowed to\ncompute any algebraic function of the point coordinates, and test the\nsign of the resulting function. The result is called a algebraic\ndecision tree.  The input to the CHSV problem is a sequence of 2n = N\nreal numbers. We can think of these numbers as forming a vector \n(z(1), z(2), ... z(N) = vector(z) in R^N, (a vector z in N-dimensional space),\n which we will call a configuration.\n\n\nEach node of the decision tree is associated with a multivariate\nalgebraic formula of degree at most d. for example:\n\nf(z) = z(1)*z(4) -  2z(3)z(6) + 5(z(6)^2).\n\nwould be an algebraic function of degree 2. The node branches in one\nof three ways, depending on whether the result is negative, zero, or\npositive. (Computing orientations and dot products both fall into this\ncategory.) Each leaf of the resulting tree corresponds to a possible\nanswer that the algorithm might give.\n\nFor each input vector z to the CHSV problem, the answer is either\n``yes'' or ``no''. The set of all ``yes'' points is just a subset of\npoints Y in R^N , that is a region in this space. Given an arbitrary\ninput z the purpose of the decision tree is to tell us whether this\npoint is in Y or not. This is done by walking down the tree,\nevaluating the functions on z and following the appropriate branches\nuntil arriving at a leaf, which is either labeled ``yes'' (meaning z\nin Y) or ``no''. An abstract example (not for the convex hull problem)\nof a region of configuration space and a possible algebraic decision\ntree (of degree 1) is shown in the following figure. (We have\nsimplified it by making it a binary tree.) In this case the input is\njust a pair of real numbers.\nFigure 15: The geometric interpretation of an algebraic decision tree.\n\nWe say that two points u, v in  Y are in the same connected component\nof Y if there is a path in R^N from u to v such that all the points\nalong the path are in the set Y . (There are two connected components\nin the figure.) We will make use of the following fundamental result\non algebraic decision trees, due to Ben Or. Intuitively, it states\nthat if your set has M connected components, then there must be at\nleast M leaves in any decision tree for the set, and the tree must\nhave height at least the logarithm of the number of leaves.\n\n\nTheorem: Let Y in R^N be any set and let T be any d th order algebraic\ndecision tree that determines membership in W . If W has M disjoint\nconnected components, then T must have height at least Omega(log M - N).\n\n\nWe will begin our proof with a simpler problem.  \n\nMultiset Size Verification Problem (MSV):\n\nGiven a multiset of n real numbers and an integer k, confirm that the\nmultiset has exactly k distinct elements.\n\nLemma: The MSV problem requires O( n log k) steps in the worst case in\nthe d th order algebraic decision tree\n\nProof: In terms of points in R^n , the set of points for which the\nanswer is ``yes'' is \n\nY = {(z(1), z(2),...  z(n) ) in R^n such that |{z(1), z(2), ... ,\nz(n)}| = k}\n\nIt suffices to show that there are at least k!*k^(n-k) different\nconnected components in this set, because by Ben Or's result it would\nfollow that the time to test membership in Y would be\n Omega(log(k!*k^( n-k )) - n) =  \n Omega(k log k + (n - k) log k - n) = \n Omega( n log k)\n\nConsider the all the tuples (z(1) ... z(n) ) with z(1), ..., z(k) set\nto the distinct integers from 1 to k, and z(k+1) ... z(n) each set to\nan arbitrary integer in the same range. Clearly there are k! ways to\nselect the first k elements and k^(n-k) ways to select the\nremaining elements.  Each such tuple has exactly k distinct items, but\nit is not hard to see that if we attempt to continuously modify one of\nthese tuples to equal another one, we must change the number of\ndistinct elements, implying that each of these tuples is in a\ndifferent connected component of Y .\n\nTo finish the lower bound proof, we argue that any instance of MSV can\nbe reduced to the convex hull size verification problem (CHSV). Thus\nany lower bound for MSV problem applies to CHSV as well.\n\nTheorem: The CHSV problem requires O( n log h) time to solve.  Proof:\nLet Z = (z(1), ..., z(n) ) and k be an instance of the MSV problem. We\ncreate a point set p(1), ...,p(n) in the plane where p(i) = (z(i),\nz(i)^), and set h = k.\n\n(Observe that the points lie on a parabola, so that all the points are\non the convex hull.) Now, if the multiset Z has exactly k distinct\nelements, then there are exactly h = k points in the point set (since\nthe others are all duplicates of these) and so there are exactly h\npoints on the hull.  Conversely, if there are h points on the convex\nhull, then there were exactly h = k distinct numbers in the multiset\nto begin with in Z.\n\nThus, we cannot solve CHSV any faster than O( n log h) time, for\notherwise we could solve MSV in the same time.\n\nThe proof is rather unsatisfying, because it relies on the fact that\nthere are many duplicate points. You might wonder, does the lower\nbound still hold if there are no duplicates? Kirkpatric and Seidel\nactually prove a stronger (but harder) result that the O( n log h)\nlower bound holds even you assume that the points are distinct.\n\n\nExpected Number of Hull Points \n\nFor different distributions a different number of points is ex pected\nto lie on the hull. One of the simplest cases to analyze is that of\npoints uniformly distributed in a unit square.\n\nTheorem: If n points are sampled from a uniform distribution in a unit\nsquare, then the expected number of points on the convex hull is O(log\nn).  Proof: First, we will break the hull into four parts, the upper\nleft, upper right, lower left, and lower right hulls. This is done by\nbreaking the hull at its leftmost, rightmost, topmost and bottommost\npoints. We will show that each has O(log n) points. By symmetry, we\nmay consider one, say the upper right hull.\n\nRather than bounding the number of points on the hull, we will bound a\nlarger quantity.  A point p(i) = (x(i), y(i) ) is said to be dominated\nby another point p(j) if x(j) >= x(i) and y(j) >= y(i) .  The maxima\nof a point set P are the points that are not dominated by any other\npoint in P . Observe that each vertex of the convex hull is a maxima\n(but not vice versa), and therefore there are at least as many maxima\npoints as convex hull vertices. So it suffices to prove that the\nexpected number of maxima is O(log n).  \n\nSuppose that the points are sorted in decreasing order of x\ncoordinate. We will assume that all points have distinct coordinates\nto simplify things. Clearly p(i) is in the maxima if and only if p(i)\nhas the largest y coordinate among the subset {p(1), p(2),\n... p(i)}. What is the probability that this is true? The y\ncoordinates of these points are independent and identically\ndistributed. Therefore each of these points is equally likely to be\nthe maximum, and so the probability that p(i) has the largest y\ncoordinate is just 1/i.  Thus, each point p(i) is a maxima with\nprobability 1/i.\nFigure 16: Upper right hull and maxima.\n\nThe expected number of maxima is the sum of these expectations: \n\nsum(i = 1..n) 1/i\n\nThe sum is the harmonic series and it is well known that it is very\nclose to ln n (see Cormen, Leiserson, and Rivest). From the comments\nmade earlier it follows that the expected number of points on all the\nhulls is at most four times this quantity, which is O(log n).\nThis course is modeled on the Computational Geometry Course taught by\nDave Mount, at the University of Maryland.  These notes are\nmodifications of his Lecture Notes, which are copyrighted as follows:\n\n\nCopyright, David M. Mount, 2000, Dept. of Computer Science,\nUniversity of Maryland, College Park, MD, 20742. These lecture notes\nwere prepared by David Mount for the course CMSC 754, Computational\nGeometry, at the University of Maryland, College Park. Permission to\nuse, copy, modify, and distribute these notes for educational purposes\nand without fee is hereby granted, provided that this copyright notice\nappear in all copies.\n"}, {"score": 892.6384, "uuid": "be0ac018-04a0-522f-8334-c788db2a8b69", "index": "cw12", "trec_id": "clueweb12-0210wb-06-15141", "target_hostname": "www.math.grin.edu", "target_uri": "http://www.math.grin.edu/~rebelsky/Courses/CS152/2000F/Outlines/outline.29.html", "page_rank": 1.3242978e-09, "spam_rank": 90, "title": "CSC-152 2000F : Class 29: More Efficient Sorting <em>Algorithms</em>", "snippet": "One of the most straightforward <em>is</em> <em>merge</em> <em>sort</em>. * In <em>merge</em> <em>sort</em>, you split the list, array, collection <em>or</em> ... into two parts, <em>sort</em> each part, and then <em>merge</em> them together. * Unlike the previous <em>algorithms</em>, <em>merge</em> <em>sort</em> requires extra space for the sorted arrays <em>or</em> subarrays. * We&#x27;ll write this as a non-inplace", "explanation": null, "document": "Back to Some Sorting Algorithms . On to Quicksort .\nHeld Friday, October 13, 2000\nSummary\nToday we visit our first divide-and-conquer sorting algorithm, merge sort.\nNotes\nExam 3 is ready.\nI'll use the last fifteen minutes of today's class for a final pre-break discussion of the project.\nNo outline for today yet, sorry. (I'll write it during break.)\nOverview\nSorting with Divide and Conquer\nIn the previous class, we identified a number of interesting sorting algorithms which took O(n2) time.\nCan we do better? Well, sometimes using divide-and-conquer helps speed up algorithms (in our experience from O(n) to O(log2n)).\nWe'll look at two different ways of ``splitting'' the array.\nMerge Sort\nWe can develop a number of sorting techniques based on the divide and conquer technique. One of the most straightforward is merge sort.\nIn merge sort, you split the list, array, collection or ... into two parts, sort each part, and then merge them together.\nUnlike the previous algorithms, merge sort requires extra space for the sorted arrays or subarrays.\nWe'll write this as a non-inplace routine which returns a new, sorted array, rather than sorting the existing array.\nWhat is the running time?\nWe can use recurrence relations:\nLet f(n) be the running time of merge sort on input of size n.\nf(1) = 1\nLet's run this for a few steps\nf(n)\n"}, {"score": 880.0898, "uuid": "38782b83-382f-50a3-bcb5-fa8f20725061", "index": "cw12", "trec_id": "clueweb12-0100tw-31-17801", "target_hostname": "drdobbs.com", "target_uri": "http://drdobbs.com/parallel/232500147", "page_rank": 1.1700305e-09, "spam_rank": 86, "title": "The Need to Rewrite Established <em>Algorithms</em> | Dr Dobb&#x27;s", "snippet": "And yet, even this mainstay of obvious logic <em>is</em> now changing beneath our feet. The imminent era of manycore processors <em>is</em> likely to bring other changes to the fore. I especially expect that <em>sort</em> routines will be dramatically affected. <em>Quicksort</em> will no longer be the default sorting <em>algorithm</em>.", "explanation": null, "document": "By Andrew Binstock , February 06, 2012\nParallel architectures, like other hardware advances before them, require us to rewrite algorithms and data structures \u0097 especially the old standbys that have served us well\nA central point of developer wisdom is to reuse code, especially data structures and collections. A few decades ago, it was common for C programmers to write innumerable implementations of linked lists from scratch. The code became almost a muscle memory as you banged it out. Today, such an exercise is more the result of ignoring established and well-tested options, rather than coding prowess. Except in exigent circumstances, writing your own collections has the whiff of cowboy programming.\nIt's safe to say that, for the most part, you should not be writing your own data structures or basic algorithms (sorts, checksums, encryption, calendars, etc.). However, this principle has a recurring exception that needs to be acknowledged; namely, that advances in hardware must find their way promptly into the implementations of common algorithms.\nIn a 1996 article on hashing efficiency that I wrote for Dr. Dobb's , I discussed the effects of the then-significant problem of memory latency on the choice of hash table design. Basically, the concern was that every fetch of a hash bucket that was not in cache created a significant performance obstacle as the processor waited for the long memory-fetch cycle. This suggested that on closed hash tables, nonlinear rehashing until an open slot was found was a costly operation. Linear rehashing (finding the closest empty slot) worked better. The problem of memory latency and small caches, in those days, made algorithm and data structure selection a task best completed with care.\nThe expansion of processor caches changed the issue, especially insofar as algorithms were concerned. Unless you come from a comp-sci background, the terms \"cache-aware\" and \"cache-oblivious\" algorithms might be new to you. Implementation of the former tend to uncover the size of the cache on the runtime hardware and then size the data structures and algorithms to minimize memory fetches. Success in this can represent significant performance gains, at the cost of some portability. Some libraries, frequently those provided by processor vendors (such as Intel and AMD, in particular) or specialized development houses, provide these implementations. Intel's Integrated Performance Primitives library , for example, checks the runtime platform characteristics and brings in the right binaries for optimal performance.\nFor most applications, however, we're dependent on the standard libraries provided with the language. (Intel's IPP library, for example, comes only for native code. Java and .NET are supported only with wrappers.) Language providers eventually do deliver library updates, but the progress can be frustratingly slow and the work uneven. The delivery of Java's support for multithread-friendly collections is a case in point. Scala's multithreaded collections were a major draw because they came at a time when Java's collections did not work well enough.\nNot only are better libraries needed, but even within standard libraries, the choice of data structures is becoming more complex. In this excellent article explaining why linked lists are pass\u00e9 , Dr. Dobb's blogger Clay Breshears discusses why trees make a better and more parallel-friendly data structure than the ever-sequential linked list. This is exactly the kind of nuance that should keep us vigilant against lazily accepting a static view of which algorithms and data structures to choose. Everyone knows, don't they, that linked lists are faster than trees? And yet, even this mainstay of obvious logic is now changing beneath our feet.\nThe imminent era of manycore processors is likely to bring other changes to the fore. I especially expect that sort routines will be dramatically affected. Quicksort will no longer be the default sorting algorithm. The choice of sort will be more carefully matched to the needs of the data and the capabilities of the platform. We already see this on a macro level in the new world of big data. Map-reduce at scale depends upon sorts being done in smaller increments and reassembled through a merge function. And even there, the basic sorting has to be capable of handling billions of data items. In which case, grabbing an early item and making it the pivot element for millions of other entries (Quicksort) can have unfortunate consequences on performance.\nBetween the proliferation of cores, the rapid expansion of caches, the faster performance of RAM, and the huge increase in memory, the traditional choices of algorithms and data structures are no longer inherently safe or appropriate at all. Once again, selection must be made with considerable care aforethought.\n\u2014 Andrew Binstock\n"}, {"score": 867.8712, "uuid": "44d6872a-6cbd-50bd-99ac-b7a99952ec1c", "index": "cw12", "trec_id": "clueweb12-1805wb-30-02911", "target_hostname": "nzacditt.org.nz", "target_uri": "http://nzacditt.org.nz/resource/level-1-computer-science-concepts-concept-of-an-algorithm", "page_rank": 1.3901621e-09, "spam_rank": 85, "title": "[Level 1] Computer Science Concepts: Concept of an <em>Algorithm</em> | NZACDITT", "snippet": "Naps has Sorting Out Sorting, an interactive way to demonstrate some common <em>sort</em> <em>algorithms</em>: * Mr Barton Maths has a great resource called Sorting <em>Algorithms</em>, <em>which</em> <em>is</em> an impressive spreadsheet <em>which</em> covers bubble <em>sort</em>, shuttle <em>sort</em>, and other sorting <em>algorithms</em>. the original list of numbers can be edited", "explanation": null, "document": "Level 1\nWhat is this Resource?\nThis document was prepared by Sumant Murugesh, Tim Bell and Vanja Venrooy at the University of Canterbury. It is not an official document, but is offered as an evolving guide to the resources that are available for teaching the new material in NCEA Digital Technologies (currently focussed on the Programming and Computer Science strand). The structure has been based on various versions of the Body of Knowledge, proposed Standards, and Teaching and Learning guide; the material comes from an extensive search for relevant resources. It is our hope that this resource will evolve based on feedback from teachers, and ultimately end up as teaching plans that are built on the resources. Feedback can be sent to tim.bell [at] canterbury.ac.nz.\nAchievement Standard\nThe resources on this page relate mostly to the Digital Technologies Achievement Standard 1.44/AS91074 (Demonstrate understanding of basic concepts from computer science)\nObjectives\nDemonstrate an understanding of the distinguishing concepts of algorithms and programming languages from Computer Science and Software Engineering\nUnderstand what an algorithm is and how it is different from a program\nComparing and contrasting algorithms , programs , and informal instructions\nDescribing an algorithm for a task, showing understanding of the way steps can be combined in sequential, conditional , and iterative structures ( control flow )\nDetermine the cost of an iterative algorithm on a problem of size n\nDownload programs that implement different algorithms for the same problem, and compare how fast they are. For example, if students are using the Scratch language, there is a zip file of programs available that include several different implementations of searching and sorting algorithms.\nContext\nThe key thing at this level is to be able to compare different algorithms for the same task, such as linear and binary search, and consider how they differ as the amount of data being searched increases. This can be done in a variety of ways without even implementing the algorithms; it could be a simple analysis of the time taken to do an algorithm by hand (such as searching phone books of various sizes), or timing an on-screen animation where the algorithms \u201crace\u201d each other , or an offline activity such as the CSUnplugged \u201cBattleships\u201d game , where the algorithm becomes a strategy for winning a competition.\nIdeas for Teaching and Learning Activities\nEstimate how long it takes to look up a name in the local telephone book using a conventional (binary search) method compared with looking through every page starting at page 1 (linear search). Now consider how the two methods compare if the phone book was 10 times larger, or even a thousand times larger. (Linear search might take days or weeks, whereas binary search doesn't take much longer even for a book with millions of pages.)\nGive students a pile of 50 cards with numbers on them, and have a competition to see who can sort them the fastest. What are the best strategies?\nFind the sum of the first n whole numbers (1 + 2 + ... + n), either by hand or using a computer program. Contrast the amount of effort needed (e.g. in terms of arithmetic operations) using the obvious method of n-1 additions, with using a formula for the sum (n*(n+1)/2).\nExplore the purpose (not the details) of a wide variety of appropriate algorithms, such as search algorithm for sorted lists, sorting algorithms, graph algorithms for finding paths and checking connectedness, maze algorithms, long multiplication (normal and \u201crussian peasant\u201d), and greatest common divisor.\nGiven two lists of the number plates of every car that went down two streets, discuss algorithms to produce a single list of all cars that travelled down both roads. (There are several ways to do this, one of the fastest is to sort the two lists before comparing them; a slow one is to search one list for each item in the other list).\nOur Picks\nHere is a shortlist of resources we have picked from the comprehensive list below that were either developed for high school use or can be easily adapted for the purpose.\n"}, {"score": 866.72284, "uuid": "25c36431-bc90-5758-ae72-2d0234373b13", "index": "cw12", "trec_id": "clueweb12-1010wb-43-26421", "target_hostname": "www.victusspiritus.com", "target_uri": "http://www.victusspiritus.com/2011/03/17/brushing-up-on-computer-science-part-4-algorithms/", "page_rank": 1.2812285e-09, "spam_rank": 81, "title": "Brushing Up on Computer Science Part 4, <em>Algorithms</em> \u2014 Victus Spiritus", "snippet": "In simple pseudocode, the <em>algorithm</em> might be expressed as this: return array &#x2F;&#x2F; an array of zero <em>or</em> one elements <em>is</em> already sorted The <em>quicksort</em> on average requires O(n log n) comparisons and worse case O(n^2). The <em>merge</em> <em>sort</em> implements a divide and conquer strategy.", "explanation": null, "document": "( source )\nUnderstanding Strategies beats Memorizing Tactics\nAlgorithms are well specified techniques for performing an unbounded variety of tasks (good luck learning all algorithms). They are the tactics employed by computer scientists day in and day out, many times without even conscious awareness until further optimization is needed. When working on application specific problems at a high level, it\u2019s a distraction to constantly dive into deep operating system and compiler details.\nThere are broad patterns common to vastly separated problem spaces. These strategies serve as motivations for both design and implementation of algorithms. Divide and conquer and greedy techniques are deployed time and time again within algorithms.\nA couple of the most famous divide and conquer techniques are the FFT and MapReduce ( slides , web article ). The FFT iteratively breaks down the Discrete Fourier Transform into a series of odd and even components until it get\u2019s down to a single multiply and add.\nI first came across map reduce while reading a set of slides by Jeff Dean, Designs, Lessons and Advice from Building Large Distributed Systems .\nI didn\u2019t give it much thought at the time, as it appeared to be a good framework for implementing a divide and conquer strategy. A few months later I hit upon MR again while reading through CouchDB\u2019s architecture for retrieving documents. It\u2019s no surprise sharp folks utilize similar strategies for very different types of problems.\nGreedy algorithms make locally optimally decisions, often keeping several potential solutions, and generate a final decision when a forcing function requires an estimate in finite time. One example of a greedy algorithm is an implementation of the Multiple Hypothesis Tracker or MHT. The MHT associates combinations of measurements (factorial growth), calculates likelihood scores, and retains a set number of associations per iteration. At each phase there is always a set of best tracks that it can report out, yet there is a possibility that the global best solution will be pruned during an iteration (heuristics make no guarantees). For understanding the nuances of greedy strategies I defer to the expert, Boss Hog.\nSorting Algorithms\nI blame the cruel mistress entropy for the heroic efforts needed to repeatedly sort unorganized data, such is nature. The methods to sort data are as varied as there are ways to visualize it, each algorithm and its implementation is crafted with an artistic touch. If you\u2019re looking for a sort to fit your problem\u2019s constraints, an excellent starting point is the Wikipedia page which I quote shamelessly below. The gifs from Wikipedia display an animation of each algorithm in action. The gifs cycle through visualizations after a brief pause.\nQuicksort\nQuicksort sorts by employing a divide and conquer strategy to divide a list into two sub-lists.\nThe steps are:\n1)Pick an element, called a pivot, from the list.\n2) Reorder the list so that all elements with values less than the pivot come before the pivot, while all elements with values greater than the pivot come after it (equal values can go either way). After this partitioning, the pivot is in its final position. This is called the partition operation. Recursively sort the sub-list of lesser elements and the sub-list of greater elements. The base case of the recursion are lists of size zero or one, which never need to be sorted.\nSimple version\nIn simple pseudocode, the algorithm might be expressed as this:\n function quicksort(array)\n     var list less, greater\n     if length(array) \u2264 1\n         return array  // an array of zero or one elements is already sorted\n     select and remove a pivot value pivot from array\n     for each x in array\n         if x \u2264 pivot then append x to less\n         else append x to greater\n     return concatenate(quicksort(less), pivot, quicksort(greater))\nThe quicksort on average requires O(n log n) comparisons and worse case O(n^2).\nMerge sort\nThe merge sort implements a divide and conquer strategy. If the size of a sublist is 0 or 1 it\u2019s sorted. The merge sort breaks up a list into two sublists of equivalent size. Each sublist is then merged.\nThe following is a pseudo code example of the algorithm\u2019s implementation ( source )\nfunction merge_sort(m)\n    if length(m) \u2264 1\n        return m\n    var list left, right, result\n    var integer middle = length(m) / 2\n    for each x in m up to middle\n         add x to left\n    for each x in m after middle\n         add x to right\n    left = merge_sort(left)\n    right = merge_sort(right)\n    result = merge(left, right)\n    return result\n\nfunction merge(left,right)\n    var list result\n    while length(left) > 0 or length(right) > 0\n        if length(left) > 0 and length(right) > 0\n            if first(left) \u2264 first(right)\n                append first(left) to result\n                left = rest(left)\n            else\n                append first(right) to result\n                right = rest(right)\n        else if length(left) > 0\n            append first(left) to result\n            left = rest(left)\n        else if length(right) > 0\n            append first(right) to result\n            right = rest(right)\n    end while\n    return result\nThe merge sort has an average and worst case performance of O(nlogn).\nBubble sort\nThe bubble sort iterates through a collection swapping adjacent elements into a specified order. The process repeats itself until no further elements are swapped. The algorithms average and worst case performance is O(n^2)\nHeapsort\nThe heapsort finds an extreme element (largest or smallest) and places it at one end of the list, continuing until the entire list is sorted. The heap is a binary tree where the root node is larger than any of it\u2019s children, key(A) >= key(B). The heap is also a maximally efficient implementation called a priority queue. The average and worst case performance is O(nlogn).\nHeapsort begins by building a heap out of the data set, and then removing the largest item and placing it at the end of the partially sorted array. After removing the largest item, it reconstructs the heap, removes the largest remaining item, and places it in the next open position from the end of the partially sorted array. This is repeated until there are no items left in the heap and the sorted array is full. Elementary implementations require two arrays \u2013 one to hold the heap and the other to hold the sorted elements.\nIt\u2019s worth mentioning another heap, the Fibonacci Heap , which I\u2019ll come back to in the tomorrow\u2019s post on graphs (Dijkstra\u2019s search algorithm).\nPacking Problems\n( source )\nThe image says it all about packing problems. Your goal is to fit as many N-dimensional structures as possible into the smallest region possible, and the maximum density algorithm wins. If you\u2019re really into this type of problem, Dropbox is hiring.\nPacking problems are a class of optimization problems in recreational mathematics which involve attempting to pack objects together (often inside a container), as densely as possible. Many of these problems can be related to real life storage and transportation issues. Each packing problem has a dual covering problem, which asks how many of the same objects are required to completely cover every region of the container, where objects are allowed to overlap.\nIn a packing problem, you are given:\n\u2018containers\u2019 (usually a single two- or three-dimensional convex region, or an infinite space)\n\u2018goods\u2019 (usually a single type of shape), some or all of which must be packed into this container\nUsually the packing must be without overlaps between goods and other goods or the container walls. The aim is to find the configuration with the maximal density. In some variants the overlapping (of goods with each other and/or with the boundary of the container) is allowed but should be minimised.\n"}, {"score": 814.3728, "uuid": "e4c60fb7-1bec-531d-8bf0-bb1b8e03424c", "index": "cw12", "trec_id": "clueweb12-0400wb-56-08808", "target_hostname": "srfi.schemers.org", "target_uri": "http://srfi.schemers.org/srfi-32/srfi-32.txt", "page_rank": 1.2605034e-09, "spam_rank": 69, "title": "The SRFI-32 <em>sort</em> libraries -*- outline -*- Olin Shivers First draft: 1998", "snippet": "This <em>is</em> a bad idea -- non-local pointer chasing bad; vector shuffling good. vector &lt; <em>sort</em> vector-<em>or</em>-list &lt; <em>merge</em>-<em>sort</em> vector-<em>or</em>-list &lt; quick-<em>sort</em> vector-<em>or</em>-list &lt; Naive vector <em>quicksort</em>: loser, for worst-case performance reasons.", "explanation": null, "document": "The SRFI-32 sort libraries -*- outline -*- Olin Shivers First draft: 1998/10/19 Last update: 2002/7/21 Emacs should display this document in outline mode. Say c-h m for instructions on how to move through it by sections (e.g., c-c c-n, c-c c-p). * Table of contents ------------------- Abstract Procedure index Introduction What's wrong with the current state of affairs? Design rules What vs. how Consistency across function signatures Data parameter first, less-than parameter after Ordering, comparison functions & stability All vector operations accept optional subrange parameters Required vs. allowed side-effects Procedure specification Procedure naming and functionality Types of parameters and return values sort-lib - general sorting package Algorithm-specific sorting packages Algorithmic properties Topics to be resolved during discussion phase Porting and optimisation References & Links Acknowledgements Copyright * Abstract ---------- Current Scheme sorting packages are, every one of them, surprisingly bad. I've designed the API for a full-featured sort toolkit, which I propose as an SRFI. The spec comes with 1200 lines of high-quality reference code: tightly written, highly commented, portable code, available for free. Implementors want this code. It's better than what you have. ------------------------------------------------------------------------------- * Procedure index ----------------- list-sorted? vector-sorted? list-merge vector-merge list-sort vector-sort list-stable-sort vector-stable-sort list-delete-neighbor-dups vector-delete-neighbor-dups list-merge! vector-merge! list-sort! vector-sort! list-stable-sort! vector-stable-sort! list-delete-neighbor-dups! vector-delete-neighbor-dups! heap-sort quick-sort insert-sort list-merge-sort vector-merge-sort heap-sort! quick-sort! insert-sort! list-merge-sort! vector-merge-sort! ------------------------------------------------------------------------------- * Introduction -------------- As I'll detail bewlow, I wasn't very happy with the state of the Scheme world for sorting and merging lists and vectors. So I have designed and written a fairly comprehensive sorting & merging toolkit. It is - very portable, - much better code than what is currently in Elk, Gambit, Bigloo, Scheme->C, MzScheme, RScheme, Scheme48, MIT Scheme, or slib, and - priced to move: free code. The package includes - Vector insert sort (stable) - Vector heap sort - Vector quick sort (with median-of-3 pivot picking) - Vector merge sort (stable) - Pure and destructive list merge sort (stable) - Stable vector and list merge - Miscellaneous sort-related procedures: Vector and list merging, sorted? predicates, vector binary search, vector and list delete-equal-neighbor procedures. - A general, non-algorithmic set of procedure names for general sorting and merging. Scheme programmers may want to adopt this package. I'd like Scheme implementors to adopt this code and its API -- in fact, the code is a bribe to make it easy for implementors to converge on the suggested API. I mean, you'd really have to be a boor to take this free code I wrote and mutate its interface over to your incompatible, unportable API, wouldn't you? But you could, of course -- it's freely available. More in the spirit of the offering, you could make this API available, and then also write a little module providing your old interface that is defined in terms of this API. \"Scheme implementors,\" in this context, includes slib, which isn't really a standalone implementation of Scheme, but is an influential collection of API's and code. The code is tightly bummed. It is clearly written, and commented in my usual voluminous style. This includes notes on porting and implementation-specific optimisations. ------------------------------------------------------------------------------- * What's wrong with the current state of affairs? ------------------------------------------------- It's just amazing to me that in 2002, sorting and merging hasn't been completely put to bed. These are well-understood algorithms, each of them well under a page of code. The straightforward algorithms are basic, core stuff -- sophomore-level. But if you tour the major Scheme implementations out there on the Net, you find badly written code that provides extremely spotty coverage of the algorithm space. One implementation even has a buggy implementation that has been in use for about 20 years! Open source-code is a wonderful thing. In a couple of hours, I was able to download and check the sources of 9 Scheme systems. Here are my notes from the systems I checked. You can skip to the next section if you aren't morbidly curious. slib sorted? vector-or-list < merge list1 list2 < merge! list1 list2 < sort vector-or-list < sort! vector-or-list < Richard O'Keefe's stable list merge sort is right idea, but implemented using gratuitous variable side effects. It also does redundant SET-CDR!s. The vector sort converts to list, merge sorts, then reconverts to vector. This is a bad idea -- non-local pointer chasing bad; vector shuffling good. MIT Scheme sort! vector < merge-sort! vector < quick-sort! vector < sort vector-or-list < merge-sort vector-or-list < quick-sort vector-or-list < Naive vector quicksort: loser, for worst-case performance reasons. List sort by \"list->vector; quicksort; vector->list,\" hence also loser. A clever stable vector merge sort, albeit not very bummed. Scheme 48 & T sort-list list < sort-list! list < list-merge! list1 list2 < Bob Nix's implementation of online merge-sort, written in the early 80's. Conses unnecessary bookkeeping structure, which isn't necessary with a proper recursive formulation. Also, does redundant SET-CDR!s. No vector sort. Also, has a bug -- is claimed to be a stable sort, but isn't! To see this, get the S48 code, and try (define (my< x y) (< (quotient x 2) (quotient y 2))) (list-merge! (list 0 2) (list 3) my<) ; -> (0 2 3) (list-merge! (list 2) (list 0 3) my<) ; -> (0 3 2) This could be fixed very easily, but it isn't worth it given the other problems with the algorithm. RScheme vector-sort! vector < sort collection < Good basic implementation of vector heapsort, which has O(n lg n) worst-case time. Code ugly, needs tuning. List sort by \"list->vector; sort; vector->list\", which allocates unneeded temp storage. Nothing for stable sorting. MzScheme Naive quicksort -- but not available for vector sorting, even though it internally uses a vector. Nothing for stable sorting, and naive quicksort has bad worst-case behaviour. Bigloo, Scheme->C Couldn't find anything -- but maybe I didn't search for the right thing, since the Bigloo names are French. (I invite correction from the Bigloo implementors.) Gambit sort-list list < Nothing for vectors. Simple, slow, unstable merge sort for lists. Elk Another naive quicksort. Lists handled by converting to vector. sort vector-or-list < sort! vector-or-list < Chez Scheme merge < list1 list2 merge! < list1 list2 sort < list sort! < list These are stable. I have not seen the source code. Common Lisp sort sequence < [key] stable-sort sequence < [key] merge result-type sequence1 sequence2 < [key] The sort procedures are allowed, but not required, to be destructive. SML/NJ sort: ('a*'a -> bool) -> 'a list -> 'a list \"Smooth applicative merge sort,\" which is stable. There is also a highly bummed quicksort for vectors. The right solution: Implement a full toolbox of carefully written standard sort routines. Having the source available for all of these above-cited Schemes made life a lot easier writing this code. I appreciate the authors making their source available under such open terms. ------------------------------------------------------------------------------- * Design rules -------------- ** What vs. how =============== There are two different interfaces: \"what\" (simple) & \"how\" (detailed). - Simple: you specify semantics: datatype (list or vector), mutability, and stability. - Detailed: you specify the actual algorithm (quick, heap, insert, merge). Different algorithms have different properties, both semantic & pragmatic, so these exports are necessary. It is necessarily the case that the specifications of these procedures make statements about execution \"pragmatics.\" For example, the sole distinction between heap sort and quick sort -- both of which are provided by this library -- is one of execution time, which is not a \"semantic\" distinction. Similar resource-use statements are made about \"iterative\" procedures, meaning that they can execute on input of arbitrary size without needing to allocate an unbounded number of stack frames. ** Consistency across function signatures ========================================= The two interfaces share common function signatures wherever possible, to facilitate switching a given call from one procedure to another. ** Data parameter first, less-than parameter after ================================================== These procedures uniformly observe the following parameter order: the data to be sorted come before the the comparison function. That is, we write (sort lis <) not (sort < lis). This is consistent with every single implementation out there, with the sole exception of Chez Scheme. In my opinion, it would be more consistent with other Scheme libraries to put the ordering function first -- the \"operation currying\" convention. (E.g., consider FOR-EACH or MAP or FIND.) I decided to leave things as they are in favor of near-total backwards compatibility with existing practice. [Perhaps this should be discussed.] ** Ordering, comparison functions & stability ============================================= These routines take a < comparison function, not a <= comparison function, and they sort into increasing order. The difference between a < spec and a <= spec comes up in two places: - the definition of an ordered or sorted data set, and - the definition of a stable sorting algorithm. + We say that a data set (a list or vector) is *sorted* or *ordered* if it contains no adjacent pair of values ... X Y ... such that Y < X. In other words, scanning across the data never takes a \"downwards\" step. If you use a <= procedure where these algorithms expect a < procedure, you may not get the answers you expect. For example, the LIST-SORTED? function will return false if you pass it a <= comparison function and an ordered list containing adjacent equal elements. + A \"stable\" sort is one that preserves the pre-existing order of equal elements. Suppose, for example, that we sort a list of numbers by comparing their absolute values, i.e., using comparison function (lambda (x y) (< (abs x) (abs y))) If we sort a list that contains both 3 and -3: ... 3 ... -3 ... then a stable sort is an algorithm that will not swap the order of these two elements, that is, the answer will look like ... 3 -3 ... not ... -3 3 ... Choosing < for the comparison function instead of <= affects how stability is coded. Given an adjacent pair X Y, (< y x) means \"Y should be moved in front of X\" -- otherwise, leave things as they are. So using a <= function where a < function is expected will *invert* stability. This is due to the definition of equality, given a < comparator: (and (not (< x y)) (not (< y x))) The definition is rather different, given a <= comparator: (and (<= x y) (<= x y)) + A \"stable\" merge is one that reliably favors one of its data sets when equal items appear in both data sets. *All merge operations in this library are stable*, breaking ties between data sets in favor of the first data set -- elements of the first list come before equal elements in the second list. So, if we are merging two lists of numbers ordered by absolute value using the stable merge operation LIST-MERGE (list-merge '(0 -2 4 8 -10) '(-1 3 -4 7) (lambda (x y) (< (abs x) (abs y)))) reliably places the 4 of the first list before the equal-comparing -4 of the second list: (0 -1 -2 4 -4 7 8 -10) In short, if your comparison function F answers true to (F x x), then using a stable sorting or merging algorithm will not give you a stable sort or merge, and LIST-SORTED? may surprise you. Note that you can synthesize a < function from a <= function with (lambda (x y) (not (<= y x))) if need be. Precise definitions give sharp edges to tools, but require care in use. \"Measure twice, cut once.\" I have adopted the choice of < from Common Lisp. I assume they had a good reason for adopting < instead of <=. I'd love to know what this reason is; send me email if you can explain it, please. ** All vector operations accept optional subrange parameters ============================================================ The vector operations specified below all take optional START/END arguments indicating a selected subrange of a vector's elements. If a START parameter or START/END parameter pair is given to such a procedure, they must be exact, non-negative integers, such that 0 <= START <= END <= (VECTOR-LENGTH V) where V is the related vector parameter. If not specified, they default to 0 and the length of the vector, respectively. They are interpreted to select the range [START,END), that is, all elements from index START (inclusive) up to, but not including, index END. ** Required vs. allowed side-effects ==================================== LIST-SORT! and LIST-STABLE-SORT! are allowed, but not required, to alter their arguments' cons cells to construct the result list. This is consistent with the what-not-how character of the group of procedures to which they belong (the \"sort-lib\" package). The LIST-DELETE-NEIGHBOR-DUPS!, LIST-MERGE! and LIST-MERGE-SORT! procedures, on the other hand, provide specific algorithms, and, as such, explicitly commit to the use of side-effects on their input lists in order to guarantee their key algorithmic properties (e.g., linear-time operation, constant-space stack use). ------------------------------------------------------------------------------- * Procedure specification ------------------------- The procedures are split into several packages. In a Scheme system that has a module or package system, these procedures should be contained in modules named as follows: Package name Functionality ------------ ------------- sort-lib General sorting for lists & vectors sorted?-lib Sorted predicates for lists & vectors list-merge-sort-lib List merge sort vector-merge-sort-lib Vector merge sort vector-heap-sort-lib Vector heap sort vector-quick-sort-lib Vector quick sort vector-insert-sort-lib Vector insertion sort delndup-lib List and vector delete neighbor duplicates A Scheme system without a module system should provide all of the bindings defined in all of these modules as components of the \"SRFI-32\" package. Note that there is no list insert sort package, as you might as well always use list merge sort. The reference implementation's destructive list merge sort will do fewer SET-CDR!s than a destructive insert sort. ** Procedure naming and functionality ===================================== Almost all of the procedures described below are variants of two basic operations: sorting and merging. These procedures are consistently named by composing a set of basic lexemes to indicate what they do. Lexeme Meaning ------ ------- \"sort\" The procedure sorts its input data set by some < comparison function. \"merge\" The procedure merges two ordered data sets into a single ordered result. \"stable\" This lexeme indicates that the sort is a stable one. \"vector\" The procedure operates upon vectors. \"list\" The procedure operates upon lists. \"!\" Procedures that end in \"!\" are allowed, and sometimes required, to reuse their input storage to construct their answer. ** Types of parameters and return values ======================================== In the procedures specified below, - A LIS parameter is a list; - A V parameter is a vector; - A < or = parameter is a procedure accepting two arguments taken from the specified procedure's data set(s), and returning a boolean; - START and END parameters are exact, non-negative integers that serve as vector indices selecting a subrange of some associated vector. When specified, they must satisfy the relation 0 <= start <= end <= (vector-length v) where V is the associated vector. Passing values to procedures with these parameters that do not satisfy these types is an error. If a procedure is said to return \"unspecified,\" this means that nothing at all is said about what the procedure returns, not even the number of return values. Such a procedure is not even required to be consistent from call to call in the nature or number of its return values. It is simply required to return a value (or values) that may be passed to a command continuation, e.g. as the value of an expression appearing as a non-terminal subform of a BEGIN expression. Note that in R5RS, this restricts such a procedure to returning a single value; non-R5RS systems may not even provide this restriction. ** sort-lib - general sorting package ===================================== This library provides basic sorting and merging functionality suitable for general programming. The procedures are named by their semantic properties, i.e., what they do to the data (sort, stable sort, merge, and so forth). Procedure Suggested algorithm ------------------------------------------------------------------------- list-sorted? lis < -> boolean list-merge lis1 lis2 < -> list list-merge! lis1 lis2 < -> list list-sort lis < -> list (vector heap or quick) list-sort! lis < -> list (list merge sort) list-stable-sort lis < -> list (vector merge sort) list-stable-sort! lis < -> list (list merge sort) list-delete-neighbor-dups lis = -> list list-delete-neighbor-dups! lis = -> list vector-sorted? v < [start end] -> boolean vector-merge v1 v2 < [start1 end1 start2 end2] -> vector vector-merge! v v1 v2 < [start start1 end1 start2 end2] -> unspecific vector-sort v < [start end] -> vector (heap or quick sort) vector-sort! v < [start end] -> unspecific (heap or quick sort) vector-stable-sort v < [start end] -> vector (vector merge sort) vector-stable-sort! v < [start end] -> unspecific (vector merge sort) vector-delete-neighbor-dups v = [start end] -> vector vector-delete-neighbor-dups! v = [start end] -> end' LIST-SORTED? and VECTOR-SORTED? return true if their input list or vector is in sorted order, as determined by their < comparison parameter. All four merge operations are stable: an element of the initial list LIS1 or vector V1 will come before an equal-comparing element in the second list LIS2 or vector V2 in the result. The procedures LIST-MERGE LIST-SORT LIST-STABLE-SORT LIST-DELETE-NEIGHBOR-DUPS do not alter their inputs and are allowed to return a value that shares a common tail with a list argument. The procedures LIST-SORT! LIST-STABLE-SORT! are \"linear update\" operators -- they are allowed, but not required, to alter the cons cells of their arguments to produce their results. On the other hand, the procedures LIST-DELETE-NEIGHBOR-DUPS! LIST-MERGE! make only a single, iterative, linear-time pass over their argument lists, using SET-CDR!s to rearrange the cells of the lists into the final result -- they work \"in place.\" Hence, any cons cell appearing in the result must have originally appeared in an input. The intent of this iterative-algorithm commitment is to allow the programmer to be sure that if, for example, LIST-MERGE! is asked to merge two ten-million-element lists, the operation will complete without performing some extremely (possibly twenty-million) deep recursion. The vector procedures VECTOR-SORT VECTOR-STABLE-SORT VECTOR-DELETE-NEIGHBOR-DUPS do not alter their inputs, but allocate a fresh vector for their result, of length END-START. The vector procedures VECTOR-SORT! VECTOR-STABLE-SORT! sort their data in-place. (But note that VECTOR-STABLE-SORT! may allocate temporary storage proportional to the size of the input -- I am not aware of O(n lg n) stable vector sorting algorithms that run in constant space.) VECTOR-MERGE returns a vector of length (END1-START1)+(END2-START2). VECTOR-MERGE! writes its result into vector V, beginning at index START0, for indices less than END0 = START0 + (END1-START1) + (END2-START2). The target subvector V[start0,end0) may not overlap either source subvector V1[start1,end1) V2[start2,end2). The DELETE-NEIGHBOR-DUP-... procedures: These procedures delete adjacent duplicate elements from a list or a vector, using a given element-equality procedure. The first/leftmost element of a run of equal elements is the one that survives. The list or vector is not otherwise disordered. These procedures are linear time -- much faster than the O(n^2) general duplicate-element deletors that do not assume any \"bunching\" of elements (such as the ones provided by SRFI-1). If you want to delete duplicate elements from a large list or vector, sort the elements to bring equal items together, then use one of these procedures, for a total time of O(n lg n). The comparison function = passed to these procedures is always applied (= x y) where X comes before Y in the containing list or vector. - LIST-DELETE-NEIGHBOR-DUPS does not alter its input list; its answer may share storage with the input list. - VECTOR-DELETE-NEIGHBOR-DUPS does not alter its input vector, but rather allocates a fresh vector to hold the result. - LIST-DELETE-NEIGHBOR-DUPS! is permitted, but not required, to mutate its input list in order to construct its answer. - VECTOR-DELETE-NEIGHBOR-DUPS! reuses its input vector to hold the answer, packing its answer into the index range [start,end'), where END' is the non-negative exact integer returned as its value. It returns END' as its result. The vector is not altered outside the range [start,end'). [Maybe this procedure should take a \"target\" vector to write?] - Examples: (list-delete-neighbor-dups '(1 1 2 7 7 7 0 -2 -2) =) => (1 2 7 0 -2) (vector-delete-neighbor-dups '#(1 1 2 7 7 7 0 -2 -2) =) => #(1 2 7 0 -2) (vector-delete-neighbor-dups '#(1 1 2 7 7 7 0 -2 -2) = 3 7) => #(7 0 -2) ;; Result left in v[3,9): (let ((v (vector 0 0 0 1 1 2 2 3 3 4 4 5 5 6 6))) (cons (vector-delete-neighbor-dups! v = 3) v)) => (9 . #(0 0 0 1 2 3 4 5 6 4 4 5 5 6 6)) ** Algorithm-specific sorting packages ====================================== These packages provide more specific sorting functionality, that is, specific committment to particular algorithms that have particular pragmatic consequences (such as memory locality, asymptotic running time) beyond their semantic behaviour (sorting, stable sorting, merging, etc.). Programmers that need a particular algorithm can use one of these packages. sorted?-lib - sorted predicates list-sorted? lis < -> boolean vector-sorted? v < [start end] -> boolean Return #f iff there is an adjacent pair ... X Y ... in the input list or vector such that Y < X. The optional START/END range arguments restrict VECTOR-SORTED? to the indicated subvector. list-merge-sort-lib - list merge sort list-merge-sort lis < -> list list-merge-sort! lis < -> list list-merge lis1 lis2 < -> list list-merge! lis1 lis2 < -> list The sort procedures sort their data using a list merge sort, which is stable. (The reference implementation is, additionally, a \"natural\" sort. See below for the properties of this algorithm.) The ! procedures are destructive -- they use SET-CDR!s to rearrange the cells of the lists into the proper order. As such, they do not allocate any extra cons cells -- they are \"in place\" sorts. Additionally, LIST-MERGE! is iterative, not recursive -- it can operate on arguments of arbitrary size without requiring an unbounded amount of stack space. The merge operations are stable: an element of LIS1 will come before an equal-comparing element in LIS2 in the result list. vector-merge-sort-lib - vector merge sort vector-merge-sort v < [start end temp] -> vector vector-merge-sort! v < [start end temp] -> unspecific vector-merge v1 v2 < [start1 end1 start2 end2] -> vector vector-merge! v v1 v2 < [start0 start1 end1 start2 end2] -> unspecific The sort procedures sort their data using vector merge sort, which is stable. (The reference implementation is, additionally, a \"natural\" sort. See below for the properties of this algorithm.) The optional START/END arguments provide for sorting of subranges, and default to 0 and the length of the corresponding vector. Merge-sorting a vector requires the allocation of a temporary \"scratch\" work vector for the duration of the sort. This scratch vector can be passed in by the client as the optional TEMP argument; if so, the supplied vector must be of size >= END, and will not be altered outside the range [start,end). If not supplied, the sort routines allocate one themselves. The merge operations are stable: an element of V1 will come before an equal-comparing element in V2 in the result vector. VECTOR-MERGE-SORT! leaves its result in V[start,end). VECTOR-MERGE-SORT returns a vector of length END-START. VECTOR-MERGE returns a vector of length (END1-START1)+(END2-START2). VECTOR-MERGE! writes its result into vector V, beginning at index START0, for indices less than END0 = START0 + (END1-START1) + (END2-START2). The target subvector V[start0,end0) may not overlap either source subvector V1[start1,end1) V2[start2,end2). vector-heap-sort-lib - vector heap sort heap-sort v < [start end] -> vector heap-sort! v < [start end] -> unspecific These procedures sort their data using heap sort, which is not a stable sorting algorithm. HEAP-SORT returns a vector of length END-START. HEAP-SORT! is in-place, leaving its result in V[start,end). vector-quick-sort-lib - vector quick sort quick-sort v < [start end] -> vector quick-sort! v < [start end] -> unspecific These procedures sort their data using quick sort, which is not a stable sorting algorithm. QUICK-SORT returns a vector of length END-START. QUICK-SORT! is in-place, leaving its result in V[start,end). vector-insert-sort-lib - vector insertion sort insert-sort v < [start end] -> vector insert-sort! v < [start end] -> unspecific These procedures stably sort their data using insertion sort. INSERT-SORT returns a vector of length END-START. INSERT-SORT! is in-place, leaving its result in V[start,end). delndup-lib - list and vector delete neighbor duplicates list-delete-neighbor-dups lis = -> list list-delete-neighbor-dups! lis = -> list vector-delete-neighbor-dups v = [start end] -> vector vector-delete-neighbor-dups! v = [start end] -> end' These procedures delete adjacent duplicate elements from a list or a vector, using a given element-equality procedure =. The first/leftmost element of a run of equal elements is the one that survives. The list or vector is not otherwise disordered. These procedures are linear time -- much faster than the O(n^2) general duplicate-element deletors that do not assume any \"bunching\" of elements (such as the ones provided by SRFI-1). If you want to delete duplicate elements from a large list or vector, sort the elements to bring equal items together, then use one of these procedures, for a total time of O(n lg n). The comparison function = passed to these procedures is always applied (= x y) where X comes before Y in the containing list or vector. LIST-DELETE-NEIGHBOR-DUPS does not alter its input list; its answer may share storage with the input list. VECTOR-DELETE-NEIGHBOR-DUPS does not alter its input vector, but rather allocates a fresh vector to hold the result. LIST-DELETE-NEIGHBOR-DUPS! is permitted, but not required, to mutate its input list in order to construct its answer. VECTOR-DELETE-NEIGHBOR-DUPS! reuses its input vector to hold the answer, packing its answer into the index range [start,end'), where END' is the non-negative exact integer returned as its value. It returns END' as its result. The vector is not altered outside the range [start,end'). Examples: (list-delete-neighbor-dups '(1 1 2 7 7 7 0 -2 -2) =) => (1 2 7 0 -2) (vector-delete-neighbor-dups '#(1 1 2 7 7 7 0 -2 -2) =) => #(1 2 7 0 -2) (vector-delete-neighbor-dups '#(1 1 2 7 7 7 0 -2 -2) = 3 7) => #(7 0 -2) ;; Result left in v[3,9): (let ((v (vector 0 0 0 1 1 2 2 3 3 4 4 5 5 6 6))) (cons (vector-delete-neighbor-dups! v = 3) v)) => (9 . #(0 0 0 1 2 3 4 5 6 4 4 5 5 6 6)) ------------------------------------------------------------------------------- * Algorithmic properties ------------------------ Different sort and merge algorithms have different properties. Choose the algorithm that matches your needs: Vector insert sort Stable, but only suitable for small vectors -- O(n^2). Vector quick sort Not stable. Is fast on average -- O(n lg n) -- but has bad worst-case behaviour. Has good memory locality for big vectors (unlike heap sort). A clever pivot-picking trick (median of three samples) helps avoid worst-case behaviour, but pathological cases can still blow up. Vector heap sort Not stable. Guaranteed fast -- O(n lg n) *worst* case. Poor locality on large vectors. A very reliable workhorse. Vector merge sort Stable. Not in-place -- requires a temporary buffer of equal size. Fast -- O(n lg n) -- and has good memory locality for large vectors. The implementation of vector merge sort provided by this SRFI's reference implementation is, additionally, a \"natural\" sort, meaning that it exploits existing order in the input data, providing O(n) best case. Destructive list merge sort Stable, fast and in-place (i.e., allocates no new cons cells). \"Fast\" means O(n lg n) worse-case, and substantially better if the data is already mostly ordered, all the way down to linear time for a completely-ordered input list (i.e., it is a \"natural\" sort). Note that sorting lists involves chasing pointers through memory, which can be a loser on modern machine architectures because of poor cache & page locality. Pointer *writing*, which is what the SET-CDR!s of a destructive list-sort algorithm do, is even worse, especially if your Scheme has a generational GC -- the writes will thrash the write-barrier. Sorting vectors has inherently better locality. This SRFIs destructive list merge and merge sort implementations are opportunistic -- they avoid redundant SET-CDR!s, and try to take long already-ordered runs of list structure as-is when doing the merges. Pure list merge sort Stable and fast -- O(n lg n) worst-case, and possibly better, depending upon the input list (see above). Algorithm Stable? Worst case Average case In-place ------------------------------------------------------ V insert Yes O(n^2) O(n^2) Yes V quick No O(n^2) O(n lg n) Yes V heap No O(n lg n) O(n lg n) Yes V merge Yes O(n lg n) O(n lg n) No L merge Yes O(n lg n) O(n lg n) Either ------------------------------------------------------------------------------- * Topics to be resolved during discussion phase ----------------------------------------------- I particularly solicit comments about the following topics. - Include VECTOR-BINARY-SEARCH ? Should we include (VECTOR-BINARY-SEARCH v key< elt->key key [start end]) in the SRFI? It sort of goes with sorting; it's exactly ten lines of code. - Comparison function before or after the list/vector argument? Should it be (list-sort < lis) or (list-sort lis <) There is overwhelming consistency among the implementations: data first, < after. Only Chez does it differently. I have done it in the backwards-compatible way. But I prefer the < first, data after way. ------------------------------------------------------------------------------- * Porting and optimisation -------------------------- This package should be trivial to port. There are only four non-R4RS bits in the code: - Use of multiple-value return, with the R5RS VALUES procedure, and the simple (RECEIVE (var ...) mv-exp body ...) multiple-value binding macro. - A VECTOR-COPY procedure. This is a tiny little procedure: (vector-copy v [start end]) - Use of the LET-OPTIONALS macro from scsh to parse and default optional arguments to three routines. Again, easy to port the macro or rewrite the code to parse, default, and error check the args by hand. - Calls to an ERROR function for complaining about bad arguments. This code is tightly bummed, as far as I can go in portable Scheme. You could speed up the vector code a lot by error-checking the procedure parameters and then shifting over to fixnum-specific arithmetic and dangerous vector-indexing and vector-setting primitives. The comments in the code indicate where the initial error checks would have to be added. There are several (QUOTIENT N 2)'s that could be changed to a fixnum right-shift, as well, in both the list and vector code. The code is designed to enable this -- each file usually exports one or two \"safe\" procedures that end up calling an internal \"dangerous\" primitive. The little exported cover procedures are where you move the error checks. This should provide *big* speedups. In fact, all the code bumming I've done pretty much disappears in the noise unless you have a good compiler and also can dump the vector-index checks and generic arithmetic -- so I've really just set things up for you to exploit. The optional-arg parsing, defaulting, and error checking is done with a portable R4RS macro. But if your Scheme has a faster mechanism (e.g., Chez), you should definitely port over to it. Note that argument defaulting and error-checking are interleaved -- you don't have to error-check defaulted START/END args to see if they are fixnums that are legal vector indices for the corresponding vector, etc. ------------------------------------------------------------------------------- * References & Links -------------------- This document, in HTML: http://srfi.schemers.org/srfi-32/srfi-32.html [This link may not be valid while the SRFI is in draft form.] This document, in simple text format: http://srfi.schemers.org/srfi-32/srfi-32.txt Archive of SRFI-32 discussion-list email: http://srfi.schemers.org/srfi-32/mail-archive/maillist.html SRFI web site: http://srfi.schemers.org/ [CommonLisp] Common Lisp: the Language Guy L. Steele Jr. (editor). Digital Press, Maynard, Mass., second edition 1990. Available at http://www.elwood.com/alu/table/references.htm#cltl2 The Common Lisp \"HyperSpec,\" produced by Kent Pitman, is essentially the ANSI spec for Common Lisp: http://www.xanalys.com/software_tools/reference/HyperSpec/ [R5RS] Revised^5 Report on the Algorithmic Language Scheme, R. Kelsey, W. Clinger, J. Rees (editors). Higher-Order and Symbolic Computation, Vol. 11, No. 1, September, 1998. and ACM SIGPLAN Notices, Vol. 33, No. 9, October, 1998. Available at http://www.schemers.org/Documents/Standards/ ------------------------------------------------------------------------------- * Acknowledgements ------------------ I thank the authors of the open source I consulted when designing this library, particularly Richard O'Keefe, Donovan Kolby and the MIT Scheme Team. ------------------------------------------------------------------------------- * Copyright ----------- ** SRFI text ============ This document is copyright (C) Olin Shivers (1998, 1999). All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ** Reference implementation =========================== Short summary: no restrictions. While I wrote all of this code myself, I read a lot of code before I began writing. However, all such code is, itself, either open source or public domain, rendering irrelevant any issue of \"copyright taint.\" The natural merge sorts (pure list, destructive list, and vector) are not only my own code, but are implementations of an algorithm of my own devising. They run in O(n lg n) worst case, O(n) best case, and require only a logarithmic number of stack frames. And they are stable. And the destructive-list variant allocates zero cons cells; it simply rearranges the cells of the input list. Hence the reference implementation is Copyright (c) 1998 by Olin Shivers. and made available under the same copyright as the SRFI text (see above).\n"}, {"score": 807.95294, "uuid": "6361e3a4-82e2-5d7f-8867-9673c8374193", "index": "cw12", "trec_id": "clueweb12-0611wb-31-13816", "target_hostname": "shop.oreilly.com", "target_uri": "http://shop.oreilly.com/product/9781565924536.do", "page_rank": 1.21072e-09, "spam_rank": 67, "title": "Mastering <em>Algorithms</em> with C - O&#x27;Reilly Media", "snippet": "The sea horse&#x27;s unusual-looking body <em>is</em> formed by 50 <em>or</em> so bony plates that encircle the body to create rings of armor. Within their narrow snouts are tubes through <em>which</em> they feed, sucking in plankton and tiny fish larvae.", "explanation": null, "document": "0-596-10343-3\nKyle Loudon\nKyle Loudon is a software engineer at Matrix Semiconductor in Santa Clara, California, where he works with file systems and applications for memory chips. Prior to Matrix, Kyle developed platform software for embedded devices, including various wireless phones and the Apple iPod. He also led the graphical user interface group at Jeppesen Dataplan (now a part of Boeing), developed flight planning software, and created system software at IBM in the early 1990s. For the past several years, Kyle has taught object-oriented programming using C++ at the University of California, Santa Cruz Extension, and has worked with C++ since the beginning of its widespread use in 1990. Kyle is the author of Mastering Algorithms with C, also published by O'Reilly and Associates.\nView Kyle Loudon's full profile page.\nColophon\nOur look is the result of reader comments, our own experimentation, and feedback from distribution channels. Distinctive covers complement our distinctive approach to technical topics, breathing personality and life into potentially dry subjects. The animals on the cover of Mastering Algorithms with C are sea horses. Sea horses are of the family Syngnathidae, genus Hippocampus. The word \"hippocampus\" comes from the Greek for \"bent horse.\" The sea horse's unusual-looking body is formed by 50 or so bony plates that encircle the body to create rings of armor. Within their narrow snouts are tubes through which they feed, sucking in plankton and tiny fish larvae. The male sea horse has a pouch in his belly, into which a female lays 100 or more eggs at a time. The male fertilizes the eggs in the pouch and carries them until they hatch, ten days to six weeks later, depending on the sea horse species. Sea horses are found mostly in shallow tropical and subtropical waters, although there are some ocean-dwelling sea horse species. All sea horses use their pelvic and pectoral fins for steering. They swim slowly, in an upright position, and take frequent breaks. During these breaks they wrap their prehensile tails around a piece of seaweed or coral to anchor themselves. In addition to providing a resting place, the seaweed and coral provide good camouflage for the sea horse. The largest sea horse species is the Pacific sea horse, measuring approximately 12 inches long. The smallest is the dwarf sea horse, which measures only an inch and a half long. Jeffrey Liggett was the production editor for Mastering Algorithms with C; Cindy Kogut was the copyeditor; Ellie Fountain Maden was the proofreader; Sebastian Banker provided production assistance; Claire Cloutier LeBlanc, Nancy Wolfe Kotary, and Melanie Wang provided quality control. Robert Romano and Rhon Porter created the illustrations using Adobe Photoshop 5 and Macromedia FreeHand 8. Mike Sierra provided FrameMaker technical support. William Meyers wrote the index with production assistance by Seth Maislin and Brenda Miller. Hanna Dyer designed the cover of this book based on a series design by Edie Freedman. The illustration is by Lorrie LeJeune. The cover layout was produced by Kathleen Wilson with QuarkXPress 3.32 using the ITC Garamond font. Kathleen Wilson designed the diskette label. Whenever possible, our books use RepKover, a durable and flexible lay-flat binding. If the page count exceeds RepKover's limit, perfect binding is used. The inside layout was designed by Alicia Cech based on a series design by Nancy Priest. The inside layout was implemented in FrameMaker 5.5.6 by Mike Sierra. The text and heading fonts are ITC Garamond Light and Garamond Book. This colophon was written by Clairemarie Fisher O'Leary.\nDescription\n"}, {"score": 802.53394, "uuid": "c129194b-719b-5027-9253-4023c284758a", "index": "cw12", "trec_id": "clueweb12-1415wb-96-28869", "target_hostname": "blog.itu.dk", "target_uri": "https://blog.itu.dk/BADS-F2009/", "page_rank": 1.1700305e-09, "spam_rank": 73, "title": "<em>Algorithms</em> and Data Structures Spring 2009", "snippet": "MIT, Introduction to Algortihms, Lecture 1 introduces asymptotic growth, analyzes insertion <em>sort</em> and <em>merge</em> <em>sort</em> (fast forward the first 17 minutes if you want to skip the announcements). YouTube has videos from many other universities.", "explanation": null, "document": "I gathered several points around the implementations\nIt is a frustrating experience to get anything accepted by the judge.\nThis experience is not very different than when you program against any other existing components in real life. It is very frequent you get almost no error message, things just do not behave as they should. So it is a useful thing to develop several habits that can help you in debugging such things. I list some.\nUnderstand the very few error messages that the online judge gives you. See what kind of messages you get when:\nprogram does not compile (send them some garbage)\nprogram does something illegal (try opening a file)\nprogram crashes (just throw an exception as the first thing)\nprogram produces an empty output (just return from main)\nprogram produces some nonsense output (just print \u201cI love algorithms and data structures !\u201d)\nprogram loops forever (create an infinite loop)\nCompare with the message you get for your seemingly good implementation. How do they compare? Sometimes this gives a hint.\nCheck the specification meticulously. Almost every single word has a meaning. Read it slowly, and check whether it complies.\nIn particular be very careful about whitespace and new lines. There should be no less and no more white space characters in the output than asked for. Sometimes it is unclear whether they want any newline characters (one, two or zero) after the last nonwhitespace character in the output. Check all these possibilities. The rule of thumb is that one newline character is needed to end the line, and only if they ask for an empty line after the output then you should add another one.\nVery importantly, test your program well.\nWhite box: use unit testing heavily on your solution (see http://junit.org ). Testing by submitting to online judge is very ineffective, so test well locally first. This is much faster.\nBlack box: test all border conditions, like minimum and maximum values of input parameters, etc. If the problem specification allows an empty input then test on empty input also. For example, in minesweeper a single cell board is probably an interesting case. Your program has to work well on all inputs that comply with the specification.\nFollow coding guidelines:\nThe code must be Java 1.2 compliant (so no generics etc)\nThe input is to be read via System.in\nThe output is to be written to System.out\nYou may not use any public classes\nThe program must not make use of any file or network access Java packages.\nThe entry point must be \u2018public static void main(String[] args)\u2019 in the class \u2018Main\u2019.\nAll classes should be placed in the default package (since you can only submit a single file)\nuse the template at: http://www.itu.dk/courses/SPT/E2007/Contest-Fall07/Template.java\nCheck another template available from the judge website.\nSome of this guidelines might be outdated, but they are conservatively safe\nThe deadline for implementation is not Thursday morning next week, so if you get stuck take time to interact with me and TAs.\n"}], [{"score": 760.248, "uuid": "53688a66-80cb-5ccc-b3e6-f098dcd43da6", "index": "cw12", "trec_id": "clueweb12-0916wb-81-11204", "target_hostname": "nltk.googlecode.com", "target_uri": "http://nltk.googlecode.com/svn/trunk/doc/book/ch06-extras.html", "page_rank": 1.1814556e-09, "spam_rank": 68, "title": "Writing Structured Programs This chapter introduces concepts in <em>algorithms</em>", "snippet": "Evidently <em>merge</em> <em>sort</em> <em>is</em> much <em>better</em> than bubble <em>sort</em>, and <em>quicksort</em> <em>is</em> <em>better</em> still. Readers are encouraged to look at nltk.misc.sort to see how these different methods work.", "explanation": null, "document": "Chinese and XML\nCodecs for processing Chinese text have been incorporated into Python (since version 2.4).\n>>> path = nltk.data.find('samples/sinorama-gb.xml') >>> f = codecs.open(path, encoding='gb2312') >>> lines = f.readlines() >>> for l in lines: ... l = l[:-1] ... utf_enc = l.encode('utf8') ... print repr(utf_enc) '<?xml version=\"1.0\" encoding=\"gb2312\" ?>' '' '<sent>' '\\xe7\\x94\\x9a\\xe8\\x87\\xb3\\xe7\\x8c\\xab\\xe4\\xbb\\xa5\\xe4\\xba\\xba\\xe8\\xb4\\xb5' '' 'In some cases, cats were valued above humans.' '</sent>'\nWith appropriate support on your terminal, the escaped text string inside the <SENT> element above will be rendered as the following string of ideographs: \u751a\u81f3\u732b\u4ee5\u4eba\u8d35.\nWe can also read in the contents of an XML file using the etree package (at least, if the file is encoded as UTF-8 \u2014 as of writing, there seems to be a problem reading GB2312-encoded files in etree).\n>>> path = nltk.data.find('samples/sinorama-utf8.xml') >>> from nltk.etree import ElementTree as ET >>> tree = ET.parse(path) >>> text = tree.findtext('sent') >>> uni_text = text.encode('utf8') >>> print repr(uni_text.splitlines()[1]) '\\xe7\\x94\\x9a\\xe8\\x87\\xb3\\xe7\\x8c\\xab\\xe4\\xbb\\xa5\\xe4\\xba\\xba\\xe8\\xb4\\xb5'\n1.2\u00a0\u00a0\u00a0Algorithm Design\nAn algorithm is a \"recipe\" for solving a problem. For example, to multiply 16 by 12 we might use any of the following methods:\nAdd 16 to itself 12 times over\nPerform \"long multiplication\", starting with the least-significant digits of both numbers\nLook up a multiplication table\nRepeatedly halve the first number and double the second, 16*12 = 8*24 = 4*48 = 2*96 = 192\nDo 10*12 to get 120, then add 6*12\nRewrite 16*12 as (x+2)(x-2), remember that 14*14=196, and add (+2)(-2) = -4\nEach of these methods is a different algorithm, and requires different amounts of computation time and different amounts of intermediate information to store. A similar situation holds for many other superficially simple tasks, such as sorting a list of words.\nSorting Algorithms\nNow, as we saw above, Python provides a built-in function sort() that performs this task efficiently. However, NLTK also provides several algorithms for sorting lists, to illustrate the variety of possible methods. To illustrate the difference in efficiency, we will create a list of 1000 numbers, randomize the list, then sort it, counting the number of list manipulations required.\n>>> from random import shuffle >>> a = range(1000) # [0,1,2,...999] >>> shuffle(a) # randomize\nNow we can try a simple sort method called bubble sort, that scans through the list many times, exchanging adjacent items if they are out of order. It sorts the list a in-place, and returns the number of times it modified the list:\n>>> from nltk.misc import sort >>> sort.bubble(a) 250918\nWe can try the same task using various sorting algorithms. Evidently merge sort is much better than bubble sort, and quicksort is better still.\n>>> shuffle(a); sort.merge(a) 6175 >>> shuffle(a); sort.quick(a) 2378\nReaders are encouraged to look at nltk.misc.sort to see how these different methods work. The collection of NLTK modules exemplify a variety of algorithm design techniques, including brute-force, divide-and-conquer, dynamic programming, and greedy search. Readers who would like a systematic introduction to algorithm design should consult the resources mentioned at the end of this tutorial.\nDecorate-Sort-Undecorate\nIn Chapter 6 we saw how to sort a list of items according to some property of the list.\n>>> words = 'I turned off the spectroroute'.split() >>> words.sort(cmp) >>> words ['I', 'off', 'spectroroute', 'the', 'turned'] >>> words.sort(lambda x, y: cmp(len(y), len(x))) >>> words ['spectroroute', 'turned', 'off', 'the', 'I']\nThis is inefficient when the list of items gets long, as we compute len() twice for every comparison (about 2nlog(n) times). The following is more efficient:\n>>> [pair[1] for pair in sorted((len(w), w) for w in words)[::-1]] ['spectroroute', 'turned', 'the', 'off', 'I']\nThis technique is called decorate-sort-undecorate. We can compare its performance by timing how long it takes to execute it a million times.\n>>> from timeit import Timer >>> Timer(\"sorted(words, lambda x, y: cmp(len(y), len(x)))\", ... \"words='I turned off the spectroroute'.split()\").timeit() 8.3548779487609863 >>> Timer(\"[pair[1] for pair in sorted((len(w), w) for w in words)]\", ... \"words='I turned off the spectroroute'.split()\").timeit() 9.9698889255523682\nMORE: consider what happens as the lists get longer...\nAnother example: sorting dates of the form \"1 Jan 1970\"\n>>> month_index = { ... \"Jan\" : 1, \"Feb\" : 2, \"Mar\" : 3, \"Apr\" : 4, ... \"May\" : 5, \"Jun\" : 6, \"Jul\" : 7, \"Aug\" : 8, ... \"Sep\" : 9, \"Oct\" : 10, \"Nov\" : 11, \"Dec\" : 12 ... } >>> def date_cmp(date_string1, date_string2): ... (d1,m1,y1) = date_string1.split() ... (d2,m2,y2) = date_string2.split() ... conv1 = y1, month_index[m1], d1 ... conv2 = y2, month_index[m2], d2 ... return cmp(a2, b2) >>> sort(date_list, date_cmp)\nThe comparison function says that we compare two times of the form ('Mar', '2004') by reversing the order of the month and year, and converting the month into a number to get ('2004', '3'), then using Python's built-in cmp function to compare them.\nNow do this using decorate-sort-undecorate, for large data size\nTime comparison\nBrute Force\nWordfinder Puzzle\nHere we will generate a grid of letters, containing words found in the dictionary. First we remove any duplicates and disregard the order in which the lexemes appeared in the dictionary. We do this by converting it to a set, then back to a list. Then we select the first 200 words, and then only keep those words having a reasonable length.\n>>> words = list(set(lexemes)) >>> words = words[:200] >>> words = [w for w in words if 3 <= len(w) <= 12]\nNow we generate the wordfinder grid, and print it out.\n>>> from nltk.misc.wordfinder import wordfinder >>> grid, used = wordfinder(words) >>> for i in range(len(grid)): ... for j in range(len(grid[i])): ... print grid[i][j], ... print O G H K U U V U V K U O R O V A K U N C K Z O T O I S E K S N A I E R E P A K C I A R A A K I O Y O V R S K A W J K U Y L R N H N K R G V U K G I A U D J K V N I I Y E A U N O K O O U K T R K Z A E L A V U K O X V K E R V T I A A E R K R K A U I U G O K U T X U I K N V V L I E O R R K O K N U A J Z T K A K O O S U T R I A U A U A S P V F O R O O K I C A O U V K R R T U I V A O A U K V V S L P E K A I O A I A K R S V K U S A A I X I K O P S V I K R O E O A R E R S E T R O J X O I I S U A G K R O R E R I T A I Y O A R R R A T O O K O I K I W A K E A A R O O E A K I K V O P I K H V O K K G I K T K K L A K A A R M U G E P A U A V Q A I O O O U K N X O G K G A R E A A P O O R K V V P U J E T Z P K B E I E T K U R A N E O A V A E O R U K B V K S Q A V U E C E K K U K I K I R A E K O J I Q K K K\nFinally we generate the words which need to be found.\n>>> for i in range(len(used)): ... print \"%-12s\" % used[i], ... if float(i+1)%5 == 0: print KOKOROPAVIRA KOROROVIVIRA KAEREASIVIRA KOTOKOTOARA  KOPUASIVIRA KATAITOAREI  KAITUTUVIRA  KERIKERISI   KOKARAPATO   KOKOVURITO KAUKAUVIRA   KOKOPUVIRA   KAEKAESOTO   KAVOVOVIRA   KOVAKOVARA KAAREKOPIE   KAEPIEVIRA   KAPUUPIEPA   KOKORUUTO    KIKIRAEKO KATAAVIRA    KOVOKOVOA    KARIVAITO    KARUVIRA     KAPOKARI KUROVIRA     KITUKITU     KAKUPUTE     KAEREASI     KUKURIKO KUPEROO      KAKAPUA      KIKISI       KAVORA       KIKIPI KAPUA        KAARE        KOETO        KATAI        KUVA KUSI         KOVO         KOAI\nProblem Transformation (aka Transform-and-Conquer)\nFind words which, when reversed, make legal words. Extremely wasteful brute force solution:\n>>> words = nltk.corpus.words.words('en') >>> for word1 in words: ... for word2 in words: ... if word1 == word2[::-1]: ... print word1\nMore efficient:\n>>> wordlist = set(words) >>> rev_wordlist = set(word[::-1] for word in words) >>> sorted(wordlist.intersection(rev_wordlist)) ['ah', 'are', 'bag', 'ban', 'bard', 'bat', 'bats', 'bib', 'bob', 'boob', 'brag', 'bud', 'buns', 'bus', 'but', 'civic', 'dad', 'dam', 'decal', 'deed', 'deeps', 'deer', 'deliver', 'denier', 'desserts', 'deus', 'devil', 'dial', 'diaper', 'did', 'dim', 'dog', 'don', 'doom', 'drab', 'draw', 'drawer', 'dub', 'dud', 'edit', 'eel', 'eke', 'em', 'emit', 'era', 'ere', 'evil', 'ewe', 'eye', 'fires', 'flog', 'flow', 'gab', 'gag', 'garb', 'gas', 'gel', 'gig', 'gnat', 'god', 'golf', 'gulp', 'gum', 'gums', 'guns', 'gut', 'ha', 'huh', 'keel', 'keels', 'keep', 'knits', 'laced', 'lager', 'laid', 'lap', 'lee', 'leek', 'leer', 'leg', 'leper', 'level', 'lever', 'liar', 'live', 'lived', 'loop', 'loops', 'loot', 'loots', 'mad', 'madam', 'me', 'meet', 'mets', 'mid', 'mood', 'mug', 'nab', 'nap', 'naps', 'net', 'nip', 'nips', 'no', 'nod', 'non', 'noon', 'not', 'now', 'nun', 'nuts', 'on', 'pal', 'pals', 'pan', 'pans', 'par', 'part', 'parts', 'pat', 'paws', 'peek', 'peels', 'peep', 'pep', 'pets', 'pin', 'pins', 'pip', 'pit', 'plug', 'pool', 'pools', 'pop', 'pot', 'pots', 'pup', 'radar', 'rail', 'rap', 'rat', 'rats', 'raw', 'redder', 'redraw', 'reed', 'reel', 'refer', 'regal', 'reined', 'remit', 'repaid', 'repel', 'revel', 'reviled', 'reviver', 'reward', 'rotator', 'rotor', 'sag', 'saw', 'sees', 'serif', 'sexes', 'slap', 'sleek', 'sleep', 'sloop', 'smug', 'snap', 'snaps', 'snip', 'snoops', 'snub', 'snug', 'solos', 'span', 'spans', 'spat', 'speed', 'spin', 'spit', 'spool', 'spoons', 'spot', 'spots', 'stab', 'star', 'stem', 'step', 'stew', 'stink', 'stool', 'stop', 'stops', 'strap', 'straw', 'stressed', 'stun', 'sub', 'sued', 'swap', 'tab', 'tang', 'tap', 'taps', 'tar', 'teem', 'ten', 'tide', 'time', 'timer', 'tip', 'tips', 'tit', 'ton', 'tool', 'top', 'tops', 'trap', 'tub', 'tug', 'war', 'ward', 'warder', 'warts', 'was', 'wets', 'wolf', 'won']\nObserve that this output contains redundant information; each word and its reverse is included. How could we remove this redundancy?\nPresorting, sets:\nFind words which have at least (or exactly) one instance of all vowels. Instead of writing extremely complex regular expressions, some simple preprocessing does the trick:\n>>> words = [\"sequoia\", \"abacadabra\", \"yiieeaouuu!\"] >>> vowels = \"aeiou\" >>> [w for w in words if set(w).issuperset(vowels)] ['sequoia', 'yiieeaouuu!'] >>> [w for w in words if sorted(c for c in w if c in vowels) == list(vowels)] ['sequoia']\nSpace-Time Tradeoffs\nFuzzy Spelling\n1.3\u00a0\u00a0\u00a0Exercises\n\u25d1 Consider again the problem of hyphenation across line-breaks. Suppose that you have successfully written a tokenizer that returns a list of strings, where some strings may contain a hyphen followed by a newline character, e.g. long-\\nterm. Write a function that iterates over the tokens in a list, removing the newline character from each, in each of the following ways:\nUse doubly-nested for loops. The outer loop will iterate over each token in the list, while the inner loop will iterate over each character of a string.\nReplace the inner loop with a call to re.sub()\nFinally, replace the outer loop with call to the map() function, to apply this substitution to each token.\nDiscuss the clarity (or otherwise) of each of these approaches.\n1.4\u00a0\u00a0\u00a0Object-Oriented Programming in Python (DRAFT)\nObject-Oriented Programming is a programming paradigm in which complex structures and processes are decomposed into classes, each encapsulating a single data type and the legal operations on that type. In this section we show you how to create simple data classes and processing classes by example. For a systematic introduction to Object-Oriented design, please consult the large literature of books on this topic.\nData Classes: Trees in NLTK\nAn important data type in language processing is the syntactic tree. Here we will review the parts of the NLTK code that defines the Tree class.\nThe first line of a class definition is the class keyword followed by the class name, in this case Tree. This class is derived from Python's built-in list class, permitting us to use standard list operations to access the children of a tree node.\n>>> class Tree(list):\nNext we define the initializer __init__(); Python knows to call this function when you ask for a new tree object by writing t = Tree(node, children). The constructor's first argument is special, and is standardly called self, giving us a way to refer to the current object from within its definition. This particular constructor calls the list initializer (similar to calling self = list(children)), then defines the node property of a tree.\n...     def __init__(self, node, children):\n...         list.__init__(self, children)\n...         self.node = node\nNext we define another special function that Python knows to call when we index a Tree. The first case is the simplest, when the index is an integer, e.g. t[2], we just ask for the list item in the obvious way. The other cases are for handling slices, like t[1:2], or t[:].\n...     def __getitem__(self, index):\n...         if isinstance(index, int):\n...             return list.__getitem__(self, index)\n...         else:\n...             if len(index) == 0:\n...                 return self\n...             elif len(index) == 1:\n...                 return self[int(index[0])]\n...             else:\n...                 return self[int(index[0])][index[1:]]\n...\nThis method was for accessing a child node. Similar methods are provided for setting and deleting a child (using __setitem__) and __delitem__).\nTwo other special member functions are __repr__() and __str__(). The __repr__() function produces a string representation of the object, one that can be executed to re-create the object, and is accessed from the interpreter simply by typing the name of the object and pressing 'enter'. The __str__() function produces a human-readable version of the object; here we call a pretty-printing function we have defined called pp().\n...     def __repr__(self):\n...         childstr = ' '.join([repr(c) for c in self])\n...         return '(%s: %s)' % (self.node, childstr)\n...     def __str__(self):\n...         return self.pp()\nNext we define some member functions that do other standard operations on trees. First, for accessing the leaves:\n...     def leaves(self):\n...         leaves = []\n...         for child in self:\n...             if isinstance(child, Tree):\n...                 leaves.extend(child.leaves())\n...             else:\n...                 leaves.append(child)\n...         return leaves\nNext, for computing the height:\n...     def height(self):\n...         max_child_height = 0\n...         for child in self:\n...             if isinstance(child, Tree):\n...                 max_child_height = max(max_child_height, child.height())\n...             else:\n...                 max_child_height = max(max_child_height, 1)\n...         return 1 + max_child_height\nAnd finally, for enumerating all the subtrees (optionally filtered):\n...     def subtrees(self, filter=None):\n...         if not filter or filter(self):\n...             yield self\n...         for child in self:\n...             if isinstance(child, Tree):\n...                 for subtree in child.subtrees(filter):\n...                     yield subtree\nProcessing Classes: N-gram Taggers in NLTK\nThis section will discuss the tag.ngram module.\nDuck Typing\n[Hunt & Thomas, 1999]\nAbout this document...\nThis chapter is a draft from Natural Language Processing, by Steven Bird , Ewan Klein and Edward Loper , Copyright \u00a9 2008 the authors. It is distributed with the Natural Language Toolkit [ http://www.nltk.org/ ], Version 0.9.8b1, under the terms of the Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License [ http://creativecommons.org/licenses/by-nc-nd/3.0/us/ ].\nThis document is Revision: 7606 Sat Feb 14 17:10:35 EST 2009\nAbout this document...\nThis chapter is a draft from Natural Language Processing, by Steven Bird , Ewan Klein and Edward Loper , Copyright \u00a9 2008 the authors. It is distributed with the Natural Language Toolkit [ http://www.nltk.org/ ], Version 0.9.8b1, under the terms of the Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License [ http://creativecommons.org/licenses/by-nc-nd/3.0/us/ ].\nThis document is Revision: 7606 Sat Feb 14 17:10:35 EST 2009\n"}, {"score": 696.5478, "uuid": "51472f33-367b-5574-98c5-abed6abef363", "index": "cw12", "trec_id": "clueweb12-0916wb-72-08783", "target_hostname": "nltk.googlecode.com", "target_uri": "http://nltk.googlecode.com/svn/trunk/doc/book/ch04-extras.html", "page_rank": 1.1814556e-09, "spam_rank": 77, "title": "Writing Structured Programs (Extras) This chapter introduces concepts in <em>algorithms</em>", "snippet": "Evidently <em>merge</em> <em>sort</em> <em>is</em> much <em>better</em> than bubble <em>sort</em>, and <em>quicksort</em> <em>is</em> <em>better</em> still. Readers are encouraged to look at nltk.misc.sort to see how these different methods work.", "explanation": null, "document": "4.2\u00a0\u00a0\u00a0Abstract Data Types\nStacks and Queues\nLists are a versatile data type. We can use lists to implement so-called abstract data types such as stacks and queues. A stack is a container that has a last-in-first-out (or LIFO) policy for adding and removing items (see Figure 4.1 ).\nFigure 4.1: Stacks and Queues\nStacks are used to keep track of the current context in computer processing of natural languages (and programming languages too). We will seldom have to deal with stacks explicitly, as the implementation of NLTK parsers, treebank corpus readers, (and even Python functions), all use stacks behind the scenes. However, it is important to understand what stacks are and how they work.\ndef check_parens(tokens):\n    stack = [] for token in tokens: if token == '(': # push stack.append(token) elif token == ')': # pop stack.pop()\n    return stack\n>>> phrase = \"( the cat ) ( sat ( on ( the mat )\" >>> print check_parens(phrase.split()) ['(', '(']\nExample 4.2 (code_check_parens.py) : Figure 4.2: Check whether parentheses are balanced\nIn Python, we can treat a list as a stack by limiting ourselves to the three operations defined on stacks: append(item) (to push item onto the stack), pop() to pop the item off the top of the stack, and [-1] to access the item on the top of the stack. The program in Example 4.2 processes a sentence with phrase markers, and checks that the parentheses are balanced. The loop pushes material onto the stack when it gets an open parenthesis, and pops the stack when it gets a close parenthesis. We see that two are left on the stack at the end; i.e. the parentheses are not balanced.\nAlthough the program in Example 4.2 is a useful illustration of stacks, it is overkill because we could have done a direct count: phrase.count('(') == phrase.count(')'). However, we can use stacks for more sophisticated processing of strings containing nested structure, as shown in Example 4.3 . Here we build a (potentially deeply-nested) list of lists. Whenever a token other than a parenthesis is encountered, we add it to a list at the appropriate level of nesting. The stack keeps track of this level of nesting, exploiting the fact that the item at the top of the stack is actually shared with a more deeply nested item. (Hint: add diagnostic print statements to the function to help you see what it is doing.)\ndef convert_parens(tokens):\n    stack = [[]] for token in tokens: if token == '(': # push sublist = []\n            stack[-1].append(sublist)\n            stack.append(sublist) elif token == ')': # pop stack.pop() else: # update top of stack stack[-1].append(token)\n    return stack[0]\n>>> phrase = \"( the cat ) ( sat ( on ( the mat ) ) )\" >>> print convert_parens(phrase.split()) [['the', 'cat'], ['sat', ['on', ['the', 'branch']]]]\nExample 4.3 (code_convert_parens.py) : Figure 4.3: Convert a nested phrase into a nested list using a stack\nLists can be used to represent another important data structure. A queue is a container that has a first-in-first-out (or FIFO) policy for adding and removing items (see Figure 4.1 ). We could use a queue of length n to create all the n-grams of a text. As with stacks, we will seldom have to deal with queues explicitly, as the implementation of NLTK n-gram taggers ( Section 5.5 ) and chart parsers use queues behind the scenes. Here's how queues can be implemented using lists.\n>>> queue = ['the', 'cat', 'sat'] >>> queue.append('on') >>> queue.append('the') >>> queue.append('branch') >>> queue.pop(0) 'the' >>> queue.pop(0) 'cat' >>> queue ['sat', 'on', 'the', 'branch']\nNote\nThe list-based implementation of queues is inefficient for large queues. In such cases, it is better to use Python's built-in support for \"double-ended queues\", collections.deque.\n4.3\u00a0\u00a0\u00a0Chinese and XML\nCodecs for processing Chinese text have been incorporated into Python (since version 2.4).\n>>> path = nltk.data.find('samples/sinorama-gb.xml') >>> f = codecs.open(path, encoding='gb2312') >>> lines = f.readlines() >>> for l in lines: ... l = l[:-1] ... utf_enc = l.encode('utf8') ... print repr(utf_enc) '<?xml version=\"1.0\" encoding=\"gb2312\" ?>' '' '<sent>' '\\xe7\\x94\\x9a\\xe8\\x87\\xb3\\xe7\\x8c\\xab\\xe4\\xbb\\xa5\\xe4\\xba\\xba\\xe8\\xb4\\xb5' '' 'In some cases, cats were valued above humans.' '</sent>'\nWith appropriate support on your terminal, the escaped text string inside the <SENT> element above will be rendered as the following string of ideographs: \u751a\u81f3\u732b\u4ee5\u4eba\u8d35.\nWe can also read in the contents of an XML file using the etree package (at least, if the file is encoded as UTF-8 \u2014 as of writing, there seems to be a problem reading GB2312-encoded files in etree).\n>>> path = nltk.data.find('samples/sinorama-utf8.xml') >>> from nltk.etree import ElementTree as ET >>> tree = ET.parse(path) >>> text = tree.findtext('sent') >>> uni_text = text.encode('utf8') >>> print repr(uni_text.splitlines()[1]) '\\xe7\\x94\\x9a\\xe8\\x87\\xb3\\xe7\\x8c\\xab\\xe4\\xbb\\xa5\\xe4\\xba\\xba\\xe8\\xb4\\xb5'\n4.4\u00a0\u00a0\u00a0More on Defensive Programming\nThe Return Statement\nAnother aspect of defensive programming concerns the return statement of a function. In order to be confident that all execution paths through a function lead to a return statement, it is best to have a single return statement at the end of the function definition. This approach has a further benefit: it makes it more likely that the function will only return a single type. Thus, the following version of our tag() function is safer. First we assign a default value\n, then in certain cases\nwe replace it with a different value\n. All paths through the function body end at the single return statement\n.\n... if word in ['a', 'the', 'all']:\n... result = 'det'\n... return result\nA return statement can be used to pass multiple values back to the calling program, by packing them into a tuple. Here we define a function that returns a tuple consisting of the average word length of a sentence, and the inventory of letters used in the sentence. It would have been clearer to write two separate functions.\n>>> def proc_words(words): ... avg_wordlen = sum(len(word) for word in words)/len(words) ... chars_used = ''.join(sorted(set(''.join(words)))) ... return avg_wordlen, chars_used >>> proc_words(['Not', 'a', 'good', 'way', 'to', 'write', 'functions']) (3, 'Nacdefginorstuwy')\n[write version with two separate functions]\n4.5\u00a0\u00a0\u00a0Algorithm Design\nAn algorithm is a \"recipe\" for solving a problem. For example, to multiply 16 by 12 we might use any of the following methods:\nAdd 16 to itself 12 times over\nPerform \"long multiplication\", starting with the least-significant digits of both numbers\nLook up a multiplication table\nRepeatedly halve the first number and double the second, 16*12 = 8*24 = 4*48 = 2*96 = 192\nDo 10*12 to get 120, then add 6*12\nRewrite 16*12 as (x+2)(x-2), remember that 14*14=196, and add (+2)(-2) = -4\nEach of these methods is a different algorithm, and requires different amounts of computation time and different amounts of intermediate information to store. A similar situation holds for many other superficially simple tasks, such as sorting a list of words.\nSorting Algorithms\nNow, as we saw above, Python provides a built-in function sort() that performs this task efficiently. However, NLTK also provides several algorithms for sorting lists, to illustrate the variety of possible methods. To illustrate the difference in efficiency, we will create a list of 1000 numbers, randomize the list, then sort it, counting the number of list manipulations required.\n>>> from random import shuffle >>> a = range(1000) # [0,1,2,...999] >>> shuffle(a) # randomize\nNow we can try a simple sort method called bubble sort, that scans through the list many times, exchanging adjacent items if they are out of order. It sorts the list a in-place, and returns the number of times it modified the list:\n>>> from nltk.misc import sort >>> sort.bubble(a) 250918\nWe can try the same task using various sorting algorithms. Evidently merge sort is much better than bubble sort, and quicksort is better still.\n>>> shuffle(a); sort.merge(a) 6175 >>> shuffle(a); sort.quick(a) 2378\nReaders are encouraged to look at nltk.misc.sort to see how these different methods work. The collection of NLTK modules exemplify a variety of algorithm design techniques, including brute-force, divide-and-conquer, dynamic programming, and greedy search. Readers who would like a systematic introduction to algorithm design should consult the resources mentioned at the end of this tutorial.\nDecorate-Sort-Undecorate\nIn Chapter 4 we saw how to sort a list of items according to some property of the list.\n>>> words = 'I turned off the spectroroute'.split() >>> words.sort(cmp) >>> words ['I', 'off', 'spectroroute', 'the', 'turned'] >>> words.sort(lambda x, y: cmp(len(y), len(x))) >>> words ['spectroroute', 'turned', 'off', 'the', 'I']\nThis is inefficient when the list of items gets long, as we compute len() twice for every comparison (about 2nlog(n) times). The following is more efficient:\n>>> [pair[1] for pair in sorted((len(w), w) for w in words)[::-1]] ['spectroroute', 'turned', 'the', 'off', 'I']\nThis technique is called decorate-sort-undecorate. We can compare its performance by timing how long it takes to execute it a million times.\n>>> from timeit import Timer >>> Timer(\"sorted(words, lambda x, y: cmp(len(y), len(x)))\", ... \"words='I turned off the spectroroute'.split()\").timeit() 8.3548779487609863 >>> Timer(\"[pair[1] for pair in sorted((len(w), w) for w in words)]\", ... \"words='I turned off the spectroroute'.split()\").timeit() 9.9698889255523682\nMORE: consider what happens as the lists get longer...\nAnother example: sorting dates of the form \"1 Jan 1970\"\n>>> month_index = { ... \"Jan\" : 1, \"Feb\" : 2, \"Mar\" : 3, \"Apr\" : 4, ... \"May\" : 5, \"Jun\" : 6, \"Jul\" : 7, \"Aug\" : 8, ... \"Sep\" : 9, \"Oct\" : 10, \"Nov\" : 11, \"Dec\" : 12 ... } >>> def date_cmp(date_string1, date_string2): ... (d1,m1,y1) = date_string1.split() ... (d2,m2,y2) = date_string2.split() ... conv1 = y1, month_index[m1], d1 ... conv2 = y2, month_index[m2], d2 ... return cmp(a2, b2) >>> sort(date_list, date_cmp)\nThe comparison function says that we compare two times of the form ('Mar', '2004') by reversing the order of the month and year, and converting the month into a number to get ('2004', '3'), then using Python's built-in cmp function to compare them.\nNow do this using decorate-sort-undecorate, for large data size\nTime comparison\nBrute Force\nWordfinder Puzzle\nHere we will generate a grid of letters, containing words found in the dictionary. First we remove any duplicates and disregard the order in which the lexemes appeared in the dictionary. We do this by converting it to a set, then back to a list. Then we select the first 200 words, and then only keep those words having a reasonable length.\n>>> words = list(set(lexemes)) >>> words = words[:200] >>> words = [w for w in words if 3 <= len(w) <= 12]\nNow we generate the wordfinder grid, and print it out.\n>>> from nltk.misc.wordfinder import wordfinder >>> grid, used = wordfinder(words) >>> for i in range(len(grid)): ... for j in range(len(grid[i])): ... print grid[i][j], ... print O G H K U U V U V K U O R O V A K U N C K Z O T O I S E K S N A I E R E P A K C I A R A A K I O Y O V R S K A W J K U Y L R N H N K R G V U K G I A U D J K V N I I Y E A U N O K O O U K T R K Z A E L A V U K O X V K E R V T I A A E R K R K A U I U G O K U T X U I K N V V L I E O R R K O K N U A J Z T K A K O O S U T R I A U A U A S P V F O R O O K I C A O U V K R R T U I V A O A U K V V S L P E K A I O A I A K R S V K U S A A I X I K O P S V I K R O E O A R E R S E T R O J X O I I S U A G K R O R E R I T A I Y O A R R R A T O O K O I K I W A K E A A R O O E A K I K V O P I K H V O K K G I K T K K L A K A A R M U G E P A U A V Q A I O O O U K N X O G K G A R E A A P O O R K V V P U J E T Z P K B E I E T K U R A N E O A V A E O R U K B V K S Q A V U E C E K K U K I K I R A E K O J I Q K K K\nFinally we generate the words which need to be found.\n>>> for i in range(len(used)): ... print \"%-12s\" % used[i], ... if float(i+1)%5 == 0: print KOKOROPAVIRA KOROROVIVIRA KAEREASIVIRA KOTOKOTOARA  KOPUASIVIRA KATAITOAREI  KAITUTUVIRA  KERIKERISI   KOKARAPATO   KOKOVURITO KAUKAUVIRA   KOKOPUVIRA   KAEKAESOTO   KAVOVOVIRA   KOVAKOVARA KAAREKOPIE   KAEPIEVIRA   KAPUUPIEPA   KOKORUUTO    KIKIRAEKO KATAAVIRA    KOVOKOVOA    KARIVAITO    KARUVIRA     KAPOKARI KUROVIRA     KITUKITU     KAKUPUTE     KAEREASI     KUKURIKO KUPEROO      KAKAPUA      KIKISI       KAVORA       KIKIPI KAPUA        KAARE        KOETO        KATAI        KUVA KUSI         KOVO         KOAI\nProblem Transformation (aka Transform-and-Conquer)\nFind words which, when reversed, make legal words. Extremely wasteful brute force solution:\n>>> words = nltk.corpus.words.words('en') >>> for word1 in words: ... for word2 in words: ... if word1 == word2[::-1]: ... print word1\nMore efficient:\n>>> wordlist = set(words) >>> rev_wordlist = set(word[::-1] for word in words) >>> sorted(wordlist.intersection(rev_wordlist)) ['ah', 'are', 'bag', 'ban', 'bard', 'bat', 'bats', 'bib', 'bob', 'boob', 'brag', 'bud', 'buns', 'bus', 'but', 'civic', 'dad', 'dam', 'decal', 'deed', 'deeps', 'deer', 'deliver', 'denier', 'desserts', 'deus', 'devil', 'dial', 'diaper', 'did', 'dim', 'dog', 'don', 'doom', 'drab', 'draw', 'drawer', 'dub', 'dud', 'edit', 'eel', 'eke', 'em', 'emit', 'era', 'ere', 'evil', 'ewe', 'eye', 'fires', 'flog', 'flow', 'gab', 'gag', 'garb', 'gas', 'gel', 'gig', 'gnat', 'god', 'golf', 'gulp', 'gum', 'gums', 'guns', 'gut', 'ha', 'huh', 'keel', 'keels', 'keep', 'knits', 'laced', 'lager', 'laid', 'lap', 'lee', 'leek', 'leer', 'leg', 'leper', 'level', 'lever', 'liar', 'live', 'lived', 'loop', 'loops', 'loot', 'loots', 'mad', 'madam', 'me', 'meet', 'mets', 'mid', 'mood', 'mug', 'nab', 'nap', 'naps', 'net', 'nip', 'nips', 'no', 'nod', 'non', 'noon', 'not', 'now', 'nun', 'nuts', 'on', 'pal', 'pals', 'pan', 'pans', 'par', 'part', 'parts', 'pat', 'paws', 'peek', 'peels', 'peep', 'pep', 'pets', 'pin', 'pins', 'pip', 'pit', 'plug', 'pool', 'pools', 'pop', 'pot', 'pots', 'pup', 'radar', 'rail', 'rap', 'rat', 'rats', 'raw', 'redder', 'redraw', 'reed', 'reel', 'refer', 'regal', 'reined', 'remit', 'repaid', 'repel', 'revel', 'reviled', 'reviver', 'reward', 'rotator', 'rotor', 'sag', 'saw', 'sees', 'serif', 'sexes', 'slap', 'sleek', 'sleep', 'sloop', 'smug', 'snap', 'snaps', 'snip', 'snoops', 'snub', 'snug', 'solos', 'span', 'spans', 'spat', 'speed', 'spin', 'spit', 'spool', 'spoons', 'spot', 'spots', 'stab', 'star', 'stem', 'step', 'stew', 'stink', 'stool', 'stop', 'stops', 'strap', 'straw', 'stressed', 'stun', 'sub', 'sued', 'swap', 'tab', 'tang', 'tap', 'taps', 'tar', 'teem', 'ten', 'tide', 'time', 'timer', 'tip', 'tips', 'tit', 'ton', 'tool', 'top', 'tops', 'trap', 'tub', 'tug', 'war', 'ward', 'warder', 'warts', 'was', 'wets', 'wolf', 'won']\nObserve that this output contains redundant information; each word and its reverse is included. How could we remove this redundancy?\nPresorting, sets:\nFind words which have at least (or exactly) one instance of all vowels. Instead of writing extremely complex regular expressions, some simple preprocessing does the trick:\n>>> words = [\"sequoia\", \"abacadabra\", \"yiieeaouuu!\"] >>> vowels = \"aeiou\" >>> [w for w in words if set(w).issuperset(vowels)] ['sequoia', 'yiieeaouuu!'] >>> [w for w in words if sorted(c for c in w if c in vowels) == list(vowels)] ['sequoia']\nSpace-Time Tradeoffs\nFuzzy Spelling\n4.6\u00a0\u00a0\u00a0Exercises\n\u25d1 Consider again the problem of hyphenation across line-breaks. Suppose that you have successfully written a tokenizer that returns a list of strings, where some strings may contain a hyphen followed by a newline character, e.g. long-\\nterm. Write a function that iterates over the tokens in a list, removing the newline character from each, in each of the following ways:\nUse doubly-nested for loops. The outer loop will iterate over each token in the list, while the inner loop will iterate over each character of a string.\nReplace the inner loop with a call to re.sub()\nFinally, replace the outer loop with call to the map() function, to apply this substitution to each token.\nDiscuss the clarity (or otherwise) of each of these approaches.\n4.7\u00a0\u00a0\u00a0Search\nMany NLP tasks can be construed as search problems. For example, the task of a parser is to identify one or more parse trees for a given sentence. As we saw in Part II, there are several algorithms for parsing. A recursive descent parser performs backtracking search, applying grammar productions in turn until a match with the next input word is found, and backtracking when there is no match. As we will see in Section 8.6 , the space of possible parse trees is very large; a parser can be thought of as providing a relatively efficient way to find the right solution(s) within a very large space of candidates.\nAs another example of search, suppose we want to find the most complex sentence in a text corpus. Before we can begin we have to be explicit about how the complexity of a sentence is to be measured: word count, verb count, character count, parse-tree depth, etc. In the context of learning this is known as the objective function, the property of candidate solutions we want to optimize.\nExhaustive Search\nenumerate search space, evaluate at each point\nthis example: search space size is 255 = 36,028,797,018,963,968\nFor a computer that can do 100,000 evaluations per second, this would take over 10,000 years!\nBacktracking search -- saw this in the recursive descent parser.\nHill-Climbing Search\nStarting from a given location in the search space, evaluate nearby locations and move to a new location only if it is an improvement on the current location.\ndef flip(segs, pos):\n    return segs[:pos] + `1-int(segs[pos])` + segs[pos+1:] def hill_climb(text, segs, iterations): for i in range(iterations):\n        pos, best = 0, evaluate(text, segs) for i in range(len(segs)):\n            score = evaluate(text, flip(segs, i)) if score < best:\n                pos, best = i, score if pos != 0:\n            segs = flip(segs, pos) print evaluate(text, segs), segment(text, segs)\n    return segs\n>>> print evaluate(text, seg1), segment(text, seg1) 63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy'] >>> hill_climb(text, seg1, 20) 61 ['doyouseethekittyseethedoggy', 'doyoulikethekitty', 'likethedoggy'] 59 ['doyouseethekittyseethedoggydoyoulikethekitty', 'likethedoggy'] 57 ['doyouseethekittyseethedoggydoyoulikethekittylikethedoggy']\nExample 4.4 (code_hill_climb.py) : Figure 4.4: Hill-Climbing Search\n4.8\u00a0\u00a0\u00a0Object-Oriented Programming in Python (DRAFT)\nObject-Oriented Programming is a programming paradigm in which complex structures and processes are decomposed into classes, each encapsulating a single data type and the legal operations on that type. In this section we show you how to create simple data classes and processing classes by example. For a systematic introduction to Object-Oriented design, please consult the large literature of books on this topic.\nData Classes: Trees in NLTK\nAn important data type in language processing is the syntactic tree. Here we will review the parts of the NLTK code that defines the Tree class.\nThe first line of a class definition is the class keyword followed by the class name, in this case Tree. This class is derived from Python's built-in list class, permitting us to use standard list operations to access the children of a tree node.\n>>> class Tree(list):\nNext we define the initializer __init__(); Python knows to call this function when you ask for a new tree object by writing t = Tree(node, children). The constructor's first argument is special, and is standardly called self, giving us a way to refer to the current object from within its definition. This particular constructor calls the list initializer (similar to calling self = list(children)), then defines the node property of a tree.\n...     def __init__(self, node, children):\n...         list.__init__(self, children)\n...         self.node = node\nNext we define another special function that Python knows to call when we index a Tree. The first case is the simplest, when the index is an integer, e.g. t[2], we just ask for the list item in the obvious way. The other cases are for handling slices, like t[1:2], or t[:].\n...     def __getitem__(self, index):\n...         if isinstance(index, int):\n...             return list.__getitem__(self, index)\n...         else:\n...             if len(index) == 0:\n...                 return self\n...             elif len(index) == 1:\n...                 return self[int(index[0])]\n...             else:\n...                 return self[int(index[0])][index[1:]]\n...\nThis method was for accessing a child node. Similar methods are provided for setting and deleting a child (using __setitem__) and __delitem__).\nTwo other special member functions are __repr__() and __str__(). The __repr__() function produces a string representation of the object, one that can be executed to re-create the object, and is accessed from the interpreter simply by typing the name of the object and pressing 'enter'. The __str__() function produces a human-readable version of the object; here we call a pretty-printing function we have defined called pp().\n...     def __repr__(self):\n...         childstr = ' '.join([repr(c) for c in self])\n...         return '(%s: %s)' % (self.node, childstr)\n...     def __str__(self):\n...         return self.pp()\nNext we define some member functions that do other standard operations on trees. First, for accessing the leaves:\n...     def leaves(self):\n...         leaves = []\n...         for child in self:\n...             if isinstance(child, Tree):\n...                 leaves.extend(child.leaves())\n...             else:\n...                 leaves.append(child)\n...         return leaves\nNext, for computing the height:\n...     def height(self):\n...         max_child_height = 0\n...         for child in self:\n...             if isinstance(child, Tree):\n...                 max_child_height = max(max_child_height, child.height())\n...             else:\n...                 max_child_height = max(max_child_height, 1)\n...         return 1 + max_child_height\nAnd finally, for enumerating all the subtrees (optionally filtered):\n...     def subtrees(self, filter=None):\n...         if not filter or filter(self):\n...             yield self\n...         for child in self:\n...             if isinstance(child, Tree):\n...                 for subtree in child.subtrees(filter):\n...                     yield subtree\nProcessing Classes: N-gram Taggers in NLTK\nThis section will discuss the tag.ngram module.\nDuck Typing\n(Hunt & Thomas, 2000)\nAbout this document...\nThis is a chapter from Natural Language Processing with Python, by Steven Bird , Ewan Klein and Edward Loper , Copyright \u00a9 2009 the authors. It is distributed with the Natural Language Toolkit [http://www.nltk.org/], Version 2.0b7, under the terms of the Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License [ http://creativecommons.org/licenses/by-nc-nd/3.0/us/ ].\nThis document is Revision: 8464 Mon 14 Dec 2009 10:58:42 EST\n"}, {"score": 684.2764, "uuid": "a93e5809-b258-5fdb-8c49-a0bb66ccf244", "index": "cw12", "trec_id": "clueweb12-1308wb-12-30604", "target_hostname": "blog.boyet.com", "target_uri": "http://blog.boyet.com/blog/javascriptlessons/", "page_rank": 5.0789577e-09, "spam_rank": 82, "title": "JavaScriptLessons : <em>Algorithms</em> for the masses - julian m bucknall", "snippet": "Well, for a start the browser\u2019s implementation of <em>sort</em> <em>is</em> in all probability done with <em>quicksort</em>. <em>Which</em> particular flavor of <em>quicksort</em> <em>is</em> unknown (the different flavors would be how the pivot element <em>is</em> chosen).", "explanation": null, "document": "No Comments\nThere\u2019s a concept in functional programming called currying which gives you the ability to create one function from another, and have the created function call the original but with some arguments prefilled in. To be ultra-rigorous, we assume the original function has n arguments, and currying produces a chain of n functions each taking one argument to produce the same results as the original. We, however, will skip that rigorous definition since it\u2019s too constricting for our discussion here. Instead\u00a0 we\u2019ll talk about partial function application, but call it currying anyway. In essence: we want to call function A that takes lots of parameters, but since some/most of them are preset to some default values, we\u2019d prefer having another function B that calls A filling in those presets to save time/effort.\nNow, ordinarily, people explain currying with reference to mystical functions that add two numbers. I\u2019m not happy with those types of examples because all I\u2019m left with is a big feeling of \u201cSo What?\u201d when I run through one of those.\nInstead let\u2019s illustrate what currying is about by looking at the setTimeout function. This, as you should know, takes a function and an expiry time in milliseconds, and, after the time has expired will execute the function. Let\u2019s suppose you\u2019re writing some code and you notice that all your timeouts tend to be 10 milliseconds long (using a small timeout like this is a technique to split up long-running code and allow the execution engine time to \u201cbreathe\u201d). If you are like me, you\u2019d write a function that called setTimeout with a default of 10 milliseconds:\nvar delay10 = function(f) {\r\n  setTimeout(f, 10);\r\n};\nNothing too difficult. After a while, you notice that there are some other calls to setTimeout that use a default of 5 seconds: for example, your code displays some element in the browser and if the user does not respond to it within 5 seconds you remove the element. Your first thought might be to write this next function:\nvar delay5secs = function(f) {\r\n  setTimeout(f, 5000);\r\n};\nBut then there\u2019s part of you that recognizes that this is code duplication. Tiny, but still there. You wrote it (just like I just did) using copy-and-paste and altered a single value. Because we understand that functions are objects in JavaScript, we should be able to write a function that takes an expiry time and that returns a function that calls setTimeout with that delay:\nvar makeDelayFunction = function(time) { return function (f) { \r\n    setTimeout(f, time); \r\n  };\r\n};\nSo here the makeDelayFunction takes a time value and then returns a function that takes a function to execute and calls setTimeout with that function and the original time value. Through the wonder of closures we encapsulate the original time argument and then use it in the function that\u2019s returned. Our two delay functions can now get created like this:\nvar delay10 = makeDelayFunction(10); var delay5secs = makeDelayFunction(5000);\nAnd, bingo, we\u2019ve removed the code duplication. What we\u2019ve just done is to curry the setTimeout function (again, I\u2019m using a looser definition of curry here). We\u2019ve created a function that returns functions that call setTimeout with a preset time value, and the preset value is stored in a closure. Now, admittedly, with my setTimeout example the benefits aren\u2019t that earth-shattering, but at least it shows the principle.\nLet\u2019s take it a little further now. The addEventListener function takes three parameters: the event type (a string with various predefined values), the listener (a function that will be called when the event happens), and the capture flag (which for our purposes can be false). The function is called on some element of the DOM. Let\u2019s suppose we want to add mouse click listeners for a whole bunch of elements. Of the four parameters to the function (counting the object we\u2019re calling it on as a parameter) we have two fixed parameters and two \u201cfloating\u201d. Our first thought, given what we\u2019ve learned above, is this:\nvar makeListener = function (type) { return function (element, func) {\r\n    element.addEventListener(type, func, false);\r\n  };\r\n}; var addClick = makeListener(\"click\");\r\naddClick(myDiv, function () { alert(\"myDiv was clicked!\"); });\nLooks pretty good, but note the code duplication. It\u2019s a little harder to spot perhaps, but it\u2019s there. (In essence, we\u2019re returning a function that calls another. The function being called has parameters that come from the closure, and others from the call itself.) Can we extract out this commonality?\nIt turns out that yes we can. The idea was first shown by Oliver Steele, and it needs us to use the arguments array and a little trick. The little trick goes like this: we pass in a complete set of arguments we need for the wrapped function to the maker function. For those arguments that are fixed, we supply the actual values. For those arguments that are floating, we pass undefined. The function we create then slots in the arguments it\u2019s given into the undefined spaces that were passed to the maker function. Perhaps looking at the call to the maker function for our first example might make this clearer:\nvar delay10 = makePartialFunction(setTimeout, undefined, 10); var delay5secs = makePartialFunction(setTimeout, undefined, 5000);\nThe new maker function, makePartialFunction, takes the function to be wrapped as the first parameter, and then the remaining arguments are the arguments to that function when it\u2019s to be called, with any unknowns passed as undefined.\nWhat might that maker function look like?\nvar makePartialFunction = function () { // the first argument is the function we're wrapping var func = arguments[0], // get the arguments pseudo-array as a local array defaultArgs = Array.prototype.slice.call(arguments); return function () { var actualArgs = [], // the arguments we'll call with arg = 0,\r\n      i; // create the arguments for the call for (i = 1; (i < defaultArgs.length) && (arg < arguments.length); i++) { if (defaultArgs[i] !== undefined) {\r\n        actualArgs.push(defaultArgs[i]);\r\n      } else{\r\n        actualArgs.push(arguments[arg]);\r\n        arg += 1;\r\n      }\r\n    } // call the wrapped function return func.apply(this, actualArgs);\r\n  };\r\n};\nFirst of all we make a copy of the data passed to the maker function: the function to be wrapped and the arguments passed. Then we return a function. This function, when called, constructs an arguments array for the wrapped function, replacing the undefined parts with arguments to its own call. It then calls the wrapped function using the apply invocation.\nSounds groovy, except it won\u2019t work for my second example. Next time we\u2019ll fix that problem.\nNow playing:\nChemical Brothers - Setting Sun [Instrumental]\n(from The Saint)\n"}, {"score": 683.9379, "uuid": "18791f9e-02ce-552c-86a0-2f48f7247045", "index": "cw12", "trec_id": "clueweb12-1313wb-25-08878", "target_hostname": "blog.boyet.com", "target_uri": "http://blog.boyet.com/blog/javascriptlessons/?p=1", "page_rank": 1.1700305e-09, "spam_rank": 83, "title": "JavaScriptLessons : <em>Algorithms</em> for the masses - julian m bucknall", "snippet": "Well, for a start the browser\u2019s implementation of <em>sort</em> <em>is</em> in all probability done with <em>quicksort</em>. <em>Which</em> particular flavor of <em>quicksort</em> <em>is</em> unknown (the different flavors would be how the pivot element <em>is</em> chosen).", "explanation": null, "document": "No Comments\nThere\u2019s a concept in functional programming called currying which gives you the ability to create one function from another, and have the created function call the original but with some arguments prefilled in. To be ultra-rigorous, we assume the original function has n arguments, and currying produces a chain of n functions each taking one argument to produce the same results as the original. We, however, will skip that rigorous definition since it\u2019s too constricting for our discussion here. Instead\u00a0 we\u2019ll talk about partial function application, but call it currying anyway. In essence: we want to call function A that takes lots of parameters, but since some/most of them are preset to some default values, we\u2019d prefer having another function B that calls A filling in those presets to save time/effort.\nNow, ordinarily, people explain currying with reference to mystical functions that add two numbers. I\u2019m not happy with those types of examples because all I\u2019m left with is a big feeling of \u201cSo What?\u201d when I run through one of those.\nInstead let\u2019s illustrate what currying is about by looking at the setTimeout function. This, as you should know, takes a function and an expiry time in milliseconds, and, after the time has expired will execute the function. Let\u2019s suppose you\u2019re writing some code and you notice that all your timeouts tend to be 10 milliseconds long (using a small timeout like this is a technique to split up long-running code and allow the execution engine time to \u201cbreathe\u201d). If you are like me, you\u2019d write a function that called setTimeout with a default of 10 milliseconds:\nvar delay10 = function(f) {\r\n  setTimeout(f, 10);\r\n};\nNothing too difficult. After a while, you notice that there are some other calls to setTimeout that use a default of 5 seconds: for example, your code displays some element in the browser and if the user does not respond to it within 5 seconds you remove the element. Your first thought might be to write this next function:\nvar delay5secs = function(f) {\r\n  setTimeout(f, 5000);\r\n};\nBut then there\u2019s part of you that recognizes that this is code duplication. Tiny, but still there. You wrote it (just like I just did) using copy-and-paste and altered a single value. Because we understand that functions are objects in JavaScript, we should be able to write a function that takes an expiry time and that returns a function that calls setTimeout with that delay:\nvar makeDelayFunction = function(time) { return function (f) { \r\n    setTimeout(f, time); \r\n  };\r\n};\nSo here the makeDelayFunction takes a time value and then returns a function that takes a function to execute and calls setTimeout with that function and the original time value. Through the wonder of closures we encapsulate the original time argument and then use it in the function that\u2019s returned. Our two delay functions can now get created like this:\nvar delay10 = makeDelayFunction(10); var delay5secs = makeDelayFunction(5000);\nAnd, bingo, we\u2019ve removed the code duplication. What we\u2019ve just done is to curry the setTimeout function (again, I\u2019m using a looser definition of curry here). We\u2019ve created a function that returns functions that call setTimeout with a preset time value, and the preset value is stored in a closure. Now, admittedly, with my setTimeout example the benefits aren\u2019t that earth-shattering, but at least it shows the principle.\nLet\u2019s take it a little further now. The addEventListener function takes three parameters: the event type (a string with various predefined values), the listener (a function that will be called when the event happens), and the capture flag (which for our purposes can be false). The function is called on some element of the DOM. Let\u2019s suppose we want to add mouse click listeners for a whole bunch of elements. Of the four parameters to the function (counting the object we\u2019re calling it on as a parameter) we have two fixed parameters and two \u201cfloating\u201d. Our first thought, given what we\u2019ve learned above, is this:\nvar makeListener = function (type) { return function (element, func) {\r\n    element.addEventListener(type, func, false);\r\n  };\r\n}; var addClick = makeListener(\"click\");\r\naddClick(myDiv, function () { alert(\"myDiv was clicked!\"); });\nLooks pretty good, but note the code duplication. It\u2019s a little harder to spot perhaps, but it\u2019s there. (In essence, we\u2019re returning a function that calls another. The function being called has parameters that come from the closure, and others from the call itself.) Can we extract out this commonality?\nIt turns out that yes we can. The idea was first shown by Oliver Steele, and it needs us to use the arguments array and a little trick. The little trick goes like this: we pass in a complete set of arguments we need for the wrapped function to the maker function. For those arguments that are fixed, we supply the actual values. For those arguments that are floating, we pass undefined. The function we create then slots in the arguments it\u2019s given into the undefined spaces that were passed to the maker function. Perhaps looking at the call to the maker function for our first example might make this clearer:\nvar delay10 = makePartialFunction(setTimeout, undefined, 10); var delay5secs = makePartialFunction(setTimeout, undefined, 5000);\nThe new maker function, makePartialFunction, takes the function to be wrapped as the first parameter, and then the remaining arguments are the arguments to that function when it\u2019s to be called, with any unknowns passed as undefined.\nWhat might that maker function look like?\nvar makePartialFunction = function () { // the first argument is the function we're wrapping var func = arguments[0], // get the arguments pseudo-array as a local array defaultArgs = Array.prototype.slice.call(arguments); return function () { var actualArgs = [], // the arguments we'll call with arg = 0,\r\n      i; // create the arguments for the call for (i = 1; (i < defaultArgs.length) && (arg < arguments.length); i++) { if (defaultArgs[i] !== undefined) {\r\n        actualArgs.push(defaultArgs[i]);\r\n      } else{\r\n        actualArgs.push(arguments[arg]);\r\n        arg += 1;\r\n      }\r\n    } // call the wrapped function return func.apply(this, actualArgs);\r\n  };\r\n};\nFirst of all we make a copy of the data passed to the maker function: the function to be wrapped and the arguments passed. Then we return a function. This function, when called, constructs an arguments array for the wrapped function, replacing the undefined parts with arguments to its own call. It then calls the wrapped function using the apply invocation.\nSounds groovy, except it won\u2019t work for my second example. Next time we\u2019ll fix that problem.\nNow playing:\nChemical Brothers - Setting Sun [Instrumental]\n(from The Saint)\n"}, {"score": 682.2031, "uuid": "78d3551b-1a28-5a6d-9fff-1f1e0f9366cc", "index": "cw12", "trec_id": "clueweb12-1501wb-96-23946", "target_hostname": "people.cis.ksu.edu", "target_uri": "http://people.cis.ksu.edu/~schmidt/CIS200/ch8V11.sort.search.html", "page_rank": 1.1700305e-09, "spam_rank": 68, "title": "Copyright \u00a9 2001 David Schmidt 8.7.3 Sorting When an array <em>is</em> used as a", "snippet": "The time complexity of <em>merge</em> <em>sort</em> <em>is</em> <em>is</em> significantly <em>better</em> than the other sorting <em>algorithms</em> seen so far; we consider the number of comparisons the <em>algorithm</em> makes. (The analysis of element exchanges goes the same.)", "explanation": null, "document": "Copyright \u00a9 2001 David Schmidt\n8.7.3 Sorting\nWhen an array is used as a database, where elements are fetched and updated frequently, there is a distinct advantage to ordering the elements by their keys---it becomes far easier to locate an element. The process of ordering an array's elements is called sorting.\nAlgorithms for sorting have a rich history, and we cannot do justice here. Instead, we focus upon the development of two traditional sorting methods, selection sort and insertion sort. To simplify the algorithms that follow, we work with arrays of integers, where we sort the elements so that they are ordered in value from smallest integer to largest. (Of course, we can use the same techniques to sort elements by their key values.)\nThe idea behind selection sort is simple: Locate the least integer in the array, and move it to the front. Then, find the next least integer, and move it second to the front. Repeat this process until all integers have been selected in order of size. The algorithm that sorts array r in this manner goes\nfor ( i = 0;  i != r.length;  i = i+1 )\n    { Find the least element in  r  within the range\n        r[i]  to  r[r.length-1]; say that it is at  r[j].\n      Exchange  r[i]  with  r[j].\n    }\nHere is the algorithm in action. Say that we have this array, r:\nWhen selection sorting starts, its loop finds the least element in the range r[0]..r[4] at index 2 and exchanges the elements at indexes 0 and 2:\nThe second loop iteration locates the least element in the range r[1]..r[4] at index 3, and the elements at indexes 1 and 3 are exchanged:\nThe algorithm next considers the elements in range r[2]..r[4] and so on until it reaches this end result:\nFigure 15 shows the method.\nFIGURE 15: selection sort================================================\n\n/** selectionSort sorts the elements of its array parameter\n  * @param  r - the array to be sorted  */\npublic void selectionSort(int[] r)\n{ for ( int i = 0;  i != r.length;  i = i+1 )\n      // invariant: subarray r[0]..r[i-1] is sorted\n      { int j = findLeast(r, i, r.length-1);  // get index of least element\n        int temp = r[i]; \n        r[i] = r[j]; \n        r[j] = temp; \n      }\n}\n\n/** findLeast finds the index of the least element in  r[start]..r[end]\n  * @param  r - the array to be searched\n  * @param  start - the starting element for the search\n  * @param  end - the ending element for the search\n  * @return the index of the smallest element in  r[start]..r[end]  */\nprivate int findLeast(int[] r, int start, int end)\n{ int least = start;\n  for ( int i = start+1;  i <= end;  i = i+1 )\n      // invariant: least is index of least element in range r[start]..r[i-1]\n      { if ( r[i] < r[least] ) { least = i; } }\n  return least; }\n\nENDFIGURE===============================================================\nThere is more than one way to sort an array; a second classic approach, called insertion sort, rearranges elements the way most people sort a hand of playing cards: Start with the first card (element), then take the second card (element) and insert it either before or after the first card, so that the two cards are in order; then take the third card and insert it in its proper position so that the three cards are ordered, and so on. Eventually, all the cards are inserted where they belong in the ordering.\nThe algorithm based on this idea is simply stated as:\nfor ( i=1;  i < r.length;  i = i+1 )\n    { Insert  r[i] in its proper place within the already sorted prefix, \n        r[0]..r[i-1].\n    }\nIf we apply the algorithm to the example array, r, seen above,\nwe see that the algorithm first inserts the 8 where it belongs with respect to the 11:\nThis makes the prefix, r[0]..r[1], correctly sorted. Next, the -2 must be inserted in its proper place with respect to the sorted prefix:\nTo make room for -2 at its proper position, r[0], the two elements, 8 and 11, must be shifted one position to the right. Now, r[0]..r[2] is correctly sorted. The last two elements are inserted similarly.\nFigure 16 show the insertion sorting method.\nFIGURE 16: insertion sort===============================================\n\n/** insertionSort sorts the elements of its array parameter\n  * @param  r - the array to be sorted  */\npublic static void insertionSort(int[] r)\n{ for ( int i = 1;  i < r.length;  i = i+1 )\n      // invariant: prefix  r[0]..r[i-1] is sorted\n      { int v = r[i];  // v  is the next element to insert into the prefix\n        int j = i;\n        while ( j != 0  &&  r[j-1] > v )\n              // invariants:\n              // (i) the original prefix, r[0]..r[i-1],\n              //     is now arranged as  r[0]..r[j-1], r[j+1]..r[i];\n              // (ii) all of  r[j+1]..r[i]  are greater than  v\n              { r[j] = r[j-1];\n                j = j-1;\n              }\n        r[j] = v;\n      }\n}\n\nENDFIGURE================================================================\nThe method's most delicate step is searching the sorted prefix to find a space for v---the while-loop searches from right to left, shifting values one by one, until it encounters a value that is not larger than v. At all iterations, position r[j] is reserved for v; when the iterations stop, v is inserted at r[j].\nExercises\nTry selection sort and insertion sort on these arrays: {4, 3, 2, 2}; {1, 2, 3, 4}; {1}; { } (the array of length 0).\nExplain which of the two sorting methods might finish faster when the array to be sorted is already or nearly sorted; when the array's elements are badly out of order.\nExplain why the for-loop in method selectionSort iterates one more time than it truly needs.\nWhy is the test expression, j != 0, required in the while-loop in method insertionSort?\nAnother sorting technique is bubble sort: over and over, compare pairs of adjacent elements and exchange them if the one on the right is less than the one on the left. In this way, the smaller elements move like ``bubbles'' to the left (``top'') of the array. The algorithm goes:\nboolean did_exchanges = true;\nwhile ( did_exchanges )\n      { did_exchanges = false;\n        for ( int i = 1;  i < r.length;  i = i+1 )\n            { If  r[i] < r[i-1},\n                  then exchange them and assign  did_exchanges = true.\n            }\n      }\nProgram this sorting method.\n8.7.4 Searching\nOnce an array is sorted, it becomes simpler to locate an element within it---rather than examining items one by one, from left to right, we can start searching in the middle, at approximately where the item might appear in the sorted collection. (This is what we do when we search for a word in a dictionary.) A standard searching algorithm, called binary search, exploits this idea.\nGiven a sorted array of integers, r, we wish to determine where a value, item, lives in r. We start searching in the middle of r; if item is not exactly the middle element, we compare what we found to it: If item is less than the middle element, then we next search the lower half of the array; if item is greater than the element, we search the upper half of the array. We repeat this strategy until item is found or the range of search narrows to nothing, which means that item is not present.\nThe algorithm goes\nSet searching = true.\nSet the lower bound of the search to be 0 and the upper bound of the\n    search to be the last index of array,  r.\nwhile ( searching  &&  lower bound <= upper bound )\n      { index = (lower bound + upper bound) / 2;\n        if   ( item == r[index] ) { found the item---set  searching = false; }\n        else if ( item < r[index] )  { reset  upper bound = index-1; }\n        else { reset  lower bound = index+1; }\n      }\nFigure 17 shows the method, which is a standard example of the searching pattern of iteration.\nFIGURE 17: binary search=================================================\n\n/** binarySearch  searches for an item in a sorted array\n  * @param  r - the array to be searched\n  * @param  item - the desired item in array  r\n  * @return the index where  item  resides in  r;  if  item  is not\n  *   found, then return  -1  */\npublic int binarySearch(int[] r, int item)\n{ int lower = 0; \n  int upper = r.length - 1; \n  int index = -1;\n  boolean searching = true;\n  while ( searching  &&  lower <= upper )\n        // (1) searching == true  implies  item is in range  r[lower]..r[upper],\n        //       if it exists in  r  at all.\n        // (2) searching == false  implies that  r[index] == item.\n        { index = (lower + upper) / 2;\n          if ( r[index] == item )    \n               { searching = false; }\n          else if ( r[index] < item )\n               { lower = index + 1; }\n          else { upper = index - 1; }\n        }\n  if ( searching )  \n       { index = -1; }  // implies  lower > upper,  hence  item  not in  r\n  return index;\n}\n\nENDFIGURE================================================================\nIf we searched for the item 10 in the sorted array r seen in the examples in the previous section, the first iteration of the loop in binarySearch gives us this configuration:\nThe search starts exactly in the middle, and the loop examines r[2] to see if it is 10. It is not, and since 10 is larger than 8, the value found at r[2], the search is revised as follows:\nSearching the upper half of the array, which is just two elements, moves the search to r[3], which locates the desired item.\nNotice that a linear search, that is,\nint index = 0;\nboolean searching = true;\nwhile ( searching  &&  index != r.length )\n      { if ( r[index] == item )\n             { searching = false; }\n        else { index = index + 1; }\n      }\nwould examine four elements of the array to locate element 10. The binary search examined just two. Binary search's speedup for larger arrays is enormous and is discussed in the next section.\nBinary search is a well-known programming challenge because it is easy to formulate incorrect versions. (Although the loop in Figure 17 is small, its invariant suggests that a lot of thought is embedded within it.) Also, small adjustments lead to fascinating variations. Here is a clever reformulation, due to N. Wirth:\npublic int binarySearch(int[] r, int item)\n{ int lower = 0; \n  int upper = r.length-1; \n  int index = -1;\n  while ( lower <= upper )\n        // (1) lower != upper+2  implies that  item   is in range\n        //     r[lower]..r[upper],  if it exists in  r  at all\n        // (2) lower == upper+2  implies that  r[index] == item\n        { index = (lower + upper) / 2;\n          if ( item <= r[index] )\n             { upper = index - 1; };\n          if ( item >= r[index] )\n             { lower = index + 1; };\n        }\n  if ( lower != upper+2 ) \n     { index = -1; }\n  return index; \n}\nThis algorithm merges variable searching in Figure 17 with the lower and upper bounds of the search so that the loop's test becomes simpler. This alters the loop invariant so that the discovery of item is indicated by lower == upper+2. Both searching algorithms must terminate, because the expression, upper-lower decreases in value at each iteration, ensuring that the loop test will eventually go false.\nExercises\nUse the binary search method in Figure 17 on the sorted array, {1, 2, 2, 4, 6}: Ask the method to search for 6; for 2; for 3. Write execution traces for these searches.\nHere is a binary search method due to R. Howell:\npublic int search(int[] r, int item)\n{ int answer = -1;\n  if ( r.length > 0 )\n     { int lower = 0; \n       int upper = r.length;\n       while ( upper - lower > 1 )\n             // item  is in  r[lower]..r[upper-1], if it is in  r\n             { int index = (lower + upper) / 2;\n               if ( r[index] > item )\n                    { upper = index; }\n               else { lower = index; }\n             }\n       if ( r[lower]== item ) { answer = lower; }\n     }\n  return answer;\n}\nExplain why the invariant and the termination of the loop ensure that the method returns a correct answer. Explain why the loop must terminate. (This is not trivial because the loop makes one extra iteration before it quits.)\n8.7.5 Time-Complexity Measures\nThe previous section stated that binary search computes its answer far faster than does linear search. We can state how much faster by doing a form of counting analysis on the respective algorithms. The analysis will introduce us to a standard method for computing the time complexity of an algorithm. We then apply the method to analyze the time complexity of selection sort and insertion sort.\nTo analyze a searching algorithm, one counts the number of elements the algorithm must examine to find an item (or to report failure). Consider linear search: If array r has, say, N elements, we know in the very worst case that a linear search must examine all N elements to find the desired item or report failure. Of course, over many randomly generated test cases, the number of elements examined will average to about N/2, but in any case, the number of examinations is directly proportional to the the array' length, and we say that the algorithm has performance of order N (also known as linear) time complexity.\nFor example, a linear search of an array of 256 elements will require at most 256 examinations and 128 examinations on the average.\nBecause it halves its range of search at each element examination, binary search does significantly better than linear time complexity: For example, a worst case binary search of a 256-element array makes one examination in the middle of the 256 elements, then one examination in the middle of the remaining 128 elements, then one examination in the middle of the remaining 64 elements, and so on---a maximum of only 9 examinations are required!\nWe can state this behavior more precisely with a recursive definition. Let E(N) stand for the number of examinations binary search makes (in worst case) to find an item in an array of N elements.\nHere is the exact number of examinations binary search does:\nE(N) = 1 + E(N/2),  for  N > 1\nE(1) = 1\nThe first equation states that a search of an array with multiple elements requires an examination of the array's middle element, and assuming the desired item is not found in the middle, a subsequent search of an array of half the length. An array of length 1 requires just one examination to terminate the search.\nTo simplify our analysis of the above equations, say the array's length is a power of 2, that is, N = 2M, for some positive M. (For example, for N = 256, M is 8. Of course, not all arrays have a length that is exactly a power of 2, but we can always pretend that an array is ``padded'' with extra elements to make its length a power of 2.)\nHere are the equations again:\nE(2M) = 1 + E(2M-1),  for  M > 0\nE(20) = 1\nAfter several calculations with this definition (and a proof by induction---see the Exercises), we can convince ourselves that\nE(2M) = M + 1\na remarkably small answer!\nWe say that the binary search algorithm has order log N (or logarithmic) time complexity. (Recall that log N, or more precisely, log2 N, is N's base-2 logarithm, that is, the exponent, M, such that 2M equals N. For example, log 256 is 8, and log 100 falls between 6 and 7.) Because we started our analysis with the assumption that N = 2M, we conclude that\nE(N) = (log N) + 1\nwhich shows that binary search has logarithmic time complexity.\nIt takes only a little experimentation to see, for large values of N, that log N is significantly less than N itself. This is reflected in the speed of execution of binary search, which behaves significantly better than linear search for large-sized arrays.\nAnalysis of Sorting Algorithms\nOf course, binary search assumes that the array it searches is sorted, so we should calculate as well the time complexity of the sorting algorithms we studied. The two factors in the performance of a sorting algorithm are (i) the number of comparisons of array elements, and (ii) the number of exchanges of array elements. If either of these measures is high, this slows the algorithm.\nConsider selection sort first (Figure 15); it locates and exchanges the smallest element, then the next smallest element, and so on. For an array of length N, it uses N-1 comparisons to find the smallest element, N-2 comparisons to find the next smallest element, and so on. The total number of comparisons is therefore\n(N-1) + (N-2) + ...downto... + 2 + 1\nFrom number theory (and an induction proof), we can discover that this sequence totals\nN * (N - 1)\n-------------\n     2\nthat is, (1/2)N2 - (1/2)N. When N has a substantial positive value, only the N2 factor matters, so we say that the algorithm has order N2 ( quadratic) time complexity.\nAlgorithms with quadratic time complexity perform significantly slower than logarithmic and linear algorithms, and this slowness can be annoying when N is very large (e.g., for N equals 100, N2 is 10,000).\nIt is easy to see that selection sort does exactly N-1 exchanges of elements---a linear time complexity---so the exchanges are not the costly part of the algorithm.\nNext, we consider insertion sort (Figure 16); recall that it shifts elements, one by one, from right to left into their proper places. In worst case, insertion sort encounters an array whose elements are in reverse order. In this case, the algorithm's first iteration makes one comparison and one exchange; the second iteration makes two comparisons and two exchanges; and so on. The total number of comparisons and exchanges are the same, namely,\n1 + 2 + ... + (N-2) + N-1\nThis is the same sequence we encountered in our analysis of selection sort, so we conclude that insertion sort also has quadratic time complexity.\nAlthough selection sort's time complexity is stable across all possible permutations of arrays to be sorted, insertion sort executes much faster when it is given an almost completely sorted array to sort. This is because insertion sort shifts elements only when they are out of order. For example, if insertion sort is given an array of length N+1 where only one element is out of order, it will take only order N (linear) time to shift the element to its proper position. For this reason, insertion sort is preferred for sorting almost-sorted arrays.\nIn contrast, insertion sort does badly at exchanging elements when sorting an arbitrary array---it makes order N2 exchanges, whereas selection sort limits its exchanges to at worst order N. Therefore, selection sort is preferred if there is substantial difficulty in moving elements of the array. (But this is not normally the case for Java arrays, because the elements of a Java array are either primitive values, like numbers, or addresses of objects. These values are easy to exchange.)\nExercises\nTo get intuition about time complexities, calculate the values of N, 5*N, log N, N2, and (1/2)(N2) - (1/2)N for each of the following values of N: 4; 64; 128; 512; 1024; 16384.\nThen, reexamine the time complexities of the searching and sorting algorithms and describe how the algorithms would behave on arrays of size N, for the above values of N. (To give some perspective to the analysis, pretend that your computer is very slow and takes 0.1 seconds to perform a comparison or exchange operation.)\nModify class Database in Figure 3 so that its insert method sorts the base array after a new record is added. (Warning---watch for null values in the array!) Because the contents of base are already sorted when a new element is inserted, does this simplify the sorting process? What form of sorting is better for this application---selection sort or insertion sort?\nNext, modify locationOf so that it uses binary search.\nPerform time-complexity analyses of the following methods:\nFor Figure 1, Chapter 7, measure the time complexity of summation(N), depending on the value of N. Count the number of assignments the method makes.\nFor Figure 3, Chapter 7, measure the time complexity of findChar(c, s), depending on the lengths of string s. Count the number of charAt operations the method makes.\nFor Figure 13, measure the time complexity of paint, depending on the size of array that must be painted. Count the number of invocations of paintPiece.\nOur time-complexity analyses are a bit simplistic: a precise time-complexity analysis would count every operation that a computer's processor makes, that is, every arithmetic operation, every comparison operation, every variable reference, every assignment, every method invocation, every method return, etc. Perform such a detailed analysis for the algorithms in the previous Exercise; for linear search; for binary search. Are your answers significantly different than before?\nUse mathematical induction to prove that E(2M) = M + 1, for all nonnegative values of M. This requires that you prove these two claims:\nbasis step: E(20) = 0 + 1\ninduction step: Assume that E(2i) = i + 1 holds true. Use this to prove E(2i+1) = (i+1) + 1.\nUse mathematical induction to prove that (N-1) + (N-2) + ...downto... + 2 + 1 equals (1/2)(N2) - (1/2)N, for all values of N that are 2 or larger. This requires that you prove these two claims:\nbasis step: (2-1) + (2-2) + ...downto... + 2 + 1 equals (1/2)(22) - (1/2)2. (Hint: read the sequence, 1 + ...downto... + 1 as being just the one-element sequence, 1.)\ninduction step: Assume that (i-1) + (i-2) + ...downto... + 2 + 1 equals (1/2)(i2) - (1/2)i. Use this to prove ((i+1)-1) + ((i+1)-2) + ...downto... + 2 + 1 equals (1/2)((i+1)2) - (1/2)(i+1).\n8.7.6 Divide-and-Conquer Algorithms\nIn the previous section, we saw that the binary search algorithm has a significantly better time complexity than the linear search algorithm. The time measurement for binary search was expressed by a recursive definition, which suggests that a recursion might be a factor in binary search's performance. This is indeed the case---binary search is an example of a style of recursion known as divide and conquer, which we study in this section.\nFirst, Figure 18 shows binary search written in recursive style.\nFIGURE 18: binary search by recursion=================================\n\n/** binarySearch  searches for an item within a segment of a sorted array\n  * @param  r - the array to be searched\n  * @param item - the desired item\n  * @param lower - the lower bound of the segment\n  * @param upper - the upper bound of the segment\n  * @return the index where  item  resides in  r[lower]..r[upper];\n  *  return -1, if  item  is not present in the segment of  r  */\npublic int binarySearch(int[] r, int item, int lower, int upper)\n{ int answer = -1;\n  if ( lower <= upper )\n     { int index = (lower + upper) / 2;\n       if ( r[index] == item )\n               { answer = index; }\n       else if ( r[index] < item )\n               { answer = binarySearch(r, item, index + 1, upper); }\n       else { answer = binarySearch(r, item, lower, index - 1); }\n     }\n  return answer;\n}\n\nENDFIGURE==============================================================\nTo search an entire array, a, for a value, v, the method is invoked as binarySearch(a, v, 0, a.length-1). The method clearly shows that, at each recursive invocation, the segment searched is divided in half. Eventually, the desired item is found or the segment is divided into nothing.\nThe method in the Figure is an example of a divide-and-conquer algorithm, so called because the algorithm divides its argment, the array, into smaller segments at each invocation. The divide-and-conquer pattern uses recursion correctly, because each recursive invocation operates on parameters (the array segments) that grow smaller until they reach a stopping value (size 0).\nMerge sort\nSorting can be accomplished with a divide-and-conquer algorithm, which proceeds as follows: To sort a complete array, r,\nDivide the array into two smaller segments, call them s1 and s2.\nSort s1.\nSort s2.\nMerge the two sorted segments to form the completely sorted array.\nThe merge step goes as follows: Say that you have a deck of cards you wish to sort. You divide the deck in half and somehow sort each half into its own pile. You merge the two piles by playing this ``game'': Turn over the top card from each pile. (The top cards represent the lowest-valued cards of the two piles.) Take the lower-valued of the two cards, form a new pile with it, and turn over the next card from the pile from which you took the lower-valued card. Repeat the game until all the cards are moved into the third pile, which will be the entire deck, sorted.\nFigure 19 shows the method based on this algorithm, called merge sort.\nFIGURE 19: merge sort=================================================\n\n/** mergeSort builds a sorted array segment\n  * @param r - the array\n  * @param lower - the lower bound of the segment to be sorted\n  * @param upper - the upper bound of the segment to be sorted\n  * @return a sorted array whose elements are those in  r[lower]..r[upper]  */\npublic int[] mergeSort(int[] r, int lower, int upper)\n{ int[] answer;\n  if ( lower > upper )  // is it an empty segment?\n     { answer = new int[0]; }  \n  else if ( lower == upper ) // is it a segment of just one element?\n     { answer = new int[1];\n       answer[0] = r[lower];\n     }\n  else // it is a segment of length 2 or more, so divide and conquer:\n     { int middle = (lower + upper) / 2;\n       int[] s1 = mergeSort(r, lower, middle);\n       int[] s2 = mergeSort(r, middle+1, upper);\n       answer = merge(s1, s2);\n     }\n  return answer;\n}\n\n/** merge builds a sorted array by merging its two sorted arguments\n  * @param r1 - the first sorted array\n  * @param r2 - the second sorted array\n  * @return a sorted array whose elements are exactly those of  r1 and r2  */\nprivate int[] merge(int[] r1, int[] r2)\n{ int length = r1.length + r2.length;\n  int[] answer = new int[length];\n  int index1 = 0;\n  int index2 = 0;\n  for ( int i = 0;  i != length;  i = i+1 )\n      // invariant: answer[0]..answer[i-1]  is sorted and holds the elements of \n      //   r1[0]..r1[index1-1]  and  r2[0]..r2[index2-1]\n      { if (    index1 == r1.length\n           || ( index2 != r2.length  &&  r2[index2] < r1[index1] ) )\n             { answer[i] = r2[index2];\n               index2 = index2 + 1;\n             }\n        else { answer[i] = r1[index1];\n               index1 = index1 + 1;\n             }\n      }\n  return answer;\n}\n\nENDFIGURE==============================================================\nLike the recursive version of binary search, mergeSort is first invoked as mergeSort(a, 0, a.length-1) to indicate that all the elements in array a should be sorted. The method returns a new array that contains a's elements reordered.\nMethod mergeSort first verifies that the segment of the array it must sort has at least two elements; if it does, the segment is divided in two, the subsegments are sorted, and merge combines the two sorted subarrays into the answer.\nThe time complexity of merge sort is is significantly better than the other sorting algorithms seen so far; we consider the number of comparisons the algorithm makes. (The analysis of element exchanges goes the same.)\nFirst, we note that merge(r1, r2) makes as many comparisons as there are elements in the shorter of its two array parameters, but it will be convenient to overestimate and state that no more than r1.length + r2.length comparisons are ever made.\nNext, we define the comparisons made by mergeSort on an array of length N as the quantity, C(N):\nC(N) = C(N / 2) + C(N / 2) + N,  if  N > 1\nC(1) = 0\nThe first equation states that the total comparisons to sort an array of length 2 or more is the sum of the comparisons needed to sort the left segment, the comparisons needed to sort the right segment, and the comparisons needed to merge the two sorted segments. Of course, an array of length 1 requires no comparisons.\nOur analysis of these equations goes simpler if we we pretend the array's length is a power of 2, that is N = 2M, for some nonnegative M:\nC(2M) = C(2M-1) + C(2M-1) +  2M C(20) = 0\nThese equations look like the ones discovered in the analysis of binary search. Indeed, if we divide both sides of the first equation by 2M, we see the pattern in the binary search equation:\nC(2M)       C(2M-1)\n------  =  -------  + 1\n  2M 2M-1\nAs with the binary search equation, we can conclude that\nC(2M)      \n-------  =  M\n  2M\nWhen we multiply both sides of the above solution by 2M, we see that\nC(2M) = 2M * M\nand since N = 2M, we have that\nC(N) = N * log N\nWe say that merge sort has order N log N time complexity. Such algorithms perform almost as well as linear-time algorithms, so our discovery is significant.\nAlas, mergeSort suffers from a significant flaw: When it sorts an array, it creates additional arrays for merging---this will prove expensive when sorting large arrays. The method in Figure 19 freely created many extra arrays, but if we are careful, we can write a version of mergeSort that creates no more than one extra array the same size as the original, unsorted array. For arrays that model large databases, even this might be unacceptable, unfortunately.\nQuicksort\nA brilliant solution to the extra-array problem was presented by C.A.R. Hoare in the guise of the ``quicksort'' algorithm. Like merge sort, quicksort uses the divide-and-conquer technique, but it cleverly rebuilds the sorted array segments within the original array: It replaces the merge step, which occurred after the recursive invocations, with a partitioning step, which occurs before the recursive invocations.\nThe idea behind partitioning can be understood this way: Say that you have a deck of unsorted playing cards. You partition the cards by (i) choosing a card at random from the deck and (ii) creating two piles from the remaining cards by placing those cards whose values are less than the chosen card in one pile and placing those cards whose values are greater than the chosen card in the other.\nIt is a small step from partitioning to sorting: If you sort the cards in each pile, then the entire deck is sorted by just concatenating the piles. This is a classic divide-and-conquer strategy and forms the algorithm for quicksort. Given an array, r, whose elements are numbered r[lower] to r[upper]:\nRearrange (partition) r into two nonempty subarrays so that there is an index, m, such that all the elements in r[lower]..r[m] are less than or equal to all elements in r[m+1]..r[upper].\nSort the partition r[lower]..r[m].\nSort the partition r[m+1]..r[upper].\nThe end result must be the array entirely sorted.\nFigure 20 gives the quickSort method, which is invoked as quickSort(r, 0, r.length-1), for array r. The hard work is done by partition(r, lower, upper), which partitions the elements in the range r[lower]..r[upper] into two groups. The method uses the element at r[lower] as the ``pivot'' value for partitioning as it scans the elements from left to right, moving those values less than the pivot to the left side of the subarray. Once all the elements are scanned, the ones less than the pivot form the first partition, and the ones greater-or-equal to the pivot form the second partition.\nFIGURE 20: quicksort===================================================\n\n/** quickSort sorts an array within the indicated bounds\n  * @param r - the array to be sorted\n  * @param lower - the lower bound of the elements to be sorted\n  * @param upper - the upper bound of the elements to be sorted  */\npublic void quickSort(int[] r, int lower, int upper)\n{  if ( lower < upper )\n     { int middle = partition(r, lower, upper);\n       quickSort(r, lower, middle);\n       quickSort(r, middle+1, upper);\n     }\n}\n\n/** partition rearranges an array's elements into two nonempty partitions\n  * @param r - an array of length 2 or more\n  * @param lower - the lower bound of the elements to be partitioned\n  * @param upper - the upper bound of the elements to be partitioned\n  * @return the index,  m,  such that all elements in the nonempty partition,\n  *   r[lower]..r[m],  are  <=  all elements in the nonempty partition, \n  *   r[m+1]..r[upper]  */\nprivate int partition(int[] r, int lower, int upper)\n{ int v = r[lower];   // the ``pivot'' value used to make the partitions\n  int m = lower - 1;  // marks the right end of the first partition\n  int i = lower + 1;  // marks the right end of the second partition\n  while ( i <= upper )\n        // invariant: (i) all of  r[lower]..r[m]  are < v\n        //           (ii) all of  r[m+1]..r[i-1]  are >= v, \n        //                and the partition is nonempty\n        { if ( r[i] < v )\n             { // insert r[i] at the end of the first partition\n               // by exchanging it with  r[m+1]:\n               m = m + 1;\n               int temp = r[i];\n               r[i] = r[m];\n               r[m] = temp;\n             }\n          i = i + 1;\n        }\n  if ( m == lower - 1 )  // after all the work, is the first partition empty?\n     { m = m + 1; }      // then place  r[lower], which is  v,  into it\n  return m;\n}\n\nENDFIGURE==============================================================\nIt is essential that both of the partitions created by partition are nonempty. For this reason, a conditional statement after the while-loop asks whether the partition of elements less than the pivot is empty. If it is, this means the pivot is the smallest value in the subarray, no exchanges were made, and the pivot remains at r[lower]. In this case, the pivot value itself becomes the first partition.\nWe can see partitioning at work in an example. Say that we invoke quickSort(r, 0 ,6), which immediately invokes partition(r, 0, 6) for the array r shown below. The variables in partition are initialized as follows:\nWe position m and i under the array to indicate the variables' values. The pivot value is r[0]---5. Values less than the pivot will be moved to the left; the other values will move to the right.\nWithin partition's while-loop, i moves right, searching for a value less than 5; it finds one at element 2. This causes r[2] to be moved to the end of the first partition---it is exchanged with r[m+1], and both m and i are incremented. Here is the resulting situation:\nA check of the loop invariant verifies that the elements in the range r[0] to r[m] are less than the pivot, and the values in the range r[m+1] to r[i-1] are greater-or-equal to the pivot.\nImmediately, i has located another value to be moved to the first partition. An exchange is undertaken between r[1] and r[3], producing the following:\nThe process continues; one more exchange is made. When the method finishes, here is the partitioned array:\nSince m is 2, the partitions are r[0]..r[2] and r[3]..r[6].\nOnce a partitioning step is complete, quickSort recursively sorts the two partitions. This causes each subarray, r[0]..r[2] and r[3]..r[6], to be partitioned and recursively sorted. (That is, the invocation, quicksort(r, 0, 2) invokes partition(r, 0, 2), and quicksort(r, 3, 6) invokes partition(r, 3, 6), and so on.) Eventually, partitions of size 1 are reached, stopping the recursive invocations.\nFor quickSort to perform at its best, the partition method must generate partitions that are equally sized. In such a case, each recursive invocation of quickSort operates on an array segment half the size of the previous one, and the time complexity is the same as mergesort---order N log N. But there is no guarantee that partition will always break an array into two equally sized partitions---if the pivot value, v, is the largest (or smallest) value in an array segment of size N, then partition creates one partition of size 1 and one of size N-1. For example, if array r was already sorted\nand we invoked partition(r, 0, 6), then partition would choose the pivot to be 1 and would create the partitions r[0] and r[1]..r[6]. The subsequent recursive invocation to quickSort(r, 1, 6) causes another such partitioning: r[1] and r[2]..r[6]. This behavior repeats for all the recursive calls.\nIn a case as the above, quickSort degenerates into a variation of insertion sort and operates with order N2 time complexity. Obviously, if quickSort is applied often to sorted or almost-sorted arrays, then partition should choose a pivot value from the middle of the array rather than from the end (see the Exercises below). Studies of randomly generated arrays shows that quicksort behaves, on the average, with order N log N time complexity.\nExercises\nTo gain understanding, apply iterative binarySearch in Figure 17 and recursive binarySearch in Figure 18 to locate the value, 9, in the array, int[] r = {-2, 5, 8, 9, 11, 14}. Write execution traces.\nWrite an execution trace of mergeSort applied to the array int[] r = {5, 8, -2, 11, 9}.\nRewrite mergeSort in Figure 19 so that it does not create multiple new arrays. Instead, use this variant:\n/** mergeSort sorts a segment of an array, r\n  * @param r - the array whose elements must be sorted\n  * @param scratch - an extra array that is the same length as  r\n  * @param lower - the lower bound of the segment to be sorted\n  * @param upper - the upper bound of the segment to be sorted  */\npublic void mergeSort(int[] r, int[] scratch, int lower, int upper)\n{ ...\n  mergeSort(r, scratch, lower, middle);\n  mergeSort(r, scratch, middle+1, upper);\n  ...\n}\nThe method is initially invoked as follows: mergeSort(a, new int[a.length], 0, a.length-1).\nFinish the execution traces for the example in this section that uses quickSort.\nWrite a partition algorithm for use by quickSort that chooses a pivot value in the middle of the subarray to be partitioned.\nBecause quickSort's partition method is sensitive to the pivot value it chooses for partitioning, a standard improvement is to revise partition so that, when it partitions a subarray of size 3 or larger, partition chooses 3 array elements from the subarray and picks the median (the ``middle value'') as the pivot. Revise partition in this way.\n"}, {"score": 679.1255, "uuid": "22347b7e-898d-5983-aaa1-621b8a0e7f34", "index": "cw12", "trec_id": "clueweb12-0105wb-90-05242", "target_hostname": "www.cs.sunysb.edu", "target_uri": "http://www.cs.sunysb.edu/~algorith/lectures-good/index.html", "page_rank": 1.3087293e-09, "spam_rank": 70, "title": "CSE 373&#x2F;548 - Analysis of <em>Algorithms</em> Spring 1996", "snippet": "8.2-3 Argue that insertion <em>sort</em> <em>is</em> <em>better</em> than <em>Quicksort</em> for sorting checks In the best case, <em>Quicksort</em> takes . Although using median-of-three turns the sorted permutation into the best case, we lose if insertion <em>sort</em> <em>is</em> <em>better</em> on the given data.", "explanation": null, "document": "36.5-5 Prove that Hamiltonian Path is NP-complete.\nThis is not a special case of Hamiltonian cycle! ( G may have a HP but not cycle)\nThe easiest argument says that G contains a HP but no HC iff (x,y) in G such that adding edge (x, y) to G causes to have a HC, so\ncalls to a HC function solves HP.\nThe cleanest proof modifies the VC and HC reduction from the book:\nListening to Part 27-3\nApproximating Vertex Cover\nAs we have seen, finding the minimum vertex cover is NP-complete. However, a very simple strategy (heuristic) can get us a cover at most twice that of the optimal.\nWhile the graph has edges\npick an arbitrary edge v, u\nadd both u and v to the cover\ndelete all edges incident on either u and v\nIf the graph is represented by an adjacency list this can be implemented in O(m+n) time.\nThis heuristic must always produce cover, since an edge is only deleted when it is adjacent to a cover vertex.\nFurther, any cover uses at least half as many vertices as the greedy cover.\nWhy? Delete all edges from the graph except the edges we selected.\nNo two of these edges share a vertex. Therefore, any cover of just these edges must include one vertex per edge, or half the greedy cover!\nListening to Part 27-4\nThings to Notice\nAlthough the heuristic is simple, it is not stupid. Many other seemingly smarter ones can give a far worse performance in the worst case.\nExample: Pick one of the two vertices instead of both (after all, the middle edge is already covered) The optimal cover is one vertex, the greedy heuristic is two vertices, while the new/bad heuristic can be as bad as n-1.\nProving a lower bound on the optimal solution is the key to getting an approximation result.\nMaking a heuristic more complicated does not necessarily make it better. It just makes it more difficult to analyze.\nA post-processing clean-up step (delete any unecessessary vertex) can only improve things in practice, but might not help the bound.\nListening to Part 27-5\nThe Euclidean Traveling Salesman\nIn the traditional version of TSP - a salesman wants to plan a drive to visit all his customers exactly once and get back home.\nEuclidean geometry satisfies the triangle inequality,\n.\nTSP remains hard even when the distances are Euclidean distances in the plane.\nNote that the cost of airfares is an example of a distance function which violates the triangle inequality.\nHowever, we can approximate the optimal Euclidean TSP tour using minimum spanning trees.\nClaim: the cost of a MST is a lower bound on the cost of a TSP tour.\nWhy? Deleting any edge from a TSP tour leaves a path, which is a tree of weight at least that of the MST!\nListening to Part 27-6\nIf we were allowed to visit cities more than once, doing a depth-first traversal of a MST, and then walking out the tour specified is at most twice the cost of MST. Why? We will be using each edge exactly twice.\nEvery edge is used exactly twice in the DFS tour: 1.\nHowever, how can we avoid revisiting cities?\nWe can take a shortest path to the next unvisited vertex. The improved tour is 1-2-3-5-8-9-6-4-7-10-11-1. Because we replaced a chain of edges by the edge, the triangle inequality ensures the tour only gets shorter. Thus this is still within twice optimal!\n37.1-3 Give an efficient greedy algorithm that finds an optimal vertex cover of a tree in linear time.\nIn a vertex cover we need to have at least one vertex for each edge.\nEvery tree has at least two leaves, meaning that there is always an edge which is adjacent to a leaf. Which vertex can we never go wrong picking? The non-leaf, since it is the only one which can also cover other edges!\nAfter trimming off the covered edges, we have a smaller tree. We can repeat the process until the tree as 0 or 1 edges. When the tree consists only of an isolated edge, pick either vertex.\nAll leaves can be identified and trimmed in O(n) time during a DFS.\nFormal Languages and the Theory of NP-completeness\nThe theory of NP-completeness is based on formal languages and Turing machines, and so we will must work on a more abstract level than usual.\nFor a given alphabet of symbols\n0, 1, &, we can form an infinite set of strings or words by arranging them in any order: `&10', `111111',`&&&', and `&'.\nA subset of the set of strings over some alphabet is a formal language.\nFormal language theory concerns the study of how powerful a machine you need to recognize whether a string is from a particular language.\nExample: Is the string a binary representation of a even number? A simple finite machine can check if the last symbol is zero:\nNo memory is required, except for the current state.\nObserve that solving decision problems can be thought of as formal language recognition. The problem instances are encoded as strings and strings in the language if and only if the answer to the decision problem is YES!\nWhat kind of machine is necessary to recognize this language? A Turing Machine!\nA Turing machine has a finite-state-control (its program), a two way infinite tape (its memory) and a read-write head (its program counter)\nSo, where are we?\nEach instance of an optimization or decision problem can be encoded as string on some alphabet. The set of all instances which return True for some problem define a language.\nHence, any problem which solves this problem is equivalent to a machine which recognizes whether an instance is in the language!\nThe goal of all this is going to be a formal way to talk about the set of problems which can be solved in polynomial time, and the set that cannot be.\nNon-deterministic Turing Machines\nSuppose we buy a guessing module peripherial for our Turing machine, which looks at a Turing machine program and problem instance and in polynomial time writes something it says is an answer. To convince ourselves it really is an answer, we can run another program to check it.\nEx: The Traveling Salesman Problem\nThe guessing module can easily write a permutation of the vertices in polynomial time. We can check if it is correct by summing up the weights of the special edges in the permutation and see that it is less than k.\nThe class of languages which we can recognize in time polynomial in the size of the string or a deterministic Turing Machine (without guessing module) is called P.\nThe class of languages we can recognize in time polynomial in the length of the string or a non-deterministic Turing Machine is called NP.\nClearly,\n, since for any DTM program we can run it on a non-deterministic machine, ignore what the guessing module is doing, and it will just as fast.\nP ?= NP\nObserve that any NDTM program which takes time P(n) can simulated in\ntime on a deterministic machine, by running the checking program\ntimes, once on each possible guessed string.\nThe $10,000 question is whether a polynomial time simulation exists, or in other words whether P=NP?. Do there exist languages which can be verified in polynomial time and still take exponential time on deterministic machines?\nThis is the most important question in computer science. Since proving an exponential time lower bound for a problem in NP would make us famous, we assume that we cannot do it.\nWhat we can do is prove that it is at least as hard as any problem in NP. A problem in NP for which a polynomial time algorithm would imply all languages in NP are in P is called NP-complete.\nTuring Machines and Cook's Theorem\nCook's Theorem proves that satisfiability is NP-complete by reducing all non-deterministic Turing machines to SAT.\nEach Turing machine has access to a two-way infinite tape (read/write) and a finite state control, which serves as the program.\nA program for a non-deterministic TM is:\nSpace on the tape for guessing a solution and certificate to permit verification.\nA finite set of tape symbols\nA finite set of states\nfor the machine, including the start state\nand final states\nA transition function, which takes the current machine state, and current tape symbol and returns the new state, symbol, and head position.\nWe know a problem is in NP if we have a NDTM program to solve it in worst-case time p[n], where p is a polynomial and n is the size of the input.\nCook's Theorem - Satisfiability is NP-complete!\nProof: We must show that any problem in NP is at least as hard as SAT. Any problem in NP has a non-deterministic TM program which solves it in polynomial time, specifically P(n).\nWe will take this program and create from it an instance of satisfiability such that it is satisfiable if and only if the input string was in the language.\nIf a polynomial time transform exists, then SAT must be NP-complete, since a polynomial solution to SAT gives a polynomial time algorithm to anything in NP.\nOur transformation will use boolean variables to maintain the state of the TM:\nVariable\n"}, {"score": 556.70715, "uuid": "7fc6d81e-16e8-58a0-bafb-71c64c39a5ea", "index": "cw12", "trec_id": "clueweb12-1410wb-26-09560", "target_hostname": "opendatastructures.org", "target_uri": "http://opendatastructures.org/versions/edition-0.1d/ods-java/node56.html", "page_rank": 1.2698046e-09, "spam_rank": 88, "title": "11.1 Comparison-Based Sorting", "snippet": "Unlike <em>merge</em>-<em>sort</em>, <em>which</em> does merging after solving the two subproblems, <em>quicksort</em> does all its work upfront.", "explanation": null, "document": "11.1.4 A Lower-Bound for Comparison-Based Sorting\nWe have now seen three comparison-based sorting algorithms that each run in\ntime. By now, we should be wondering if faster algorithms exist. The short answer to this question is no. If the only operations allowed on the elements of\nare comparisons then no algorithm can avoid doing roughly\ncomparisons. This is not difficult to prove, but requires a little imagination. Ultimately, it follows from the fact that\n(Proving this fact is left as Exercise 11.4 .)\nWe will first focus our attention on deterministic algorithms like merge-sort and heap-sort and on a particular fixed value of\n. Imagine such an algorithm is being used to sort\ndistinct elements. The key to proving the lower-bound is to observe that, for a deterministic algorithm with a fixed value of\n, the first pair of elements that are compared is always the same. For example, in\n, when\nis even, the first call to\nis with\nand the first comparison is between elements\nand\n.\nSince all input elements are distinct, this first comparison has only two possible outcomes. The second comparison done by the algorithm may depend on the outcome of the first comparison. The third comparison may depend on the results of the first two, and so on. In this way, any deterministic comparison-based sorting algorithm can be viewed as a rooted binary comparison-tree. Each internal node,\n, of this tree is labelled with a pair of of indices\nand\n. If\nthe algorithm proceeds to the left subtree, otherwise it proceeds to the right subtree. Each leaf\nof this tree is labelled with a permutation\nof\n. This permutation represents the permutation that is required to sort\nif the comparison tree reaches this leaf. That is,\nAn example of a comparison tree for an array of size\nis shown in Figure 11.5 .\nFigure 11.5: A comparison tree for sorting an array\nof length\n.\nThe comparison tree for a sorting algorithm tells us everything about the algorithm. It tells us exactly the sequence of comparisons that will be performed for any input array\nhaving\ndistinct elements and it tells us how the algorithm will reorder\nto sort it. An immediate consequence of this is that the comparison tree must have at least\nleaves; if not, then there are two distinct permutations that lead to the same leaf, so the algorithm does not correctly sort at least one of these permutations.\nFor example, the comparison tree in Figure 11.6 has only\nleaves. Inspecting this tree, we see that the two input arrays\nand\nboth lead to the rightmost leaf. On the input\nthis leaf correctly outputs\n"}, {"score": 553.4206, "uuid": "cbb05424-f904-5603-b266-fb1a8e4cffe7", "index": "cw12", "trec_id": "clueweb12-1411wb-37-10488", "target_hostname": "opendatastructures.org", "target_uri": "http://opendatastructures.org/ods-java/11_1_Comparison_Based_Sorti.html", "page_rank": 1.2410147e-09, "spam_rank": 89, "title": "11.1 Comparison-Based Sorting", "snippet": "Unlike <em>merge</em>-<em>sort</em>, <em>which</em> does merging after solving the two subproblems, <em>quicksort</em> does all its work upfront.", "explanation": null, "document": "11.1.4 A Lower-Bound for Comparison-Based Sorting\nWe have now seen three comparison-based sorting algorithms that each run in\ntime. By now, we should be wondering if faster algorithms exist. The short answer to this question is no. If the only operations allowed on the elements of\nare comparisons then no algorithm can avoid doing roughly\ncomparisons. This is not difficult to prove, but requires a little imagination. Ultimately, it follows from the fact that\n(Proving this fact is left as Exercise 11.4 .)\nWe will first focus our attention on deterministic algorithms like merge-sort and heap-sort and on a particular fixed value of\n. Imagine such an algorithm is being used to sort\ndistinct elements. The key to proving the lower-bound is to observe that, for a deterministic algorithm with a fixed value of\n, the first pair of elements that are compared is always the same. For example, in\n, when\nis even, the first call to\nis with\nand the first comparison is between elements\nand\n.\nSince all input elements are distinct, this first comparison has only two possible outcomes. The second comparison done by the algorithm may depend on the outcome of the first comparison. The third comparison may depend on the results of the first two, and so on. In this way, any deterministic comparison-based sorting algorithm can be viewed as a rooted binary comparison-tree. Each internal node,\n, of this tree is labelled with a pair of of indices\nand\n. If\nthe algorithm proceeds to the left subtree, otherwise it proceeds to the right subtree. Each leaf\nof this tree is labelled with a permutation\nof\n. This permutation represents the permutation that is required to sort\nif the comparison tree reaches this leaf. That is,\nAn example of a comparison tree for an array of size\nis shown in Figure 11.5 .\nFigure 11.5: A comparison tree for sorting an array\nof length\n.\nThe comparison tree for a sorting algorithm tells us everything about the algorithm. It tells us exactly the sequence of comparisons that will be performed for any input array\nhaving\ndistinct elements and it tells us how the algorithm will reorder\nto sort it. An immediate consequence of this is that the comparison tree must have at least\nleaves; if not, then there are two distinct permutations that lead to the same leaf, so the algorithm does not correctly sort at least one of these permutations.\nFor example, the comparison tree in Figure 11.6 has only\nleaves. Inspecting this tree, we see that the two input arrays\nand\nboth lead to the rightmost leaf. On the input\nthis leaf correctly outputs\n"}, {"score": 553.34906, "uuid": "d956a2cd-08d8-5890-a4e6-460ec7f985aa", "index": "cw12", "trec_id": "clueweb12-1808wb-80-10341", "target_hostname": "jorgetavares.com", "target_uri": "http://jorgetavares.com/tag/survey/", "page_rank": 1.2053321e-09, "spam_rank": 69, "title": "Survey &laquo; Jorge Tavares weblog", "snippet": "In ECL (and ABCL) <em>quicksort</em> implements a partition scheme <em>which</em> deals <em>better</em> with duplicate elements (although <em>is</em> not the three-way partitioning) but it always picks as pivot the first element.", "explanation": null, "document": "merge sort (lists) / heapsort\nmerge sort\nIn terms of the implementation of sort, quicksort is the most used algorithm, followed by heapsort . The choice for these algorithms is expected. Both have an average-case performance of O(nlgn) and heapsort guarantees a worst-case performace of O(nlgn) too. Quicksort has a worst-case performance of O(n2) but it can be optimized in several ways so that it also gives an expected worst-case performance of O(nlgn). However, it seems that the quicksort implementations are not completely optimized. In ECL (and ABCL) quicksort implements a partition scheme which deals better with duplicate elements (although is not the three-way partitioning) but it always picks as pivot the first element. CCL chooses the pivot with a median-of-3 method and always sorts the smaller partition to ensure a worst-case stack depth of O(lgn).\nAs for CLISP, I think it uses a tree sort but I am not entirely sure. The only source file I could find with a sort implementation was sort.d and it looks like it contains an implementation of tree sort with a self-balanced binary tree, which also gives this algorithm an average and worst-case performance of O(nlgn).\nAs expected, most of the implementations use merge sort to implement stable-sort since it is a stable sort with average and worst-case performance of O(nlgn). Apparently, all implementations are bottom-up merge sorts with the exception of CCL and ECL. Another interesting thing is that merge sort is also used for lists in sort, in most of the implementations. However, I found it surprising to find quicksort in the stable-sort column because it is not a stable algorithm. Since it is only used for strings and bit vectors, it is not really an issue. While reading the source code of the implementations, I realized that ABCL was using quicksort in stable-sort for all non-list sequences. This is a problem that exists in the current 1.0.1 release but I\u2019ve sent a bug report with a quick fix to the maintainers. The next release should have stable-sort fixed.\nThis exploration of the sorting algorithms used in the open source implementations was very educational and interesting to me. I\u2019ve learned what algorithms are actually used and enjoyed seing how they were implemented. Just spotting the issue in ABCL stable-sort made this review worthwhile. I think there is still room for improvement in some implementations but knowing now the strengths and weaknesses of the sorts in CL is already good enough. On a final note, I just wonder what are the algorithms used in ACL and LW .\nWritten by Jorge Tavares\n"}, {"score": 529.08484, "uuid": "ea00b31d-ffb6-589d-b8e3-5e79454b161b", "index": "cw12", "trec_id": "clueweb12-1600wb-30-12308", "target_hostname": "www.ai.mit.edu", "target_uri": "http://www.ai.mit.edu/projects/reinventing_computing/papers/theses/pshuang/meng/node20.html", "page_rank": 1.1792828e-09, "spam_rank": 72, "title": "eqntott", "snippet": "Investigation readily uncovered that the Linux system qsort() <em>is</em> supplied by the GNU implementation of the libc library; by default, it does not use the <em>quicksort</em> <em>algorithm</em>, but rather uses the <em>merge</em> <em>sort</em> <em>algorithm</em> if it can allocate enough temporary buffer to perform a standard two-space <em>merge</em> <em>sort</em>,", "explanation": null, "document": "eqntott (SPECint92)\nIn a 1975 paper [ ], Donald E. Knuth asserted (without further citation) that at that time computer manufacturers believed that about a quarter of computer time was spent sorting. Regardless of whether that fraction was true then, or what that fraction would be today, sorting is an example of software algorithms as technology. A civil engineer chooses from a wide variety of different building material technologies (e.g., wood frame, reinforced concrete, steel I-beam), based on the desired tradeoffs between cost, aesthetics, and strength, which depend on the needs of the particular project. A software engineer similarly chooses from a wide variety of different sorting algorithms based on the desired tradeoffs between coding complexity, suitability to size of problem (i.e., internal vs. external sorts), average space consumption, average time consumption, variability of space/time consumption, and sort stability. In both fields, particular technologies are refined over time, and new technologies may be created with potentially completely different tradeoffs. Taking advantage of evolutionary and revolutionary changes in sorting technologies sounds like fertile ground for a clever compiler.\nThe eqntott application from the SPECint92 suite of benchmarks [ ] translates a logical representation of a boolean equation to a truth table. It includes the following characterization in its description:\nThis characterization of eqntott execution is strictly speaking correct but rather misleading. Using gprof profiling, we see that although eqntott does spends about 90% of its time executing qsort(), about 85% of the total run-time was actually spent in cmppt(), which is only called from one particular call site of qsort().\nAttacking qsort()\nThe source code files for eqntott include an implementation for qsort(), which makes sense, since the stability of standard C library qsort() --- that is, whether it will or will not preserve the initial ordering of elements being sorted which compare as equal --- is intentionally undefined. By including its own qsort(), those who submitted eqntott to the SPEC organization as a benchmark could provide a reference output file to go with the reference input file; whereas if eqntott used the system\nqsort(), then the reference output file might not match eqntott's output on a given platform even though the difference would be entirely due to sort stability, and would not make the resulting output incorrect.\nSince the qsort() routine supplied with eqntott dates back to at least 1983 (it claims to be better than the system qsort() of the day...), I was curious if qsort() implementations had noticeably improved since then. Forcing eqntott to use the system qsort() under SunOS 4.1.3 resulted in nearly identical timings as using the included qsort(); however, forcing eqntott to use the system qsort() on Linux dramatically reduced run-time, from just over 20 seconds to 2.4 seconds.\nInvestigation readily uncovered that the Linux system qsort() is supplied by the GNU implementation of the libc library; by default, it does not use the quicksort algorithm, but rather uses the merge sort algorithm if it can allocate enough temporary buffer to perform a standard two-space merge sort, only falling back to an in-place quicksort if the attempt to allocate memory fails. Both merge sort and quicksort are on average O(N log N) algorithms; however, in the real world, constant factors and not just asymptotic algorithmic complexity matters. It is generally acknowledged that implementations of some of the quicksort variants have the lowest constant factors of well-known comparison-based sorting routines; this is borne out by some basic experimentation --- each row of Table\nwas generated by summing the times and comparisons for 5 different runs, each run sorting 1,000,000 integers generated by random() with seed values 0, 1, 2, 3, and 4 respectively. (Times were quite consistent for runs which used the same sort and had the same sortedness of input, differing only on the random seed.)\nTable: Merge sort and median-of-3 quicksort on arrays of integers.\nSo if the particular quicksort implementation beats the merge sort implementation on both randomized and on already-sorted inputs, what accounts for the nearly an order of magnitude difference in eqntott performance? (A difference in the wrong direction to boot: Table\nwould suggest that eqntott using quicksort would be faster than eqntott using merge sort, when in fact the reverse is true.) One possibility is that most analyses of sort algorithms assume that comparisons take constant time; this is not always true. In particular, every comparison routine defined in eqntott may take variable amounts of time to execute. Hence it can matter greatly to the total run-time exactly which elements are compared with each other, something which was not true for sorting integers. I.e., one hypothesis is that in this case merge sort tends to be comparing elements which the comparison routine can quickly decide whether a<b, a=b, or a>b, whereas quicksort tends to be comparing elements which the comparison routine cannot quickly determine the ordering relationship. It should be noted that such comparison functions are not particularly unusual: an ordering string comparison function, for example, takes variable time to run, being highly dependent on the nature of input data.\nAs attractive a hypothesis as this might be, however, it does not stand up to scrutiny. In particular, profiling indicates that there is a 79-fold reduction in the number of calls to cmppt(): quicksort calls it 2,841,621 times; merge sort, a mere 35,944 times. That is responsible for the reduced run time for eqntott, and not a hypothesized reduction in the average latency for a call to cmppt(). The eqntott call site to qsort() which provides cmppt() as a comparison function hands qsort() an array with just 4,106 items; note that 2,841,621 comparisons to sort 4,106 items seems more nearly O(N\n), despite the fact that the quicksort implementation does have the median-of-3 modification which should prevent most (but clearly not all) cases of such degenerate behavior. The number of calls to cmppt() which the merge sort issues, on the other hand, seems much closer to O(N log N). Is the SPECint92 reference input file particularly unusual in eliciting this kind of behavior from the quicksort implementation?\nThis kind of effort with blind alleys, hypotheses to shoot down, etc., is typical of of a human programmer attempting to optimize a program. If the system qsort() had been written with a qif statement to quasistatically select between a quicksort implementation or a merge sort implementation, it would not be necessary to go to this kind of effort, nor necessary to speculate about whether typical user files provided as input to eqntott tend to elicit O(N\n) behavior from the quicksort used; whether or not quicksort often behaved poorly or not on user inputs would be directly observed and taken into account. Furthermore, casual trials with another similarly-sized input file for eqntott (modeling a 4-bit 16-input/output multiplexer) suggests that the reference input file for the SPECint92 benchmark use of eqntott is in fact somewhat anomalous for eliciting such poor behavior from quicksort; however, program performance for the multiplexor input file was still about 20% better with merge sort than with quicksort, consistent with the results in Table\nwhich shows that the merge sort implementation tends to perform fewer comparisons --- and the number of comparisons, when each comparison can be quite expensive, will dominate the running time more than the efficiency of the inner loops of the sorting algorithm. Hence which sort should be selected in a given usage will depend on both the input data pattern and on the cost of the comparison function provided to qsort(). To reiterate, if the library writer had written the system qsort() with a qif statement, then sorting performance would be improved on average, without any effort from the application programmer's part and without complicating the semantics of qsort() usage.\nAttacking cmppt()\nAnother obvious approach to improving the performance of eqntott would be to try to make cmppt() execute faster. Figure\nshows the inner loop of the comparison function. It's not hard to convince oneself that the code in Figure\nis equivalent Figure\n, but might execute faster on average if\n,\n, or\nwere common conditions. However, it's unclear which of the three terms should come first in the conditionally evaluated boolean-OR expression for best performance; presumably, this is dependent on the input to eqntott. It would be highly tedious to try them all out by hand; but a quasistatic compiler would not be deterred by impatience. With quasistatic if support, the programmer can simply write the code sequence in Figure\nand forget about it.\nNote the total number of permutations is actually far greater than the three which happen to be shown: the three clauses OR'ed together can be arranged 6 different ways; plus, each of the\nand\nclauses contains two sub-clauses AND'ed together, which can also be permuted --- for a total of 24 possible versions.\nFigure: Comparison routine cmppt()'s inner loop.\nFigure: A restructuring of the cmppt() inner loop.\nFigure: Quasistatic version of cmppt() inner loop.\nTable: Performance of different rearrangements of boolean terms.\nTable\nshows the performance numbers for a few of the possible versions; only one of the listed versions performs better than the original version. Note that careless use of quasistatic constructs in tight inner loops resulted in spectacular profiling overhead, far greater than even that for pixie. This is in part because the current real-time profiling implementation does not customize the stopwatch start and stop routines for each place where they are used, even though they are inlined; hence the routines still have greater overhead than pixie's instrumentation code per invocation. Furthermore, it so happens that the problem is vastly excaberated by the fact that the current implementation of real-time profiling has a bug whereby the GNU gcc code generator believes two registers contain live data between the stopwatch start and stop routines, and this causes several of the inner loop variables to be spilled from the unusually small register set of the Intel Pentium processor. Fortunately, the overhead did not in this case mask which alternative would run faster when instrumentation is turned back off. We see that although there is some improvement, the cmppt() comparison function did not offer great opportunity for optimization using quasistatic constructs. However, it does serve as an example where the programmer can use the clever compiler's capabilities as a tool to simplify the task of exploring what-if scenarios to improve program performance, although judiciously (i.e., not in inner loops). It's less tedious and less error-prone to let the clever compiler manage program versions than to do so by hand.\nSeveral other possible transformations to improve eqntott performance, which might benefit from quasistatic constructs, were not fully explored for lack of time. They are described in Appendix\n.\n"}], [{"score": 513.7448, "uuid": "1b9d072f-db77-5d98-a6de-b2b129374b0c", "index": "cw12", "trec_id": "clueweb12-1809wb-09-21271", "target_hostname": "jorgetavares.com", "target_uri": "http://jorgetavares.com/tag/zsort/", "page_rank": 1.2069589e-09, "spam_rank": 81, "title": "zsort &laquo; Jorge Tavares weblog", "snippet": "Right now the implemented <em>algorithms</em> are: insertion <em>sort</em>, <em>quicksort</em>, randomized <em>quicksort</em>, <em>merge</em> <em>sort</em>, heapsort and counting <em>sort</em>. The plan <em>is</em> to add more <em>algorithms</em>, for example, bucket <em>sort</em> and timsort.", "explanation": null, "document": "with 6 comments\nzsort is a library that I started working on as a simple hobby project. More or less around the same time I decided to check which algorithms the different Common Lisp implementations use . It is now part of Quicklisp so it can be easily used (thanks Zack !).\nThe main goal of zsort is to be a collection of portable sorting algorithms. If they can be fast, even better. Common lisp provides the sort and stable-sort functions but these can have different algorithms implemented according to each implementation, which can make an application unportable if you rely on a specific type of sorting. Also, the standard functions might not be the best for a certain situation and as such you might need a specialized sort. Even if for most situations the standard functions are more than enough, the zsort library could be a useful complement.\nRight now the implemented algorithms are: insertion sort , quicksort , randomized quicksort , merge sort , heapsort and counting sort . The plan is to add more algorithms, for example, bucket sort and timsort . However, the main thing on the todo list is adding the possibility of external sorting (to handle large amounts of data) and parallel versions of some sorting algorithms. I am considering using lparallel for this but I am still undecided.\nThere is still a lot of work to be done, but I think the library as it is can already be a little useful. And of course, all kind of suggestions and improvements are welcome!\nWritten by Jorge Tavares\n"}, {"score": 509.83853, "uuid": "d176724d-ce77-5fae-a04e-fcc4149b3341", "index": "cw12", "trec_id": "clueweb12-1809wb-06-11871", "target_hostname": "jorgetavares.com", "target_uri": "http://jorgetavares.com/tag/library/", "page_rank": 1.2069589e-09, "spam_rank": 80, "title": "library &laquo; Jorge Tavares weblog", "snippet": "Right now the implemented <em>algorithms</em> are: insertion <em>sort</em>, <em>quicksort</em>, randomized <em>quicksort</em>, <em>merge</em> <em>sort</em>, heapsort and counting <em>sort</em>. The plan <em>is</em> to add more <em>algorithms</em>, for example, bucket <em>sort</em> and timsort.", "explanation": null, "document": "with 6 comments\nzsort is a library that I started working on as a simple hobby project. More or less around the same time I decided to check which algorithms the different Common Lisp implementations use . It is now part of Quicklisp so it can be easily used (thanks Zack !).\nThe main goal of zsort is to be a collection of portable sorting algorithms. If they can be fast, even better. Common lisp provides the sort and stable-sort functions but these can have different algorithms implemented according to each implementation, which can make an application unportable if you rely on a specific type of sorting. Also, the standard functions might not be the best for a certain situation and as such you might need a specialized sort. Even if for most situations the standard functions are more than enough, the zsort library could be a useful complement.\nRight now the implemented algorithms are: insertion sort , quicksort , randomized quicksort , merge sort , heapsort and counting sort . The plan is to add more algorithms, for example, bucket sort and timsort . However, the main thing on the todo list is adding the possibility of external sorting (to handle large amounts of data) and parallel versions of some sorting algorithms. I am considering using lparallel for this but I am still undecided.\nThere is still a lot of work to be done, but I think the library as it is can already be a little useful. And of course, all kind of suggestions and improvements are welcome!\nWritten by Jorge Tavares\n"}, {"score": 498.12122, "uuid": "a972ed26-49f2-5d2d-8b74-527255828a6c", "index": "cw12", "trec_id": "clueweb12-1808wb-76-25768", "target_hostname": "jorgetavares.com", "target_uri": "http://jorgetavares.com/", "page_rank": 3.3167749e-09, "spam_rank": 77, "title": "Jorge Tavares weblog", "snippet": "Right now the implemented <em>algorithms</em> are: insertion <em>sort</em>, <em>quicksort</em>, randomized <em>quicksort</em>, <em>merge</em> <em>sort</em>, heapsort and counting <em>sort</em>. The plan <em>is</em> to add more <em>algorithms</em>, for example, bucket <em>sort</em> and timsort.", "explanation": null, "document": "with 4 comments\nI was reading the thread What are some fun or useful macros? on reddit and it reminded me of another thread that appeared in the Pro mailing list. These kind of threads are always enjoyable because each time you learn something new and see really interesting things. While reading them, it crossed my mind that another variant of this question would be what are some fun or useful macros design patterns. Instead of examples of specific code macros (general or not) it would be nice to see common programming practices using macros. So, for a lack of a better expression name, let\u2019s call them macros design patterns.\nMy favorite one is to configure an algorithm, especially when we want to use the correct types. Essentially, we write an algorithm using a macro that takes types or configuration arguments and then we expand it to the appropriate desired configurations. For example, if you have an algorithm that operates on different types of sequences, instead of writing several duplicate functions with the same algorithm but with the associated type declarations, just apply this pattern. A simple but contrived example: we want to compute the mean of a vector but use its proper type. In addition, we might also want the possibility of using a key to access the vector elements. We can write the following macro:\n(defmacro mean-body (vector vector-type vector-ref key)\r\n  (let ((size (gensym)) (i (gensym)))\r\n    `(locally \r\n\t (declare (type ,vector-type ,vector))\r\n       (let ((,size (length ,vector)))\r\n\t (/ (loop for ,i from 0 below ,size\r\n\t\t  sum ,(if key \r\n\t\t\t   `(funcall ,key (,vector-ref ,vector ,i))\r\n\t\t\t   `(,vector-ref ,vector ,i)))\r\n\t    ,size)))))\nThe macro contains the algorithm (in this simple case the mean) and the arguments allow us to configure the multiple versions we need. If we want a simple-vector, the macro will expand to use the correct type declaration and svref. If a key function is needed it will also include it. Then, we can call the macro with the several configurations value inside the main function:\n(defun mean (vector &optional key)\r\n  (typecase vector\r\n    (simple-vector \r\n     (if key \r\n\t (mean-body vector simple-vector svref key)\r\n\t (mean-body vector simple-vector svref nil)))\r\n    (vector \r\n     (if key \r\n\t (mean-body vector vector aref key)\r\n\t (mean-body vector vector aref nil)))\r\n    (otherwise \r\n      (if key \r\n\t (mean-body vector sequence elt key)\r\n\t (mean-body vector sequence elt nil)))))\nThis can be very useful in situations where we want to optimize code since it becomes easy to add the proper type declarations to the input arguments of an algorithm. Moreover, we keep the algorithm in a single place, making it easier to maintain. Depending on the situation, we can also define a function for each configuration. In the example we could have a mean-simple-vector and mean-vector.\nI don't know if it has already a specific name but I like to call it the configurable algorithm pattern. I find it very useful. And thinking back to the reddit thread , what are your favorite macros design patterns? Which ones do you find useful and use them regularly? If you want to share, feel free to drop a line. I am interested in seing and learning other patterns!\nWritten by Jorge Tavares\n"}, {"score": 494.1595, "uuid": "33941b2f-7786-56f1-bf2d-69380e84b261", "index": "cw12", "trec_id": "clueweb12-1809wb-06-11864", "target_hostname": "jorgetavares.com", "target_uri": "http://jorgetavares.com/2012/02/", "page_rank": 1.1700305e-09, "spam_rank": 72, "title": "February &laquo; 2012 &laquo; Jorge Tavares weblog", "snippet": "Another interesting thing <em>is</em> that <em>merge</em> <em>sort</em> <em>is</em> also used for lists in <em>sort</em>, in most of the implementations. However, I found it surprising to find <em>quicksort</em> in the stable-<em>sort</em> column because it <em>is</em> not a stable <em>algorithm</em>. Since it <em>is</em> only used for strings and bit vectors, it <em>is</em> not really an issue.", "explanation": null, "document": "with 4 comments\nI was reading the thread What are some fun or useful macros? on reddit and it reminded me of another thread that appeared in the Pro mailing list. These kind of threads are always enjoyable because each time you learn something new and see really interesting things. While reading them, it crossed my mind that another variant of this question would be what are some fun or useful macros design patterns. Instead of examples of specific code macros (general or not) it would be nice to see common programming practices using macros. So, for a lack of a better expression name, let\u2019s call them macros design patterns.\nMy favorite one is to configure an algorithm, especially when we want to use the correct types. Essentially, we write an algorithm using a macro that takes types or configuration arguments and then we expand it to the appropriate desired configurations. For example, if you have an algorithm that operates on different types of sequences, instead of writing several duplicate functions with the same algorithm but with the associated type declarations, just apply this pattern. A simple but contrived example: we want to compute the mean of a vector but use its proper type. In addition, we might also want the possibility of using a key to access the vector elements. We can write the following macro:\n(defmacro mean-body (vector vector-type vector-ref key)\r\n  (let ((size (gensym)) (i (gensym)))\r\n    `(locally \r\n\t (declare (type ,vector-type ,vector))\r\n       (let ((,size (length ,vector)))\r\n\t (/ (loop for ,i from 0 below ,size\r\n\t\t  sum ,(if key \r\n\t\t\t   `(funcall ,key (,vector-ref ,vector ,i))\r\n\t\t\t   `(,vector-ref ,vector ,i)))\r\n\t    ,size)))))\nThe macro contains the algorithm (in this simple case the mean) and the arguments allow us to configure the multiple versions we need. If we want a simple-vector, the macro will expand to use the correct type declaration and svref. If a key function is needed it will also include it. Then, we can call the macro with the several configurations value inside the main function:\n(defun mean (vector &optional key)\r\n  (typecase vector\r\n    (simple-vector \r\n     (if key \r\n\t (mean-body vector simple-vector svref key)\r\n\t (mean-body vector simple-vector svref nil)))\r\n    (vector \r\n     (if key \r\n\t (mean-body vector vector aref key)\r\n\t (mean-body vector vector aref nil)))\r\n    (otherwise \r\n      (if key \r\n\t (mean-body vector sequence elt key)\r\n\t (mean-body vector sequence elt nil)))))\nThis can be very useful in situations where we want to optimize code since it becomes easy to add the proper type declarations to the input arguments of an algorithm. Moreover, we keep the algorithm in a single place, making it easier to maintain. Depending on the situation, we can also define a function for each configuration. In the example we could have a mean-simple-vector and mean-vector.\nI don't know if it has already a specific name but I like to call it the configurable algorithm pattern. I find it very useful. And thinking back to the reddit thread , what are your favorite macros design patterns? Which ones do you find useful and use them regularly? If you want to share, feel free to drop a line. I am interested in seing and learning other patterns!\nWritten by Jorge Tavares\n"}, {"score": 465.89566, "uuid": "022cc656-120f-5fe5-89a8-9e4ec26e3420", "index": "cw12", "trec_id": "clueweb12-1809wb-04-21644", "target_hostname": "jorgetavares.com", "target_uri": "http://jorgetavares.com/2012/04/", "page_rank": 1.1700305e-09, "spam_rank": 72, "title": "April &laquo; 2012 &laquo; Jorge Tavares weblog", "snippet": "Right now the implemented <em>algorithms</em> are: insertion <em>sort</em>, <em>quicksort</em>, randomized <em>quicksort</em>, <em>merge</em> <em>sort</em>, heapsort and counting <em>sort</em>. The plan <em>is</em> to add more <em>algorithms</em>, for example, bucket <em>sort</em> and timsort.", "explanation": null, "document": "with 6 comments\nzsort is a library that I started working on as a simple hobby project. More or less around the same time I decided to check which algorithms the different Common Lisp implementations use . It is now part of Quicklisp so it can be easily used (thanks Zack !).\nThe main goal of zsort is to be a collection of portable sorting algorithms. If they can be fast, even better. Common lisp provides the sort and stable-sort functions but these can have different algorithms implemented according to each implementation, which can make an application unportable if you rely on a specific type of sorting. Also, the standard functions might not be the best for a certain situation and as such you might need a specialized sort. Even if for most situations the standard functions are more than enough, the zsort library could be a useful complement.\nRight now the implemented algorithms are: insertion sort , quicksort , randomized quicksort , merge sort , heapsort and counting sort . The plan is to add more algorithms, for example, bucket sort and timsort . However, the main thing on the todo list is adding the possibility of external sorting (to handle large amounts of data) and parallel versions of some sorting algorithms. I am considering using lparallel for this but I am still undecided.\nThere is still a lot of work to be done, but I think the library as it is can already be a little useful. And of course, all kind of suggestions and improvements are welcome!\nWritten by Jorge Tavares\n"}, {"score": 502.3886, "uuid": "fd6d365c-d02e-560d-bb31-9a47226a6199", "index": "cw12", "trec_id": "clueweb12-1607wb-18-00270", "target_hostname": "cr.openjdk.java.net", "target_uri": "http://cr.openjdk.java.net/~alanb/6880672/webrev.00/raw_files/new/src/share/classes/java/util/Arrays.java", "page_rank": 1.1900824e-09, "spam_rank": 73, "title": ". * DO NOT ALTER <em>OR</em>", "snippet": "The sorting <em>algorithm</em> <em>is</em> the Dual-Pivot <em>Quicksort</em>, suggested by * Vladimir Yaroslavskiy on February 2009.", "explanation": null, "document": "/* * Copyright 1997-2008 Sun Microsystems, Inc. All Rights Reserved. * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER. * * This code is free software; you can redistribute it and/or modify it * under the terms of the GNU General Public License version 2 only, as * published by the Free Software Foundation. Sun designates this * particular file as subject to the \"Classpath\" exception as provided * by Sun in the LICENSE file that accompanied this code. * * This code is distributed in the hope that it will be useful, but WITHOUT * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or * FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License * version 2 for more details (a copy is included in the LICENSE file that * accompanied this code). * * You should have received a copy of the GNU General Public License version * 2 along with this work; if not, write to the Free Software Foundation, * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA. * * Please contact Sun Microsystems, Inc., 4150 Network Circle, Santa Clara, * CA 95054 USA or visit www.sun.com if you need additional information or * have any questions. */ package java.util; import java.lang.reflect.*; /** * This class contains various methods for manipulating arrays (such as * sorting and searching). This class also contains a static factory * that allows arrays to be viewed as lists. * *\nThe methods in this class all throw a NullPointerException if * the specified array reference is null, except where noted. * *\nThe documentation for the methods contained in this class includes * briefs description of the implementations. Such descriptions should * be regarded as implementation notes, rather than parts of the * specification. Implementors should feel free to substitute other * algorithms, so long as the specification itself is adhered to. (For * example, the algorithm used by sort(Object[]) does not have to be * a mergesort, but it does have to be stable.) * *\nThis class is a member of the * * Java Collections Framework . * * @author Josh Bloch * @author Neal Gafter * @author John Rose * @author Vladimir Yaroslavskiy * @since 1.2 */ public class Arrays { // Suppresses default constructor, ensuring non-instantiability. private Arrays() { } // Sorting /** * Sorts the specified array of longs into ascending numerical order. * *\nThe sorting algorithm is the Dual-Pivot Quicksort, suggested by * Vladimir Yaroslavskiy on February 2009. This algorithm offers * n*ln(n) performance on many data sets that cause other quicksorts * to degrade to quadratic (n^2) performance. * * @param a the array to be sorted */ public static void sort(long[] a) { sort(a, 0, a.length); } /** * Sorts the specified range of the specified array of longs into * ascending numerical order. The range to be sorted extends from index * fromIndex, inclusive, to index toIndex, exclusive * (if fromIndex == toIndex, the range to be sorted is empty). * *\nThe sorting algorithm is the Dual-Pivot Quicksort, suggested by * Vladimir Yaroslavskiy on February 2009. This algorithm offers * n*ln(n) performance on many data sets that cause other quicksorts * to degrade to quadratic (n^2) performance. * * @param a the array to be sorted * @param fromIndex the index of the first element (inclusive) to be sorted * @param toIndex the index of the last element (exclusive) to be sorted * @throws IllegalArgumentException if fromIndex > toIndex * @throws ArrayIndexOutOfBoundsException if fromIndex < 0 * or toIndex > a.length */ public static void sort(long[] a, int fromIndex, int toIndex) { rangeCheck(a.length, fromIndex, toIndex); dualPivotQuicksort(a, fromIndex, toIndex - 1, 3); } /** * Sorts the specified array of ints into ascending numerical order. * *\nThe sorting algorithm is the Dual-Pivot Quicksort, suggested by * Vladimir Yaroslavskiy on February 2009. This algorithm offers * n*ln(n) performance on many data sets that cause other quicksorts * to degrade to quadratic (n^2) performance. * * @param a the array to be sorted */ public static void sort(int[] a) { sort(a, 0, a.length); } /** * Sorts the specified range of the specified array of ints into * ascending numerical order. The range to be sorted extends from index * fromIndex, inclusive, to index toIndex, exclusive * (if fromIndex == toIndex, the range to be sorted is empty). * *\nThe sorting algorithm is the Dual-Pivot Quicksort, suggested by * Vladimir Yaroslavskiy on February 2009. This algorithm offers * n*ln(n) performance on many data sets that cause other quicksorts * to degrade to quadratic (n^2) performance. * * @param a the array to be sorted * @param fromIndex the index of the first element (inclusive) to be sorted * @param toIndex the index of the last element (exclusive) to be sorted * @throws IllegalArgumentException if fromIndex > toIndex * @throws ArrayIndexOutOfBoundsException if fromIndex < 0 * or toIndex > a.length */ public static void sort(int[] a, int fromIndex, int toIndex) { rangeCheck(a.length, fromIndex, toIndex); dualPivotQuicksort(a, fromIndex, toIndex - 1, 3); } /** * Sorts the specified array of shorts into ascending numerical order. * *\nThe sorting algorithm is the Dual-Pivot Quicksort, suggested by * Vladimir Yaroslavskiy on February 2009. This algorithm offers * n*ln(n) performance on many data sets that cause other quicksorts * to degrade to quadratic (n^2) performance. * * @param a the array to be sorted */ public static void sort(short[] a) { sort(a, 0, a.length); } /** * Sorts the specified range of the specified array of shorts into * ascending numerical order. The range to be sorted extends from index * fromIndex, inclusive, to index toIndex, exclusive * (if fromIndex == toIndex, the range to be sorted is empty). * *\nThe sorting algorithm is the Dual-Pivot Quicksort, suggested by * Vladimir Yaroslavskiy on February 2009. This algorithm offers * n*ln(n) performance on many data sets that cause other quicksorts * to degrade to quadratic (n^2) performance. * * @param a the array to be sorted * @param fromIndex the index of the first element (inclusive) to be sorted * @param toIndex the index of the last element (exclusive) to be sorted * @throws IllegalArgumentException if fromIndex > toIndex * @throws ArrayIndexOutOfBoundsException if fromIndex < 0 * or toIndex > a.length */ public static void sort(short[] a, int fromIndex, int toIndex) { rangeCheck(a.length, fromIndex, toIndex); dualPivotQuicksort(a, fromIndex, toIndex - 1, 3); } /** * Sorts the specified array of chars into ascending numerical order. * *\nThe sorting algorithm is the Dual-Pivot Quicksort, suggested by * Vladimir Yaroslavskiy on February 2009. This algorithm offers * n*ln(n) performance on many data sets that cause other quicksorts * to degrade to quadratic (n^2) performance. * * @param a the array to be sorted */ public static void sort(char[] a) { sort(a, 0, a.length); } /** * Sorts the specified range of the specified array of chars into * ascending numerical order. The range to be sorted extends from index * fromIndex, inclusive, to index toIndex, exclusive * (if fromIndex == toIndex, the range to be sorted is empty). * *\nThe sorting algorithm is the Dual-Pivot Quicksort, suggested by * Vladimir Yaroslavskiy on February 2009. This algorithm offers * n*ln(n) performance on many data sets that cause other quicksorts * to degrade to quadratic (n^2) performance. * * @param a the array to be sorted * @param fromIndex the index of the first element (inclusive) to be sorted * @param toIndex the index of the last element (exclusive) to be sorted * @throws IllegalArgumentException if fromIndex > toIndex * @throws ArrayIndexOutOfBoundsException if fromIndex < 0 * or toIndex > a.length */ public static void sort(char[] a, int fromIndex, int toIndex) { rangeCheck(a.length, fromIndex, toIndex); dualPivotQuicksort(a, fromIndex, toIndex - 1, 3); } /** * Sorts the specified array of bytes into ascending numerical order. * *\nThe sorting algorithm is the Dual-Pivot Quicksort, suggested by * Vladimir Yaroslavskiy on February 2009. This algorithm offers * n*ln(n) performance on many data sets that cause other quicksorts * to degrade to quadratic (n^2) performance. * * @param a the array to be sorted */ public static void sort(byte[] a) { sort(a, 0, a.length); } /** * Sorts the specified range of the specified array of bytes into * ascending numerical order. The range to be sorted extends from index * fromIndex, inclusive, to index toIndex, exclusive * (if fromIndex == toIndex, the range to be sorted is empty). * *\nThe sorting algorithm is the Dual-Pivot Quicksort, suggested by * Vladimir Yaroslavskiy on February 2009. This algorithm offers * n*ln(n) performance on many data sets that cause other quicksorts * to degrade to quadratic (n^2) performance. * * @param a the array to be sorted * @param fromIndex the index of the first element (inclusive) to be sorted * @param toIndex the index of the last element (exclusive) to be sorted * @throws IllegalArgumentException if fromIndex > toIndex * @throws ArrayIndexOutOfBoundsException if fromIndex < 0 * or toIndex > a.length */ public static void sort(byte[] a, int fromIndex, int toIndex) { rangeCheck(a.length, fromIndex, toIndex); dualPivotQuicksort(a, fromIndex, toIndex - 1, 3); } /** * Sorts the specified array of doubles into ascending numerical order. * *\nThe < relation does not provide a total order on * all floating-point values; although they are distinct numbers * -0.0d == 0.0d is true and a NaN value * compares neither less than, greater than, nor equal to any * floating-point value, even itself. To allow the sort to * proceed, instead of using the < relation to * determine ascending numerical order, this method uses the total * order imposed by {@link Double#compareTo}. This ordering * differs from the < relation in that * -0.0d is treated as less than 0.0d and * NaN is considered greater than any other floating-point value. * For the purposes of sorting, all NaN values are considered * equivalent and equal. * *\nThe sorting algorithm is the Dual-Pivot Quicksort, suggested by * Vladimir Yaroslavskiy on February 2009. This algorithm offers * n*ln(n) performance on many data sets that cause other quicksorts * to degrade to quadratic (n^2) performance. * * @param a the array to be sorted */ public static void sort(double[] a) { sort(a, 0, a.length); } /** * Sorts the specified range of the specified array of doubles into * ascending numerical order. The range to be sorted extends from index * fromIndex, inclusive, to index toIndex, exclusive * (if fromIndex==toIndex, the range to be sorted is empty). * *\nThe < relation does not provide a total order on * all floating-point values; although they are distinct numbers * -0.0d == 0.0d is true and a NaN value * compares neither less than, greater than, nor equal to any * floating-point value, even itself. To allow the sort to * proceed, instead of using the < relation to * determine ascending numerical order, this method uses the total * order imposed by {@link Double#compareTo}. This ordering * differs from the < relation in that * -0.0d is treated as less than 0.0d and * NaN is considered greater than any other floating-point value. * For the purposes of sorting, all NaN values are considered * equivalent and equal. * *\nThe sorting algorithm is the Dual-Pivot Quicksort, suggested by * Vladimir Yaroslavskiy on February 2009. This algorithm offers * n*ln(n) performance on many data sets that cause other quicksorts * to degrade to quadratic (n^2) performance. * * @param a the array to be sorted * @param fromIndex the index of the first element (inclusive) to be sorted * @param toIndex the index of the last element (exclusive) to be sorted * @throws IllegalArgumentException if fromIndex > toIndex * @throws ArrayIndexOutOfBoundsException if fromIndex < 0 * or toIndex > a.length */ public static void sort(double[] a, int fromIndex, int toIndex) { rangeCheck(a.length, fromIndex, toIndex); sortNegZeroAndNaN(a, fromIndex, toIndex); } private static void sortNegZeroAndNaN(double[] a, int fromIndex, int toIndex) { final long NEG_ZERO_BITS = Double.doubleToLongBits(-0.0d); /* * The sort is done in three phases to avoid the expense of using * NaN and -0.0d aware comparisons during the main sort. * * Preprocessing phase: move any NaN's to end of array, count the * number of -0.0d's, and turn them into 0.0d's. */ int numNegZeros = 0; int i = fromIndex; int n = toIndex; while (i < n) { if (a[i] != a[i]) { swap(a, i, --n); } else { if (a[i] == 0 && Double.doubleToLongBits(a[i]) == NEG_ZERO_BITS) { a[i] = 0.0d; numNegZeros++; } i++; } } // Main sort phase: quicksort everything but the NaN's dualPivotQuicksort(a, fromIndex, n - 1, 3); // Postprocessing phase: change 0.0d's to -0.0d's as required if (numNegZeros != 0) { int j = binarySearch0(a, fromIndex, n, 0.0d); // position of ANY zero do { j--; } while (j >= fromIndex && a[j] == 0.0d); // j is now one less than the index of the FIRST zero for (int k = 0; k < numNegZeros; k++) { a[++j] = -0.0d; } } } /** * Sorts the specified array of floats into ascending numerical order. * *\nThe < relation does not provide a total order on * all floating-point values; although they are distinct numbers * -0.0f == 0.0f is true and a NaN value * compares neither less than, greater than, nor equal to any * floating-point value, even itself. To allow the sort to * proceed, instead of using the < relation to * determine ascending numerical order, this method uses the total * order imposed by {@link Float#compareTo}. This ordering * differs from the < relation in that * -0.0f is treated as less than 0.0f and * NaN is considered greater than any other floating-point value. * For the purposes of sorting, all NaN values are considered * equivalent and equal. * *\nThe sorting algorithm is the Dual-Pivot Quicksort, suggested by * Vladimir Yaroslavskiy on February 2009. This algorithm offers * n*ln(n) performance on many data sets that cause other quicksorts * to degrade to quadratic (n^2) performance. * * @param a the array to be sorted */ public static void sort(float[] a) { sort(a, 0, a.length); } /** * Sorts the specified range of the specified array of floats into * ascending numerical order. The range to be sorted extends from index * fromIndex, inclusive, to index toIndex, exclusive * (if fromIndex==toIndex, the range to be sorted is empty). * *\nThe < relation does not provide a total order on * all floating-point values; although they are distinct numbers * -0.0f == 0.0f is true and a NaN value * compares neither less than, greater than, nor equal to any * floating-point value, even itself. To allow the sort to * proceed, instead of using the < relation to * determine ascending numerical order, this method uses the total * order imposed by {@link Float#compareTo}. This ordering * differs from the < relation in that * -0.0f is treated as less than 0.0f and * NaN is considered greater than any other floating-point value. * For the purposes of sorting, all NaN values are considered * equivalent and equal. * *\nThe sorting algorithm is the Dual-Pivot Quicksort, suggested by * Vladimir Yaroslavskiy on February 2009. This algorithm offers * n*ln(n) performance on many data sets that cause other quicksorts * to degrade to quadratic (n^2) performance. * * @param a the array to be sorted * @param fromIndex the index of the first element (inclusive) to be sorted * @param toIndex the index of the last element (exclusive) to be sorted * @throws IllegalArgumentException if fromIndex > toIndex * @throws ArrayIndexOutOfBoundsException if fromIndex < 0 * or toIndex > a.length */ public static void sort(float[] a, int fromIndex, int toIndex) { rangeCheck(a.length, fromIndex, toIndex); sortNegZeroAndNaN(a, fromIndex, toIndex); } private static void sortNegZeroAndNaN(float[] a, int fromIndex, int toIndex) { final int NEG_ZERO_BITS = Float.floatToIntBits(-0.0f); /* * The sort is done in three phases to avoid the expense of using * NaN and -0.0f aware comparisons during the main sort. * * Preprocessing phase: move any NaN's to end of array, count the * number of -0.0f's, and turn them into 0.0f's. */ int numNegZeros = 0; int i = fromIndex; int n = toIndex; while (i < n) { if (a[i] != a[i]) { swap(a, i, --n); } else { if (a[i] == 0 && Float.floatToIntBits(a[i]) == NEG_ZERO_BITS) { a[i] = 0.0f; numNegZeros++; } i++; } } // Main sort phase: quicksort everything but the NaN's dualPivotQuicksort(a, fromIndex, n - 1, 3); // Postprocessing phase: change 0.0f's to -0.0f's as required if (numNegZeros != 0) { int j = binarySearch0(a, fromIndex, n, 0.0f); // position of ANY zero do { j--; } while (j >= fromIndex && a[j] == 0.0f); // j is now one less than the index of the FIRST zero for (int k = 0; k < numNegZeros; k++) { a[++j] = -0.0f; } } } /* * The code for each of the seven primitive types is largely identical. * C'est la vie. */ /** * Sorts the specified sub-array of longs into ascending order. */ private static void dualPivotQuicksort(long[] a, int left, int right, int div) { int len = right - left; if (len < 27) { // insertion sort for tiny array for (int i = left + 1; i <= right; i++) { for (int j = i; j > left && a[j] < a[j - 1]; j--) { swap(a, j, j - 1); } } return; } int third = len / div; // \"medians\" int m1 = left + third; int m2 = right - third; if (m1 <= left) { m1 = left + 1; } if (m2 >= right) { m2 = right - 1; } if (a[m1] < a[m2]) { swap(a, m1, left); swap(a, m2, right); } else { swap(a, m1, right); swap(a, m2, left); } // pivots long pivot1 = a[left]; long pivot2 = a[right]; // pointers int less = left + 1; int great = right - 1; // sorting for (int k = less; k <= great; k++) { if (a[k] < pivot1) { swap(a, k, less++); } else if (a[k] > pivot2) { while (k < great && a[great] > pivot2) { great--; } swap(a, k, great--); if (a[k] < pivot1) { swap(a, k, less++); } } } // swaps int dist = great - less; if (dist < 13) { div++; } swap(a, less - 1, left); swap(a, great + 1, right); // subarrays dualPivotQuicksort(a, left, less - 2, div); dualPivotQuicksort(a, great + 2, right, div); // equal elements if (dist > len - 13 && pivot1 != pivot2) { for (int k = less; k <= great; k++) { if (a[k] == pivot1) { swap(a, k, less++); } else if (a[k] == pivot2) { swap(a, k, great--); if (a[k] == pivot1) { swap(a, k, less++); } } } } // subarray if (pivot1 < pivot2) { dualPivotQuicksort(a, less, great, div); } } /** * Swaps a[i] with a[j]. */ private static void swap(long[] a, int i, int j) { long temp = a[i]; a[i] = a[j]; a[j] = temp; } /** * Sorts the specified sub-array of ints into ascending order. */ private static void dualPivotQuicksort(int[] a, int left, int right, int div) { int len = right - left; if (len < 27) { // insertion sort for tiny array for (int i = left + 1; i <= right; i++) { for (int j = i; j > left && a[j] < a[j - 1]; j--) { swap(a, j, j - 1); } } return; } int third = len / div; // \"medians\" int m1 = left + third; int m2 = right - third; if (m1 <= left) { m1 = left + 1; } if (m2 >= right) { m2 = right - 1; } if (a[m1] < a[m2]) { swap(a, m1, left); swap(a, m2, right); } else { swap(a, m1, right); swap(a, m2, left); } // pivots int pivot1 = a[left]; int pivot2 = a[right]; // pointers int less = left + 1; int great = right - 1; // sorting for (int k = less; k <= great; k++) { if (a[k] < pivot1) { swap(a, k, less++); } else if (a[k] > pivot2) { while (k < great && a[great] > pivot2) { great--; } swap(a, k, great--); if (a[k] < pivot1) { swap(a, k, less++); } } } // swaps int dist = great - less; if (dist < 13) { div++; } swap(a, less - 1, left); swap(a, great + 1, right); // subarrays dualPivotQuicksort(a, left, less - 2, div); dualPivotQuicksort(a, great + 2, right, div); // equal elements if (dist > len - 13 && pivot1 != pivot2) { for (int k = less; k <= great; k++) { if (a[k] == pivot1) { swap(a, k, less++); } else if (a[k] == pivot2) { swap(a, k, great--); if (a[k] == pivot1) { swap(a, k, less++); } } } } // subarray if (pivot1 < pivot2) { dualPivotQuicksort(a, less, great, div); } } /** * Swaps a[i] with a[j]. */ private static void swap(int[] a, int i, int j) { int temp = a[i]; a[i] = a[j]; a[j] = temp; } /** * Sorts the specified sub-array of shorts into ascending order. */ private static void dualPivotQuicksort(short[] a, int left, int right, int div) { int len = right - left; if (len < 27) { // insertion sort for tiny array for (int i = left + 1; i <= right; i++) { for (int j = i; j > left && a[j] < a[j - 1]; j--) { swap(a, j, j - 1); } } return; } int third = len / div; // \"medians\" int m1 = left + third; int m2 = right - third; if (m1 <= left) { m1 = left + 1; } if (m2 >= right) { m2 = right - 1; } if (a[m1] < a[m2]) { swap(a, m1, left); swap(a, m2, right); } else { swap(a, m1, right); swap(a, m2, left); } // pivots short pivot1 = a[left]; short pivot2 = a[right]; // pointers int less = left + 1; int great = right - 1; // sorting for (int k = less; k <= great; k++) { if (a[k] < pivot1) { swap(a, k, less++); } else if (a[k] > pivot2) { while (k < great && a[great] > pivot2) { great--; } swap(a, k, great--); if (a[k] < pivot1) { swap(a, k, less++); } } } // swaps int dist = great - less; if (dist < 13) { div++; } swap(a, less - 1, left); swap(a, great + 1, right); // subarrays dualPivotQuicksort(a, left, less - 2, div); dualPivotQuicksort(a, great + 2, right, div); // equal elements if (dist > len - 13 && pivot1 != pivot2) { for (int k = less; k <= great; k++) { if (a[k] == pivot1) { swap(a, k, less++); } else if (a[k] == pivot2) { swap(a, k, great--); if (a[k] == pivot1) { swap(a, k, less++); } } } } // subarray if (pivot1 < pivot2) { dualPivotQuicksort(a, less, great, div); } } /** * Swaps a[i] with a[j]. */ private static void swap(short[] a, int i, int j) { short temp = a[i]; a[i] = a[j]; a[j] = temp; } /** * Sorts the specified sub-array of chars into ascending order. */ private static void dualPivotQuicksort(char[] a, int left, int right, int div) { int len = right - left; if (len < 27) { // insertion sort for tiny array for (int i = left + 1; i <= right; i++) { for (int j = i; j > left && a[j] < a[j - 1]; j--) { swap(a, j, j - 1); } } return; } int third = len / div; // \"medians\" int m1 = left + third; int m2 = right - third; if (m1 <= left) { m1 = left + 1; } if (m2 >= right) { m2 = right - 1; } if (a[m1] < a[m2]) { swap(a, m1, left); swap(a, m2, right); } else { swap(a, m1, right); swap(a, m2, left); } // pivots char pivot1 = a[left]; char pivot2 = a[right]; // pointers int less = left + 1; int great = right - 1; // sorting for (int k = less; k <= great; k++) { if (a[k] < pivot1) { swap(a, k, less++); } else if (a[k] > pivot2) { while (k < great && a[great] > pivot2) { great--; } swap(a, k, great--); if (a[k] < pivot1) { swap(a, k, less++); } } } // swaps int dist = great - less; if (dist < 13) { div++; } swap(a, less - 1, left); swap(a, great + 1, right); // subarrays dualPivotQuicksort(a, left, less - 2, div); dualPivotQuicksort(a, great + 2, right, div); // equal elements if (dist > len - 13 && pivot1 != pivot2) { for (int k = less; k <= great; k++) { if (a[k] == pivot1) { swap(a, k, less++); } else if (a[k] == pivot2) { swap(a, k, great--); if (a[k] == pivot1) { swap(a, k, less++); } } } } // subarray if (pivot1 < pivot2) { dualPivotQuicksort(a, less, great, div); } } /** * Swaps a[i] with a[j]. */ private static void swap(char[] a, int i, int j) { char temp = a[i]; a[i] = a[j]; a[j] = temp; } /** * Sorts the specified sub-array of bytes into ascending order. */ private static void dualPivotQuicksort(byte[] a, int left, int right, int div) { int len = right - left; if (len < 27) { // insertion sort for tiny array for (int i = left + 1; i <= right; i++) { for (int j = i; j > left && a[j] < a[j - 1]; j--) { swap(a, j, j - 1); } } return; } int third = len / div; // \"medians\" int m1 = left + third; int m2 = right - third; if (m1 <= left) { m1 = left + 1; } if (m2 >= right) { m2 = right - 1; } if (a[m1] < a[m2]) { swap(a, m1, left); swap(a, m2, right); } else { swap(a, m1, right); swap(a, m2, left); } // pivots byte pivot1 = a[left]; byte pivot2 = a[right]; // pointers int less = left + 1; int great = right - 1; // sorting for (int k = less; k <= great; k++) { if (a[k] < pivot1) { swap(a, k, less++); } else if (a[k] > pivot2) { while (k < great && a[great] > pivot2) { great--; } swap(a, k, great--); if (a[k] < pivot1) { swap(a, k, less++); } } } // swaps int dist = great - less; if (dist < 13) { div++; } swap(a, less - 1, left); swap(a, great + 1, right); // subarrays dualPivotQuicksort(a, left, less - 2, div); dualPivotQuicksort(a, great + 2, right, div); // equal elements if (dist > len - 13 && pivot1 != pivot2) { for (int k = less; k <= great; k++) { if (a[k] == pivot1) { swap(a, k, less++); } else if (a[k] == pivot2) { swap(a, k, great--); if (a[k] == pivot1) { swap(a, k, less++); } } } } // subarray if (pivot1 < pivot2) { dualPivotQuicksort(a, less, great, div); } } /** * Swaps a[i] with a[j]. */ private static void swap(byte[] a, int i, int j) { byte temp = a[i]; a[i] = a[j]; a[j] = temp; } /** * Sorts the specified sub-array of doubles into ascending order. */ private static void dualPivotQuicksort(double[] a, int left, int right, int div) { int len = right - left; if (len < 27) { // insertion sort for tiny array for (int i = left + 1; i <= right; i++) { for (int j = i; j > left && a[j] < a[j - 1]; j--) { swap(a, j, j - 1); } } return; } int third = len / div; // \"medians\" int m1 = left + third; int m2 = right - third; if (m1 <= left) { m1 = left + 1; } if (m2 >= right) { m2 = right - 1; } if (a[m1] < a[m2]) { swap(a, m1, left); swap(a, m2, right); } else { swap(a, m1, right); swap(a, m2, left); } // pivots double pivot1 = a[left]; double pivot2 = a[right]; // pointers int less = left + 1; int great = right - 1; // sorting for (int k = less; k <= great; k++) { if (a[k] < pivot1) { swap(a, k, less++); } else if (a[k] > pivot2) { while (k < great && a[great] > pivot2) { great--; } swap(a, k, great--); if (a[k] < pivot1) { swap(a, k, less++); } } } // swaps int dist = great - less; if (dist < 13) { div++; } swap(a, less - 1, left); swap(a, great + 1, right); // subarrays dualPivotQuicksort(a, left, less - 2, div); dualPivotQuicksort(a, great + 2, right, div); // equal elements if (dist > len - 13 && pivot1 != pivot2) { for (int k = less; k <= great; k++) { if (a[k] == pivot1) { swap(a, k, less++); } else if (a[k] == pivot2) { swap(a, k, great--); if (a[k] == pivot1) { swap(a, k, less++); } } } } // subarray if (pivot1 < pivot2) { dualPivotQuicksort(a, less, great, div); } } /** * Swaps a[i] with a[j]. */ private static void swap(double[] a, int i, int j) { double temp = a[i]; a[i] = a[j]; a[j] = temp; } /** * Sorts the specified sub-array of floats into ascending order. */ private static void dualPivotQuicksort(float[] a, int left, int right, int div) { int len = right - left; if (len < 27) { // insertion sort for tiny array for (int i = left + 1; i <= right; i++) { for (int j = i; j > left && a[j] < a[j - 1]; j--) { swap(a, j, j - 1); } } return; } int third = len / div; // \"medians\" int m1 = left + third; int m2 = right - third; if (m1 <= left) { m1 = left + 1; } if (m2 >= right) { m2 = right - 1; } if (a[m1] < a[m2]) { swap(a, m1, left); swap(a, m2, right); } else { swap(a, m1, right); swap(a, m2, left); } // pivots float pivot1 = a[left]; float pivot2 = a[right]; // pointers int less = left + 1; int great = right - 1; // sorting for (int k = less; k <= great; k++) { if (a[k] < pivot1) { swap(a, k, less++); } else if (a[k] > pivot2) { while (k < great && a[great] > pivot2) { great--; } swap(a, k, great--); if (a[k] < pivot1) { swap(a, k, less++); } } } // swaps int dist = great - less; if (dist < 13) { div++; } swap(a, less - 1, left); swap(a, great + 1, right); // subarrays dualPivotQuicksort(a, left, less - 2, div); dualPivotQuicksort(a, great + 2, right, div); // equal elements if (dist > len - 13 && pivot1 != pivot2) { for (int k = less; k <= great; k++) { if (a[k] == pivot1) { swap(a, k, less++); } else if (a[k] == pivot2) { swap(a, k, great--); if (a[k] == pivot1) { swap(a, k, less++); } } } } // subarray if (pivot1 < pivot2) { dualPivotQuicksort(a, less, great, div); } } /** * Swaps a[i] with a[j]. */ private static void swap(float[] a, int i, int j) { float temp = a[i]; a[i] = a[j]; a[j] = temp; } /** * Old merge sort implementation can be selected (for * compatibility with broken comparators) using a system property. * Cannot be a static boolean in the enclosing class due to * circular dependencies. To be removed in a future release. */ static final class LegacyMergeSort { private static final boolean userRequested = java.security.AccessController.doPrivileged( new sun.security.action.GetBooleanAction( \"java.util.Arrays.useLegacyMergeSort\")).booleanValue(); } /* * If this platform has an optimizing VM, check whether ComparableTimSort * offers any performance benefit over TimSort in conjunction with a * comparator that returns: * {@code ((Comparable)first).compareTo(Second)}. * If not, you are better off deleting ComparableTimSort to * eliminate the code duplication. In other words, the commented * out code below is the preferable implementation for sorting * arrays of Comparables if it offers sufficient performance. */ // /** // * A comparator that implements the natural ordering of a group of // * mutually comparable elements. Using this comparator saves us // * from duplicating most of the code in this file (one version for // * Comparables, one for explicit Comparators). // */ // private static final Comparator\n"}, {"score": 493.08176, "uuid": "d86fa0e8-2947-59f9-adeb-64b36ba2cdbf", "index": "cw12", "trec_id": "clueweb12-1216wb-91-12287", "target_hostname": "net.pku.edu.cn", "target_uri": "http://net.pku.edu.cn/~yhf/course/DAAlgorithms/960118.html", "page_rank": 1.1700305e-09, "spam_rank": 86, "title": "Sorting by divide-and-conquer", "snippet": "Note that this <em>is</em> worse than either <em>merge</em> <em>sort</em> <em>or</em> heap <em>sort</em>, and requires random number generator to avoid being really bad. But it&#x27;s pretty commonly used, and can be tuned in various ways to work <em>better</em>. (For instance, let x be the median of three randomly chosen values rather than just one value).", "explanation": null, "document": "ICS 161: Design and Analysis of Algorithms\nLecture notes for January 18, 1996\nThree Divide and Conquer Sorting Algorithms\nToday we'll finish heapsort, and describe both mergesort and quicksort. Why do we need multiple sorting algorithms? Different methods work better in different applications.\nHeapsort uses close to the right number of comparisons but needs to move data around quite a bit. It can be done in a way that uses very little extra memory. It's probably good when memory is tight, and you are sorting many small items that come stored in an array.\nMerge sort is good for data that's too big to have in memory at once, because its pattern of storage access is very regular. It also uses even fewer comparisons than heapsort, and is especially suited for data stored as linked lists.\nQuicksort also uses few comparisons (somewhat more than the other two). Like heapsort it can sort \"in place\" by moving data in an array.\nHeapification\nRecall the idea of heapsort:\nheapsort(list L)\r\n    {\r\n    make heap H from L\r\n    make empty list X\r\n    while H nonempty\r\n        remove smallest from H and add it to X\r\n    return X\r\n    }\nRemember that a heap is just a balanced binary tree in which the value at any node is smaller than the values at its children. We went over most of this last time. The total number of comparisons is n log n + however many are needed to make H. The only missing step: how to make a heap?\nTo start with, we can set up a binary tree of the right size and shape, and put the objects into the tree in any old order. This is all easy and doesn't require any comparisons. Now we have to switch objects around to get them back in order.\nThe divide and conquer idea: find natural subproblems, solve them recursively, and combine them to get an overall solution. Here the obvious subproblems are the subtrees. If we solve them recursively, we get something that is close to being a heap, except that perhaps the root doesn't satisfy the heap property. To make the whole thing a heap, we merely have to percolate that value down to a lower level in the tree.\nheapify(tree T)\r\n    {\r\n        if (T is nonempty) {\r\n            heapify(left subtree)\r\n            heapify(right subtree)\r\n            let x = value at tree root\r\n            while node containing x doesn't satisfy heap propert\r\n                switch values of node and its smallest child\r\n        }\r\n    }\nThe while loop performs two comparisons per iteration, and takes at most log n iterations, so the time for this satisfies a recurrence\nT(n) <= 2 T(n/2) + 2 log n\nHow to solve it?\nDivide and conquer recurrences\nIn general, divide and conquer is based on the following idea. The whole problem we want to solve may too big to understand or solve at once. We break it up into smaller pieces, solve the pieces separately, and combine the separate pieces together.\nWe analyze this in some generality: suppose we have a pieces, each of size n/b and merging takes time f(n). (In the heapification example a=b=2 and f(n)=O(log n) but it will not always be true that a=b -- sometimes the pieces will overlap.)\nThe easiest way to understand what's going on here is to draw a tree with nodes corresponding to subproblems (labeled with the size of the subproblem)\nn\r\n    /  |  \\\r\n    n/b   n/b   n/b\r\n    /|\\   /|\\   /|\\ \r\n     .     .     .\r\n     .     .     .\r\n     .     .     .\nFor simplicity, let's assume n is a power of b, and that the recursion stops when n is 1. Notice that the size of a node depends only on its level:\nsize(i) = n/(b^i).\nWhat is time taken by a node at level i?\ntime(i) = f(n/b^i)\nHow many levels can we have before we get down to n=1? For bottom level, n/b^i=1, so n=b^i and i=(log n)/(log b). How many items at level i? a^i. So putting these together we have\n(log n)/(log b)\r\n    T(n) =      sum       a^i f(n/b^i)\r\n        i=0\nThis looks messy, but it's not too bad. There are only a few terms (logarithmically many) and often the sum is dominated by the terms at one end (f(n)) or the other (n^(log a/log b)). In fact, you will generally only be a logarithmic factor away from the truth if you approximate the solution by the sum of these two, O(f(n) + n^(log a/log b)).\nLet's use this to analyze heapification. By plugging in parameters a=b=2, f(n)=log n, we get\nlog n\r\n    T(n) = 2  sum  2^i log(n/2^i)\r\n          i=0\nRewriting the same terms in the opposite order, this turns out to equal\nlog n\r\n    T(n) = 2  sum  n/2^i log(2^i)\r\n          i=0\r\n\r\n           log n\r\n     = 2n   sum  i/2^i\r\n        i=0\r\n\r\n           infty\r\n     <= 2n   sum  i/2^i\r\n        i=0\r\n\r\n     = 4n\nSo heapification takes at most 4n comparisons and heapsort takes at most n log n + 4n. (There's an n log n - 1.44n lower bound so we're only within O(n) of the absolute best possible.)\nThis was an example of a sorting algorithm where one part used divide and conquer. What about doing the whole algorithm that way?\nMerge sort\nAccording to Knuth, merge sort was one of the earliest sorting algorithms, invented by John von Neumann in 1945.\nLet's look at the combine step first. Suppose you have some data that's close to sorted -- it forms two sorted lists. You want to merge the two sorted lists quickly rather than having to resort to a general purpose sorting algorithm. This is easy enough:\nmerge(L1,L2)\r\n    {\r\n    list X = empty\r\n    while (neither L1 nor L2 empty)\r\n    {\r\n        compare first items of L1 & L2\r\n        remove smaller of the two from its list\r\n        add to end of X\r\n    }\r\n    catenate remaining list to end of X\r\n    return X\r\n    }\nTime analysis: in the worst case both lists empty at about same time, so everything has to be compared. Each comparison adds one item to X so the worst case is |X|-1 = |L1|+|L2|-1 comparisons. One can do a little better sometimes e.g. if L1 is smaller than most of L2.\nOnce we know how to combine two sorted lists, we can construct a divide and conquer sorting algorithm that simply divides the list in two, sorts the two recursively, and merges the results:\nmerge sort(L)\r\n    {\r\n        if (length(L) < 2) return L\r\n        else {\r\n            split L into lists L1 and L2, each of n/2 elements\r\n            L1 = merge sort(L1)\r\n            L2 = merge sort(L2)\r\n            return merge(L1,L2)\r\n        }\r\n    }\nThis is simpler than heapsort (so easier to program) and works pretty well. How many comparisons does it use? We can use the analysis of the merge step to write down a recurrence:\nC(n) <= n-1 + 2C(n/2)\nAs you saw in homework 1.31, for n = power of 2, the solution to this is n log n - n + 1. For other n, it's similar but more complicated. To prove this (at least the power of 2 version), you can use the formula above to produce\nlog n\r\n    C(N) <=  sum  2^i (n/2^i - 1)\r\n         i=0\r\n\r\n          log n\r\n        =  sum  n - 2^i\r\n           i=0\r\n\r\n        = n(log n + 1) - (2n - 1)\r\n\r\n        = n log n - n + 1\nSo the number of comparisons is even less than heapsort.\nQuicksort\nQuicksort, invented by Tony Hoare , follows a very similar divide and conquer idea: partition into two lists and put them back together again It does more work on the divide side, less on the combine side.\nMerge sort worked no matter how you split the lists (one obvious way is to take first n/2 and last n/2 elements, another is to take every other element). But if you could perform the splits so that everything in one list was smaller than everything in the other, this information could be used to make merging much easier: you could merge just by concatenating the lists.\nHow to split so one list smaller than the other? e.g. for alphabetical order, you could split into A-M, N-Z so could use some split depending on what data looks like, but we want a comparison sorting algorithm that works for any data.\nQuicksort uses a simple idea: pick one object x from the list, and split the rest into those before x and those after x.\nquicksort(L)\r\n    {\r\n        if (length(L) < 2) return L\r\n        else {\r\n            pick some x in L\r\n            L1 = { y in L : y < x }\r\n            L2 = { y in L : y > x }\r\n            L3 = { y in L : y = x }\r\n            quicksort(L1)\r\n            quicksort(L2)\r\n            return concatenation of L1, L3, and L2\r\n        }\r\n    }\n(We don't need to sort L3 because everything in it is equal).\nQuicksort analysis\nThe partition step of quicksort takes n-1 comparisons. So we can write a recurrence for the total number of comparisons done by quicksort:\nC(n) = n-1 + C(a) + C(b)\nwhere a and b are the sizes of L1 and L2, generally satisfying a+b=n-1. In the worst case, we might pick x to be the minimum element in L. Then a=0, b=n-1, and the recurrence simplifies to C(n)=n-1 + C(n-1) = O(n^2). So this seems like a very bad algorithm.\nWhy do we call it quicksort? How can we make it less bad? Randomization!\nSuppose we pick x=a[k] where k is chosen randomly. Then any value of a is equally likely from 0 to n-1. To do average case analysis, we write out the sum over possible random choices of the probability of that choice times the time for that choice. Here the choices are the values of k, the probabilities are all 1/n, and the times can be described by formulas involving the time for the recursive calls to the algorithm. So average case analysis of a randomized algorithm gives a randomized recurrence:\nn-1\r\n    C(n) = sum (1/n)[n - 1 + C(a) + C(n-a-1)]\r\n       a=0\nTo simplify the recurrence, note that if C(a) occurs one place in the sum, the same number will occur as C(n-a-1) in another term -- we rearrange the sum to group the two together. We can also take the (n-1) parts out of the sum since the sum of 1/n copies of 1/n times n-1 is just n-1.\nn-1\r\n    C(n) = n - 1 + sum (2/n) C(a)\r\n           a=0\nThe book gives two proofs that this is O(n log n). Of these, induction is easier.\nOne useful idea here: we want to prove f(n) is O(g(n)). The O() hides too much information, instead we need to prove f(n) <= a g(n) but we don't know what value a should take. We work it out with a left as a variable then use the analysis to see what values of a work.\nWe have C(1) = 0 = a (1 log 1) for all a. Suppose C(i) <= a i log i for some a, all i<n. Then\nC(n) = n-1 + sum(2/n) C(a)\r\n     <= n-1 + sum(2/n)ai log i\r\n     = n-1 + 2a/n sum(i=2 to n-1) (i log i)\r\n     <= n-1 + 2a/n integral(i=2 to n)(i log i)\r\n     = n-1 + 2a/n (n^2 log n / 2 - n^2/4 - 2 ln 2 + 1)\r\n     = n-1 + a n log n - an/2 - O(1)\nand this will work if n-1 < an/2, and in particular if a=2. So we can conclude that C(n) <= 2 n log n.\nNote that this is worse than either merge sort or heap sort, and requires random number generator to avoid being really bad. But it's pretty commonly used, and can be tuned in various ways to work better. (For instance, let x be the median of three randomly chosen values rather than just one value).\n"}, {"score": 488.6523, "uuid": "02605fab-d965-5069-a689-dd081fd1fcb1", "index": "cw12", "trec_id": "clueweb12-0809wb-02-27111", "target_hostname": "docs.activestate.com", "target_uri": "http://docs.activestate.com/activeperl/5.8/lib/pods/perl58delta.html", "page_rank": 1.2253631e-09, "spam_rank": 88, "title": "perl58delta - what <em>is</em> new for perl v5.8.0", "snippet": "With the <em>quicksort</em> <em>algorithm</em> used to implement Perl 5.6 and earlier, the order of ties <em>is</em> left up to the <em>sort</em>.", "explanation": null, "document": "Locale::Maketext, by Sean Burke, is a localization framework. See the Locale::Maketext manpage , and the Locale::Maketext::TPJ13 manpage . The latter is an article about software localization, originally published in The Perl Journal #13, and republished here with kind permission.\nMath::BigRat for big rational numbers, to accompany Math::BigInt and Math::BigFloat, from Tels. See the Math::BigRat manpage .\nMemoize can make your functions faster by trading space for time, from Mark-Jason Dominus. See the Memoize manpage .\nMIME::Base64, by Gisle Aas, allows you to encode data in base64, as defined in RFC 2045 - MIME (Multipurpose Internet Mail Extensions).\nuse MIME::Base64;\n$encoded = encode_base64('Aladdin:open sesame'); $decoded = decode_base64($encoded);\nprint $encoded, \"\\n\"; # \"QWxhZGRpbjpvcGVuIHNlc2FtZQ==\"\nMIME::QuotedPrint, by Gisle Aas, allows you to encode data in quoted-printable encoding, as defined in RFC 2045 - MIME (Multipurpose Internet Mail Extensions).\nuse MIME::QuotedPrint;\n$encoded = encode_qp(\"\\xDE\\xAD\\xBE\\xEF\"); $decoded = decode_qp($encoded);\nprint $encoded, \"\\n\"; # \"=DE=AD=BE=EF\\n\" print $decoded, \"\\n\"; # \"\\xDE\\xAD\\xBE\\xEF\\n\"\nNEXT, by Damian Conway, is a pseudo-class for method redispatch. See the NEXT manpage .\nopen is a new pragma for setting the default I/O layers for open() .\nPerlIO::scalar, by Nick Ing-Simmons, provides the implementation of IO to \"in memory\" Perl scalars as discussed above. It also serves as an example of a loadable PerlIO layer. Other future possibilities include PerlIO::Array and PerlIO::Code. See the PerlIO::scalar manpage .\nPerlIO::via, by Nick Ing-Simmons, acts as a PerlIO layer and wraps PerlIO layer functionality provided by a class (typically implemented in Perl code).\nPerlIO::via::QuotedPrint, by Elizabeth Mattijsen, is an example of a PerlIO::via class:\nuse PerlIO::via::QuotedPrint; open($fh,\">:via(QuotedPrint)\",$path);\nThis will automatically convert everything output to $fh to Quoted-Printable. See the PerlIO::via manpage and the PerlIO::via::QuotedPrint manpage .\nPod::ParseLink, by Russ Allbery, has been added, to parse L<> links in pods as described in the new perlpodspec.\nPod::Text::Overstrike, by Joe Smith, has been added. It converts POD data to formatted overstrike text. See the Pod::Text::Overstrike manpage . [561+]\nScalar::Util is a selection of general-utility scalar subroutines, such as blessed(), reftype(), and tainted(). See the Scalar::Util manpage .\nsort is a new pragma for controlling the behaviour of sort() .\nStorable gives persistence to Perl data structures by allowing the storage and retrieval of Perl data to and from files in a fast and compact binary format. Because in effect Storable does serialisation of Perl data structures, with it you can also clone deep, hierarchical datastructures. Storable was originally created by Raphael Manfredi, but it is now maintained by Abhijit Menon-Sen. Storable has been enhanced to understand the two new hash features, Unicode keys and restricted hashes. See the Storable manpage .\nSwitch, by Damian Conway, has been added. Just by saying\nuse Switch;\nyou have switch and case available in Perl.\nuse Switch;\nswitch ($val) {\ncase 1 { print \"number 1\" } case \"a\" { print \"string a\" } case [1..10,42] { print \"number in list\" } case (@array) { print \"number in list\" } case /\\w+/      { print \"pattern\" }\r\n                case qr/\\w+/    { print \"pattern\" }\r\n                case (%hash)    { print \"entry in hash\" }\r\n                case (\\%hash)   { print \"entry in hash\" }\r\n                case (\\&sub)    { print \"arg to subroutine\" }\r\n                else            { print \"previous case not true\" }\r\n                    }\nTest::More, by Michael Schwern, is yet another framework for writing test scripts, more extensive than Test::Simple. See the Test::More manpage .\nTest::Simple, by Michael Schwern, has basic utilities for writing tests. See the Test::Simple manpage .\nText::Balanced, by Damian Conway, has been added, for extracting delimited text sequences from strings.\nuse Text::Balanced 'extract_delimited';\n($a, $b) = extract_delimited(\"'never say never', he never said\", \"'\", '');\n$a will be \"'never say never'\", $b will be ', he never said'.\nIn addition to extract_delimited(), there are also extract_bracketed(), extract_quotelike(), extract_codeblock(), extract_variable(), extract_tagged(), extract_multiple(), gen_delimited_pat(), and gen_extract_tagged(). With these, you can implement rather advanced parsing algorithms. See the Text::Balanced manpage .\nthreads, by Arthur Bergman, is an interface to interpreter threads. Interpreter threads (ithreads) is the new thread model introduced in Perl 5.6 but only available as an internal interface for extension writers (and for Win32 Perl for fork() emulation). See the threads manpage , the threads::shared manpage , and the perlthrtut manpage .\nthreads::shared, by Arthur Bergman, allows data sharing for interpreter threads. See the threads::shared manpage .\nTie::File, by Mark-Jason Dominus, associates a Perl array with the lines of a file. See the Tie::File manpage .\nTie::Memoize, by Ilya Zakharevich, provides on-demand loaded hashes. See the Tie::Memoize manpage .\nTie::RefHash::Nestable, by Edward Avis, allows storing hash references (unlike the standard Tie::RefHash) The module is contained within Tie::RefHash. See the Tie::RefHash manpage .\nTime::HiRes, by Douglas E. Wegscheid, provides high resolution timing (ualarm, usleep, and gettimeofday). See the Time::HiRes manpage .\nUnicode::UCD offers a querying interface to the Unicode Character Database. See the Unicode::UCD manpage .\nUnicode::Collate, by SADAHIRO Tomoyuki, implements the UCA (Unicode Collation Algorithm) for sorting Unicode strings. See the Unicode::Collate manpage .\nUnicode::Normalize, by SADAHIRO Tomoyuki, implements the various Unicode normalization forms. See the Unicode::Normalize manpage .\nXS::APItest, by Tim Jenness, is a test extension that exercises XS APIs. Currently only printf() is tested: how to output various basic data types from XS.\nXS::Typemap, by Tim Jenness, is a test extension that exercises XS typemaps. Nothing gets installed, but the code is worth studying for extension writers.\nThe following independently supported modules have been updated to the newest versions from CPAN: CGI, CPAN, DB_File, File::Spec, File::Temp, Getopt::Long, Math::BigFloat, Math::BigInt, the podlators bundle (Pod::Man, Pod::Text), Pod::LaTeX [561+], Pod::Parser, Storable, Term::ANSIColor, Test, Text-Tabs+Wrap.\nattributes::reftype() now works on tied arguments.\nAutoLoader can now be disabled with no AutoLoader;.\nB::Deparse has been significantly enhanced by Robin Houston. It can now deparse almost all of the standard test suite (so that the tests still succeed). There is a make target \"test.deparse\" for trying this out.\nCarp now has better interface documentation, and the @CARP_NOT interface has been added to get optional control over where errors are reported independently of @ISA, by Ben Tilly.\nClass::Struct can now define the classes in compile time.\nClass::Struct now assigns the array/hash element if the accessor is called with an array/hash element as the sole argument.\nThe return value of Cwd::fastcwd() is now tainted.\nData::Dumper now has an option to sort hashes.\nData::Dumper now has an option to dump code references using B::Deparse.\nDB_File now supports newer Berkeley DB versions, among other improvements.\nDevel::Peek now has an interface for the Perl memory statistics (this works only if you are using perl's malloc, and if you have compiled with debugging).\nThe English module can now be used without the infamous performance hit by saying\nuse English '-no_match_vars';\n(Assuming, of course, that you don't need the troublesome variables $` , $& , or $' .) Also, introduced @LAST_MATCH_START and @LAST_MATCH_END English aliases for @- and @+ .\nExtUtils::MakeMaker has been significantly cleaned up and fixed. The enhanced version has also been backported to earlier releases of Perl and submitted to CPAN so that the earlier releases can enjoy the fixes.\nThe arguments of WriteMakefile() in Makefile.PL are now checked for sanity much more carefully than before. This may cause new warnings when modules are being installed. See the ExtUtils::MakeMaker manpage for more details.\nExtUtils::MakeMaker now uses File::Spec internally, which hopefully leads to better portability.\nFcntl, Socket, and Sys::Syslog have been rewritten by Nicholas Clark to use the new-style constant dispatch section (see the ExtUtils::Constant manpage ). This means that they will be more robust and hopefully faster.\nFile::Find now chdir()s correctly when chasing symbolic links. [561]\nFile::Find now has pre- and post-processing callbacks. It also correctly changes directories when chasing symbolic links. Callbacks (naughtily) exiting with \"next;\" instead of \"return;\" now work.\nFile::Find is now (again) reentrant. It also has been made more portable.\nThe warnings issued by File::Find now belong to their own category. You can enable/disable them with use/no warnings 'File::Find';.\nFile::Glob::glob() has been renamed to File::Glob::bsd_glob() because the name clashes with the builtin glob() . The older name is still available for compatibility, but is deprecated. [561]\nFile::Glob now supports GLOB_LIMIT constant to limit the size of the returned list of filenames.\nIPC::Open3 now allows the use of numeric file descriptors.\nIO::Socket now has an atmark() method, which returns true if the socket is positioned at the out-of-band mark. The method is also exportable as a sockatmark() function.\nIO::Socket::INET failed to open the specified port if the service name was not known. It now correctly uses the supplied port number as is. [561]\nIO::Socket::INET has support for the ReusePort option (if your platform supports it). The Reuse option now has an alias, ReuseAddr. For clarity, you may want to prefer ReuseAddr.\nIO::Socket::INET now supports a value of zero for LocalPort (usually meaning that the operating system will make one up.)\n'use lib' now works identically to @INC. Removing directories with 'no lib' now works.\nMath::BigFloat and Math::BigInt have undergone a full rewrite by Tels. They are now magnitudes faster, and they support various bignum libraries such as GMP and PARI as their backends.\nMath::Complex handles inf, NaN etc., better.\nNet::Ping has been considerably enhanced by Rob Brown: multihoming is now supported, Win32 functionality is better, there is now time measuring functionality (optionally high-resolution using Time::HiRes), and there is now \"external\" protocol which uses Net::Ping::External module which runs your external ping utility and parses the output. A version of Net::Ping::External is available in CPAN.\nNote that some of the Net::Ping tests are disabled when running under the Perl distribution since one cannot assume one or more of the following: enabled echo port at localhost, full Internet connectivity, or sympathetic firewalls. You can set the environment variable PERL_TEST_Net_Ping to \"1\" (one) before running the Perl test suite to enable all the Net::Ping tests.\nPOSIX::sigaction() is now much more flexible and robust. You can now install coderef handlers, 'DEFAULT', and 'IGNORE' handlers, installing new handlers was not atomic.\nIn Safe, %INC is now localised in a Safe compartment so that use/require work.\nIn SDBM_File on dosish platforms, some keys went missing because of lack of support for files with \"holes\". A workaround for the problem has been added.\nIn Search::Dict one can now have a pre-processing hook for the lines being searched.\nThe Shell module now has an OO interface.\nIn Sys::Syslog there is now a failover mechanism that will go through alternative connection mechanisms until the message is successfully logged.\nThe Test module has been significantly enhanced.\nTime::Local::timelocal() does not handle fractional seconds anymore. The rationale is that neither does localtime() , and timelocal() and localtime() are supposed to be inverses of each other.\nThe vars pragma now supports declaring fully qualified variables. (Something that our() does not and will not support.)\nThe utf8:: name space (as in the pragma) provides various Perl-callable functions to provide low level access to Perl's internal Unicode representation. At the moment only length() has been implemented.\nEmacs perl mode (emacs/cperl-mode.el) has been updated to version 4.31.\nemacs/e2ctags.pl is now much faster.\nenc2xs is a tool for people adding their own encodings to the Encode module.\nh2ph now supports C trigraphs.\nh2xs now produces a template README.\nh2xs now uses Devel::PPPort for better portability between different versions of Perl.\nh2xs uses the new ExtUtils::Constant module which will affect newly created extensions that define constants. Since the new code is more correct (if you have two constants where the first one is a prefix of the second one, the first constant never got defined), less lossy (it uses integers for integer constant, as opposed to the old code that used floating point numbers even for integer constants), and slightly faster, you might want to consider regenerating your extension code (the new scheme makes regenerating easy). h2xs now also supports C trigraphs.\nlibnetcfg has been added to configure libnet.\nperlbug is now much more robust. It also sends the bug report to perl.org, not perl.com.\nperlcc has been rewritten and its user interface (that is, command line) is much more like that of the UNIX C compiler, cc. (The perlbc tools has been removed. Use perlcc -B instead.) Note that perlcc is still considered very experimental and unsupported. [561]\nperlivp is a new Installation Verification Procedure utility for running any time after installing Perl.\npiconv is an implementation of the character conversion utility iconv, demonstrating the new Encode module.\npod2html now allows specifying a cache directory.\npod2html now produces XHTML 1.0.\npod2html now understands POD written using different line endings (PC-like CRLF versus UNIX-like LF versus MacClassic-like CR).\ns2p has been completely rewritten in Perl. (It is in fact a full implementation of sed in Perl: you can use the sed functionality by using the psed utility.)\nxsubpp now understands POD documentation embedded in the *.xs files. [561]\nxsubpp now supports the OUT keyword.\nNew Documentation\nperl56delta details the changes between the 5.005 release and the 5.6.0 release.\nperlclib documents the internal replacements for standard C library functions. (Interesting only for extension writers and Perl core hackers.) [561+]\nperldebtut is a Perl debugging tutorial. [561+]\nperlebcdic contains considerations for running Perl on EBCDIC platforms. [561+]\nperlintro is a gentle introduction to Perl.\nperliol documents the internals of PerlIO with layers.\nperlmodstyle is a style guide for writing modules.\nperlnewmod tells about writing and submitting a new module. [561+]\nperlpacktut is a pack() tutorial.\nperlpod has been rewritten to be clearer and to record the best practices gathered over the years.\nperlpodspec is a more formal specification of the pod format, mainly of interest for writers of pod applications, not to people writing in pod.\nperlretut is a regular expression tutorial. [561+]\nperlrequick is a regular expressions quick-start guide. Yes, much quicker than perlretut. [561]\nperltodo has been updated.\nperltootc has been renamed as perltooc (to not to conflict with perltoot in filesystems restricted to \"8.3\" names).\nperluniintro is an introduction to using Unicode in Perl. (perlunicode is more of a detailed reference and background information)\nperlutil explains the command line utilities packaged with the Perl distribution. [561+]\nThe following platform-specific documents are available before the installation as README.platform, and after the installation as perlplatform:\nperlaix perlamiga perlapollo perlbeos perlbs2000 perlce perlcygwin perldgux perldos perlepoc perlfreebsd perlhpux perlhurd perlirix perlmachten perlmacos perlmint perlmpeix perlnetware perlos2 perlos390 perlplan9 perlqnx perlsolaris perltru64 perluts perlvmesa perlvms perlvos perlwin32\nThese documents usually detail one or more of the following subjects: configuring, building, testing, installing, and sometimes also using Perl on the said platform.\nEastern Asian Perl users are now welcomed in their own languages: README.jp (Japanese), README.ko (Korean), README.cn (simplified Chinese) and README.tw (traditional Chinese), which are written in normal pod but encoded in EUC-JP, EUC-KR, EUC-CN and Big5. These will get installed as\nperljp perlko perlcn perltw\nThe documentation for the POSIX-BC platform is called \"BS2000\", to avoid confusion with the Perl POSIX module.\nThe documentation for the WinCE platform is called perlce (README.ce in the source code kit), to avoid confusion with the perlwin32 documentation on 8.3-restricted filesystems.\nPerformance Enhancements\nmap() could get pathologically slow when the result list it generates is larger than the source list. The performance has been improved for common scenarios. [561]\nsort() is also fully reentrant, in the sense that the sort function can itself call sort() . This did not work reliably in previous releases. [561]\nsort() has been changed to use primarily mergesort internally as opposed to the earlier quicksort. For very small lists this may result in slightly slower sorting times, but in general the speedup should be at least 20%. Additional bonuses are that the worst case behaviour of sort() is now better (in computer science terms it now runs in time O(N log N), as opposed to quicksort's Theta(N**2) worst-case run time behaviour), and that sort() is now stable (meaning that elements with identical keys will stay ordered as they were before the sort). See the sort pragma for information.\nThe story in more detail: suppose you want to serve yourself a little slice of Pi.\n@digits = ( 3,1,4,1,5,9 );\nA numerical sort of the digits will yield (1,1,3,4,5,9), as expected. Which 1 comes first is hard to know, since one 1 looks pretty much like any other. You can regard this as totally trivial, or somewhat profound. However, if you just want to sort the even digits ahead of the odd ones, then what will\nsort { ($a % 2) <=> ($b % 2) } @digits;\nyield? The only even digit, 4, will come first. But how about the odd numbers, which all compare equal? With the quicksort algorithm used to implement Perl 5.6 and earlier, the order of ties is left up to the sort. So, as you add more and more digits of Pi, the order in which the sorted even and odd digits appear will change. and, for sufficiently large slices of Pi, the quicksort algorithm in Perl 5.8 won't return the same results even if reinvoked with the same input. The justification for this rests with quicksort's worst case behavior. If you run\nsort { $a <=> $b } ( 1 .. $N , 1 .. $N );\n(something you might approximate if you wanted to merge two sorted arrays using sort), doubling $N doesn't just double the quicksort time, it quadruples it. Quicksort has a worst case run time that can grow like N**2, so-called quadratic behaviour, and it can happen on patterns that may well arise in normal use. You won't notice this for small arrays, but you will notice it with larger arrays, and you may not live long enough for the sort to complete on arrays of a million elements. So the 5.8 quicksort scrambles large arrays before sorting them, as a statistical defence against quadratic behaviour. But that means if you sort the same large array twice, ties may be broken in different ways.\nBecause of the unpredictability of tie-breaking order, and the quadratic worst-case behaviour, quicksort was almost replaced completely with a stable mergesort. Stable means that ties are broken to preserve the original order of appearance in the input array. So\nsort { ($a % 2) <=> ($b % 2) } (3,1,4,1,5,9);\nwill yield (4,3,1,1,5,9), guaranteed. The even and odd numbers appear in the output in the same order they appeared in the input. Mergesort has worst case O(N log N) behaviour, the best value attainable. And, ironically, this mergesort does particularly well where quicksort goes quadratic: mergesort sorts (1..$N, 1..$N) in O(N) time. But quicksort was rescued at the last moment because it is faster than mergesort on certain inputs and platforms. For example, if you really don't care about the order of even and odd digits, quicksort will run in O(N) time; it's very good at sorting many repetitions of a small number of distinct elements. The quicksort divide and conquer strategy works well on platforms with relatively small, very fast, caches. Eventually, the problem gets whittled down to one that fits in the cache, from which point it benefits from the increased memory speed.\nQuicksort was rescued by implementing a sort pragma to control aspects of the sort. The stable subpragma forces stable behaviour, regardless of algorithm. The _quicksort and _mergesort subpragmas are heavy-handed ways to select the underlying implementation. The leading _ is a reminder that these subpragmas may not survive beyond 5.8. More appropriate mechanisms for selecting the implementation exist, but they wouldn't have arrived in time to save quicksort.\nHashes now use Bob Jenkins \"One-at-a-Time\" hashing key algorithm ( http://burtleburtle.net/bob/hash/doobs.html ). This algorithm is reasonably fast while producing a much better spread of values than the old hashing algorithm (originally by Chris Torek, later tweaked by Ilya Zakharevich). Hash values output from the algorithm on a hash of all 3-char printable ASCII keys comes much closer to passing the DIEHARD random number generation tests. According to perlbench, this change has not affected the overall speed of Perl.\nunshift() should now be noticeably faster.\nGeneric Improvements\nINSTALL now explains how you can configure Perl to use 64-bit integers even on non-64-bit platforms.\nPolicy.sh policy change: if you are reusing a Policy.sh file (see INSTALL) and you use Configure -Dprefix=/foo/bar and in the old Policy $prefix eq $siteprefix and $prefix eq $vendorprefix, all of them will now be changed to the new prefix, /foo/bar. (Previously only $prefix changed.) If you do not like this new behaviour, specify prefix, siteprefix, and vendorprefix explicitly.\nA new optional location for Perl libraries, otherlibdirs, is available. It can be used for example for vendor add-ons without disturbing Perl's own library directories.\nIn many platforms, the vendor-supplied 'cc' is too stripped-down to build Perl (basically, 'cc' doesn't do ANSI C). If this seems to be the case and 'cc' does not seem to be the GNU C compiler 'gcc', an automatic attempt is made to find and use 'gcc' instead.\ngcc needs to closely track the operating system release to avoid build problems. If Configure finds that gcc was built for a different operating system release than is running, it now gives a clearly visible warning that there may be trouble ahead.\nSince Perl 5.8 is not binary-compatible with previous releases of Perl, Configure no longer suggests including the 5.005 modules in @INC.\nConfigure -S can now run non-interactively. [561]\nConfigure support for pdp11-style memory models has been removed due to obsolescence. [561]\nconfigure.gnu now works with options with whitespace in them.\ninstallperl now outputs everything to STDERR.\nBecause PerlIO is now the default on most platforms, \"-perlio\" doesn't get appended to the $Config{archname} (also known as $^O) anymore. Instead, if you explicitly choose not to use perlio (Configure command line option -Uuseperlio), you will get \"-stdio\" appended.\nAnother change related to the architecture name is that \"-64all\" (-Duse64bitall, or \"maximally 64-bit\") is appended only if your pointers are 64 bits wide. (To be exact, the use64bitall is ignored.)\nIn AFS installations, one can configure the root of the AFS to be somewhere else than the default /afs by using the Configure parameter -Dafsroot=/some/where/else.\nAPPLLIB_EXP, a lesser-known configuration-time definition, has been documented. It can be used to prepend site-specific directories to Perl's default search path (@INC); see INSTALL for information.\nThe version of Berkeley DB used when the Perl (and, presumably, the DB_File extension) was built is now available as @Config{qw(db_version_major db_version_minor db_version_patch)} from Perl and as DB_VERSION_MAJOR_CFG DB_VERSION_MINOR_CFG DB_VERSION_PATCH_CFG from C.\nBuilding Berkeley DB3 for compatibility modes for DB, NDBM, and ODBM has been documented in INSTALL.\nIf you have CPAN access (either network or a local copy such as a CD-ROM) you can during specify extra modules to Configure to build and install with Perl using the -Dextras=... option. See INSTALL for more details.\nIn addition to config.over, a new override file, config.arch, is available. This file is supposed to be used by hints file writers for architecture-wide changes (as opposed to config.over which is for site-wide changes).\nIf your file system supports symbolic links, you can build Perl outside of the source directory by\nmkdir perl/build/directory\r\n        cd perl/build/directory\r\n        sh /path/to/perl/source/Configure -Dmksymlinks ...\nThis will create in perl/build/directory a tree of symbolic links pointing to files in /path/to/perl/source. The original files are left unaffected. After Configure has finished, you can just say\nmake all test\nand Perl will be built and tested, all in perl/build/directory. [561]\nFor Perl developers, several new make targets for profiling and debugging have been added; see the perlhack manpage .\nUse of the gprof tool to profile Perl has been documented in the perlhack manpage . There is a make target called \"perl.gprof\" for generating a gprofiled Perl executable.\nIf you have GCC 3, there is a make target called \"perl.gcov\" for creating a gcoved Perl executable for coverage analysis. See the perlhack manpage .\nIf you are on IRIX or Tru64 platforms, new profiling/debugging options have been added; see the perlhack manpage for more information about pixie and Third Degree.\nGuidelines of how to construct minimal Perl installations have been added to INSTALL.\nThe Thread extension is now not built at all under ithreads (Configure -Duseithreads) because it wouldn't work anyway (the Thread extension requires being Configured with -Duse5005threads).\nNote that the 5.005 threads are unsupported and deprecated: if you have code written for the old threads you should migrate it to the new ithreads model.\nThe Gconvert macro ($Config{d_Gconvert}) used by perl for stringifying floating-point numbers is now more picky about using sprintf %.*g rules for the conversion. Some platforms that used to use gcvt may now resort to the slower sprintf.\nThe obsolete method of making a special (e.g., debugging) flavor of perl by saying\nmake LIBPERL=libperld.a\n"}, {"score": 487.25217, "uuid": "033c5d52-b2d6-51cc-8eab-7ca5619d05b5", "index": "cw12", "trec_id": "clueweb12-0511wb-31-30152", "target_hostname": "www.mkssoftware.com", "target_uri": "http://www.mkssoftware.com/docs/perl/pod/perl58delta.asp", "page_rank": 1.3697558e-09, "spam_rank": 72, "title": "perl58delta - what <em>is</em> new for perl v5.8.0", "snippet": "With the <em>quicksort</em> <em>algorithm</em> used to implement Perl 5.6 and earlier, the order of ties <em>is</em> left up to the <em>sort</em>.", "explanation": null, "document": "Locale::Maketext, by Sean Burke, is a localization framework. See the Locale::Maketext manpage , and the Locale::Maketext::TPJ13 manpage . The latter is an article about software localization, originally published in The Perl Journal #13, and republished here with kind permission.\nMath::BigRat for big rational numbers, to accompany Math::BigInt and Math::BigFloat, from Tels. See the Math::BigRat manpage .\nMemoize can make your functions faster by trading space for time, from Mark-Jason Dominus. See the Memoize manpage .\nMIME::Base64, by Gisle Aas, allows you to encode data in base64, as defined in RFC 2045 - MIME (Multipurpose Internet Mail Extensions).\nuse MIME::Base64;\n$encoded = encode_base64('Aladdin:open sesame');\r\n    $decoded = decode_base64($encoded);\nprint $encoded, \"\\n\"; # \"QWxhZGRpbjpvcGVuIHNlc2FtZQ==\"\nMIME::QuotedPrint, by Gisle Aas, allows you to encode data in quoted-printable encoding, as defined in RFC 2045 - MIME (Multipurpose Internet Mail Extensions).\nuse MIME::QuotedPrint;\n$encoded = encode_qp(\"\\xDE\\xAD\\xBE\\xEF\");\r\n    $decoded = decode_qp($encoded);\nprint $encoded, \"\\n\"; # \"=DE=AD=BE=EF\\n\"\r\n    print $decoded, \"\\n\"; # \"\\xDE\\xAD\\xBE\\xEF\\n\"\nNEXT, by Damian Conway, is a pseudo-class for method redispatch. See the NEXT manpage .\nopen is a new pragma for setting the default I/O layers for open().\nPerlIO::scalar, by Nick Ing-Simmons, provides the implementation of IO to ``in memory'' Perl scalars as discussed above. It also serves as an example of a loadable PerlIO layer. Other future possibilities include PerlIO::Array and PerlIO::Code. See the PerlIO::scalar manpage .\nPerlIO::via, by Nick Ing-Simmons, acts as a PerlIO layer and wraps PerlIO layer functionality provided by a class (typically implemented in Perl code).\nPerlIO::via::QuotedPrint, by Elizabeth Mattijsen, is an example of a PerlIO::via class:\nuse PerlIO::via::QuotedPrint;\r\n    open($fh,\">:via(QuotedPrint)\",$path);\nThis will automatically convert everything output to $fh to Quoted-Printable. See the PerlIO::via manpage and the PerlIO::via::QuotedPrint manpage .\nPod::ParseLink, by Russ Allbery, has been added, to parse L<> links in pods as described in the new perlpodspec.\nPod::Text::Overstrike, by Joe Smith, has been added. It converts POD data to formatted overstrike text. See the Pod::Text::Overstrike manpage . [561+]\nScalar::Util is a selection of general-utility scalar subroutines, such as blessed(), reftype(), and tainted(). See the Scalar::Util manpage .\nsort is a new pragma for controlling the behaviour of sort().\nStorable gives persistence to Perl data structures by allowing the storage and retrieval of Perl data to and from files in a fast and compact binary format. Because in effect Storable does serialisation of Perl data structures, with it you can also clone deep, hierarchical datastructures. Storable was originally created by Raphael Manfredi, but it is now maintained by Abhijit Menon-Sen. Storable has been enhanced to understand the two new hash features, Unicode keys and restricted hashes. See the Storable manpage .\nSwitch, by Damian Conway, has been added. Just by saying\nuse Switch;\nyou have switch and case available in Perl.\nuse Switch;\nswitch ($val) {\ncase 1          { print \"number 1\" }\r\n                case \"a\"        { print \"string a\" }\r\n                case [1..10,42] { print \"number in list\" }\r\n                case (@array)   { print \"number in list\" }\r\n                case /\\w+/      { print \"pattern\" }\r\n                case qr/\\w+/    { print \"pattern\" }\r\n                case (%hash)    { print \"entry in hash\" }\r\n                case (\\%hash)   { print \"entry in hash\" }\r\n                case (\\&sub)    { print \"arg to subroutine\" }\r\n                else            { print \"previous case not true\" }\r\n    }\nTest::More, by Michael Schwern, is yet another framework for writing test scripts, more extensive than Test::Simple. See the Test::More manpage .\nTest::Simple, by Michael Schwern, has basic utilities for writing tests. See the Test::Simple manpage .\nText::Balanced, by Damian Conway, has been added, for extracting delimited text sequences from strings.\nuse Text::Balanced 'extract_delimited';\n($a, $b) = extract_delimited(\"'never say never', he never said\", \"'\", '');\n$a will be ``'never say never''', $b will be ', he never said'.\nIn addition to extract_delimited(), there are also extract_bracketed(), extract_quotelike(), extract_codeblock(), extract_variable(), extract_tagged(), extract_multiple(), gen_delimited_pat(), and gen_extract_tagged(). With these, you can implement rather advanced parsing algorithms. See the Text::Balanced manpage .\nthreads, by Arthur Bergman, is an interface to interpreter threads. Interpreter threads (ithreads) is the new thread model introduced in Perl 5.6 but only available as an internal interface for extension writers (and for Win32 Perl for fork() emulation). See the threads manpage , the threads::shared manpage , and the perlthrtut manpage .\nthreads::shared, by Arthur Bergman, allows data sharing for interpreter threads. See the threads::shared manpage .\nTie::File, by Mark-Jason Dominus, associates a Perl array with the lines of a file. See the Tie::File manpage .\nTie::Memoize, by Ilya Zakharevich, provides on-demand loaded hashes. See the Tie::Memoize manpage .\nTie::RefHash::Nestable, by Edward Avis, allows storing hash references (unlike the standard Tie::RefHash) The module is contained within Tie::RefHash. See the Tie::RefHash manpage .\nTime::HiRes, by Douglas E. Wegscheid, provides high resolution timing (ualarm, usleep, and gettimeofday). See the Time::HiRes manpage .\nUnicode::UCD offers a querying interface to the Unicode Character Database. See the Unicode::UCD manpage .\nUnicode::Collate, by SADAHIRO Tomoyuki, implements the UCA (Unicode Collation Algorithm) for sorting Unicode strings. See the Unicode::Collate manpage .\nUnicode::Normalize, by SADAHIRO Tomoyuki, implements the various Unicode normalization forms. See the Unicode::Normalize manpage .\nXS::APItest, by Tim Jenness, is a test extension that exercises XS APIs. Currently only printf() is tested: how to output various basic data types from XS.\nXS::Typemap, by Tim Jenness, is a test extension that exercises XS typemaps. Nothing gets installed, but the code is worth studying for extension writers.\nThe following independently supported modules have been updated to the newest versions from CPAN: CGI, CPAN, DB_File, File::Spec, File::Temp, Getopt::Long, Math::BigFloat, Math::BigInt, the podlators bundle (Pod::Man, Pod::Text), Pod::LaTeX [561+], Pod::Parser, Storable, Term::ANSIColor, Test, Text-Tabs+Wrap.\nattributes::reftype() now works on tied arguments.\nAutoLoader can now be disabled with no AutoLoader;.\nB::Deparse has been significantly enhanced by Robin Houston. It can now deparse almost all of the standard test suite (so that the tests still succeed). There is a make target ``test.deparse'' for trying this out.\nCarp now has better interface documentation, and the @CARP_NOT interface has been added to get optional control over where errors are reported independently of @ISA, by Ben Tilly.\nClass::Struct can now define the classes in compile time.\nClass::Struct now assigns the array/hash element if the accessor is called with an array/hash element as the sole argument.\nThe return value of Cwd::fastcwd() is now tainted.\nData::Dumper now has an option to sort hashes.\nData::Dumper now has an option to dump code references using B::Deparse.\nDB_File now supports newer Berkeley DB versions, among other improvements.\nDevel::Peek now has an interface for the Perl memory statistics (this works only if you are using perl's malloc, and if you have compiled with debugging).\nThe English module can now be used without the infamous performance hit by saying\nuse English '-no_match_vars';\n(Assuming, of course, that you don't need the troublesome variables $` , $& , or $' .) Also, introduced @LAST_MATCH_START and @LAST_MATCH_END English aliases for @- and @+ .\nExtUtils::MakeMaker has been significantly cleaned up and fixed. The enhanced version has also been backported to earlier releases of Perl and submitted to CPAN so that the earlier releases can enjoy the fixes.\nThe arguments of WriteMakefile() in Makefile.PL are now checked for sanity much more carefully than before. This may cause new warnings when modules are being installed. See the ExtUtils::MakeMaker manpage for more details.\nExtUtils::MakeMaker now uses File::Spec internally, which hopefully leads to better portability.\nFcntl, Socket, and Sys::Syslog have been rewritten by Nicholas Clark to use the new-style constant dispatch section (see the ExtUtils::Constant manpage ). This means that they will be more robust and hopefully faster.\nFile::Find now chdir()s correctly when chasing symbolic links. [561]\nFile::Find now has pre- and post-processing callbacks. It also correctly changes directories when chasing symbolic links. Callbacks (naughtily) exiting with ``next;'' instead of ``return;'' now work.\nFile::Find is now (again) reentrant. It also has been made more portable.\nThe warnings issued by File::Find now belong to their own category. You can enable/disable them with use/no warnings 'File::Find';.\nFile::Glob::glob() has been renamed to File::Glob::bsd_glob() because the name clashes with the builtin glob(). The older name is still available for compatibility, but is deprecated. [561]\nFile::Glob now supports GLOB_LIMIT constant to limit the size of the returned list of filenames.\nIPC::Open3 now allows the use of numeric file descriptors.\nIO::Socket now has an atmark() method, which returns true if the socket is positioned at the out-of-band mark. The method is also exportable as a sockatmark() function.\nIO::Socket::INET failed to open the specified port if the service name was not known. It now correctly uses the supplied port number as is. [561]\nIO::Socket::INET has support for the ReusePort option (if your platform supports it). The Reuse option now has an alias, ReuseAddr. For clarity, you may want to prefer ReuseAddr.\nIO::Socket::INET now supports a value of zero for LocalPort (usually meaning that the operating system will make one up.)\n'use lib' now works identically to @INC. Removing directories with 'no lib' now works.\nMath::BigFloat and Math::BigInt have undergone a full rewrite by Tels. They are now magnitudes faster, and they support various bignum libraries such as GMP and PARI as their backends.\nMath::Complex handles inf, NaN etc., better.\nNet::Ping has been considerably enhanced by Rob Brown: multihoming is now supported, Win32 functionality is better, there is now time measuring functionality (optionally high-resolution using Time::HiRes), and there is now ``external'' protocol which uses Net::Ping::External module which runs your external ping utility and parses the output. A version of Net::Ping::External is available in CPAN.\nNote that some of the Net::Ping tests are disabled when running under the Perl distribution since one cannot assume one or more of the following: enabled echo port at localhost, full Internet connectivity, or sympathetic firewalls. You can set the environment variable PERL_TEST_Net_Ping to ``1'' (one) before running the Perl test suite to enable all the Net::Ping tests.\nPOSIX::sigaction() is now much more flexible and robust. You can now install coderef handlers, 'DEFAULT', and 'IGNORE' handlers, installing new handlers was not atomic.\nIn Safe, %INC is now localised in a Safe compartment so that use/require work.\nIn SDBM_File on dosish platforms, some keys went missing because of lack of support for files with ``holes''. A workaround for the problem has been added.\nIn Search::Dict one can now have a pre-processing hook for the lines being searched.\nThe Shell module now has an OO interface.\nIn Sys::Syslog there is now a failover mechanism that will go through alternative connection mechanisms until the message is successfully logged.\nThe Test module has been significantly enhanced.\nTime::Local::timelocal() does not handle fractional seconds anymore. The rationale is that neither does localtime(), and timelocal() and localtime() are supposed to be inverses of each other.\nThe vars pragma now supports declaring fully qualified variables. (Something that our() does not and will not support.)\nThe utf8:: name space (as in the pragma) provides various Perl-callable functions to provide low level access to Perl's internal Unicode representation. At the moment only length() has been implemented.\nEmacs perl mode (emacs/cperl-mode.el) has been updated to version 4.31.\nemacs/e2ctags.pl is now much faster.\nenc2xs is a tool for people adding their own encodings to the Encode module.\nh2ph now supports C trigraphs.\nh2xs now produces a template README.\nh2xs now uses Devel::PPPort for better portability between different versions of Perl.\nh2xs uses the new ExtUtils::Constant module which will affect newly created extensions that define constants. Since the new code is more correct (if you have two constants where the first one is a prefix of the second one, the first constant never got defined), less lossy (it uses integers for integer constant, as opposed to the old code that used floating point numbers even for integer constants), and slightly faster, you might want to consider regenerating your extension code (the new scheme makes regenerating easy). h2xs now also supports C trigraphs.\nlibnetcfg has been added to configure libnet.\nperlbug is now much more robust. It also sends the bug report to perl.org, not perl.com.\nperlcc has been rewritten and its user interface (that is, command line) is much more like that of the UNIX C compiler, cc. (The perlbc tools has been removed. Use perlcc -B instead.) Note that perlcc is still considered very experimental and unsupported. [561]\nperlivp is a new Installation Verification Procedure utility for running any time after installing Perl.\npiconv is an implementation of the character conversion utility iconv, demonstrating the new Encode module.\npod2html now allows specifying a cache directory.\npod2html now produces XHTML 1.0.\npod2html now understands POD written using different line endings (PC-like CRLF versus UNIX-like LF versus MacClassic-like CR).\ns2p has been completely rewritten in Perl. (It is in fact a full implementation of sed in Perl: you can use the sed functionality by using the psed utility.)\nxsubpp now understands POD documentation embedded in the *.xs files. [561]\nxsubpp now supports the OUT keyword.\nNew Documentation\nperl56delta details the changes between the 5.005 release and the 5.6.0 release.\nperlclib documents the internal replacements for standard C library functions. (Interesting only for extension writers and Perl core hackers.) [561+]\nperldebtut is a Perl debugging tutorial. [561+]\nperlebcdic contains considerations for running Perl on EBCDIC platforms. [561+]\nperlintro is a gentle introduction to Perl.\nperliol documents the internals of PerlIO with layers.\nperlmodstyle is a style guide for writing modules.\nperlnewmod tells about writing and submitting a new module. [561+]\nperlpacktut is a pack() tutorial.\nperlpod has been rewritten to be clearer and to record the best practices gathered over the years.\nperlpodspec is a more formal specification of the pod format, mainly of interest for writers of pod applications, not to people writing in pod.\nperlretut is a regular expression tutorial. [561+]\nperlrequick is a regular expressions quick-start guide. Yes, much quicker than perlretut. [561]\nperltodo has been updated.\nperltootc has been renamed as perltooc (to not to conflict with perltoot in filesystems restricted to ``8.3'' names).\nperluniintro is an introduction to using Unicode in Perl. (perlunicode is more of a detailed reference and background information)\nperlutil explains the command line utilities packaged with the Perl distribution. [561+]\nThe following platform-specific documents are available before the installation as README.platform, and after the installation as perlplatform:\nperlaix perlamiga perlapollo perlbeos perlbs2000 perlce perlcygwin perldgux perldos perlepoc perlfreebsd perlhpux perlhurd perlirix perlmachten perlmacos perlmint perlmpeix perlnetware perlos2 perlos390 perlplan9 perlqnx perlsolaris perltru64 perluts perlvmesa perlvms perlvos perlwin32\nThese documents usually detail one or more of the following subjects: configuring, building, testing, installing, and sometimes also using Perl on the said platform.\nEastern Asian Perl users are now welcomed in their own languages: README.jp (Japanese), README.ko (Korean), README.cn (simplified Chinese) and README.tw (traditional Chinese), which are written in normal pod but encoded in EUC-JP, EUC-KR, EUC-CN and Big5. These will get installed as\nperljp perlko perlcn perltw\nThe documentation for the POSIX-BC platform is called ``BS2000'', to avoid confusion with the Perl POSIX module.\nThe documentation for the WinCE platform is called perlce (README.ce in the source code kit), to avoid confusion with the perlwin32 documentation on 8.3-restricted filesystems.\nPerformance Enhancements\nmap() could get pathologically slow when the result list it generates is larger than the source list. The performance has been improved for common scenarios. [561]\nsort() is also fully reentrant, in the sense that the sort function can itself call sort(). This did not work reliably in previous releases. [561]\nsort() has been changed to use primarily mergesort internally as opposed to the earlier quicksort. For very small lists this may result in slightly slower sorting times, but in general the speedup should be at least 20%. Additional bonuses are that the worst case behaviour of sort() is now better (in computer science terms it now runs in time O(N log N), as opposed to quicksort's Theta(N**2) worst-case run time behaviour), and that sort() is now stable (meaning that elements with identical keys will stay ordered as they were before the sort). See the sort pragma for information.\nThe story in more detail: suppose you want to serve yourself a little slice of Pi.\n@digits = ( 3,1,4,1,5,9 );\nA numerical sort of the digits will yield (1,1,3,4,5,9), as expected. Which 1 comes first is hard to know, since one 1 looks pretty much like any other. You can regard this as totally trivial, or somewhat profound. However, if you just want to sort the even digits ahead of the odd ones, then what will\nsort { ($a % 2) <=> ($b % 2) } @digits;\nyield? The only even digit, 4, will come first. But how about the odd numbers, which all compare equal? With the quicksort algorithm used to implement Perl 5.6 and earlier, the order of ties is left up to the sort. So, as you add more and more digits of Pi, the order in which the sorted even and odd digits appear will change. and, for sufficiently large slices of Pi, the quicksort algorithm in Perl 5.8 won't return the same results even if reinvoked with the same input. The justification for this rests with quicksort's worst case behavior. If you run\nsort { $a <=> $b } ( 1 .. $N , 1 .. $N );\n(something you might approximate if you wanted to merge two sorted arrays using sort), doubling $N doesn't just double the quicksort time, it quadruples it. Quicksort has a worst case run time that can grow like N**2, so-called quadratic behaviour, and it can happen on patterns that may well arise in normal use. You won't notice this for small arrays, but you will notice it with larger arrays, and you may not live long enough for the sort to complete on arrays of a million elements. So the 5.8 quicksort scrambles large arrays before sorting them, as a statistical defence against quadratic behaviour. But that means if you sort the same large array twice, ties may be broken in different ways.\nBecause of the unpredictability of tie-breaking order, and the quadratic worst-case behaviour, quicksort was almost replaced completely with a stable mergesort. Stable means that ties are broken to preserve the original order of appearance in the input array. So\nsort { ($a % 2) <=> ($b % 2) } (3,1,4,1,5,9);\nwill yield (4,3,1,1,5,9), guaranteed. The even and odd numbers appear in the output in the same order they appeared in the input. Mergesort has worst case O(N log N) behaviour, the best value attainable. And, ironically, this mergesort does particularly well where quicksort goes quadratic: mergesort sorts (1..$N, 1..$N) in O(N) time. But quicksort was rescued at the last moment because it is faster than mergesort on certain inputs and platforms. For example, if you really don't care about the order of even and odd digits, quicksort will run in O(N) time; it's very good at sorting many repetitions of a small number of distinct elements. The quicksort divide and conquer strategy works well on platforms with relatively small, very fast, caches. Eventually, the problem gets whittled down to one that fits in the cache, from which point it benefits from the increased memory speed.\nQuicksort was rescued by implementing a sort pragma to control aspects of the sort. The stable subpragma forces stable behaviour, regardless of algorithm. The _quicksort and _mergesort subpragmas are heavy-handed ways to select the underlying implementation. The leading _ is a reminder that these subpragmas may not survive beyond 5.8. More appropriate mechanisms for selecting the implementation exist, but they wouldn't have arrived in time to save quicksort.\nHashes now use Bob Jenkins ``One-at-a-Time'' hashing key algorithm ( http://burtleburtle.net/bob/hash/doobs.html ). This algorithm is reasonably fast while producing a much better spread of values than the old hashing algorithm (originally by Chris Torek, later tweaked by Ilya Zakharevich). Hash values output from the algorithm on a hash of all 3-char printable ASCII keys comes much closer to passing the DIEHARD random number generation tests. According to perlbench, this change has not affected the overall speed of Perl.\nunshift() should now be noticeably faster.\nGeneric Improvements\nINSTALL now explains how you can configure Perl to use 64-bit integers even on non-64-bit platforms.\nPolicy.sh policy change: if you are reusing a Policy.sh file (see INSTALL) and you use Configure -Dprefix=/foo/bar and in the old Policy $prefix eq $siteprefix and $prefix eq $vendorprefix, all of them will now be changed to the new prefix, /foo/bar. (Previously only $prefix changed.) If you do not like this new behaviour, specify prefix, siteprefix, and vendorprefix explicitly.\nA new optional location for Perl libraries, otherlibdirs, is available. It can be used for example for vendor add-ons without disturbing Perl's own library directories.\nIn many platforms, the vendor-supplied 'cc' is too stripped-down to build Perl (basically, 'cc' doesn't do ANSI C). If this seems to be the case and 'cc' does not seem to be the GNU C compiler 'gcc', an automatic attempt is made to find and use 'gcc' instead.\ngcc needs to closely track the operating system release to avoid build problems. If Configure finds that gcc was built for a different operating system release than is running, it now gives a clearly visible warning that there may be trouble ahead.\nSince Perl 5.8 is not binary-compatible with previous releases of Perl, Configure no longer suggests including the 5.005 modules in @INC.\nConfigure -S can now run non-interactively. [561]\nConfigure support for pdp11-style memory models has been removed due to obsolescence. [561]\nconfigure.gnu now works with options with whitespace in them.\ninstallperl now outputs everything to STDERR.\nBecause PerlIO is now the default on most platforms, ``-perlio'' doesn't get appended to the $Config{archname} (also known as $^O) anymore. Instead, if you explicitly choose not to use perlio (Configure command line option -Uuseperlio), you will get ``-stdio'' appended.\nAnother change related to the architecture name is that ``-64all'' (-Duse64bitall, or ``maximally 64-bit'') is appended only if your pointers are 64 bits wide. (To be exact, the use64bitall is ignored.)\nIn AFS installations, one can configure the root of the AFS to be somewhere else than the default /afs by using the Configure parameter -Dafsroot=/some/where/else.\nAPPLLIB_EXP, a lesser-known configuration-time definition, has been documented. It can be used to prepend site-specific directories to Perl's default search path (@INC); see INSTALL for information.\nThe version of Berkeley DB used when the Perl (and, presumably, the DB_File extension) was built is now available as @Config{qw(db_version_major db_version_minor db_version_patch)} from Perl and as DB_VERSION_MAJOR_CFG DB_VERSION_MINOR_CFG DB_VERSION_PATCH_CFG from C.\nBuilding Berkeley DB3 for compatibility modes for DB, NDBM, and ODBM has been documented in INSTALL.\nIf you have CPAN access (either network or a local copy such as a CD-ROM) you can during specify extra modules to Configure to build and install with Perl using the -Dextras=... option. See INSTALL for more details.\nIn addition to config.over, a new override file, config.arch, is available. This file is supposed to be used by hints file writers for architecture-wide changes (as opposed to config.over which is for site-wide changes).\nIf your file system supports symbolic links, you can build Perl outside of the source directory by\nmkdir perl/build/directory\r\n        cd perl/build/directory\r\n        sh /path/to/perl/source/Configure -Dmksymlinks ...\nThis will create in perl/build/directory a tree of symbolic links pointing to files in /path/to/perl/source. The original files are left unaffected. After Configure has finished, you can just say\nmake all test\nand Perl will be built and tested, all in perl/build/directory. [561]\nFor Perl developers, several new make targets for profiling and debugging have been added; see the perlhack manpage .\nUse of the gprof tool to profile Perl has been documented in the perlhack manpage . There is a make target called ``perl.gprof'' for generating a gprofiled Perl executable.\nIf you have GCC 3, there is a make target called ``perl.gcov'' for creating a gcoved Perl executable for coverage analysis. See the perlhack manpage .\nIf you are on IRIX or Tru64 platforms, new profiling/debugging options have been added; see the perlhack manpage for more information about pixie and Third Degree.\nGuidelines of how to construct minimal Perl installations have been added to INSTALL.\nThe Thread extension is now not built at all under ithreads ( Configure -Duseithreads) because it wouldn't work anyway (the Thread extension requires being Configured with -Duse5005threads).\nNote that the 5.005 threads are unsupported and deprecated: if you have code written for the old threads you should migrate it to the new ithreads model.\nThe Gconvert macro ($Config{d_Gconvert}) used by perl for stringifying floating-point numbers is now more picky about using sprintf %.*g rules for the conversion. Some platforms that used to use gcvt may now resort to the slower sprintf.\nThe obsolete method of making a special (e.g., debugging) flavor of perl by saying\nmake LIBPERL=libperld.a\n"}, {"score": 468.37668, "uuid": "55459e9a-2205-5772-8ec5-d374a91acd63", "index": "cw12", "trec_id": "clueweb12-0711wb-80-08718", "target_hostname": "acs.lbl.gov", "target_uri": "http://acs.lbl.gov/software/colt/api/cern/colt/list/AbstractList.html", "page_rank": 1.8168812e-09, "spam_rank": 66, "title": "AbstractList (Colt 1.2.0 - API Specification)", "snippet": "It <em>is</em> generally <em>better</em> to call <em>sort</em>() <em>or</em> sortFromTo(...) instead, because those methods automatically choose the best sorting <em>algorithm</em>. <em>Sorts</em> the specified range of the receiver into ascending order. The sorting <em>algorithm</em> <em>is</em> a tuned <em>quicksort</em>, adapted from Jon L. Bentley and M.", "explanation": null, "document": "public void addAllOf( Collection collection)\nAppends all of the elements of the specified Collection to the receiver.\nThrows:\nClassCastException - if an element in the collection is not of the same parameter type of the receiver.\nbeforeInsertAllOf\npublic void beforeInsertAllOf(int\u00a0index, Collection collection)\nInserts all elements of the specified collection before the specified position into the receiver. Shifts the element currently at that position (if any) and any subsequent elements to the right (increases their indices).\nParameters:\nindex - index before which to insert first element from the specified collection.\ncollection - the collection to be inserted\nThrows:\nClassCastException - if an element in the collection is not of the same parameter type of the receiver.\nIndexOutOfBoundsException - if index < 0 || index > size().\nclear\npublic void clear()\nRemoves all elements from the receiver. The receiver will be empty after this call returns, but keep its current capacity.\nSpecified by:\nmergeSort\npublic final void mergeSort()\nSorts the receiver into ascending order. This sort is guaranteed to be stable: equal elements will not be reordered as a result of the sort.\nThe sorting algorithm is a modified mergesort (in which the merge is omitted if the highest element in the low sublist is less than the lowest element in the high sublist). This algorithm offers guaranteed n*log(n) performance, and can approach linear performance on nearly sorted lists.\nYou should never call this method unless you are sure that this particular sorting algorithm is the right one for your data set. It is generally better to call sort() or sortFromTo(...) instead, because those methods automatically choose the best sorting algorithm.\nmergeSortFromTo\npublic abstract void mergeSortFromTo(int\u00a0from,\n                                     int\u00a0to)\nSorts the receiver into ascending order. This sort is guaranteed to be stable: equal elements will not be reordered as a result of the sort.\nThe sorting algorithm is a modified mergesort (in which the merge is omitted if the highest element in the low sublist is less than the lowest element in the high sublist). This algorithm offers guaranteed n*log(n) performance, and can approach linear performance on nearly sorted lists.\nYou should never call this method unless you are sure that this particular sorting algorithm is the right one for your data set. It is generally better to call sort() or sortFromTo(...) instead, because those methods automatically choose the best sorting algorithm.\nParameters:\nfrom - the index of the first element (inclusive) to be sorted.\nto - the index of the last element (inclusive) to be sorted.\nThrows:\nIndexOutOfBoundsException - if (from<0 || from>to || to>=size()) && to!=from-1.\nquickSort\npublic final void quickSort()\nSorts the receiver into ascending order. The sorting algorithm is a tuned quicksort, adapted from Jon L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 1993). This algorithm offers n*log(n) performance on many data sets that cause other quicksorts to degrade to quadratic performance.\nYou should never call this method unless you are sure that this particular sorting algorithm is the right one for your data set. It is generally better to call sort() or sortFromTo(...) instead, because those methods automatically choose the best sorting algorithm.\nquickSortFromTo\npublic abstract void quickSortFromTo(int\u00a0from,\n                                     int\u00a0to)\nSorts the specified range of the receiver into ascending order. The sorting algorithm is a tuned quicksort, adapted from Jon L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 1993). This algorithm offers n*log(n) performance on many data sets that cause other quicksorts to degrade to quadratic performance.\nYou should never call this method unless you are sure that this particular sorting algorithm is the right one for your data set. It is generally better to call sort() or sortFromTo(...) instead, because those methods automatically choose the best sorting algorithm.\nParameters:\nfrom - the index of the first element (inclusive) to be sorted.\nto - the index of the last element (inclusive) to be sorted.\nThrows:\nIndexOutOfBoundsException - if (from<0 || from>to || to>=size()) && to!=from-1.\nremove\npublic void remove(int\u00a0index)\nRemoves the element at the specified position from the receiver. Shifts any subsequent elements to the left.\nParameters:\nindex - the index of the element to removed.\nThrows:\nIndexOutOfBoundsException - if index < 0 || index >= size().\nremoveFromTo\npublic abstract void removeFromTo(int\u00a0fromIndex,\n                                  int\u00a0toIndex)\nRemoves from the receiver all elements whose index is between from, inclusive and to, inclusive. Shifts any succeeding elements to the left (reduces their index). This call shortens the list by (to - from + 1) elements.\nThrows:\nIndexOutOfBoundsException - if (from<0 || from>to || to>=size()) && to!=from-1.\nreplaceFromWith\npublic abstract void replaceFromWith(int\u00a0from, Collection other)\nReplaces the part of the receiver starting at from (inclusive) with all the elements of the specified collection. Does not alter the size of the receiver. Replaces exactly Math.max(0,Math.min(size()-from, other.size())) elements.\nParameters:\nfrom - the index at which to copy the first element from the specified collection.\nother - Collection to replace part of the receiver\nThrows:\nIndexOutOfBoundsException - if index < 0 || index >= size().\nreverse\npublic abstract void reverse()\nReverses the elements of the receiver. Last becomes first, second last becomes second first, and so on.\nsetSize\npublic void setSize(int\u00a0newSize)\nSets the size of the receiver. If the new size is greater than the current size, new null or zero items are added to the end of the receiver. If the new size is less than the current size, all components at index newSize and greater are discarded. This method does not release any superfluos internal memory. Use method trimToSize to release superfluos internal memory.\nParameters:\nnewSize - the new size of the receiver.\nThrows:\n"}], [{"score": 465.5833, "uuid": "0220a4b6-feb2-52bd-a7ec-892caf0eac36", "index": "cw12", "trec_id": "clueweb12-1808wb-61-16441", "target_hostname": "programmer.97things.oreilly.com", "target_uri": "http://programmer.97things.oreilly.com/wiki/index.php/Execution_Speed_versus_Maintenance_Effort", "page_rank": 1.2216826e-09, "spam_rank": 92, "title": "Execution Speed versus Maintenance Effort - Programmer 97-things", "snippet": "An example <em>is</em> supporting uniprocessor and multiprocessor modes. When sequential, <em>quicksort</em> <em>is</em> preferable to <em>merge</em> <em>sort</em>, but for concurrency, more research effort has been devoted to producing excellent <em>merge</em> <em>sort</em> and radix <em>sorts</em> than <em>quicksort</em>.", "explanation": null, "document": "Execution Speed versus Maintenance Effort\nFrom Programmer 97-things\nJump to: navigation , search\nMost of the time spent in the execution of a program is in a small proportion of the code. An approach based on simplicity is suitable for the majority of a codebase. It is maintainable without adversely slowing down the application. Following Amdahl's Law, to make the program fast we should concentrate on those few lines which are run most of the time. Determine bottlenecks by empirically profiling representative runs instead of merely relying on algorithmic complexity theory or a hunch.\nThe need for speed can encourage the use of an unobvious algorithm. Typical dividers are slower than multipliers, so it is faster to multiply by the reciprocal of the divisor than to divide by it. Given typical hardware with no choice to use better hardware, division should (if efficiency is important) be performed by multiplication, even though the algorithmic complexity of division and multiplication are identical.\nOther bottlenecks provide an incentive for several alternative algorithms to be used in the same application for the same problem (sometimes even for the same inputs!). Unfortunately, a practical demand for speed punishes having strictly one algorithm per problem. An example is supporting uniprocessor and multiprocessor modes. When sequential, quicksort is preferable to merge sort, but for concurrency, more research effort has been devoted to producing excellent merge sort and radix sorts than quicksort. You should be aware that the best uniprocessor algorithm is not necessarily the best multiprocessor algorithm. You should also be aware that algorithm choice is not merely a question of one uniprocessor architecture versus one multiprocessor architecture. For example, a primitive (and hence cheaper) embedded uniprocessor may lack a branch predictor so a radix sort algorithm may not be advantageous. Different kinds of multiprocessors exist. In 2009, the best published algorithm for multiplying typical m\u00d7n matrices (by P. D'Alberto and A. Nicolau) was designed for a small quantity of desktop multicore machines, whereas other algorithms are viable for machine clusters.\nChanging the quantity or architecture of processors is not the only motivation for diverse algorithms. Special features of different inputs may be exploitable for a faster algorithm. E.g., a practical method for multiplying general square matrices would be O(n>2.376) but the special case of (tri)diagonal matrices admits an O(n) method.\nDivide-and-conquer algorithms, such as quicksort, start out well but suffer from excessive subprogram call overhead when their recursive invocations inevitably reach small subproblems. It is faster to apply a cut-off problem size, at which point recursion is stopped. A nonrecursive algorithm can finish off the remaining work.\nSome applications need a problem solved more than once for the same instance. Exploit dynamic programming instead of recomputing. Dynamic programming is suitable for chained matrix multiplication and optimizing searching binary trees. Unlike a web browser's cache, it guarantees correctness.\nGiven vertex coloring, sometimes graph coloring should be directly performed, sometimes clique partitioning should be performed instead. Determining the vertex-chromatic index is NP-hard. Check whether the graph has many edges. If so, get the graph's complement. Find a minimum clique partitioning of the complement. The algorithmic complexity is unchanged but the speed is improved. Graph coloring is applicable to networking and numerical differentiation.\nLittering a codebase with unintelligible tricks throughout would be bad, as would letting the application run too slowly. Find the right balance for you. Consult books and papers on algorithms. Measure the performance.\n"}, {"score": 451.90463, "uuid": "dde30df4-f2f8-529f-a504-29a8dc2152e2", "index": "cw12", "trec_id": "clueweb12-0917wb-00-29074", "target_hostname": "pine.cs.yale.edu", "target_uri": "http://pine.cs.yale.edu/pinewiki/AlgorithmDesignTechniques", "page_rank": 1.2349278e-09, "spam_rank": 85, "title": "AlgorithmDesignTechniques - Pinewiki", "snippet": "This <em>algorithm</em> <em>is</em> known as MergeSort, and <em>is</em> one of the fastest general-purpose sorting <em>algorithms</em>. The <em>merge</em> can be avoided by carefully splitting the array into elements less than and elements greater than some pivot, then sorting the two resulting piles; this gives <em>QuickSort</em>.", "explanation": null, "document": "More Actions:\n1. Basic principles of algorithm design\nThe fundamental principle of algorithm design was best expressed by the mathematician George_Polya : \"If there is a problem you can't solve, then there is an easier problem you can solve: find it.\" For computers, the situation is even easier: if there is any technique to make a problem easier even by a tiny bit, then you can repeat the technique until the problem becomes trivial.\nFor example, suppose we want to find the maximum element of an array of n ints, but we are as dumb as bricks, so it doesn't occur to us to iterate through the array keeping track of the largest value seen so far. We might instead be able to solve the problem by observing that the maximum element is either (a) the last element, or (b) the maximum of the first n-1 elements, depending on which is bigger. Figuring out (b) is an easier version of the original problem, so we are pretty much done once we've realized we can split the problem in this way. Here's the code:\n1 /* returns maximum of the n elements in a */ 2 int 3 max_element(int a[], int n) 4 { 5 int prefix_max; 6 7 assert(n > 0); 8 9 if(n == 1) { 10 return a[0]; 11 } else { 12 prefix_max = max_element(a, n-1); 13 if(prefix_max < a[n-1]) { 14 return a[n-1]; 15 } else { 16 return prefix_max; 17 } 18 } 19 }\nNote that we need a special case for a 1-element array, because the empty prefix of such an array has no maximum element. We also assert that the array contains at least one element, just to avoid mischief.\nOne problem with this algorithm (at least when coding in C) is that the recursion may get very deep. Fortunately, there is a straightforward way to convert the recursion to a loop. The idea is that instead of returning a value from the recursive call, we put it in a variable that gets used in the next pass through the loop. The result is\n1 /* returns maximum of the n elements in a */ 2 int 3 max_element(int a[], int n) 4 { 5 int i; /* this replaces n-1 from the recursive version */ 6 int prefix_max; 7 8 assert(n > 0); 9 10 prefix_max = a[0]; /* this is the i == 0 case */ 11 12 for(i = 1; i < n; i++) { 13 if(prefix_max < a[i]) { 14 prefix_max = a[i]; /* was return a[n-1] */ 15 } 16 /* else case becomes prefix_max = prefix_max, a noop */ 17 } 18 19 /* at the end we have to return a value for real */ 20 return prefix_max; 21 }\n2. Classifying algorithms by structure\nAlgorithm design often requires both creativity and problem-specific knowledge, but there are certain common techniques that appear over and over again. The following classification is adapted from LevitinBook :\nBruteForce : Try all possible solutions until you find the right one.\nDivideAndConquer : Split the problem into two or more subproblems, solve the subproblems recursively, and then combine the solutions.\nDecreaseAndConquer : Reduce the problem to a single smaller problem, solve that problem recursively, and then use that solution to solve the original problem.\nTransformAndConquer : Either (a) transform the input to a form that makes the problem easy to solve, or (b) transform the input into the input to another problem whose solution solves the original problem.\nUseSpace : Solve the problem using some auxiliary data structure.\nDynamicProgramming : Construct a table of solutions for increasingly large subproblems, where each new entry in the table is computed using previous entries in the table.\nGreedyMethod : Run through your problem one step at a time, keeping track of the single best solution at each step. Hope sincerely that this will not lead you to make a seemingly-good choice early with bad consequences later.\nSome of these approaches work better than others---it is the role of AlgorithmAnalysis (and experiments with real computers) to figure out which are likely to be both correct and efficient in practice. But having all of them in your toolbox lets you try different possibilities for a given problem.\n3. Example: Finding the maximum\nThough this classification is not completely well-defined, and is a bit arbitrary for some algorithms, it does provide a useful list of things to try in solving a problem. Here are some examples of applying the different approaches to a simple problem, the problem of finding the maximum of an array of integers.\nBruteForce : For index i, test if A[i] is greater than or equal to every element in the array. When you find such an A[i], return it. For this algorithm, T(n) = n*Theta(n) = Theta(n2) if implemented in the most natural way.\nDivideAndConquer : If A has only one element, return it. Otherwise, let m1 be the maximum of A[1]..A[n/2]. Let m2 be the maximum of A[n/2+1]..A[n]. Return the larger of m1 and m2. The running time is given by T(n) = 2T(n/2) + Theta(1) = Theta(n).\nDecreaseAndConquer : If A has only one element, return it. Otherwise, let m be the maximum of A[2]..A[n]. Return the larger of A[0] and m. Now the running time is given by T(n) = T(n-1) + Theta(1) = Sigmai=1 to n Theta(1) = Theta(n).\nTransformAndConquer : Sort the array, then return A[n]. Using an optimal comparison-based sort, this takes Theta(n log n) + Theta(1) = Theta(n log n) time. The advantage of this approach is that you probably don't have to code up the sort.\nUseSpace : Insert all elements into a balanced binary search tree, then return the rightmost element. Cost is Theta(n log n) to do n insertions, plus Theta(log n) to find the rightmost element, for a total of Theta(n log n). Sorting is equivalent and probably easier.\nDynamicProgramming : Create an auxiliary array B with indices 1 to n. Set B[1] = A[1]. As i goes from 2 to n, set B[i] to the larger of B[i-1] and A[i]. Return B[n]. Cost: Theta(n).\nGreedyMethod : Let max = A[1]. For each element A[i] in A[2..n], if A[i] > max, set max = A[i]. Return the final value of max. Cost: Theta(n); this algorithm is pretty much identical to the previous one.\n4. Example: Sorting\nThe sorting problem asks, given an array of n elements in arbitrary order, for an array containing the same n elements in nondecreasing order, i.e. with A[i] <= A[i+1] for all i. We can apply each of the techniques above to this problem and get a sorting algorithm (though some are not very good).\nBruteForce : For each of the n! permutations of the input, test if it is sorted by checking A[i] <= A[i+1] for all i. Cost if implemented naively: n!*Theta(n) = Theta(n*n!). This algorithm is known as deterministic monkeysort or deterministic bogosort. It also has a randomized variant, where the careful generation of all n! permutations is replaced by shuffling. The randomized variant is easier to code and runs about a factor of two faster than the deterministic variant, but does not guarantee termination if the shuffling is consistently unlucky.\nDivideAndConquer : Sort A[1..floor(n/2)] and A[floor(n/2+1)..n] separately, then merge the results (which takes Theta(n) time and Theta(n) additional space if implemented in the most straightforward way). Cost: T(n)=2T(n/2)+Theta(n)=Theta(n log n) by the Master Theorem. This algorithm is known as MergeSort , and is one of the fastest general-purpose sorting algorithms. The merge can be avoided by carefully splitting the array into elements less than and elements greater than some pivot, then sorting the two resulting piles; this gives QuickSort . The performance of QuickSort is often faster than MergeSort in practice, but its worst-case performance (when the pivot is chosen badly) is just as bad as the result of:\nDecreaseAndConquer : Remove A[n], sort the remainder, then insert A[n] in the appropriate place. This algorithm is called InsertionSort . The final insertion step requires finding the right place (which can be done fairly quickly if one is clever) but then moving up to n-1 elements to make room for A[n]. Total cost is given by T(n) = T(n-1) + Theta(n) = T(n2).\nTransformAndConquer : I'm not aware of any good general TransformAndConquer approach to sorting (there are some bad ones), but in some cases one can transform seemingly general sorting problem (e.g. sorting strings) into specialized sorting problems that permit faster solutions (e.g. sorting small integers).\nUseSpace : Insert the elements into a balanced binary search tree, then read them out from left to right. Another version: insert them into a heap. Both take Theta(n log n) time, but are more complicated to implement than MergeSort or QuickSort unless you have BST or heap code lying around already.\nDynamicProgramming : ( InsertionSort may be seen as an example of this.)\nGreedyMethod : Find the smallest element, mark it as used, and output it. Repeat until no elements are left. The result is SelectionSort , which runs in a respectable but suboptimal Theta(n2) time.\nAlgorithmDesignTechniques (last edited 2007-12-25 23:42:02 by localhost)\nImmutable Page\n"}, {"score": 451.6702, "uuid": "a7f08b2f-09e0-5fee-8cd6-f7271e09c065", "index": "cw12", "trec_id": "clueweb12-1001wb-45-26777", "target_hostname": "pine.cs.yale.edu", "target_uri": "http://pine.cs.yale.edu/pinewiki/AlgorithmDesignTechniques?action=show&redirect=AlgorithmDesignTechnique", "page_rank": 1.1700305e-09, "spam_rank": 84, "title": "AlgorithmDesignTechniques - Pinewiki", "snippet": "This <em>algorithm</em> <em>is</em> known as MergeSort, and <em>is</em> one of the fastest general-purpose sorting <em>algorithms</em>. The <em>merge</em> can be avoided by carefully splitting the array into elements less than and elements greater than some pivot, then sorting the two resulting piles; this gives <em>QuickSort</em>.", "explanation": null, "document": "More Actions:\n1. Basic principles of algorithm design\nThe fundamental principle of algorithm design was best expressed by the mathematician George_Polya : \"If there is a problem you can't solve, then there is an easier problem you can solve: find it.\" For computers, the situation is even easier: if there is any technique to make a problem easier even by a tiny bit, then you can repeat the technique until the problem becomes trivial.\nFor example, suppose we want to find the maximum element of an array of n ints, but we are as dumb as bricks, so it doesn't occur to us to iterate through the array keeping track of the largest value seen so far. We might instead be able to solve the problem by observing that the maximum element is either (a) the last element, or (b) the maximum of the first n-1 elements, depending on which is bigger. Figuring out (b) is an easier version of the original problem, so we are pretty much done once we've realized we can split the problem in this way. Here's the code:\n1 /* returns maximum of the n elements in a */ 2 int 3 max_element(int a[], int n) 4 { 5 int prefix_max; 6 7 assert(n > 0); 8 9 if(n == 1) { 10 return a[0]; 11 } else { 12 prefix_max = max_element(a, n-1); 13 if(prefix_max < a[n-1]) { 14 return a[n-1]; 15 } else { 16 return prefix_max; 17 } 18 } 19 }\nNote that we need a special case for a 1-element array, because the empty prefix of such an array has no maximum element. We also assert that the array contains at least one element, just to avoid mischief.\nOne problem with this algorithm (at least when coding in C) is that the recursion may get very deep. Fortunately, there is a straightforward way to convert the recursion to a loop. The idea is that instead of returning a value from the recursive call, we put it in a variable that gets used in the next pass through the loop. The result is\n1 /* returns maximum of the n elements in a */ 2 int 3 max_element(int a[], int n) 4 { 5 int i; /* this replaces n-1 from the recursive version */ 6 int prefix_max; 7 8 assert(n > 0); 9 10 prefix_max = a[0]; /* this is the i == 0 case */ 11 12 for(i = 1; i < n; i++) { 13 if(prefix_max < a[i]) { 14 prefix_max = a[i]; /* was return a[n-1] */ 15 } 16 /* else case becomes prefix_max = prefix_max, a noop */ 17 } 18 19 /* at the end we have to return a value for real */ 20 return prefix_max; 21 }\n2. Classifying algorithms by structure\nAlgorithm design often requires both creativity and problem-specific knowledge, but there are certain common techniques that appear over and over again. The following classification is adapted from LevitinBook :\nBruteForce : Try all possible solutions until you find the right one.\nDivideAndConquer : Split the problem into two or more subproblems, solve the subproblems recursively, and then combine the solutions.\nDecreaseAndConquer : Reduce the problem to a single smaller problem, solve that problem recursively, and then use that solution to solve the original problem.\nTransformAndConquer : Either (a) transform the input to a form that makes the problem easy to solve, or (b) transform the input into the input to another problem whose solution solves the original problem.\nUseSpace : Solve the problem using some auxiliary data structure.\nDynamicProgramming : Construct a table of solutions for increasingly large subproblems, where each new entry in the table is computed using previous entries in the table.\nGreedyMethod : Run through your problem one step at a time, keeping track of the single best solution at each step. Hope sincerely that this will not lead you to make a seemingly-good choice early with bad consequences later.\nSome of these approaches work better than others---it is the role of AlgorithmAnalysis (and experiments with real computers) to figure out which are likely to be both correct and efficient in practice. But having all of them in your toolbox lets you try different possibilities for a given problem.\n3. Example: Finding the maximum\nThough this classification is not completely well-defined, and is a bit arbitrary for some algorithms, it does provide a useful list of things to try in solving a problem. Here are some examples of applying the different approaches to a simple problem, the problem of finding the maximum of an array of integers.\nBruteForce : For index i, test if A[i] is greater than or equal to every element in the array. When you find such an A[i], return it. For this algorithm, T(n) = n*Theta(n) = Theta(n2) if implemented in the most natural way.\nDivideAndConquer : If A has only one element, return it. Otherwise, let m1 be the maximum of A[1]..A[n/2]. Let m2 be the maximum of A[n/2+1]..A[n]. Return the larger of m1 and m2. The running time is given by T(n) = 2T(n/2) + Theta(1) = Theta(n).\nDecreaseAndConquer : If A has only one element, return it. Otherwise, let m be the maximum of A[2]..A[n]. Return the larger of A[0] and m. Now the running time is given by T(n) = T(n-1) + Theta(1) = Sigmai=1 to n Theta(1) = Theta(n).\nTransformAndConquer : Sort the array, then return A[n]. Using an optimal comparison-based sort, this takes Theta(n log n) + Theta(1) = Theta(n log n) time. The advantage of this approach is that you probably don't have to code up the sort.\nUseSpace : Insert all elements into a balanced binary search tree, then return the rightmost element. Cost is Theta(n log n) to do n insertions, plus Theta(log n) to find the rightmost element, for a total of Theta(n log n). Sorting is equivalent and probably easier.\nDynamicProgramming : Create an auxiliary array B with indices 1 to n. Set B[1] = A[1]. As i goes from 2 to n, set B[i] to the larger of B[i-1] and A[i]. Return B[n]. Cost: Theta(n).\nGreedyMethod : Let max = A[1]. For each element A[i] in A[2..n], if A[i] > max, set max = A[i]. Return the final value of max. Cost: Theta(n); this algorithm is pretty much identical to the previous one.\n4. Example: Sorting\nThe sorting problem asks, given an array of n elements in arbitrary order, for an array containing the same n elements in nondecreasing order, i.e. with A[i] <= A[i+1] for all i. We can apply each of the techniques above to this problem and get a sorting algorithm (though some are not very good).\nBruteForce : For each of the n! permutations of the input, test if it is sorted by checking A[i] <= A[i+1] for all i. Cost if implemented naively: n!*Theta(n) = Theta(n*n!). This algorithm is known as deterministic monkeysort or deterministic bogosort. It also has a randomized variant, where the careful generation of all n! permutations is replaced by shuffling. The randomized variant is easier to code and runs about a factor of two faster than the deterministic variant, but does not guarantee termination if the shuffling is consistently unlucky.\nDivideAndConquer : Sort A[1..floor(n/2)] and A[floor(n/2+1)..n] separately, then merge the results (which takes Theta(n) time and Theta(n) additional space if implemented in the most straightforward way). Cost: T(n)=2T(n/2)+Theta(n)=Theta(n log n) by the Master Theorem. This algorithm is known as MergeSort , and is one of the fastest general-purpose sorting algorithms. The merge can be avoided by carefully splitting the array into elements less than and elements greater than some pivot, then sorting the two resulting piles; this gives QuickSort . The performance of QuickSort is often faster than MergeSort in practice, but its worst-case performance (when the pivot is chosen badly) is just as bad as the result of:\nDecreaseAndConquer : Remove A[n], sort the remainder, then insert A[n] in the appropriate place. This algorithm is called InsertionSort . The final insertion step requires finding the right place (which can be done fairly quickly if one is clever) but then moving up to n-1 elements to make room for A[n]. Total cost is given by T(n) = T(n-1) + Theta(n) = T(n2).\nTransformAndConquer : I'm not aware of any good general TransformAndConquer approach to sorting (there are some bad ones), but in some cases one can transform seemingly general sorting problem (e.g. sorting strings) into specialized sorting problems that permit faster solutions (e.g. sorting small integers).\nUseSpace : Insert the elements into a balanced binary search tree, then read them out from left to right. Another version: insert them into a heap. Both take Theta(n log n) time, but are more complicated to implement than MergeSort or QuickSort unless you have BST or heap code lying around already.\nDynamicProgramming : ( InsertionSort may be seen as an example of this.)\nGreedyMethod : Find the smallest element, mark it as used, and output it. Repeat until no elements are left. The result is SelectionSort , which runs in a respectable but suboptimal Theta(n2) time.\nAlgorithmDesignTechniques (last edited 2007-12-25 23:42:02 by localhost)\nImmutable Page\n"}, {"score": 449.24243, "uuid": "f29d6c6f-8921-5449-9601-65fd94d37095", "index": "cw12", "trec_id": "clueweb12-1607wb-18-00274", "target_hostname": "cr.openjdk.java.net", "target_uri": "http://cr.openjdk.java.net/~alanb/6880672/webrev.00/src/share/classes/java/util/Arrays.java.sdiff.html", "page_rank": 1.1900824e-09, "spam_rank": 77, "title": "jdk Sdiff src/share/classes/java/util", "snippet": "(For 43 * example, the <em>algorithm</em> used by &lt;tt&gt;<em>sort</em>(Object[])&lt;&#x2F;tt&gt; does not have to be 64 * <em>Sorts</em> the specified array of longs into ascending numerical order. 65 * The sorting <em>algorithm</em> <em>is</em> a tuned <em>quicksort</em>, adapted from Jon Bentley and M.", "explanation": null, "document": "Print this page\n33  * that allows arrays to be viewed as lists.\n  34  *\n  35  * <p>The methods in this class all throw a <tt>NullPointerException</tt> if\n  36  * the specified array reference is null, except where noted.\n  37  *\n  38  * <p>The documentation for the methods contained in this class includes\n  39  * briefs description of the <i>implementations</i>.  Such descriptions should\n  40  * be regarded as <i>implementation notes</i>, rather than parts of the\n  41  * <i>specification</i>.  Implementors should feel free to substitute other\n  42  * algorithms, so long as the specification itself is adhered to.  (For\n  43  * example, the algorithm used by <tt>sort(Object[])</tt> does not have to be\n  44  * a mergesort, but it does have to be <i>stable</i>.)\n  45  *\n  46  * <p>This class is a member of the\n  47  * <a href=\"{@docRoot}/../technotes/guides/collections/index.html\">\n  48  * Java Collections Framework</a>.\n  49  *\n  50  * @author  Josh Bloch\n  51  * @author  Neal Gafter\n  52  * @author  John Rose\n\n  53  * @since   1.2\n  54  */\n  55 \n  56 public class Arrays {\n  57     // Suppresses default constructor, ensuring non-instantiability.\n  58     private Arrays() {\n  59     }\n  60 \n  61     // Sorting\n  62 \n  63     /**\n  64      * Sorts the specified array of longs into ascending numerical order. 65      * The sorting algorithm is a tuned quicksort, adapted from Jon 66      * L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", 67      * Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 68      * 1993).  This algorithm offers n*log(n) performance on many data sets 69      * that cause other quicksorts to degrade to quadratic performance. 70      *\n\n\n\n\n\n  71      * @param a the array to be sorted\n  72      */\n  73     public static void sort(long[] a) { 74         sort1(a, 0, a.length); 75     }\n  76 \n  77     /**\n  78      * Sorts the specified range of the specified array of longs into\n  79      * ascending numerical order.  The range to be sorted extends from index 80      * <tt>fromIndex</tt>, inclusive, to index <tt>toIndex</tt>, exclusive. 81      * (If <tt>fromIndex==toIndex</tt>, the range to be sorted is empty.) 82      * 83      * <p>The sorting algorithm is a tuned quicksort, adapted from Jon 84      * L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", 85      * Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 86      * 1993).  This algorithm offers n*log(n) performance on many data sets 87      * that cause other quicksorts to degrade to quadratic performance. 88      *\n  89      * @param a the array to be sorted 90      * @param fromIndex the index of the first element (inclusive) to be 91      *        sorted 92      * @param toIndex the index of the last element (exclusive) to be sorted\n  93      * @throws IllegalArgumentException if <tt>fromIndex &gt; toIndex</tt> 94      * @throws ArrayIndexOutOfBoundsException if <tt>fromIndex &lt; 0</tt> or 95      * <tt>toIndex &gt; a.length</tt> 96      */\n  97     public static void sort(long[] a, int fromIndex, int toIndex) {\n  98         rangeCheck(a.length, fromIndex, toIndex); 99         sort1(a, fromIndex, toIndex-fromIndex); 100     }\n 101 \n 102     /**\n 103      * Sorts the specified array of ints into ascending numerical order. 104      * The sorting algorithm is a tuned quicksort, adapted from Jon 105      * L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", 106      * Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 107      * 1993).  This algorithm offers n*log(n) performance on many data sets 108      * that cause other quicksorts to degrade to quadratic performance. 109      *\n\n\n\n\n\n 110      * @param a the array to be sorted\n 111      */\n 112     public static void sort(int[] a) { 113         sort1(a, 0, a.length); 114     }\n 115 \n 116     /**\n 117      * Sorts the specified range of the specified array of ints into\n 118      * ascending numerical order.  The range to be sorted extends from index 119      * <tt>fromIndex</tt>, inclusive, to index <tt>toIndex</tt>, exclusive. 120      * (If <tt>fromIndex==toIndex</tt>, the range to be sorted is empty.)<p> 121      * 122      * The sorting algorithm is a tuned quicksort, adapted from Jon 123      * L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", 124      * Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 125      * 1993).  This algorithm offers n*log(n) performance on many data sets 126      * that cause other quicksorts to degrade to quadratic performance. 127      *\n 128      * @param a the array to be sorted 129      * @param fromIndex the index of the first element (inclusive) to be 130      *        sorted 131      * @param toIndex the index of the last element (exclusive) to be sorted\n 132      * @throws IllegalArgumentException if <tt>fromIndex &gt; toIndex</tt> 133      * @throws ArrayIndexOutOfBoundsException if <tt>fromIndex &lt; 0</tt> or 134      *         <tt>toIndex &gt; a.length</tt> 135      */\n 136     public static void sort(int[] a, int fromIndex, int toIndex) {\n 137         rangeCheck(a.length, fromIndex, toIndex); 138         sort1(a, fromIndex, toIndex-fromIndex); 139     }\n 140 \n 141     /**\n 142      * Sorts the specified array of shorts into ascending numerical order. 143      * The sorting algorithm is a tuned quicksort, adapted from Jon 144      * L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", 145      * Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 146      * 1993).  This algorithm offers n*log(n) performance on many data sets 147      * that cause other quicksorts to degrade to quadratic performance. 148      *\n\n\n\n\n\n 149      * @param a the array to be sorted\n 150      */\n 151     public static void sort(short[] a) { 152         sort1(a, 0, a.length); 153     }\n 154 \n 155     /**\n 156      * Sorts the specified range of the specified array of shorts into\n 157      * ascending numerical order.  The range to be sorted extends from index 158      * <tt>fromIndex</tt>, inclusive, to index <tt>toIndex</tt>, exclusive. 159      * (If <tt>fromIndex==toIndex</tt>, the range to be sorted is empty.)<p> 160      * 161      * The sorting algorithm is a tuned quicksort, adapted from Jon 162      * L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", 163      * Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 164      * 1993).  This algorithm offers n*log(n) performance on many data sets 165      * that cause other quicksorts to degrade to quadratic performance. 166      *\n 167      * @param a the array to be sorted 168      * @param fromIndex the index of the first element (inclusive) to be 169      *        sorted 170      * @param toIndex the index of the last element (exclusive) to be sorted\n 171      * @throws IllegalArgumentException if <tt>fromIndex &gt; toIndex</tt> 172      * @throws ArrayIndexOutOfBoundsException if <tt>fromIndex &lt; 0</tt> or 173      *         <tt>toIndex &gt; a.length</tt> 174      */\n 175     public static void sort(short[] a, int fromIndex, int toIndex) {\n 176         rangeCheck(a.length, fromIndex, toIndex); 177         sort1(a, fromIndex, toIndex-fromIndex); 178     }\n 179 \n 180     /**\n 181      * Sorts the specified array of chars into ascending numerical order. 182      * The sorting algorithm is a tuned quicksort, adapted from Jon 183      * L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", 184      * Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 185      * 1993).  This algorithm offers n*log(n) performance on many data sets 186      * that cause other quicksorts to degrade to quadratic performance. 187      *\n\n\n\n\n\n 188      * @param a the array to be sorted\n 189      */\n 190     public static void sort(char[] a) { 191         sort1(a, 0, a.length); 192     }\n 193 \n 194     /**\n 195      * Sorts the specified range of the specified array of chars into\n 196      * ascending numerical order.  The range to be sorted extends from index 197      * <tt>fromIndex</tt>, inclusive, to index <tt>toIndex</tt>, exclusive. 198      * (If <tt>fromIndex==toIndex</tt>, the range to be sorted is empty.)<p> 199      * 200      * The sorting algorithm is a tuned quicksort, adapted from Jon 201      * L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", 202      * Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 203      * 1993).  This algorithm offers n*log(n) performance on many data sets 204      * that cause other quicksorts to degrade to quadratic performance. 205      *\n 206      * @param a the array to be sorted 207      * @param fromIndex the index of the first element (inclusive) to be 208      *        sorted 209      * @param toIndex the index of the last element (exclusive) to be sorted\n 210      * @throws IllegalArgumentException if <tt>fromIndex &gt; toIndex</tt> 211      * @throws ArrayIndexOutOfBoundsException if <tt>fromIndex &lt; 0</tt> or 212      *         <tt>toIndex &gt; a.length</tt> 213      */\n 214     public static void sort(char[] a, int fromIndex, int toIndex) {\n 215         rangeCheck(a.length, fromIndex, toIndex); 216         sort1(a, fromIndex, toIndex-fromIndex); 217     }\n 218 \n 219     /**\n 220      * Sorts the specified array of bytes into ascending numerical order. 221      * The sorting algorithm is a tuned quicksort, adapted from Jon 222      * L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", 223      * Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 224      * 1993).  This algorithm offers n*log(n) performance on many data sets 225      * that cause other quicksorts to degrade to quadratic performance. 226      *\n\n\n\n\n\n 227      * @param a the array to be sorted\n 228      */\n 229     public static void sort(byte[] a) { 230         sort1(a, 0, a.length); 231     }\n 232 \n 233     /**\n 234      * Sorts the specified range of the specified array of bytes into\n 235      * ascending numerical order.  The range to be sorted extends from index 236      * <tt>fromIndex</tt>, inclusive, to index <tt>toIndex</tt>, exclusive. 237      * (If <tt>fromIndex==toIndex</tt>, the range to be sorted is empty.)<p> 238      * 239      * The sorting algorithm is a tuned quicksort, adapted from Jon 240      * L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", 241      * Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 242      * 1993).  This algorithm offers n*log(n) performance on many data sets 243      * that cause other quicksorts to degrade to quadratic performance. 244      *\n 245      * @param a the array to be sorted 246      * @param fromIndex the index of the first element (inclusive) to be 247      *        sorted 248      * @param toIndex the index of the last element (exclusive) to be sorted\n 249      * @throws IllegalArgumentException if <tt>fromIndex &gt; toIndex</tt> 250      * @throws ArrayIndexOutOfBoundsException if <tt>fromIndex &lt; 0</tt> or 251      *         <tt>toIndex &gt; a.length</tt> 252      */\n 253     public static void sort(byte[] a, int fromIndex, int toIndex) {\n 254         rangeCheck(a.length, fromIndex, toIndex); 255         sort1(a, fromIndex, toIndex-fromIndex); 256     }\n 257 \n 258     /**\n 259      * Sorts the specified array of doubles into ascending numerical order. 260      * <p> 261      * The <code>&lt;</code> relation does not provide a total order on 262      * all floating-point values; although they are distinct numbers 263      * <code>-0.0 == 0.0</code> is <code>true</code> and a NaN value 264      * compares neither less than, greater than, nor equal to any\n 265      * floating-point value, even itself.  To allow the sort to\n 266      * proceed, instead of using the <code>&lt;</code> relation to\n 267      * determine ascending numerical order, this method uses the total\n 268      * order imposed by {@link Double#compareTo}.  This ordering\n 269      * differs from the <code>&lt;</code> relation in that 270      * <code>-0.0</code> is treated as less than <code>0.0</code> and 271      * NaN is considered greater than any other floating-point value.\n 272      * For the purposes of sorting, all NaN values are considered\n 273      * equivalent and equal. 274      * <p> 275      * The sorting algorithm is a tuned quicksort, adapted from Jon 276      * L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", 277      * Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 278      * 1993).  This algorithm offers n*log(n) performance on many data sets 279      * that cause other quicksorts to degrade to quadratic performance. 280      *\n\n\n\n\n\n 281      * @param a the array to be sorted\n 282      */\n 283     public static void sort(double[] a) { 284         sort2(a, 0, a.length); 285     }\n 286 \n 287     /**\n 288      * Sorts the specified range of the specified array of doubles into\n 289      * ascending numerical order.  The range to be sorted extends from index 290      * <tt>fromIndex</tt>, inclusive, to index <tt>toIndex</tt>, exclusive. 291      * (If <tt>fromIndex==toIndex</tt>, the range to be sorted is empty.) 292      * <p> 293      * The <code>&lt;</code> relation does not provide a total order on 294      * all floating-point values; although they are distinct numbers 295      * <code>-0.0 == 0.0</code> is <code>true</code> and a NaN value 296      * compares neither less than, greater than, nor equal to any\n 297      * floating-point value, even itself.  To allow the sort to\n 298      * proceed, instead of using the <code>&lt;</code> relation to\n 299      * determine ascending numerical order, this method uses the total\n 300      * order imposed by {@link Double#compareTo}.  This ordering\n 301      * differs from the <code>&lt;</code> relation in that 302      * <code>-0.0</code> is treated as less than <code>0.0</code> and 303      * NaN is considered greater than any other floating-point value.\n 304      * For the purposes of sorting, all NaN values are considered\n 305      * equivalent and equal. 306      * <p> 307      * The sorting algorithm is a tuned quicksort, adapted from Jon 308      * L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", 309      * Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 310      * 1993).  This algorithm offers n*log(n) performance on many data sets 311      * that cause other quicksorts to degrade to quadratic performance. 312      *\n\n\n\n\n\n 313      * @param a the array to be sorted 314      * @param fromIndex the index of the first element (inclusive) to be 315      *        sorted 316      * @param toIndex the index of the last element (exclusive) to be sorted\n 317      * @throws IllegalArgumentException if <tt>fromIndex &gt; toIndex</tt> 318      * @throws ArrayIndexOutOfBoundsException if <tt>fromIndex &lt; 0</tt> or 319      *         <tt>toIndex &gt; a.length</tt> 320      */\n 321     public static void sort(double[] a, int fromIndex, int toIndex) {\n 322         rangeCheck(a.length, fromIndex, toIndex); 323         sort2(a, fromIndex, toIndex); 324     }\n 325 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 326     /**\n 327      * Sorts the specified array of floats into ascending numerical order. 328      * <p> 329      * The <code>&lt;</code> relation does not provide a total order on 330      * all floating-point values; although they are distinct numbers\n 331      * <code>-0.0f == 0.0f</code> is <code>true</code> and a NaN value\n 332      * compares neither less than, greater than, nor equal to any\n 333      * floating-point value, even itself.  To allow the sort to\n 334      * proceed, instead of using the <code>&lt;</code> relation to\n 335      * determine ascending numerical order, this method uses the total\n 336      * order imposed by {@link Float#compareTo}.  This ordering\n 337      * differs from the <code>&lt;</code> relation in that\n 338      * <code>-0.0f</code> is treated as less than <code>0.0f</code> and\n 339      * NaN is considered greater than any other floating-point value.\n 340      * For the purposes of sorting, all NaN values are considered\n 341      * equivalent and equal. 342      * <p> 343      * The sorting algorithm is a tuned quicksort, adapted from Jon 344      * L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", 345      * Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 346      * 1993).  This algorithm offers n*log(n) performance on many data sets 347      * that cause other quicksorts to degrade to quadratic performance. 348      *\n\n\n\n\n\n 349      * @param a the array to be sorted\n 350      */\n 351     public static void sort(float[] a) { 352         sort2(a, 0, a.length); 353     }\n 354 \n 355     /**\n 356      * Sorts the specified range of the specified array of floats into\n 357      * ascending numerical order.  The range to be sorted extends from index 358      * <tt>fromIndex</tt>, inclusive, to index <tt>toIndex</tt>, exclusive. 359      * (If <tt>fromIndex==toIndex</tt>, the range to be sorted is empty.) 360      * <p> 361      * The <code>&lt;</code> relation does not provide a total order on 362      * all floating-point values; although they are distinct numbers\n 363      * <code>-0.0f == 0.0f</code> is <code>true</code> and a NaN value\n 364      * compares neither less than, greater than, nor equal to any\n 365      * floating-point value, even itself.  To allow the sort to\n 366      * proceed, instead of using the <code>&lt;</code> relation to\n 367      * determine ascending numerical order, this method uses the total\n 368      * order imposed by {@link Float#compareTo}.  This ordering\n 369      * differs from the <code>&lt;</code> relation in that\n 370      * <code>-0.0f</code> is treated as less than <code>0.0f</code> and\n 371      * NaN is considered greater than any other floating-point value.\n 372      * For the purposes of sorting, all NaN values are considered\n 373      * equivalent and equal. 374      * <p> 375      * The sorting algorithm is a tuned quicksort, adapted from Jon 376      * L. Bentley and M. Douglas McIlroy's \"Engineering a Sort Function\", 377      * Software-Practice and Experience, Vol. 23(11) P. 1249-1265 (November 378      * 1993).  This algorithm offers n*log(n) performance on many data sets 379      * that cause other quicksorts to degrade to quadratic performance. 380      *\n\n\n\n\n\n 381      * @param a the array to be sorted 382      * @param fromIndex the index of the first element (inclusive) to be 383      *        sorted 384      * @param toIndex the index of the last element (exclusive) to be sorted\n 385      * @throws IllegalArgumentException if <tt>fromIndex &gt; toIndex</tt> 386      * @throws ArrayIndexOutOfBoundsException if <tt>fromIndex &lt; 0</tt> or 387      *         <tt>toIndex &gt; a.length</tt> 388      */\n 389     public static void sort(float[] a, int fromIndex, int toIndex) {\n 390         rangeCheck(a.length, fromIndex, toIndex); 391         sort2(a, fromIndex, toIndex); 392     }\n 393 394     private static void sort2(double a[], int fromIndex, int toIndex) { 395         final long NEG_ZERO_BITS = Double.doubleToLongBits(-0.0d); 396         /* 397          * The sort is done in three phases to avoid the expense of using 398          * NaN and -0.0 aware comparisons during the main sort. 399          */ 400 401         /* 402          * Preprocessing phase:  Move any NaN's to end of array, count the 403          * number of -0.0's, and turn them into 0.0's. 404          */ 405         int numNegZeros = 0; 406         int i = fromIndex, n = toIndex; 407         while(i < n) { 408             if (a[i] != a[i]) { 409                 swap(a, i, --n); 410             } else { 411                 if (a[i]==0 && Double.doubleToLongBits(a[i])==NEG_ZERO_BITS) { 412                     a[i] = 0.0d; 413                     numNegZeros++; 414                 } 415                 i++; 416             } 417         } 418 419         // Main sort phase: quicksort everything but the NaN's 420         sort1(a, fromIndex, n-fromIndex); 421 422         // Postprocessing phase: change 0.0's to -0.0's as required 423         if (numNegZeros != 0) { 424             int j = binarySearch0(a, fromIndex, n, 0.0d); // posn of ANY zero 425             do { 426                 j--; 427             } while (j>=fromIndex && a[j]==0.0d); 428 429             // j is now one less than the index of the FIRST zero 430             for (int k=0; k<numNegZeros; k++) 431                 a[++j] = -0.0d; 432         } 433     } 434 435 436     private static void sort2(float a[], int fromIndex, int toIndex) { 437         final int NEG_ZERO_BITS = Float.floatToIntBits(-0.0f);\n 438         /*\n 439          * The sort is done in three phases to avoid the expense of using 440          * NaN and -0.0 aware comparisons during the main sort. 441          */ 442 443         /* 444          * Preprocessing phase:  Move any NaN's to end of array, count the 445          * number of -0.0's, and turn them into 0.0's. 446          */ 447         int numNegZeros = 0; 448         int i = fromIndex, n = toIndex; 449         while(i < n) { 450             if (a[i] != a[i]) {\n 451                 swap(a, i, --n); 452             } else { 453                 if (a[i]==0 && Float.floatToIntBits(a[i])==NEG_ZERO_BITS) { 454                     a[i] = 0.0f;\n 455                     numNegZeros++;\n 456                 }\n 457                 i++;\n 458             }\n 459         } 460 461         // Main sort phase: quicksort everything but the NaN's 462         sort1(a, fromIndex, n-fromIndex); 463 464         // Postprocessing phase: change 0.0's to -0.0's as required 465         if (numNegZeros != 0) { 466             int j = binarySearch0(a, fromIndex, n, 0.0f); // posn of ANY zero 467             do {\n 468                 j--; 469             } while (j>=fromIndex && a[j]==0.0f); 470 \n 471             // j is now one less than the index of the FIRST zero 472             for (int k=0; k<numNegZeros; k++) 473                 a[++j] = -0.0f;\n 474         }\n 475     }\n\n 476 477 478     /*\n 479      * The code for each of the seven primitive types is largely identical.\n 480      * C'est la vie.\n 481      */\n 482 \n 483     /**\n 484      * Sorts the specified sub-array of longs into ascending order.\n 485      */ 486     private static void sort1(long x[], int off, int len) { 487         // Insertion sort on smallest arrays 488         if (len < 7) { 489             for (int i=off; i<len+off; i++) 490                 for (int j=i; j>off && x[j-1]>x[j]; j--) 491                     swap(x, j, j-1); 492             return;\n 493         }\n\n 494 495         // Choose a partition element, v 496         int m = off + (len >> 1);       // Small arrays, middle element 497         if (len > 7) { 498             int l = off; 499             int n = off + len - 1; 500             if (len > 40) {        // Big arrays, pseudomedian of 9 501                 int s = len/8; 502                 l = med3(x, l,     l+s, l+2*s); 503                 m = med3(x, m-s,   m,   m+s); 504                 n = med3(x, n-2*s, n-s, n); 505             } 506             m = med3(x, l, m, n); // Mid-size, med of 3 507         } 508         long v = x[m]; 509 510         // Establish Invariant: v* (<v)* (>v)* v* 511         int a = off, b = a, c = off + len - 1, d = c; 512         while(true) { 513             while (b <= c && x[b] <= v) { 514                 if (x[b] == v) 515                     swap(x, a++, b); 516                 b++; 517             } 518             while (c >= b && x[c] >= v) { 519                 if (x[c] == v) 520                     swap(x, c, d--); 521                 c--; 522             } 523             if (b > c) 524                 break; 525             swap(x, b++, c--); 526         }\n\n\n\n\n 527 528         // Swap partition elements back to middle 529         int s, n = off + len; 530         s = Math.min(a-off, b-a  );  vecswap(x, off, b-s, s); 531         s = Math.min(d-c,   n-d-1);  vecswap(x, b,   n-s, s); 532 533         // Recursively sort non-partition-elements 534         if ((s = b-a) > 1) 535             sort1(x, off, s); 536         if ((s = d-c) > 1) 537             sort1(x, n-s, s); 538     }\n\n\n 539 540     /** 541      * Swaps x[a] with x[b]. 542      */ 543     private static void swap(long x[], int a, int b) { 544         long t = x[a]; 545         x[a] = x[b]; 546         x[b] = t; 547     }\n\n\n 548 549     /** 550      * Swaps x[a .. (a+n-1)] with x[b .. (b+n-1)]. 551      */ 552     private static void vecswap(long x[], int a, int b, int n) { 553         for (int i=0; i<n; i++, a++, b++) 554             swap(x, a, b); 555     }\n\n\n\n\n\n\n\n\n 556 \n 557     /** 558      * Returns the index of the median of the three indexed longs. 559      */ 560     private static int med3(long x[], int a, int b, int c) { 561         return (x[a] < x[b] ? 562                 (x[b] < x[c] ? b : x[a] < x[c] ? c : a) : 563                 (x[b] > x[c] ? b : x[a] > x[c] ? c : a)); 564     }\n 565 \n 566     /** 567      * Sorts the specified sub-array of integers into ascending order. 568      */ 569     private static void sort1(int x[], int off, int len) { 570         // Insertion sort on smallest arrays 571         if (len < 7) { 572             for (int i=off; i<len+off; i++) 573                 for (int j=i; j>off && x[j-1]>x[j]; j--) 574                     swap(x, j, j-1); 575             return;\n 576         }\n\n 577 578         // Choose a partition element, v 579         int m = off + (len >> 1);       // Small arrays, middle element 580         if (len > 7) { 581             int l = off; 582             int n = off + len - 1; 583             if (len > 40) {        // Big arrays, pseudomedian of 9 584                 int s = len/8; 585                 l = med3(x, l,     l+s, l+2*s); 586                 m = med3(x, m-s,   m,   m+s); 587                 n = med3(x, n-2*s, n-s, n); 588             } 589             m = med3(x, l, m, n); // Mid-size, med of 3 590         } 591         int v = x[m]; 592 593         // Establish Invariant: v* (<v)* (>v)* v* 594         int a = off, b = a, c = off + len - 1, d = c; 595         while(true) { 596             while (b <= c && x[b] <= v) { 597                 if (x[b] == v) 598                     swap(x, a++, b); 599                 b++; 600             } 601             while (c >= b && x[c] >= v) { 602                 if (x[c] == v) 603                     swap(x, c, d--); 604                 c--; 605             } 606             if (b > c) 607                 break; 608             swap(x, b++, c--); 609         }\n\n\n\n\n 610 611         // Swap partition elements back to middle 612         int s, n = off + len; 613         s = Math.min(a-off, b-a  );  vecswap(x, off, b-s, s); 614         s = Math.min(d-c,   n-d-1);  vecswap(x, b,   n-s, s); 615 616         // Recursively sort non-partition-elements 617         if ((s = b-a) > 1) 618             sort1(x, off, s); 619         if ((s = d-c) > 1) 620             sort1(x, n-s, s); 621     }\n\n\n 622 623     /** 624      * Swaps x[a] with x[b]. 625      */ 626     private static void swap(int x[], int a, int b) { 627         int t = x[a]; 628         x[a] = x[b]; 629         x[b] = t; 630     }\n\n\n 631 632     /** 633      * Swaps x[a .. (a+n-1)] with x[b .. (b+n-1)]. 634      */ 635     private static void vecswap(int x[], int a, int b, int n) { 636         for (int i=0; i<n; i++, a++, b++) 637             swap(x, a, b); 638     }\n\n\n\n\n\n\n\n\n 639 \n 640     /** 641      * Returns the index of the median of the three indexed integers. 642      */ 643     private static int med3(int x[], int a, int b, int c) { 644         return (x[a] < x[b] ? 645                 (x[b] < x[c] ? b : x[a] < x[c] ? c : a) : 646                 (x[b] > x[c] ? b : x[a] > x[c] ? c : a)); 647     }\n 648 \n 649     /**\n 650      * Sorts the specified sub-array of shorts into ascending order.\n 651      */ 652     private static void sort1(short x[], int off, int len) { 653         // Insertion sort on smallest arrays 654         if (len < 7) { 655             for (int i=off; i<len+off; i++) 656                 for (int j=i; j>off && x[j-1]>x[j]; j--) 657                     swap(x, j, j-1); 658             return;\n 659         }\n\n 660 661         // Choose a partition element, v 662         int m = off + (len >> 1);       // Small arrays, middle element 663         if (len > 7) { 664             int l = off; 665             int n = off + len - 1; 666             if (len > 40) {        // Big arrays, pseudomedian of 9 667                 int s = len/8; 668                 l = med3(x, l,     l+s, l+2*s); 669                 m = med3(x, m-s,   m,   m+s); 670                 n = med3(x, n-2*s, n-s, n); 671             } 672             m = med3(x, l, m, n); // Mid-size, med of 3 673         } 674         short v = x[m]; 675 676         // Establish Invariant: v* (<v)* (>v)* v* 677         int a = off, b = a, c = off + len - 1, d = c; 678         while(true) { 679             while (b <= c && x[b] <= v) { 680                 if (x[b] == v) 681                     swap(x, a++, b); 682                 b++; 683             } 684             while (c >= b && x[c] >= v) { 685                 if (x[c] == v) 686                     swap(x, c, d--); 687                 c--; 688             } 689             if (b > c) 690                 break; 691             swap(x, b++, c--); 692         }\n\n\n\n\n 693 694         // Swap partition elements back to middle 695         int s, n = off + len; 696         s = Math.min(a-off, b-a  );  vecswap(x, off, b-s, s); 697         s = Math.min(d-c,   n-d-1);  vecswap(x, b,   n-s, s); 698 699         // Recursively sort non-partition-elements 700         if ((s = b-a) > 1) 701             sort1(x, off, s); 702         if ((s = d-c) > 1) 703             sort1(x, n-s, s); 704     }\n\n\n 705 706     /** 707      * Swaps x[a] with x[b]. 708      */ 709     private static void swap(short x[], int a, int b) { 710         short t = x[a]; 711         x[a] = x[b]; 712         x[b] = t; 713     }\n\n\n 714 715     /** 716      * Swaps x[a .. (a+n-1)] with x[b .. (b+n-1)]. 717      */ 718     private static void vecswap(short x[], int a, int b, int n) { 719         for (int i=0; i<n; i++, a++, b++) 720             swap(x, a, b); 721     }\n\n\n\n\n\n\n\n\n 722 \n 723     /** 724      * Returns the index of the median of the three indexed shorts. 725      */ 726     private static int med3(short x[], int a, int b, int c) { 727         return (x[a] < x[b] ? 728                 (x[b] < x[c] ? b : x[a] < x[c] ? c : a) : 729                 (x[b] > x[c] ? b : x[a] > x[c] ? c : a)); 730     }\n 731 732 733     /**\n 734      * Sorts the specified sub-array of chars into ascending order.\n 735      */ 736     private static void sort1(char x[], int off, int len) { 737         // Insertion sort on smallest arrays 738         if (len < 7) { 739             for (int i=off; i<len+off; i++) 740                 for (int j=i; j>off && x[j-1]>x[j]; j--) 741                     swap(x, j, j-1); 742             return;\n 743         }\n\n 744 745         // Choose a partition element, v 746         int m = off + (len >> 1);       // Small arrays, middle element 747         if (len > 7) { 748             int l = off; 749             int n = off + len - 1; 750             if (len > 40) {        // Big arrays, pseudomedian of 9 751                 int s = len/8; 752                 l = med3(x, l,     l+s, l+2*s); 753                 m = med3(x, m-s,   m,   m+s); 754                 n = med3(x, n-2*s, n-s, n); 755             } 756             m = med3(x, l, m, n); // Mid-size, med of 3 757         } 758         char v = x[m]; 759 760         // Establish Invariant: v* (<v)* (>v)* v* 761         int a = off, b = a, c = off + len - 1, d = c; 762         while(true) { 763             while (b <= c && x[b] <= v) { 764                 if (x[b] == v) 765                     swap(x, a++, b); 766                 b++; 767             } 768             while (c >= b && x[c] >= v) { 769                 if (x[c] == v) 770                     swap(x, c, d--); 771                 c--; 772             } 773             if (b > c) 774                 break; 775             swap(x, b++, c--); 776         }\n\n\n\n\n 777 778         // Swap partition elements back to middle 779         int s, n = off + len; 780         s = Math.min(a-off, b-a  );  vecswap(x, off, b-s, s); 781         s = Math.min(d-c,   n-d-1);  vecswap(x, b,   n-s, s); 782 783         // Recursively sort non-partition-elements 784         if ((s = b-a) > 1) 785             sort1(x, off, s); 786         if ((s = d-c) > 1) 787             sort1(x, n-s, s); 788     }\n\n\n 789 790     /** 791      * Swaps x[a] with x[b]. 792      */ 793     private static void swap(char x[], int a, int b) { 794         char t = x[a]; 795         x[a] = x[b]; 796         x[b] = t; 797     }\n\n\n 798 799     /** 800      * Swaps x[a .. (a+n-1)] with x[b .. (b+n-1)]. 801      */ 802     private static void vecswap(char x[], int a, int b, int n) { 803         for (int i=0; i<n; i++, a++, b++) 804             swap(x, a, b); 805     }\n\n\n\n\n\n\n\n\n 806 \n 807     /** 808      * Returns the index of the median of the three indexed chars. 809      */ 810     private static int med3(char x[], int a, int b, int c) { 811         return (x[a] < x[b] ? 812                 (x[b] < x[c] ? b : x[a] < x[c] ? c : a) : 813                 (x[b] > x[c] ? b : x[a] > x[c] ? c : a)); 814     }\n 815 816 817     /**\n 818      * Sorts the specified sub-array of bytes into ascending order.\n 819      */ 820     private static void sort1(byte x[], int off, int len) { 821         // Insertion sort on smallest arrays 822         if (len < 7) { 823             for (int i=off; i<len+off; i++) 824                 for (int j=i; j>off && x[j-1]>x[j]; j--) 825                     swap(x, j, j-1); 826             return;\n 827         }\n\n 828 829         // Choose a partition element, v 830         int m = off + (len >> 1);       // Small arrays, middle element 831         if (len > 7) { 832             int l = off; 833             int n = off + len - 1; 834             if (len > 40) {        // Big arrays, pseudomedian of 9 835                 int s = len/8; 836                 l = med3(x, l,     l+s, l+2*s); 837                 m = med3(x, m-s,   m,   m+s); 838                 n = med3(x, n-2*s, n-s, n); 839             } 840             m = med3(x, l, m, n); // Mid-size, med of 3 841         } 842         byte v = x[m]; 843 844         // Establish Invariant: v* (<v)* (>v)* v* 845         int a = off, b = a, c = off + len - 1, d = c; 846         while(true) { 847             while (b <= c && x[b] <= v) { 848                 if (x[b] == v) 849                     swap(x, a++, b); 850                 b++; 851             } 852             while (c >= b && x[c] >= v) { 853                 if (x[c] == v) 854                     swap(x, c, d--); 855                 c--; 856             } 857             if (b > c) 858                 break; 859             swap(x, b++, c--); 860         }\n\n\n\n\n 861 862         // Swap partition elements back to middle 863         int s, n = off + len; 864         s = Math.min(a-off, b-a  );  vecswap(x, off, b-s, s); 865         s = Math.min(d-c,   n-d-1);  vecswap(x, b,   n-s, s); 866 867         // Recursively sort non-partition-elements 868         if ((s = b-a) > 1) 869             sort1(x, off, s); 870         if ((s = d-c) > 1) 871             sort1(x, n-s, s); 872     }\n\n\n 873 874     /** 875      * Swaps x[a] with x[b]. 876      */ 877     private static void swap(byte x[], int a, int b) { 878         byte t = x[a]; 879         x[a] = x[b]; 880         x[b] = t; 881     }\n\n\n 882 883     /** 884      * Swaps x[a .. (a+n-1)] with x[b .. (b+n-1)]. 885      */ 886     private static void vecswap(byte x[], int a, int b, int n) { 887         for (int i=0; i<n; i++, a++, b++) 888             swap(x, a, b); 889     }\n\n\n\n\n\n\n\n\n 890 \n 891     /** 892      * Returns the index of the median of the three indexed bytes. 893      */ 894     private static int med3(byte x[], int a, int b, int c) { 895         return (x[a] < x[b] ? 896                 (x[b] < x[c] ? b : x[a] < x[c] ? c : a) : 897                 (x[b] > x[c] ? b : x[a] > x[c] ? c : a)); 898     }\n 899 900 901     /**\n 902      * Sorts the specified sub-array of doubles into ascending order.\n 903      */ 904     private static void sort1(double x[], int off, int len) { 905         // Insertion sort on smallest arrays 906         if (len < 7) { 907             for (int i=off; i<len+off; i++) 908                 for (int j=i; j>off && x[j-1]>x[j]; j--) 909                     swap(x, j, j-1); 910             return;\n 911         }\n\n 912 913         // Choose a partition element, v 914         int m = off + (len >> 1);       // Small arrays, middle element 915         if (len > 7) { 916             int l = off; 917             int n = off + len - 1; 918             if (len > 40) {        // Big arrays, pseudomedian of 9 919                 int s = len/8; 920                 l = med3(x, l,     l+s, l+2*s); 921                 m = med3(x, m-s,   m,   m+s); 922                 n = med3(x, n-2*s, n-s, n); 923             } 924             m = med3(x, l, m, n); // Mid-size, med of 3 925         } 926         double v = x[m]; 927 928         // Establish Invariant: v* (<v)* (>v)* v* 929         int a = off, b = a, c = off + len - 1, d = c; 930         while(true) { 931             while (b <= c && x[b] <= v) { 932                 if (x[b] == v) 933                     swap(x, a++, b); 934                 b++; 935             } 936             while (c >= b && x[c] >= v) { 937                 if (x[c] == v) 938                     swap(x, c, d--); 939                 c--; 940             } 941             if (b > c) 942                 break; 943             swap(x, b++, c--); 944         }\n\n\n\n\n 945 946         // Swap partition elements back to middle 947         int s, n = off + len; 948         s = Math.min(a-off, b-a  );  vecswap(x, off, b-s, s); 949         s = Math.min(d-c,   n-d-1);  vecswap(x, b,   n-s, s); 950 951         // Recursively sort non-partition-elements 952         if ((s = b-a) > 1) 953             sort1(x, off, s); 954         if ((s = d-c) > 1) 955             sort1(x, n-s, s); 956     }\n\n\n 957 958     /** 959      * Swaps x[a] with x[b]. 960      */ 961     private static void swap(double x[], int a, int b) { 962         double t = x[a]; 963         x[a] = x[b]; 964         x[b] = t; 965     }\n\n\n 966 967     /** 968      * Swaps x[a .. (a+n-1)] with x[b .. (b+n-1)]. 969      */ 970     private static void vecswap(double x[], int a, int b, int n) { 971         for (int i=0; i<n; i++, a++, b++) 972             swap(x, a, b); 973     }\n\n\n\n\n\n\n\n\n 974 \n 975     /** 976      * Returns the index of the median of the three indexed doubles. 977      */ 978     private static int med3(double x[], int a, int b, int c) { 979         return (x[a] < x[b] ? 980                 (x[b] < x[c] ? b : x[a] < x[c] ? c : a) : 981                 (x[b] > x[c] ? b : x[a] > x[c] ? c : a)); 982     }\n 983 984 985     /**\n 986      * Sorts the specified sub-array of floats into ascending order.\n 987      */ 988     private static void sort1(float x[], int off, int len) { 989         // Insertion sort on smallest arrays 990         if (len < 7) { 991             for (int i=off; i<len+off; i++) 992                 for (int j=i; j>off && x[j-1]>x[j]; j--) 993                     swap(x, j, j-1); 994             return;\n 995         }\n\n 996 997         // Choose a partition element, v 998         int m = off + (len >> 1);       // Small arrays, middle element 999         if (len > 7) { 1000             int l = off; 1001             int n = off + len - 1; 1002             if (len > 40) {        // Big arrays, pseudomedian of 9 1003                 int s = len/8; 1004                 l = med3(x, l,     l+s, l+2*s); 1005                 m = med3(x, m-s,   m,   m+s); 1006                 n = med3(x, n-2*s, n-s, n); 1007             } 1008             m = med3(x, l, m, n); // Mid-size, med of 3 1009         } 1010         float v = x[m]; 1011 1012         // Establish Invariant: v* (<v)* (>v)* v* 1013         int a = off, b = a, c = off + len - 1, d = c; 1014         while(true) { 1015             while (b <= c && x[b] <= v) { 1016                 if (x[b] == v) 1017                     swap(x, a++, b); 1018                 b++; 1019             } 1020             while (c >= b && x[c] >= v) { 1021                 if (x[c] == v) 1022                     swap(x, c, d--); 1023                 c--; 1024             } 1025             if (b > c) 1026                 break; 1027             swap(x, b++, c--); 1028         }\n\n\n\n\n1029 1030         // Swap partition elements back to middle 1031         int s, n = off + len; 1032         s = Math.min(a-off, b-a  );  vecswap(x, off, b-s, s); 1033         s = Math.min(d-c,   n-d-1);  vecswap(x, b,   n-s, s); 1034 1035         // Recursively sort non-partition-elements 1036         if ((s = b-a) > 1) 1037             sort1(x, off, s); 1038         if ((s = d-c) > 1) 1039             sort1(x, n-s, s); 1040     }\n\n\n1041 1042     /** 1043      * Swaps x[a] with x[b]. 1044      */ 1045     private static void swap(float x[], int a, int b) { 1046         float t = x[a]; 1047         x[a] = x[b]; 1048         x[b] = t; 1049     }\n\n\n1050 1051     /** 1052      * Swaps x[a .. (a+n-1)] with x[b .. (b+n-1)]. 1053      */ 1054     private static void vecswap(float x[], int a, int b, int n) { 1055         for (int i=0; i<n; i++, a++, b++) 1056             swap(x, a, b); 1057     }\n\n\n\n\n\n\n\n\n1058 \n1059     /** 1060      * Returns the index of the median of the three indexed floats. 1061      */ 1062     private static int med3(float x[], int a, int b, int c) { 1063         return (x[a] < x[b] ? 1064                 (x[b] < x[c] ? b : x[a] < x[c] ? c : a) : 1065                 (x[b] > x[c] ? b : x[a] > x[c] ? c : a)); 1066     }\n1067 \n\n\n1068     /**\n1069      * Old merge sort implementation can be selected (for\n1070      * compatibility with broken comparators) using a system property.\n1071      * Cannot be a static boolean in the enclosing class due to\n1072      * circular dependencies.  To be removed in a future release.\n1073      */\n1074     static final class LegacyMergeSort {\n1075         private static final boolean userRequested =\n1076             java.security.AccessController.doPrivileged(\n1077                 new sun.security.action.GetBooleanAction(\n1078                     \"java.util.Arrays.useLegacyMergeSort\")).booleanValue();\n1079     }\n1080 \n1081     /*\n1082      * If this platform has an optimizing VM, check whether ComparableTimSort\n1083      * offers any performance benefit over TimSort in conjunction with a\n1084      * comparator that returns:\n1085      *    {@code ((Comparable)first).compareTo(Second)}.\n1086      * If not, you are better off deleting ComparableTimSort to\n1087      * eliminate the code duplication.  In other words, the commented\n1130      * randomly ordered.  If the input array is nearly sorted, the\n1131      * implementation requires approximately n comparisons.  Temporary\n1132      * storage requirements vary from a small constant for nearly sorted\n1133      * input arrays to n/2 object references for randomly ordered input\n1134      * arrays.\n1135      *\n1136      * <p>The implementation takes equal advantage of ascending and\n1137      * descending order in its input array, and can take advantage of\n1138      * ascending and descending order in different parts of the the same\n1139      * input array.  It is well-suited to merging two or more sorted arrays:\n1140      * simply concatenate the arrays and sort the resulting array.\n1141      *\n1142      * <p>The implementation was adapted from Tim Peters's list sort for Python\n1143      * (<a href=\"http://svn.python.org/projects/python/trunk/Objects/listsort.txt\">\n1144      * TimSort</a>).  It uses techiques from Peter McIlroy's \"Optimistic\n1145      * Sorting and Information Theoretic Complexity\", in Proceedings of the\n1146      * Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, pp 467-474,\n1147      * January 1993.\n1148      *\n1149      * @param a the array to be sorted\n\n1150      * @throws ClassCastException if the array contains elements that are not\n1151      *         <i>mutually comparable</i> (for example, strings and integers)\n1152      * @throws IllegalArgumentException (optional) if the natural\n1153      *         ordering of the array elements is found to violate the\n1154      *         {@link Comparable} contract\n1155      */\n1156     public static void sort(Object[] a) {\n1157         if (LegacyMergeSort.userRequested)\n1158             legacyMergeSort(a);\n1159         else\n1160             ComparableTimSort.sort(a);\n1161     }\n1162 \n1163     /** To be removed in a future release. */\n1164     private static void legacyMergeSort(Object[] a) {\n1165         Object[] aux = a.clone();\n1166         mergeSort(aux, a, 0, a.length, 0);\n1167     }\n1168 \n1169     /**\n1460         // If list is already sorted, just copy from src to dest.  This is an\n1461         // optimization that results in faster sorts for nearly ordered lists.\n1462         if (c.compare(src[mid-1], src[mid]) <= 0) {\n1463            System.arraycopy(src, low, dest, destLow, length);\n1464            return;\n1465         }\n1466 \n1467         // Merge sorted halves (now in src) into dest\n1468         for(int i = destLow, p = low, q = mid; i < destHigh; i++) {\n1469             if (q >= high || p < mid && c.compare(src[p], src[q]) <= 0)\n1470                 dest[i] = src[p++];\n1471             else\n1472                 dest[i] = src[q++];\n1473         }\n1474     }\n1475 \n1476     /**\n1477      * Check that fromIndex and toIndex are in range, and throw an\n1478      * appropriate exception if they aren't.\n1479      */ 1480     private static void rangeCheck(int arrayLen, int fromIndex, int toIndex) { 1481         if (fromIndex > toIndex) 1482             throw new IllegalArgumentException(\"fromIndex(\" + fromIndex + 1483                        \") > toIndex(\" + toIndex+\")\"); 1484         if (fromIndex < 0) 1485             throw new ArrayIndexOutOfBoundsException(fromIndex); 1486         if (toIndex > arrayLen) 1487             throw new ArrayIndexOutOfBoundsException(toIndex);\n1488     }\n\n1489 \n1490     // Searching\n1491 \n1492     /**\n1493      * Searches the specified array of longs for the specified value using the\n1494      * binary search algorithm.  The array must be sorted (as\n1495      * by the {@link #sort(long[])} method) prior to making this call.  If it\n1496      * is not sorted, the results are undefined.  If the array contains\n1497      * multiple elements with the specified value, there is no guarantee which\n1498      * one will be found.\n1499      *\n1500      * @param a the array to be searched\n1501      * @param key the value to be searched for\n1502      * @return index of the search key, if it is contained in the array;\n1503      *         otherwise, <tt>(-(<i>insertion point</i>) - 1)</tt>.  The\n1504      *         <i>insertion point</i> is defined as the point at which the\n1505      *         key would be inserted into the array: the index of the first\n1506      *         element greater than the key, or <tt>a.length</tt> if all\n1507      *         elements in the array are less than the specified key.  Note\n1508      *         that this guarantees that the return value will be &gt;= 0 if\n33  * that allows arrays to be viewed as lists.\n  34  *\n  35  * <p>The methods in this class all throw a <tt>NullPointerException</tt> if\n  36  * the specified array reference is null, except where noted.\n  37  *\n  38  * <p>The documentation for the methods contained in this class includes\n  39  * briefs description of the <i>implementations</i>.  Such descriptions should\n  40  * be regarded as <i>implementation notes</i>, rather than parts of the\n  41  * <i>specification</i>. Implementors should feel free to substitute other\n  42  * algorithms, so long as the specification itself is adhered to. (For\n  43  * example, the algorithm used by <tt>sort(Object[])</tt> does not have to be\n  44  * a mergesort, but it does have to be <i>stable</i>.)\n  45  *\n  46  * <p>This class is a member of the\n  47  * <a href=\"{@docRoot}/../technotes/guides/collections/index.html\">\n  48  * Java Collections Framework</a>.\n  49  *\n  50  * @author Josh Bloch\n  51  * @author Neal Gafter\n  52  * @author John Rose 53  * @author Vladimir Yaroslavskiy 54  * @since  1.2\n  55  */\n  56 \n  57 public class Arrays {\n  58     // Suppresses default constructor, ensuring non-instantiability.\n  59     private Arrays() {\n  60     }\n  61 \n  62     // Sorting\n  63 \n  64     /**\n  65      * Sorts the specified array of longs into ascending numerical order.\n\n\n\n\n\n  66      * 67      * <p>The sorting algorithm is the Dual-Pivot Quicksort, suggested by 68      * Vladimir Yaroslavskiy on February 2009. This algorithm offers 69      * n*ln(n) performance on many data sets that cause other quicksorts 70      * to degrade to quadratic (n^2) performance. 71      * 72      * @param a the array to be sorted\n  73      */\n  74     public static void sort(long[] a) { 75         sort(a, 0, a.length); 76     }\n  77 \n  78     /**\n  79      * Sorts the specified range of the specified array of longs into\n  80      * ascending numerical order. The range to be sorted extends from index 81      * <tt>fromIndex</tt>, inclusive, to index <tt>toIndex</tt>, exclusive 82      * (if <tt>fromIndex == toIndex</tt>, the range to be sorted is empty). 83      * 84      * <p>The sorting algorithm is the Dual-Pivot Quicksort, suggested by 85      * Vladimir Yaroslavskiy on February 2009. This algorithm offers 86      * n*ln(n) performance on many data sets that cause other quicksorts 87      * to degrade to quadratic (n^2) performance. 88      *\n  89      * @param a the array to be sorted 90      * @param fromIndex the index of the first element (inclusive) to be sorted 91      * @param toIndex the index of the last element (exclusive) to be sorted\n  92      * @throws IllegalArgumentException if <tt>fromIndex &gt; toIndex</tt> 93      * @throws ArrayIndexOutOfBoundsException if <tt>fromIndex &lt; 0</tt> 94      *         or <tt>toIndex &gt; a.length</tt> 95      */\n  96     public static void sort(long[] a, int fromIndex, int toIndex) {\n  97         rangeCheck(a.length, fromIndex, toIndex); 98         dualPivotQuicksort(a, fromIndex, toIndex - 1, 3); 99     }\n 100 \n 101     /**\n 102      * Sorts the specified array of ints into ascending numerical order.\n\n\n\n\n\n 103      * 104      * <p>The sorting algorithm is the Dual-Pivot Quicksort, suggested by 105      * Vladimir Yaroslavskiy on February 2009. This algorithm offers 106      * n*ln(n) performance on many data sets that cause other quicksorts 107      * to degrade to quadratic (n^2) performance. 108      * 109      * @param a the array to be sorted\n 110      */\n 111     public static void sort(int[] a) { 112         sort(a, 0, a.length); 113     }\n 114 \n 115     /**\n 116      * Sorts the specified range of the specified array of ints into\n 117      * ascending numerical order. The range to be sorted extends from index 118      * <tt>fromIndex</tt>, inclusive, to index <tt>toIndex</tt>, exclusive 119      * (if <tt>fromIndex == toIndex</tt>, the range to be sorted is empty). 120      * 121      * <p>The sorting algorithm is the Dual-Pivot Quicksort, suggested by 122      * Vladimir Yaroslavskiy on February 2009. This algorithm offers 123      * n*ln(n) performance on many data sets that cause other quicksorts 124      * to degrade to quadratic (n^2) performance. 125      *\n 126      * @param a the array to be sorted 127      * @param fromIndex the index of the first element (inclusive) to be sorted 128      * @param toIndex the index of the last element (exclusive) to be sorted\n 129      * @throws IllegalArgumentException if <tt>fromIndex &gt; toIndex</tt> 130      * @throws ArrayIndexOutOfBoundsException if <tt>fromIndex &lt; 0</tt> 131      *         or <tt>toIndex &gt; a.length</tt> 132      */\n 133     public static void sort(int[] a, int fromIndex, int toIndex) {\n 134         rangeCheck(a.length, fromIndex, toIndex); 135         dualPivotQuicksort(a, fromIndex, toIndex - 1, 3); 136     }\n 137 \n 138     /**\n 139      * Sorts the specified array of shorts into ascending numerical order.\n\n\n\n\n\n 140      * 141      * <p>The sorting algorithm is the Dual-Pivot Quicksort, suggested by 142      * Vladimir Yaroslavskiy on February 2009. This algorithm offers 143      * n*ln(n) performance on many data sets that cause other quicksorts 144      * to degrade to quadratic (n^2) performance. 145      * 146      * @param a the array to be sorted\n 147      */\n 148     public static void sort(short[] a) { 149         sort(a, 0, a.length); 150     }\n 151 \n 152     /**\n 153      * Sorts the specified range of the specified array of shorts into\n 154      * ascending numerical order. The range to be sorted extends from index 155      * <tt>fromIndex</tt>, inclusive, to index <tt>toIndex</tt>, exclusive 156      * (if <tt>fromIndex == toIndex</tt>, the range to be sorted is empty). 157      * 158      * <p>The sorting algorithm is the Dual-Pivot Quicksort, suggested by 159      * Vladimir Yaroslavskiy on February 2009. This algorithm offers 160      * n*ln(n) performance on many data sets that cause other quicksorts 161      * to degrade to quadratic (n^2) performance. 162      *\n 163      * @param a the array to be sorted 164      * @param fromIndex the index of the first element (inclusive) to be sorted 165      * @param toIndex the index of the last element (exclusive) to be sorted\n 166      * @throws IllegalArgumentException if <tt>fromIndex &gt; toIndex</tt> 167      * @throws ArrayIndexOutOfBoundsException if <tt>fromIndex &lt; 0</tt> 168      *         or <tt>toIndex &gt; a.length</tt> 169      */\n 170     public static void sort(short[] a, int fromIndex, int toIndex) {\n 171         rangeCheck(a.length, fromIndex, toIndex); 172         dualPivotQuicksort(a, fromIndex, toIndex - 1, 3); 173     }\n 174 \n 175     /**\n 176      * Sorts the specified array of chars into ascending numerical order.\n\n\n\n\n\n 177      * 178      * <p>The sorting algorithm is the Dual-Pivot Quicksort, suggested by 179      * Vladimir Yaroslavskiy on February 2009. This algorithm offers 180      * n*ln(n) performance on many data sets that cause other quicksorts 181      * to degrade to quadratic (n^2) performance. 182      * 183      * @param a the array to be sorted\n 184      */\n 185     public static void sort(char[] a) { 186         sort(a, 0, a.length); 187     }\n 188 \n 189     /**\n 190      * Sorts the specified range of the specified array of chars into\n 191      * ascending numerical order. The range to be sorted extends from index 192      * <tt>fromIndex</tt>, inclusive, to index <tt>toIndex</tt>, exclusive 193      * (if <tt>fromIndex == toIndex</tt>, the range to be sorted is empty). 194      * 195      * <p>The sorting algorithm is the Dual-Pivot Quicksort, suggested by 196      * Vladimir Yaroslavskiy on February 2009. This algorithm offers 197      * n*ln(n) performance on many data sets that cause other quicksorts 198      * to degrade to quadratic (n^2) performance. 199      *\n 200      * @param a the array to be sorted 201      * @param fromIndex the index of the first element (inclusive) to be sorted 202      * @param toIndex the index of the last element (exclusive) to be sorted\n 203      * @throws IllegalArgumentException if <tt>fromIndex &gt; toIndex</tt> 204      * @throws ArrayIndexOutOfBoundsException if <tt>fromIndex &lt; 0</tt> 205      *         or <tt>toIndex &gt; a.length</tt> 206      */\n 207     public static void sort(char[] a, int fromIndex, int toIndex) {\n 208         rangeCheck(a.length, fromIndex, toIndex); 209         dualPivotQuicksort(a, fromIndex, toIndex - 1, 3); 210     }\n 211 \n 212     /**\n 213      * Sorts the specified array of bytes into ascending numerical order.\n\n\n\n\n\n 214      * 215      * <p>The sorting algorithm is the Dual-Pivot Quicksort, suggested by 216      * Vladimir Yaroslavskiy on February 2009. This algorithm offers 217      * n*ln(n) performance on many data sets that cause other quicksorts 218      * to degrade to quadratic (n^2) performance. 219      * 220      * @param a the array to be sorted\n 221      */\n 222     public static void sort(byte[] a) { 223         sort(a, 0, a.length); 224     }\n 225 \n 226     /**\n 227      * Sorts the specified range of the specified array of bytes into\n 228      * ascending numerical order. The range to be sorted extends from index 229      * <tt>fromIndex</tt>, inclusive, to index <tt>toIndex</tt>, exclusive 230      * (if <tt>fromIndex == toIndex</tt>, the range to be sorted is empty). 231      * 232      * <p>The sorting algorithm is the Dual-Pivot Quicksort, suggested by 233      * Vladimir Yaroslavskiy on February 2009. This algorithm offers 234      * n*ln(n) performance on many data sets that cause other quicksorts 235      * to degrade to quadratic (n^2) performance. 236      *\n 237      * @param a the array to be sorted 238      * @param fromIndex the index of the first element (inclusive) to be sorted 239      * @param toIndex the index of the last element (exclusive) to be sorted\n 240      * @throws IllegalArgumentException if <tt>fromIndex &gt; toIndex</tt> 241      * @throws ArrayIndexOutOfBoundsException if <tt>fromIndex &lt; 0</tt> 242      *         or <tt>toIndex &gt; a.length</tt> 243      */\n 244     public static void sort(byte[] a, int fromIndex, int toIndex) {\n 245         rangeCheck(a.length, fromIndex, toIndex); 246         dualPivotQuicksort(a, fromIndex, toIndex - 1, 3); 247     }\n 248 \n 249     /**\n 250      * Sorts the specified array of doubles into ascending numerical order. 251      * 252      * <p>The <code>&lt;</code> relation does not provide a total order on 253      * all floating-point values; although they are distinct numbers 254      * <code>-0.0d == 0.0d</code> is <code>true</code> and a NaN value 255      * compares neither less than, greater than, nor equal to any\n 256      * floating-point value, even itself. To allow the sort to\n 257      * proceed, instead of using the <code>&lt;</code> relation to\n 258      * determine ascending numerical order, this method uses the total\n 259      * order imposed by {@link Double#compareTo}. This ordering\n 260      * differs from the <code>&lt;</code> relation in that 261      * <code>-0.0d</code> is treated as less than <code>0.0d</code> and 262      * NaN is considered greater than any other floating-point value.\n 263      * For the purposes of sorting, all NaN values are considered\n 264      * equivalent and equal.\n\n\n\n\n\n\n 265      * 266      * <p>The sorting algorithm is the Dual-Pivot Quicksort, suggested by 267      * Vladimir Yaroslavskiy on February 2009. This algorithm offers 268      * n*ln(n) performance on many data sets that cause other quicksorts 269      * to degrade to quadratic (n^2) performance. 270      * 271      * @param a the array to be sorted\n 272      */\n 273     public static void sort(double[] a) { 274         sort(a, 0, a.length); 275     }\n 276 \n 277     /**\n 278      * Sorts the specified range of the specified array of doubles into\n 279      * ascending numerical order. The range to be sorted extends from index 280      * <tt>fromIndex</tt>, inclusive, to index <tt>toIndex</tt>, exclusive 281      * (if <tt>fromIndex==toIndex</tt>, the range to be sorted is empty). 282      * 283      * <p>The <code>&lt;</code> relation does not provide a total order on 284      * all floating-point values; although they are distinct numbers 285      * <code>-0.0d == 0.0d</code> is <code>true</code> and a NaN value 286      * compares neither less than, greater than, nor equal to any\n 287      * floating-point value, even itself. To allow the sort to\n 288      * proceed, instead of using the <code>&lt;</code> relation to\n 289      * determine ascending numerical order, this method uses the total\n 290      * order imposed by {@link Double#compareTo}. This ordering\n 291      * differs from the <code>&lt;</code> relation in that 292      * <code>-0.0d</code> is treated as less than <code>0.0d</code> and 293      * NaN is considered greater than any other floating-point value.\n 294      * For the purposes of sorting, all NaN values are considered\n 295      * equivalent and equal.\n\n\n\n\n\n\n 296      * 297      * <p>The sorting algorithm is the Dual-Pivot Quicksort, suggested by 298      * Vladimir Yaroslavskiy on February 2009. This algorithm offers 299      * n*ln(n) performance on many data sets that cause other quicksorts 300      * to degrade to quadratic (n^2) performance. 301      * 302      * @param a the array to be sorted 303      * @param fromIndex the index of the first element (inclusive) to be sorted 304      * @param toIndex the index of the last element (exclusive) to be sorted\n 305      * @throws IllegalArgumentException if <tt>fromIndex &gt; toIndex</tt> 306      * @throws ArrayIndexOutOfBoundsException if <tt>fromIndex &lt; 0</tt> 307      *         or <tt>toIndex &gt; a.length</tt> 308      */\n 309     public static void sort(double[] a, int fromIndex, int toIndex) {\n 310         rangeCheck(a.length, fromIndex, toIndex); 311         sortNegZeroAndNaN(a, fromIndex, toIndex); 312     }\n 313 314     private static void sortNegZeroAndNaN(double[] a, int fromIndex, int toIndex) { 315         final long NEG_ZERO_BITS = Double.doubleToLongBits(-0.0d); 316         /* 317          * The sort is done in three phases to avoid the expense of using 318          * NaN and -0.0d aware comparisons during the main sort. 319          * 320          * Preprocessing phase: move any NaN's to end of array, count the 321          * number of -0.0d's, and turn them into 0.0d's. 322          */ 323         int numNegZeros = 0; 324         int i = fromIndex; 325         int n = toIndex; 326 327         while (i < n) { 328             if (a[i] != a[i]) { 329                 swap(a, i, --n); 330             } 331             else { 332                 if (a[i] == 0 && Double.doubleToLongBits(a[i]) == NEG_ZERO_BITS) { 333                     a[i] = 0.0d; 334                     numNegZeros++; 335                 } 336                 i++; 337             } 338         } 339         // Main sort phase: quicksort everything but the NaN's 340         dualPivotQuicksort(a, fromIndex, n - 1, 3); 341 342         // Postprocessing phase: change 0.0d's to -0.0d's as required 343         if (numNegZeros != 0) { 344             int j = binarySearch0(a, fromIndex, n, 0.0d); // position of ANY zero 345 346             do { 347                 j--; 348             } 349             while (j >= fromIndex && a[j] == 0.0d); 350 351             // j is now one less than the index of the FIRST zero 352             for (int k = 0; k < numNegZeros; k++) { 353                 a[++j] = -0.0d; 354             } 355         } 356     } 357 358     /**\n 359      * Sorts the specified array of floats into ascending numerical order. 360      * 361      * <p>The <code>&lt;</code> relation does not provide a total order on 362      * all floating-point values; although they are distinct numbers\n 363      * <code>-0.0f == 0.0f</code> is <code>true</code> and a NaN value\n 364      * compares neither less than, greater than, nor equal to any\n 365      * floating-point value, even itself. To allow the sort to\n 366      * proceed, instead of using the <code>&lt;</code> relation to\n 367      * determine ascending numerical order, this method uses the total\n 368      * order imposed by {@link Float#compareTo}. This ordering\n 369      * differs from the <code>&lt;</code> relation in that\n 370      * <code>-0.0f</code> is treated as less than <code>0.0f</code> and\n 371      * NaN is considered greater than any other floating-point value.\n 372      * For the purposes of sorting, all NaN values are considered\n 373      * equivalent and equal.\n\n\n\n\n\n\n 374      * 375      * <p>The sorting algorithm is the Dual-Pivot Quicksort, suggested by 376      * Vladimir Yaroslavskiy on February 2009. This algorithm offers 377      * n*ln(n) performance on many data sets that cause other quicksorts 378      * to degrade to quadratic (n^2) performance. 379      * 380      * @param a the array to be sorted\n 381      */\n 382     public static void sort(float[] a) { 383         sort(a, 0, a.length); 384     }\n 385 \n 386     /**\n 387      * Sorts the specified range of the specified array of floats into\n 388      * ascending numerical order. The range to be sorted extends from index 389      * <tt>fromIndex</tt>, inclusive, to index <tt>toIndex</tt>, exclusive 390      * (if <tt>fromIndex==toIndex</tt>, the range to be sorted is empty). 391      * 392      * <p>The <code>&lt;</code> relation does not provide a total order on 393      * all floating-point values; although they are distinct numbers\n 394      * <code>-0.0f == 0.0f</code> is <code>true</code> and a NaN value\n 395      * compares neither less than, greater than, nor equal to any\n 396      * floating-point value, even itself. To allow the sort to\n 397      * proceed, instead of using the <code>&lt;</code> relation to\n 398      * determine ascending numerical order, this method uses the total\n 399      * order imposed by {@link Float#compareTo}. This ordering\n 400      * differs from the <code>&lt;</code> relation in that\n 401      * <code>-0.0f</code> is treated as less than <code>0.0f</code> and\n 402      * NaN is considered greater than any other floating-point value.\n 403      * For the purposes of sorting, all NaN values are considered\n 404      * equivalent and equal.\n\n\n\n\n\n\n 405      * 406      * <p>The sorting algorithm is the Dual-Pivot Quicksort, suggested by 407      * Vladimir Yaroslavskiy on February 2009. This algorithm offers 408      * n*ln(n) performance on many data sets that cause other quicksorts 409      * to degrade to quadratic (n^2) performance. 410      * 411      * @param a the array to be sorted 412      * @param fromIndex the index of the first element (inclusive) to be sorted 413      * @param toIndex the index of the last element (exclusive) to be sorted\n 414      * @throws IllegalArgumentException if <tt>fromIndex &gt; toIndex</tt> 415      * @throws ArrayIndexOutOfBoundsException if <tt>fromIndex &lt; 0</tt> 416      *         or <tt>toIndex &gt; a.length</tt> 417      */\n 418     public static void sort(float[] a, int fromIndex, int toIndex) {\n 419         rangeCheck(a.length, fromIndex, toIndex); 420         sortNegZeroAndNaN(a, fromIndex, toIndex); 421     }\n 422 423     private static void sortNegZeroAndNaN(float[] a, int fromIndex, int toIndex) { 424         final int NEG_ZERO_BITS = Float.floatToIntBits(-0.0f);\n 425         /*\n 426          * The sort is done in three phases to avoid the expense of using 427          * NaN and -0.0f aware comparisons during the main sort. 428          * 429          * Preprocessing phase: move any NaN's to end of array, count the 430          * number of -0.0f's, and turn them into 0.0f's. 431          */\n\n\n\n\n\n 432         int numNegZeros = 0; 433         int i = fromIndex; 434         int n = toIndex; 435 436         while (i < n) { 437             if (a[i] != a[i]) {\n 438                 swap(a, i, --n); 439             } 440             else { 441                 if (a[i] == 0 && Float.floatToIntBits(a[i]) == NEG_ZERO_BITS) { 442                     a[i] = 0.0f;\n 443                     numNegZeros++;\n 444                 }\n 445                 i++;\n 446             }\n 447         }\n\n 448         // Main sort phase: quicksort everything but the NaN's 449         dualPivotQuicksort(a, fromIndex, n - 1, 3); 450 451         // Postprocessing phase: change 0.0f's to -0.0f's as required 452         if (numNegZeros != 0) { 453             int j = binarySearch0(a, fromIndex, n, 0.0f); // position of ANY zero 454 455             do {\n 456                 j--; 457             } 458             while (j >= fromIndex && a[j] == 0.0f); 459 \n 460             // j is now one less than the index of the FIRST zero 461             for (int k = 0; k < numNegZeros; k++) { 462                 a[++j] = -0.0f;\n 463             }\n 464         } 465     } 466 \n\n 467     /*\n 468      * The code for each of the seven primitive types is largely identical.\n 469      * C'est la vie.\n 470      */\n 471 \n 472     /**\n 473      * Sorts the specified sub-array of longs into ascending order.\n 474      */ 475     private static void dualPivotQuicksort(long[] a, int left, int right, int div) { 476         int len = right - left; 477 478         if (len < 27) { // insertion sort for tiny array 479             for (int i = left + 1; i <= right; i++) { 480                 for (int j = i; j > left && a[j] < a[j - 1]; j--) { 481                     swap(a, j, j - 1); 482                 } 483             } 484             return;\n 485         } 486         int third = len / div; 487 488         // \"medians\" 489         int m1 = left  + third; 490         int m2 = right - third; 491 492         if (m1 <= left) { 493             m1 = left + 1; 494         } 495         if (m2 >= right) { 496             m2 = right - 1; 497         } 498         if (a[m1] < a[m2]) { 499             swap(a, m1, left); 500             swap(a, m2, right); 501         } 502         else { 503             swap(a, m1, right); 504             swap(a, m2, left); 505         } 506         // pivots 507         long pivot1 = a[left]; 508         long pivot2 = a[right]; 509 510         // pointers 511         int less  = left  + 1; 512         int great = right - 1; 513 514         // sorting 515         for (int k = less; k <= great; k++) { 516             if (a[k] < pivot1) { 517                 swap(a, k, less++); 518             } 519             else if (a[k] > pivot2) { 520                 while (k < great && a[great] > pivot2) { 521                     great--; 522                 } 523                 swap(a, k, great--); 524 525                 if (a[k] < pivot1) { 526                     swap(a, k, less++); 527                 } 528             } 529         } 530         // swaps 531         int dist = great - less; 532 533         if (dist < 13) { 534            div++; 535         } 536         swap(a, less  - 1, left); 537         swap(a, great + 1, right); 538 539         // subarrays 540         dualPivotQuicksort(a, left,   less - 2, div); 541         dualPivotQuicksort(a, great + 2, right, div); 542 543         // equal elements 544         if (dist > len - 13 && pivot1 != pivot2) { 545             for (int k = less; k <= great; k++) { 546                 if (a[k] == pivot1) { 547                     swap(a, k, less++); 548                 } 549                 else if (a[k] == pivot2) { 550                     swap(a, k, great--); 551 552                     if (a[k] == pivot1) { 553                         swap(a, k, less++); 554                     } 555                 } 556             } 557         } 558         // subarray 559         if (pivot1 < pivot2) { 560             dualPivotQuicksort(a, less, great, div); 561         } 562     } 563 \n 564     /** 565      * Swaps a[i] with a[j]. 566      */ 567     private static void swap(long[] a, int i, int j) { 568         long temp = a[i]; 569         a[i] = a[j]; 570         a[j] = temp; 571     }\n 572 \n 573     /** 574      * Sorts the specified sub-array of ints into ascending order. 575      */ 576     private static void dualPivotQuicksort(int[] a, int left, int right, int div) { 577         int len = right - left; 578 579         if (len < 27) { // insertion sort for tiny array 580             for (int i = left + 1; i <= right; i++) { 581                 for (int j = i; j > left && a[j] < a[j - 1]; j--) { 582                     swap(a, j, j - 1); 583                 } 584             } 585             return;\n 586         } 587         int third = len / div; 588 589         // \"medians\" 590         int m1 = left  + third; 591         int m2 = right - third; 592 593         if (m1 <= left) { 594             m1 = left + 1; 595         } 596         if (m2 >= right) { 597             m2 = right - 1; 598         } 599         if (a[m1] < a[m2]) { 600             swap(a, m1, left); 601             swap(a, m2, right); 602         } 603         else { 604             swap(a, m1, right); 605             swap(a, m2, left); 606         } 607         // pivots 608         int pivot1 = a[left]; 609         int pivot2 = a[right]; 610 611         // pointers 612         int less  = left  + 1; 613         int great = right - 1; 614 615         // sorting 616         for (int k = less; k <= great; k++) { 617             if (a[k] < pivot1) { 618                 swap(a, k, less++); 619             } 620             else if (a[k] > pivot2) { 621                 while (k < great && a[great] > pivot2) { 622                     great--; 623                 } 624                 swap(a, k, great--); 625 626                 if (a[k] < pivot1) { 627                     swap(a, k, less++); 628                 } 629             } 630         } 631         // swaps 632         int dist = great - less; 633 634         if (dist < 13) { 635            div++; 636         } 637         swap(a, less  - 1, left); 638         swap(a, great + 1, right); 639 640         // subarrays 641         dualPivotQuicksort(a, left,   less - 2, div); 642         dualPivotQuicksort(a, great + 2, right, div); 643 644         // equal elements 645         if (dist > len - 13 && pivot1 != pivot2) { 646             for (int k = less; k <= great; k++) { 647                 if (a[k] == pivot1) { 648                     swap(a, k, less++); 649                 } 650                 else if (a[k] == pivot2) { 651                     swap(a, k, great--); 652 653                     if (a[k] == pivot1) { 654                         swap(a, k, less++); 655                     } 656                 } 657             } 658         } 659         // subarray 660         if (pivot1 < pivot2) { 661             dualPivotQuicksort(a, less, great, div); 662         } 663     } 664 \n 665     /** 666      * Swaps a[i] with a[j]. 667      */ 668     private static void swap(int[] a, int i, int j) { 669         int temp = a[i]; 670         a[i] = a[j]; 671         a[j] = temp; 672     }\n 673 \n 674     /**\n 675      * Sorts the specified sub-array of shorts into ascending order.\n 676      */ 677     private static void dualPivotQuicksort(short[] a, int left, int right, int div) { 678         int len = right - left; 679 680         if (len < 27) { // insertion sort for tiny array 681             for (int i = left + 1; i <= right; i++) { 682                 for (int j = i; j > left && a[j] < a[j - 1]; j--) { 683                     swap(a, j, j - 1); 684                 } 685             } 686             return;\n 687         } 688         int third = len / div; 689 690         // \"medians\" 691         int m1 = left  + third; 692         int m2 = right - third; 693 694         if (m1 <= left) { 695             m1 = left + 1; 696         } 697         if (m2 >= right) { 698             m2 = right - 1; 699         } 700         if (a[m1] < a[m2]) { 701             swap(a, m1, left); 702             swap(a, m2, right); 703         } 704         else { 705             swap(a, m1, right); 706             swap(a, m2, left); 707         } 708         // pivots 709         short pivot1 = a[left]; 710         short pivot2 = a[right]; 711 712         // pointers 713         int less  = left  + 1; 714         int great = right - 1; 715 716         // sorting 717         for (int k = less; k <= great; k++) { 718             if (a[k] < pivot1) { 719                 swap(a, k, less++); 720             } 721             else if (a[k] > pivot2) { 722                 while (k < great && a[great] > pivot2) { 723                     great--; 724                 } 725                 swap(a, k, great--); 726 727                 if (a[k] < pivot1) { 728                     swap(a, k, less++); 729                 } 730             } 731         } 732         // swaps 733         int dist = great - less; 734 735         if (dist < 13) { 736            div++; 737         } 738         swap(a, less  - 1, left); 739         swap(a, great + 1, right); 740 741         // subarrays 742         dualPivotQuicksort(a, left,   less - 2, div); 743         dualPivotQuicksort(a, great + 2, right, div); 744 745         // equal elements 746         if (dist > len - 13 && pivot1 != pivot2) { 747             for (int k = less; k <= great; k++) { 748                 if (a[k] == pivot1) { 749                     swap(a, k, less++); 750                 } 751                 else if (a[k] == pivot2) { 752                     swap(a, k, great--); 753 754                     if (a[k] == pivot1) { 755                         swap(a, k, less++); 756                     } 757                 } 758             } 759         } 760         // subarray 761         if (pivot1 < pivot2) { 762             dualPivotQuicksort(a, less, great, div); 763         } 764     } 765 \n 766     /** 767      * Swaps a[i] with a[j]. 768      */ 769     private static void swap(short[] a, int i, int j) { 770         short temp = a[i]; 771         a[i] = a[j]; 772         a[j] = temp; 773     }\n 774 \n\n 775     /**\n 776      * Sorts the specified sub-array of chars into ascending order.\n 777      */ 778     private static void dualPivotQuicksort(char[] a, int left, int right, int div) { 779         int len = right - left; 780 781         if (len < 27) { // insertion sort for tiny array 782             for (int i = left + 1; i <= right; i++) { 783                 for (int j = i; j > left && a[j] < a[j - 1]; j--) { 784                     swap(a, j, j - 1); 785                 } 786             } 787             return;\n 788         } 789         int third = len / div; 790 791         // \"medians\" 792         int m1 = left  + third; 793         int m2 = right - third; 794 795         if (m1 <= left) { 796             m1 = left + 1; 797         } 798         if (m2 >= right) { 799             m2 = right - 1; 800         } 801         if (a[m1] < a[m2]) { 802             swap(a, m1, left); 803             swap(a, m2, right); 804         } 805         else { 806             swap(a, m1, right); 807             swap(a, m2, left); 808         } 809         // pivots 810         char pivot1 = a[left]; 811         char pivot2 = a[right]; 812 813         // pointers 814         int less  = left  + 1; 815         int great = right - 1; 816 817         // sorting 818         for (int k = less; k <= great; k++) { 819             if (a[k] < pivot1) { 820                 swap(a, k, less++); 821             } 822             else if (a[k] > pivot2) { 823                 while (k < great && a[great] > pivot2) { 824                     great--; 825                 } 826                 swap(a, k, great--); 827 828                 if (a[k] < pivot1) { 829                     swap(a, k, less++); 830                 } 831             } 832         } 833         // swaps 834         int dist = great - less; 835 836         if (dist < 13) { 837            div++; 838         } 839         swap(a, less  - 1, left); 840         swap(a, great + 1, right); 841 842         // subarrays 843         dualPivotQuicksort(a, left,   less - 2, div); 844         dualPivotQuicksort(a, great + 2, right, div); 845 846         // equal elements 847         if (dist > len - 13 && pivot1 != pivot2) { 848             for (int k = less; k <= great; k++) { 849                 if (a[k] == pivot1) { 850                     swap(a, k, less++); 851                 } 852                 else if (a[k] == pivot2) { 853                     swap(a, k, great--); 854 855                     if (a[k] == pivot1) { 856                         swap(a, k, less++); 857                     } 858                 } 859             } 860         } 861         // subarray 862         if (pivot1 < pivot2) { 863             dualPivotQuicksort(a, less, great, div); 864         } 865     } 866 \n 867     /** 868      * Swaps a[i] with a[j]. 869      */ 870     private static void swap(char[] a, int i, int j) { 871         char temp = a[i]; 872         a[i] = a[j]; 873         a[j] = temp; 874     }\n 875 \n\n 876     /**\n 877      * Sorts the specified sub-array of bytes into ascending order.\n 878      */ 879     private static void dualPivotQuicksort(byte[] a, int left, int right, int div) { 880         int len = right - left; 881 882         if (len < 27) { // insertion sort for tiny array 883             for (int i = left + 1; i <= right; i++) { 884                 for (int j = i; j > left && a[j] < a[j - 1]; j--) { 885                     swap(a, j, j - 1); 886                 } 887             } 888             return;\n 889         } 890         int third = len / div; 891 892         // \"medians\" 893         int m1 = left  + third; 894         int m2 = right - third; 895 896         if (m1 <= left) { 897             m1 = left + 1; 898         } 899         if (m2 >= right) { 900             m2 = right - 1; 901         } 902         if (a[m1] < a[m2]) { 903             swap(a, m1, left); 904             swap(a, m2, right); 905         } 906         else { 907             swap(a, m1, right); 908             swap(a, m2, left); 909         } 910         // pivots 911         byte pivot1 = a[left]; 912         byte pivot2 = a[right]; 913 914         // pointers 915         int less  = left  + 1; 916         int great = right - 1; 917 918         // sorting 919         for (int k = less; k <= great; k++) { 920             if (a[k] < pivot1) { 921                 swap(a, k, less++); 922             } 923             else if (a[k] > pivot2) { 924                 while (k < great && a[great] > pivot2) { 925                     great--; 926                 } 927                 swap(a, k, great--); 928 929                 if (a[k] < pivot1) { 930                     swap(a, k, less++); 931                 } 932             } 933         } 934         // swaps 935         int dist = great - less; 936 937         if (dist < 13) { 938            div++; 939         } 940         swap(a, less  - 1, left); 941         swap(a, great + 1, right); 942 943         // subarrays 944         dualPivotQuicksort(a, left,   less - 2, div); 945         dualPivotQuicksort(a, great + 2, right, div); 946 947         // equal elements 948         if (dist > len - 13 && pivot1 != pivot2) { 949             for (int k = less; k <= great; k++) { 950                 if (a[k] == pivot1) { 951                     swap(a, k, less++); 952                 } 953                 else if (a[k] == pivot2) { 954                     swap(a, k, great--); 955 956                     if (a[k] == pivot1) { 957                         swap(a, k, less++); 958                     } 959                 } 960             } 961         } 962         // subarray 963         if (pivot1 < pivot2) { 964             dualPivotQuicksort(a, less, great, div); 965         } 966     } 967 \n 968     /** 969      * Swaps a[i] with a[j]. 970      */ 971     private static void swap(byte[] a, int i, int j) { 972         byte temp = a[i]; 973         a[i] = a[j]; 974         a[j] = temp; 975     }\n 976 \n\n 977     /**\n 978      * Sorts the specified sub-array of doubles into ascending order.\n 979      */ 980     private static void dualPivotQuicksort(double[] a, int left, int right, int div) { 981         int len = right - left; 982 983         if (len < 27) { // insertion sort for tiny array 984             for (int i = left + 1; i <= right; i++) { 985                 for (int j = i; j > left && a[j] < a[j - 1]; j--) { 986                     swap(a, j, j - 1); 987                 } 988             } 989             return;\n 990         } 991         int third = len / div; 992 993         // \"medians\" 994         int m1 = left  + third; 995         int m2 = right - third; 996 997         if (m1 <= left) { 998             m1 = left + 1; 999         } 1000         if (m2 >= right) { 1001             m2 = right - 1; 1002         } 1003         if (a[m1] < a[m2]) { 1004             swap(a, m1, left); 1005             swap(a, m2, right); 1006         } 1007         else { 1008             swap(a, m1, right); 1009             swap(a, m2, left); 1010         } 1011         // pivots 1012         double pivot1 = a[left]; 1013         double pivot2 = a[right]; 1014 1015         // pointers 1016         int less  = left  + 1; 1017         int great = right - 1; 1018 1019         // sorting 1020         for (int k = less; k <= great; k++) { 1021             if (a[k] < pivot1) { 1022                 swap(a, k, less++); 1023             } 1024             else if (a[k] > pivot2) { 1025                 while (k < great && a[great] > pivot2) { 1026                     great--; 1027                 } 1028                 swap(a, k, great--); 1029 1030                 if (a[k] < pivot1) { 1031                     swap(a, k, less++); 1032                 } 1033             } 1034         } 1035         // swaps 1036         int dist = great - less; 1037 1038         if (dist < 13) { 1039            div++; 1040         } 1041         swap(a, less  - 1, left); 1042         swap(a, great + 1, right); 1043 1044         // subarrays 1045         dualPivotQuicksort(a, left,   less - 2, div); 1046         dualPivotQuicksort(a, great + 2, right, div); 1047 1048         // equal elements 1049         if (dist > len - 13 && pivot1 != pivot2) { 1050             for (int k = less; k <= great; k++) { 1051                 if (a[k] == pivot1) { 1052                     swap(a, k, less++); 1053                 } 1054                 else if (a[k] == pivot2) { 1055                     swap(a, k, great--); 1056 1057                     if (a[k] == pivot1) { 1058                         swap(a, k, less++); 1059                     } 1060                 } 1061             } 1062         } 1063         // subarray 1064         if (pivot1 < pivot2) { 1065             dualPivotQuicksort(a, less, great, div); 1066         } 1067     } 1068 \n1069     /** 1070      * Swaps a[i] with a[j]. 1071      */ 1072     private static void swap(double[] a, int i, int j) { 1073         double temp = a[i]; 1074         a[i] = a[j]; 1075         a[j] = temp; 1076     }\n1077 \n\n1078     /**\n1079      * Sorts the specified sub-array of floats into ascending order.\n1080      */ 1081     private static void dualPivotQuicksort(float[] a, int left, int right, int div) { 1082         int len = right - left; 1083 1084         if (len < 27) { // insertion sort for tiny array 1085             for (int i = left + 1; i <= right; i++) { 1086                 for (int j = i; j > left && a[j] < a[j - 1]; j--) { 1087                     swap(a, j, j - 1); 1088                 } 1089             } 1090             return;\n1091         } 1092         int third = len / div; 1093 1094         // \"medians\" 1095         int m1 = left  + third; 1096         int m2 = right - third; 1097 1098         if (m1 <= left) { 1099             m1 = left + 1; 1100         } 1101         if (m2 >= right) { 1102             m2 = right - 1; 1103         } 1104         if (a[m1] < a[m2]) { 1105             swap(a, m1, left); 1106             swap(a, m2, right); 1107         } 1108         else { 1109             swap(a, m1, right); 1110             swap(a, m2, left); 1111         } 1112         // pivots 1113         float pivot1 = a[left]; 1114         float pivot2 = a[right]; 1115 1116         // pointers 1117         int less  = left  + 1; 1118         int great = right - 1; 1119 1120         // sorting 1121         for (int k = less; k <= great; k++) { 1122             if (a[k] < pivot1) { 1123                 swap(a, k, less++); 1124             } 1125             else if (a[k] > pivot2) { 1126                 while (k < great && a[great] > pivot2) { 1127                     great--; 1128                 } 1129                 swap(a, k, great--); 1130 1131                 if (a[k] < pivot1) { 1132                     swap(a, k, less++); 1133                 } 1134             } 1135         } 1136         // swaps 1137         int dist = great - less; 1138 1139         if (dist < 13) { 1140            div++; 1141         } 1142         swap(a, less  - 1, left); 1143         swap(a, great + 1, right); 1144 1145         // subarrays 1146         dualPivotQuicksort(a, left,   less - 2, div); 1147         dualPivotQuicksort(a, great + 2, right, div); 1148 1149         // equal elements 1150         if (dist > len - 13 && pivot1 != pivot2) { 1151             for (int k = less; k <= great; k++) { 1152                 if (a[k] == pivot1) { 1153                     swap(a, k, less++); 1154                 } 1155                 else if (a[k] == pivot2) { 1156                     swap(a, k, great--); 1157 1158                     if (a[k] == pivot1) { 1159                         swap(a, k, less++); 1160                     } 1161                 } 1162             } 1163         } 1164         // subarray 1165         if (pivot1 < pivot2) { 1166             dualPivotQuicksort(a, less, great, div); 1167         } 1168     } 1169 \n1170     /** 1171      * Swaps a[i] with a[j]. 1172      */ 1173     private static void swap(float[] a, int i, int j) { 1174         float temp = a[i]; 1175         a[i] = a[j]; 1176         a[j] = temp; 1177     }\n1178 1179 1180 1181     /**\n1182      * Old merge sort implementation can be selected (for\n1183      * compatibility with broken comparators) using a system property.\n1184      * Cannot be a static boolean in the enclosing class due to\n1185      * circular dependencies.  To be removed in a future release.\n1186      */\n1187     static final class LegacyMergeSort {\n1188         private static final boolean userRequested =\n1189             java.security.AccessController.doPrivileged(\n1190                 new sun.security.action.GetBooleanAction(\n1191                     \"java.util.Arrays.useLegacyMergeSort\")).booleanValue();\n1192     }\n1193 \n1194     /*\n1195      * If this platform has an optimizing VM, check whether ComparableTimSort\n1196      * offers any performance benefit over TimSort in conjunction with a\n1197      * comparator that returns:\n1198      *    {@code ((Comparable)first).compareTo(Second)}.\n1199      * If not, you are better off deleting ComparableTimSort to\n1200      * eliminate the code duplication.  In other words, the commented\n1243      * randomly ordered.  If the input array is nearly sorted, the\n1244      * implementation requires approximately n comparisons.  Temporary\n1245      * storage requirements vary from a small constant for nearly sorted\n1246      * input arrays to n/2 object references for randomly ordered input\n1247      * arrays.\n1248      *\n1249      * <p>The implementation takes equal advantage of ascending and\n1250      * descending order in its input array, and can take advantage of\n1251      * ascending and descending order in different parts of the the same\n1252      * input array.  It is well-suited to merging two or more sorted arrays:\n1253      * simply concatenate the arrays and sort the resulting array.\n1254      *\n1255      * <p>The implementation was adapted from Tim Peters's list sort for Python\n1256      * (<a href=\"http://svn.python.org/projects/python/trunk/Objects/listsort.txt\">\n1257      * TimSort</a>).  It uses techiques from Peter McIlroy's \"Optimistic\n1258      * Sorting and Information Theoretic Complexity\", in Proceedings of the\n1259      * Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, pp 467-474,\n1260      * January 1993.\n1261      *\n1262      * @param a the array to be sorted 1263 1264      * @throws ClassCastException if the array contains elements that are not\n1265      *         <i>mutually comparable</i> (for example, strings and integers)\n1266      * @throws IllegalArgumentException (optional) if the natural\n1267      *         ordering of the array elements is found to violate the\n1268      *         {@link Comparable} contract\n1269      */\n1270     public static void sort(Object[] a) {\n1271         if (LegacyMergeSort.userRequested)\n1272             legacyMergeSort(a);\n1273         else\n1274             ComparableTimSort.sort(a);\n1275     }\n1276 \n1277     /** To be removed in a future release. */\n1278     private static void legacyMergeSort(Object[] a) {\n1279         Object[] aux = a.clone();\n1280         mergeSort(aux, a, 0, a.length, 0);\n1281     }\n1282 \n1283     /**\n1574         // If list is already sorted, just copy from src to dest.  This is an\n1575         // optimization that results in faster sorts for nearly ordered lists.\n1576         if (c.compare(src[mid-1], src[mid]) <= 0) {\n1577            System.arraycopy(src, low, dest, destLow, length);\n1578            return;\n1579         }\n1580 \n1581         // Merge sorted halves (now in src) into dest\n1582         for(int i = destLow, p = low, q = mid; i < destHigh; i++) {\n1583             if (q >= high || p < mid && c.compare(src[p], src[q]) <= 0)\n1584                 dest[i] = src[p++];\n1585             else\n1586                 dest[i] = src[q++];\n1587         }\n1588     }\n1589 \n1590     /**\n1591      * Check that fromIndex and toIndex are in range, and throw an\n1592      * appropriate exception if they aren't.\n1593      */ 1594     private static void rangeCheck(int arrayLength, int fromIndex, int toIndex) { 1595         if (fromIndex > toIndex) { 1596             throw new IllegalArgumentException(\"fromIndex(\" + fromIndex + 1597                 \") > toIndex(\" + toIndex + \")\"); 1598         } 1599         if (fromIndex < 0) { 1600             throw new ArrayIndexOutOfBoundsException(fromIndex); 1601         } 1602         if (toIndex > arrayLength) { 1603             throw new ArrayIndexOutOfBoundsException(toIndex);\n1604         } 1605     } 1606 \n1607     // Searching\n1608 \n1609     /**\n1610      * Searches the specified array of longs for the specified value using the\n1611      * binary search algorithm.  The array must be sorted (as\n1612      * by the {@link #sort(long[])} method) prior to making this call.  If it\n1613      * is not sorted, the results are undefined.  If the array contains\n1614      * multiple elements with the specified value, there is no guarantee which\n1615      * one will be found.\n1616      *\n1617      * @param a the array to be searched\n1618      * @param key the value to be searched for\n1619      * @return index of the search key, if it is contained in the array;\n1620      *         otherwise, <tt>(-(<i>insertion point</i>) - 1)</tt>.  The\n1621      *         <i>insertion point</i> is defined as the point at which the\n1622      *         key would be inserted into the array: the index of the first\n1623      *         element greater than the key, or <tt>a.length</tt> if all\n1624      *         elements in the array are less than the specified key.  Note\n1625      *         that this guarantees that the return value will be &gt;= 0 if\n"}, {"score": 444.8506, "uuid": "a1ccb3be-76f4-5510-84ef-a7fbf1587992", "index": "cw12", "trec_id": "clueweb12-1808wb-76-25775", "target_hostname": "jorgetavares.com", "target_uri": "http://jorgetavares.com/tag/lisp/", "page_rank": 2.3403988e-09, "spam_rank": 73, "title": "Lisp &laquo; Jorge Tavares weblog", "snippet": "Right now the implemented <em>algorithms</em> are: insertion <em>sort</em>, <em>quicksort</em>, randomized <em>quicksort</em>, <em>merge</em> <em>sort</em>, heapsort and counting <em>sort</em>. The plan <em>is</em> to add more <em>algorithms</em>, for example, bucket <em>sort</em> and timsort.", "explanation": null, "document": "with 2 comments\nThis last weekend I was in Amsterdam to attend the European Common Lisp Meeting . This was my third participation in a organized Lisp meeting (after the first ZSLUG in Zurich and the ELS 2011 in Hamburg) and I am happy I\u2019ve decided to go. I was only present at the meeting itself since going to the dinners and city tour would have been way out of my budget. Anyway, the ECLM was a nice venue. I enjoyed most of the talks and still had an opportunity to talk with fellow lispers. I enjoyed talking with Lu\u00eds Oliveira and meeting Zach Beane .\nThe first talk was given by Nick Levine and it can be viewed in two parts. In the first one, he talked about his experiences of trying to write a CL book for O\u2019Reilly . It was quite interesting to see how hard can it be to prepare a book, especially for a publisher who was (is?) not very lisp-friendly . The second part was mostly about the community, although presented with a rant on libraries. This is a topic that has been debated several times. Thanks to Quicklisp , the problem now is not installing libraries but finding them and knowing which ones are good. I am not sure if creating another site as suggested would be a good thing since resources are already scarce. Perhaps more thought must be made in how to improve the current ones. CLiki still seems to me the best starting point. Still, Nick Levine talk was good and entertaining. One of the best in the meeting.\nThe following talks were mostly about companies that use CL as their main programming language. Jack Harper talked about the company he recently started, Secure Outcomes , that produces a unique portable fingerprint scanner . I must say his talk was quite inspiring! He talked about how to get a startup running and the decisions that took him to choose Lisp as the main development language. In addition, he also explained why prefers Lispworks to any other implementation.\nNext, it was the talk given by Dave Cooper. I must confess his talk was the weaker of the day mainly because he talked about two different subjects without any connection. He started talking about GDL , the main product from his company, Genworks . I\u2019m sure GDL can be a great thing but I didn\u2019t get much from his talk. About halftime, the talk suddenly changed to the Common Lisp Foundation . This was the interesting part of the talk since he explained the aims of CLF, the people behind it, etc. However, it was not clear how it will distinguish itself from ALU in terms of operation (in terms of purpose, CLF just focus on Common Lisp while ALU in all Lisp dialects) and this was the main concern that was expressed during the questions time. After presenting CLF, and since there was still some time left for the next presenter, he went back to GDL.\nAfterwards, it was the turn of Luke Gorrie to present his lisp-hacker startup Teclo Networks . His talk was an expanded/updated version of the one given in Zurich. Still, it was also quite interesting. He started by telling how a group of hackers with a Lisp and/or Erlang background got together to improve the mobile TCP/IP communications. Then, he showed us how TCP badly misbehaves in a mobile network and how their product, Sambal , can give 10% to 27% improvements. Another interesting point of the talk was that CL is used as their main development language. In short, it is used to develop and study all their algorithms. They have a TCP/IP stack fully implemented in CL! Moreover, all their analysis and maintenance tools are also all in CL. However, in the actual product boxes they have reimplemented the algorithms in C. The reason: extreme pragmatism. Luke concluded by hinting that the sales of their product is going very well!\nIn the afternoon the talks started with Paul Miller from Xanalys . The talk was dedicated to Link Explorer , a windows desktop tool to analyze data. The application is quite impressive and was developed using just CL. Paul also gave us a demonstration of the tool as well as some notes on future development.\nThe best and most awaited talk, Quicklisp, technically and socially, was given by Zach Beane . The talk focused on several aspects of Quicklisp . Zach started by giving an overview of the famous library problem of CL, the solutions that existed before QL, explaining their advantages and disadvantages. Also, and very important, what people were actually using and what difficulties they were facing. In a survey he did, most CL programmers were installing libraries by hand, including Zach! Then he proceed to how Quicklisp was developed, some technical issues, what is the role of Quicklisp and what is the reception after one year. The talk focused then on the social impact of Quicklisp in the community. One of the things that makes Zach happy it\u2019s the number of emails he gets saying that people are back to using CL and contributing more to the community (i.e., making libraries available) because of QL. Finally, some indications of what is to come. My perception is that the possibility to enable hacking as it was possible with clbuild is one of the most exciting future features for Quicklisp. Zach\u2019s talk was excellent from all points of view!\nThe last talk of the day was by Hans H\u00fcbner . This was my second favorite presentation. Although the topic, code style and conventions, can start some heated discussions, I must say that I agree with almost everything Hans H\u00fcbner mentioned. However, like everything, some common sense is always necessary. One of the main points was that lispers should not use constructions which are not part of the standard language when the standard provide options, just because you want to save some typing. It is more important for another programmer to understand faster what is written than forcing him to look for the definition of the unusual constructs. The if* , bind were examples given. Hans also talked abut the 80-column rule, style guides, etc. In the end, it always depends on the project, the people, etc, but code style is important and should not be ignored.\nThe meeting ended with several lightning talks. The most interesting bits were: Marco Antoniotti announced ELS 2012 , to be held in Zadar, Croatia, around April-May; Christophe Rhodes talked again about swankr , a swank and slime for R ; the announcement of ABCL 1.0.0 by Erik Huelsmann.\nSome words on the organization. Organizing a meeting of this kind is not easy and Edi Weitz and Arthur Lemmens must be congratulated for making a great event. Not all was perfect but everything went smoothly. I wish that it continues to happen in the coming years!\nWritten by Jorge Tavares\n"}, {"score": 436.97876, "uuid": "a2d5f96d-db06-5fb1-bc6b-1fb49752ef07", "index": "cw12", "trec_id": "clueweb12-0005wb-53-22584", "target_hostname": "academicearth.org", "target_uri": "http://academicearth.org/courses/programming-abstractions", "page_rank": 1.2729409e-09, "spam_rank": 93, "title": "Computer Science II: Programming Abstractions | Stanford Video Course", "snippet": ", Insertion vs Selection, Quadratic Growth of the <em>Algorithm</em>, <em>Merge</em> <em>Sort</em>, <em>Merge</em> <em>Sort</em>: Working&#x2F;execution Demo, <em>Merge</em> <em>Sort</em> Code Explanation, <em>Merge</em> <em>Sort</em> Analysis, Quadratic vs Linear Arithmetic, <em>Sort</em> &#x27;Race&#x27;, Quick <em>Sort</em> Idea Partitioning for <em>Quicksort</em>, <em>Quicksort</em> Code Working&#x2F;execution, <em>Quicksort</em> Code, Live", "explanation": null, "document": "Lecture 2 - Similarity between C++ & Java: - syntax - variable types - operators - control structures\nSimilarity between C++ & Java: - syntax - variable types - operators - control structures, Looking at an Example C++ code: - comment, #include Statements, Global Declarations (constant), Declaring a Function Prototype, The main() Function, Decomposed Function Definition, Example Live Coding: To Calculate the Average, for loop -> a while : Another Purpose of the Same Code, C++ User Defined Data Types: -enums -records, C++ Parameters Passing: -pass by value - pass by reference\nLecture 3 - C++ Libraries - Standard Libraries\nC++ Libraries - Standard Libraries, CS106 Libraries, CS106 random.h Library, C++ String Type, Operations on String Type, String Class' Member Functions, C++ string vs Java String, Live Example Code : Working on Strings, CS106 strutils.h Library, C++ String vs C String, Concatenation Pitfall (C++ vs C string cont.), C++ Console I/O\nLecture 4 - C++ Console I/O\nC++ Console I/O, C++ File I/O, Stream Operations, Live Example Coding : Working with Files, Live Coding Continuation: Function to Operate on the Opened File Stream, Passing the File Stream by Reference, Error Function, Class Libraries OO Features, Why OO is So Successful, CS106 Class Library, CS106: Scanner Library, Scanner Client Interface, Client Use of Scanner, Container Classes, Template Containers, Vector Interface\nLecture 5 - Client Use of Templates\nClient Use of Templates, Vector Class, Vector Client Interface, Client Use of Vector, Type-safety in Templates, Grid Class, Grid Client Interface, Client Use of Grid, Stack Class, Stack Client Interface, Queue Class, Queue Client Interface, Client Use of Queue, Nested Templates, Learning a New API, CS106B Library Documentation\nLecture 6 - More Containers\nMore Containers, Map Class, Uses of Map, Map Client Interface, Live Coding Example: Use of Map, More information on Maps, What\u0019 s Missing? Iterator Operation Through the Map, Iterating Over the Map, Set Class, Set Client Interface, Live Coding Example : Use of Set, Set Higher-level Operations, Why Set is Different\nLecture 7 - Seeing Functions as Data: Specific Plot Functions\nSeeing Functions as Data: Specific Plot Functions, Generic Plot Function, Back to the Set, Live Coding Example: Use of Set with User Defined Data Types, Client Callback Function, Review of the Classes Seen,5 Using Nested ADTs (Abstract Data Types), Live Coding Example, Recursion, Recursive Decomposition\nLecture 8 - Common Mistakes Stumbled Upon: 'I'terator\nStumbled Upon: 'I'terator, Common Mistakes Stumbled Upon: Concatenating Strings, Solving Problems Recursively, Functional Recursion, Example of Recursion: Calculating Raise to Power, Demo of \"Raise to the Power Example\" Through Live Coding, Mechanics of What\u0019 s Going to Happen in Recursion, More Efficient Recursion, Being Wary of Too Many Base Cases, Recursion & Efficiency, Example: Palindromes, Example: Binary Search, Binary Search Code Walk Through, Choosing a Subset; Choose Code\n"}, {"score": 432.8406, "uuid": "b31409cc-a718-584f-8650-4bf446da161a", "index": "cw12", "trec_id": "clueweb12-0000wb-66-19003", "target_hostname": "cis.stvincent.edu", "target_uri": "http://cis.stvincent.edu/html/tutorials/swd/extsort/extsort.html", "page_rank": 1.1862603e-09, "spam_rank": 80, "title": "CIS Department &gt; Tutorials &gt; Software Design Using C++ &gt; External", "snippet": "Instead, on a single-user PC, it <em>is</em> <em>better</em> to read a block of each of the 2 (<em>or</em> more) files into RAM and carry out the <em>merge</em> <em>algorithm</em> there, with the output also kept in a buffer in RAM until the buffer <em>is</em> filled (<em>or</em> we are out of data) and only then writing it out to disk.", "explanation": null, "document": "External Sorting\nIntroduction\nExternal sorting refers to the sorting of a file that is on disk (or tape). Internal sorting refers to the sorting of an array of data that is in RAM. The main concern with external sorting is to minimize disk access since reading a disk block takes about a million times longer than accessing an item in RAM (according to Shaffer -- see the reference at the end of this document).\nPerhaps the simplest form of external sorting is to use a fast internal sort with good locality of reference (which means that it tends to reference nearby items, not widely scattered items) and hope that your operating system's virtual memory can handle it. (Quicksort is one sort algorithm that is generally very fast and has good locality of reference.) If the file is too huge, however, even virtual memory might be unable to fit it. Also, the performance may not be too great due to the large amount of time it takes to access data on disk.\nMethods\nMost external sort routines are based on mergesort. They typically break a large data file into a number of shorter, sorted runs. These can be produced by repeatedly reading a section of the data file into RAM, sorting it with ordinary quicksort, and writing the sorted data to disk. After the sorted runs have been generated, a merge algorithm is used to combine sorted files into longer sorted files. The simplest scheme is to use a 2-way merge: merge 2 sorted files into one sorted file, then merge 2 more, and so on until there is just one large sorted file. A better scheme is a multiway merge algorithm: it might merge perhaps 128 shorter runs together.\nAnalysis\nAccording to Shaffer, a multiway merge using half a megabyte of RAM and a disk block size of 4 KB could hold 128 disk blocks in RAM at once. This would allow 128 runs to be merged together in one pass. The average initial run size would be 1 MB. (See Shaffer on how that can be obtained with only 1/2 MB of RAM.) A file of size 128 MB could be sorted in 2 passes (one to build the initial runs and one to merge them). A file of size 16 gigabytes could be sorted in just 3 passes.\nNote that you do not want to jump back and forth between 2 or more files in trying to merge them (while writing to a third file). This would likely produce a lot of time-consuming disk seeks. Instead, on a single-user PC, it is better to read a block of each of the 2 (or more) files into RAM and carry out the merge algorithm there, with the output also kept in a buffer in RAM until the buffer is filled (or we are out of data) and only then writing it out to disk. When the merge algorithm exhausts one of the blocks of data, refill it by reading from disk another block of the associated file. This is called buffering. On a larger machine where the disk drive is being shared among many users, it may not make sense to worry about this as the read/write head is going to be seeking all over the place anyway.\nPractical Data\nShaffer presents the following practical data concerning external sorting. In this experiment a 4 MB file was sorted on a particular computer. A simple mergesort that did not build initial sorted runs took 451 seconds. A 2-way mergesort that used initial runs of 128 KB took only 160 seconds. A multiway mergesort that used the same initial runs took only 103 seconds. Clearly, using initial sorted runs dramatically speeds up the sorting.\nExample Program\nThe ideas behind an external sort seem simple enough, but implementing a working program is fairly complex. The following example attempts to show the main features used in most any external sort: producing sorted initial runs, the merging of sorted runs, and the buffering of data. However, the design used is simpler than that which is most likely used in a real-world external sort. A key place where this is true is that the example program merges only 2 sorted files at a time; it does not attempt to do a multiway merge (such as the 128-way merge mentioned above). The program also uses buffers of size 64 KB, which is no doubt smaller than necessary. Performance would probably be better with larger buffers. The buffers were kept small so that the merge portion of the algorithm could be observed without needing a huge test file. Note that when the program is creating a sorted run, it uses a single 64 KB buffer, but when it is merging a couple of sorted runs, it uses three 64 KB buffers. A more likely scenario in a good external sort is that the same amount of memory is used in both cases, no matter how many buffers exist in each case. Buffers all of the same size were used to keep the example simpler.\nextsort.cpp\nThe example program does a case-insensitive sort of the text file that the user supplies when prompted by the program. It is assumed that each line of the file contains a word and is no more than 31 characters in length. No attempt is made to order in any way words that are identical except for capitalization. (For example, there is no guarantee what order the sort will place the words MacIntosh and Macintosh since they are seen as identical. Duplicate words such as these are kept in the file.) The output data is placed back under the original file name.\nThe MakeSortedRuns function copies into a buffer a chunk of data from the file being sorted. This buffer is then sorted in main memory by using quicksort. The sorted data is written out to a temporary file. The temporary files for these sorted runs are placed in the current directory in files named ExtSortTemp.0, ExtSortTemp.1, etc.\nThe HandleMerges function has the job of merging all of these sorted runs, two at a time, until all of the data is merged back under the original file name. Other than the special cases, the typical pattern used is to merge ExtSortTemp.0 and ExtSortTemp.1 into ExtSortTempA.0, then the next 2 sorted runs are merged into ExtSortTempA.1, etc. Next we merge the files with the A in their names into ExtSortTemp files. We also merge the top-numbered pair of ExtSortTempA files first, placing the merged data into a file named ExtSortTemp.0, then we merge the next pair of ExtSortTempA files into a file named ExtSortTemp.1, etc. The reason to take the higher-numbered files first this time is that the highest numbered sorted run may be a short remnant that was left over because we had an odd number of runs. That remnant was simply renamed instead of being merged with another file. By now taking the highest-numbered run first, we merge that short remnant with another run.\nNote that the header file has a symbol DEBUG that can be defined if you want to see debugging output. Comment this line off if you do not want this information to appear on the screen. Shown below are the debugging messages produced when sorting a modified copy of the linux.words file. (This file had a couple of extra words added. Also, since the words were already in order, the file was sorted, with the Linux sort command, starting at the third character of each line. This scrambled the order of the words, giving appropriate test data for the external sort program.) Note that the test data file contained 45429 words, one per line.\nEnter the name of the text file to be sorted: linux.txt\r\nMerging ExtSortTemp.0 and ExtSortTemp.1 to ExtSortTempA.0\r\nMerging ExtSortTemp.2 and ExtSortTemp.3 to ExtSortTempA.1\r\nMerging ExtSortTemp.4 and ExtSortTemp.5 to ExtSortTempA.2\r\nMerging ExtSortTemp.6 and ExtSortTemp.7 to ExtSortTempA.3\r\nMerging ExtSortTemp.8 and ExtSortTemp.9 to ExtSortTempA.4\r\nMerging ExtSortTemp.10 and ExtSortTemp.11 to ExtSortTempA.5\r\nMerging ExtSortTemp.12 and ExtSortTemp.13 to ExtSortTempA.6\r\nMerging ExtSortTemp.14 and ExtSortTemp.15 to ExtSortTempA.7\r\nMerging ExtSortTemp.16 and ExtSortTemp.17 to ExtSortTempA.8\r\nMerging ExtSortTemp.18 and ExtSortTemp.19 to ExtSortTempA.9\r\nMerging ExtSortTemp.20 and ExtSortTemp.21 to ExtSortTempA.10\r\nRenaming ExtSortTemp.22 as ExtSortTempA.11\r\nMerging ExtSortTempA.11 and ExtSortTempA.10 to ExtSortTemp.0\r\nMerging ExtSortTempA.9 and ExtSortTempA.8 to ExtSortTemp.1\r\nMerging ExtSortTempA.7 and ExtSortTempA.6 to ExtSortTemp.2\r\nMerging ExtSortTempA.5 and ExtSortTempA.4 to ExtSortTemp.3\r\nMerging ExtSortTempA.3 and ExtSortTempA.2 to ExtSortTemp.4\r\nMerging ExtSortTempA.1 and ExtSortTempA.0 to ExtSortTemp.5\r\nMerging ExtSortTemp.0 and ExtSortTemp.1 to ExtSortTempA.0\r\nMerging ExtSortTemp.2 and ExtSortTemp.3 to ExtSortTempA.1\r\nMerging ExtSortTemp.4 and ExtSortTemp.5 to ExtSortTempA.2\r\nMerging ExtSortTempA.2 and ExtSortTempA.1 to ExtSortTemp.0\r\nRenaming ExtSortTempA.0 as ExtSortTemp.1\r\nMerging ExtSortTemp.0 and ExtSortTemp.1 to linux.txt\r\nPress ENTER:\nThe above output shows that the promised pattern of merges was indeed used by the external sort program. Note that when the number of sorted runs is odd, the remaining file is simply renamed. The final merge shows that when only 2 sorted runs remain, they are merged back into the original file.\nThose who are interested might want to try modifying the example program to use a multiway merge and possibly the \"replacement selection\" algorithm discussed in Shaffer's text. See the references below for more information.\nReferences\nSee the references below for more complete information and more advanced methods. Then try writing your own external sort!\nA Practical Introduction to Data Structures and Algorithm Analysis. Clifford A. Shaffer. Prentice-Hall (1997). See chapter 9.\nData Structures with C++. William Ford, William Topp. Prentice-Hall (1996). See pages 830 and following.\nData Structures: Form and Function. Harry F. Smith. Harcourt Brace Jovanovich (1987). See pages 712 and following.\nRelated Items\n"}, {"score": 422.65204, "uuid": "e9912a4d-1bf9-54e7-89c3-a1b0f715165f", "index": "cw12", "trec_id": "clueweb12-0915wb-36-17169", "target_hostname": "philcrissman.com", "target_uri": "http://philcrissman.com/blog/page/2/", "page_rank": 1.2054959e-09, "spam_rank": 78, "title": "Automagical Thinking", "snippet": "Well, the main reason <em>is</em> that both of these methods are significantly slower than good old Array::<em>sort</em>, <em>which</em> <em>is</em> built in to Ruby (and <em>which</em>, as far as I know, <em>is</em> implemented using the <em>quicksort</em> <em>algorithm</em>).", "explanation": null, "document": "Photo source\nI recently attempted to fit the quicksort algorithm into under 140 characters of Ruby. It looks like:\ndef q(s);t=s.dup;t.size<=1 ? t :(p=t[i=t.size/2];t.delete_at(i);x,y=[],[];t.each{|e|e<p ? x<<e : y<<e};n=q(x);m=q(y);n+[p]+m)end\nThat\u2019s a little hard to read, so here\u2019s the first working version I wrote before attempting to squeeze it into twitterable form:\ndef quicksort(sequence)\n  tmp = sequence.clone\n  if tmp.count <= 1 \n    return tmp\n  else\n    pivot_index = (tmp.count / 2)\n    pivot = tmp[pivot_index]\n    # pull the pivot out of the array.\n    tmp.delete_at(pivot_index)\n    \n    # create 2 sub-sequences, one < pivot, one >= pivot\n    sub_sequence_a = []\n    sub_sequence_b = []\n    tmp.each do |element|\n      if element < pivot\n        sub_sequence_a << element\n      elsif element >= pivot\n        sub_sequence_b << element\n      end\n    end\n\n    # quicksort sub-sequence 1\n    new_sub_sequence_a = quicksort(sub_sequence_a)\n    \n    # quicksort sub-sequence 2\n    new_sub_sequence_b = quicksort(sub_sequence_b)\n    \n    # return sub1 plus pivot plus sub2\n    new_sub_sequence_a + [pivot] + new_sub_sequence_b\n  end\nend\nA key step in this implementation is pulling the pivot out of the array (see around line 9). In my first naive attempt at quicksort I just deleted the pivot right from sequence, the array object that is passed in to be sorted.\nAnyone see the problem there?\nAt first, I sure didn\u2019t. After all, I put the pivot back in the return value, right? (Second to last line.)\nThat does indeed ensure that the returned, sorted array has all the same elements as the original array. But the original array that I passed in would now have one less item. It would (did) lose an element every time I\u2019d run my naive (read: incorrect) implementation of quicksort on it.\nWTF?\nEnter passing objects by reference. In Ruby, everything is passed by reference.(1) It can be easy to forget this, though, especially if you\u2019re used to changing objects with the assignment (\u201c=\u201d) operator.\n1 Just in case that\u2019s not a familiar phrase, \u201cby reference\u201d refers to the idea that a reference to the actual object (like a pointer, if you will) is being passed, not just a copy of the object, or just the value of the object (which is called passing \u201cby value\u201d). (And no, by \u201cpointer\u201d I\u2019m not talking about pointers in C/C++, I\u2019m just trying to say that a reference refers to or in a sense, points to, the actual original object.)\nFor example:\ndef reassign(obj)\n  obj = \"Not the same!\"\nend\n\n#irb\n> greeting = \"Hello!\"\n=> \"Hello!\"\n> reassign(greeting)\n=> \"Not the same!\"\n> puts greeting\nHello!\n=> nil\nSo, assignment inside a method doesn\u2019t change our object. Ruby must not really be passing a referece, right? Wrong, it sure is.\ndef shiftit(obj)\n  obj.shift\nend\n\n#irb\n> h = {:foo => \"bar\", :bar => \"baz\", :baz => \"qux\"}\n=> {:foo=>\"bar\",:bar =>\"baz\",:baz=>\"qux\"}\n> a = [1,2,3,4,5,6]\n=> [1,2,3,4,5,6]\n> shiftit(h)\n=> {:bar =>\"baz\",:baz=>\"qux\"}\n> shiftit(a)\n=> [2,3,4,5,6]\n> # BUT WAIT\n> h\n=> {:bar =>\"baz\",:baz=>\"qux\"}\n> a\n=> [2,3,4,5,6]\nHuh. How about that. Our original objects were changed when they were modified inside the method.\nYou\u2019ll find that any method of modifying an object inside another method with actually change (even delete, if you go that far) the original object that you passed in. Ruby quite definitely, always, passed objects by reference.\nThen How come assignment in a method doesn\u2019t change an object?\nThat\u2019s a good question. I was stumped on this one myself for a bit. The answer comes down to the idea of bindings and scope. When I tell Ruby that foo = \"bar\", Ruby binds the local variable named foo to a String object, \u201cbar\u201d. foo is a String object, and when I pass it to my reassign method, I am passing a reference to the real object, foo.\nHowever, inside the method, I\u2019m in a new scope. When I type foo = \"Some new value\" inside the method scope, trying to reassign foo, Ruby sees that I\u2019m binding foo to a new value; but from inside method scope, Ruby is not going to let me change the binding from another scope. So instead, Ruby figures that I want a local variable that is also named foo, inside my method scope, and it assigns my new value to the local foo, not the original foo.\nI could alter it, still: if I changed the reassign method so that it did obj.replace(\"Some new value!\") inside the method body, the original object would be changed, just like shiftit did to the array and the hash.\nI think I\u2019m a clone, now\nBack to the quicksort implementation I started with: the way to get around this is to use .clone or .dup to get a copy of the object. That way you aren\u2019t actually modifying the original object. Even when I realized this was happening, I still made the mistake of just doing tmp = sequence on my first try. No good; same exact problem. tmp is now just a reference straight to sequence, so altering tmp modifies sequence as well. Need to use tmp = sequence.clone, or something like it.\nThat\u2019s all I have to say about that\nSay it with me now: \u201cRuby passes objects by reference. Ruby passes objects by reference. Every time. All the time. By reference.\u201d\nFun With the Major Scale\nAug 11th, 2010\nGot to chatting with Nate (@fowlduck) about musical notation last night. He was pointing out that bits of the notation seem arbitrary and probably archaic, likening it to legacy code of a sort. For example, why no whole tone between B and C, E and F? This post is NOT meant to be a defense of music notation, a \u201csetting straight\u201d of anything, or even a definitive description of musical theory (at which I\u2019m a rank amateur). It just got me thinking about the major scale and the circle of fourths & fifths, which led to some playing around on the command line, which led to this. Enjoy.\nThe thing that makes it seem so strange is, at least in part, the major scale. The octave consists of twelve semi-tones, in which the major scale is set in this pattern: whole tone, whole tone, semitone, whole tone, whole tone, whole tone, whole tone, semi-tone. Like so:\n1   2   3   4   5   6   7   8   9   10  11  12  13\n| C |   | D |   | E | F |   | G |   | A |   | B | C | ... (etc)\nThe next root note (13th semi-tone, above) is only a semi-tone off from the last note of the scale. And so on. The semi-tone before or after any given note is a flat or a sharp respectively, giving rise to the B# == C, Cb == B, E# == F, Fb == E identities. So, let\u2019s just accept the scale as a given; that this pattern of whole tones and semi-tones sounds pleasing to the ear.\nSo why is it C that has all natural notes (no sharps or flats)? Good question. That part, to me at least, does seem arbitrary, but effectively it doesn\u2019t matter. Given the above system, the major scale in one key and one key only will have no sharps or flats. But still, why structure the notes this way? It still doesn\u2019t necessarily seem to make sense.\nThere are a few reasons. For one, we want to be able to express every scale by naming off the notes, and we\u2019d like each note to be a different letter. So we need 7 letters.\nWhere it gets really interesting is the circle of fourths and fifths.\nSo, if you go around clockwise, you\u2019re counting by fifths (G is the fifth note of the C major scale, D is the fifth note of the G major scale, etc) and if you go around counter-clockwise you\u2019re counting by fourths (F is the fourth note of C, Bb is the fourth note of F, etc). If you go around clockwise, each scale has one additional sharped note until you get to 180 degrees around, then it has n-1 flat notes, and loses one flat each fifth till you get back to C (no sharps or flats); vis versa the other direction. This is actually really cool; each scale, in addition to being unique in that, well, it has a different root note, is unique in that it is the only scale to have its specific number of sharps or flats. If a scale has 3 sharps, it\u2019s A. Period. There isn\u2019t another choice. Neat!\nSo for fun, we can explore the circle of fifths in the command line. First lets set it up:\n> circle = (0..11).map{|n| n*7%12 }\n => [0, 7, 2, 9, 4, 11, 6, 1, 8, 3, 10, 5]\nWell. That doesn\u2019t mean a whole lot. How can we map it to notes? Let\u2019s just construct a reference array really quick. (I\u2019m making it an instance variable so I can use it in a method later; this was all done in irb.)\n> @notes = [\"C\", \"C#/Db\", \"D\", \"D#/Eb\", \"E\", \"E#/F\", \"F#/Gb\", \"G\", \"G#/Ab\", \"A\", \"A#/Bb\", \"B/Cb\"]\n => [\"C\", \"C#/Db\", \"D\", \"D#/Eb\", \"E\", \"E#/F\", \"F#/Gb\", \"G\", \"G#/Ab\", \"A\", \"A#/Bb\", \"B/Cb\"]\nSo, to read our circle, we\u2019d do:\n> circle.map{|n| @notes[n]}\n => [\"C\", \"G\", \"D\", \"A\", \"E\", \"B/Cb\", \"F#/Gb\", \"C#/Db\", \"G#/Ab\", \"D#/Eb\", \"A#/Bb\", \"E#/F\"]\nHooray. The circle of fifths. Now, so what? Well, let\u2019s make a little method to construct a major scale from the @notes variable. We\u2019ll take a root note as an argument, so we can construct any major scale we like.\n> def major(root=0)\n?>  offset = (0..11).map{|n| @notes[(n+root)%12]}\n?>  [offset[0], offset[2], offset[4], offset[5], offset[7], offset[9], offset[11]].join(\" \")\n?>  end\n => nil\nSo, just calling major with no arguments should give us the C major scale; calling with, say, 5 should give us F major (5 in this case is the index of F in @notes; but you were already following that, I know):\n> major\n => \"C D E E#/F G A B/Cb\" \n> major(5)\n => \"E#/F G A A#/Bb C D E\"\nSo we said that the circle of fifths added a sharp each increment, and then flipped to flats and removed on each increment. Let\u2019s watch it happen:\n>> circle.each{|n| puts major(n) }\nC D E F G A B\nG A B C D E F#/Gb\nD E F#/Gb G A B C#/Db\nA B C#/Db D E F#/Gb G#/Ab\nE F#/Gb G#/Ab A B C#/Db D#/Eb\nB C#/Db D#/Eb E F#/Gb G#/Ab A#/Bb\nF#/Gb G#/Ab A#/Bb B C#/Db D#/Eb F\nC#/Db D#/Eb F F#/Gb G#/Ab A#/Bb C\nG#/Ab A#/Bb C C#/Db D#/Eb F G\nD#/Eb F G G#/Ab A#/Bb C D\nA#/Bb C D D#/Eb F G A\nF G A A#/Bb C D E\nNot as neat as it could look since I put the #/b both in the original notes array; the note you\u2019d read would be the one with a consecutive letter after the previous note. So in G, the note following E is F#. In Eb, the note following G is Ab\u2026 and so on. The same note letter only appears once in any given scale. A sharp or flat is added or removed each increment you go around the circle.\nI don\u2019t have any startling conclusions to close this off with. Just that music is pretty cool, and that a few of the things that seem arbitrary just have to be that way \u2013 assuming we\u2019re going to base the system around 12 equally tempered semitones (see Equal Temperment , or even Bach\u2019s Well-Tempered Clavier (evidently there\u2019s some dispute over whether or not Bach\u2019s intended tuning was equal or, uh, \u201cwell\u201d tempered; I was not aware of this!)) and a 7 note major scale \u2013 neither of which are absolutely necessary, of course, and there are other ways to do it. See Mathematics of musical scales and tuning systems for more.\nHappy musicking.\n"}, {"score": 422.50665, "uuid": "53ddd270-aaa8-5653-b118-284ce795ee2b", "index": "cw12", "trec_id": "clueweb12-0703wb-35-15267", "target_hostname": "misc-tech.livejournal.com", "target_uri": "http://misc-tech.livejournal.com/", "page_rank": 2.3765274e-09, "spam_rank": 66, "title": "misc tech", "snippet": "The trick <em>is</em> to write a comparison function that constructs the worst input on the fly. So what would I like to see people using instead? <em>Merge</em> <em>sort</em> and Radix <em>sort</em> would be my preferred candidates. <em>Merge</em> <em>sort</em> <em>is</em> stable and has a worst case runtime of O", "explanation": null, "document": "figg\nWe've been trying to hire some more programmers at work, and being able to sort the wheat from the chaff is a useful thing. Normally we ask a handful of basic java questions (abstract vs interface) and writing a compareTo method.\nAsking ridiculously easy questions is a good thing, as it was surprising to me how often people get them wrong. Given a chance, I'd love to ask harder questions, like:\nWhat's wrong with Quicksort?\nQuicksort is often the first sorting algorithm that comes to mind. It's easy to define and generally accepted to be 'fast enough'. Unfortunately, most people aren't aware of the drawbacks of using quicksort:\n( Can you guess what my answer is? )\nA large problem with quicksort is that it's easy to implement badly, and hard to implement well.\nImplementing quicksort well is described in 'Engineering a Sort Function' by McIllroy and Bentley. They describe how to pick a pivot element and how to partition and swap efficiently. And the trick of defaulting to insertion sort for small lists. This describes the current implementation of quicksort in java and the c standard libraries.\nGenerating malicious input is described by McIllroy in 'A Killer Adversary for Quicksort' . The trick is to write a comparison function that constructs the worst input on the fly.\nSo what would I like to see people using instead? Merge sort and Radix sort would be my preferred candidates.\nMerge sort is stable and has a worst case runtime of O(nlogn). It is also very easy to write, and if you're sneaky you can make it run closer to O(n) on mostly sorted data. The trick is to partition the list into ascending and descending sequences, and then merge these.\nThis trick is used in a few places. Notably in python's default sorting method 'timsort' . Tim peters has written an excellent explanation of the code, and a few clever optimizations for speeding things up in an imperative version.\nAdaptive merge sort can be written in functional languages too. 'Runsort' outlines the same trick in prolog, and a handful of tricks to make it faster. (I managed to rattle off a version of this in haskell )\nRadix sort on the other hand shines when sorting lists of strings (i.e. when comparisons become relatively expensive), usually outperforming quicksort by a factor of two. A good introduction to this is in Engineering Radix Sort' by McIllroy (again!) et al.\nThey describe a variant of Radix Sort - 'American Flag Sort' that tries to minimize space overhead by permuting the list in-place.\nAdmittedly - quicksort is not 'the great satan of sorting functions', but knowing when to use it involves knowing the weaknesses. Even so, Naive quicksort is still the easiest sort function to write on a whiteboard.\n(And the recent two-pivot quicksort variant is still a damn cool bit of work.)\n"}, {"score": 415.95602, "uuid": "a6a7a25b-5d16-591f-8d0e-0a46b6c79496", "index": "cw12", "trec_id": "clueweb12-0011wb-53-23964", "target_hostname": "theory.stanford.edu", "target_uri": "http://theory.stanford.edu/~amitp/rants/c++-vs-c/", "page_rank": 1.2187941e-09, "spam_rank": 67, "title": "Sorting in C++ vs. C", "snippet": "I\u2019d be <em>better</em> off using a general-case library routine <em>or</em> my own code (<em>which</em> I can copy and modify in new situations) for flexibility.", "explanation": null, "document": "Sorting in C++ vs. C\nHome:\nLinks \u2192\n4 Jan 2000 (a long time ago)\nNote: I wrote this a long time ago, using compilers that are now a decade old. At the time, the general opinion was that C++ was \u201cmuch slower\u201d than C, and I wrote this to point out an example of how C++ can be faster in some cases. It seems that the general opinion has changed in the last decade, and people believe that C++ is roughly on par with, or faster than C.\nThere is a tradeoff between writing special-case code yourself, calling a special-case library routine, and calling a general-case library routine. I would like to have:\nSpeed: Naturally, I want my program to run as fast as possible. General-case library code usually does not give me speed, because it\u2019s not optimized for my particular situation. I\u2019d be better off using special-case code. Hand-written code is usually (but not always) best here, because it can take advantage of specialized knowledge of the data.\nFlexibility: I want to have code that works in many different situations. Special-case library code does not give me flexibility, because it\u2019s only written for certain situations. I\u2019d be better off using a general-case library routine or my own code (which I can copy and modify in new situations) for flexibility. A general-case library is best because I don\u2019t have to copy and paste (a maintenance nightmare).\nEase of Coding: I want to write as little as possible to get the job done. Writing code myself does not satisfy this goal. I\u2019d be better off using a library routine. Special-case library routines are best because I don\u2019t have to specify as many parameters.\nAs you can see, none of the solutions gives me all three. Given any one goal, there is a corresponding best solution. Given any one solution, I can only get two out of three goals. In this document I present a comparison of sorting in C and C++, and show that with C++ STL, you can get all three.\nTradeoffs in C\nI would like to sort an array of numbers in C. I have ten minutes to do this. There are more important things I have to do today than to sort numbers. My choices are to use the qsort built-in library routine, to find a sorting routine that sorts whichever type of numbers I am dealing with (and such a routine may not always be available), or to implement a sorting algorithm myself. A program that uses qsort requires not only the call to the sorting routine, but the definition of a comparison function for numbers. To use a special-case library routine, it\u2019s likely that I have to go find the routine somewhere on the net, but I don\u2019t have to specify the data type or comparison function. A program that has its own sort routine does not need a comparison function; it can use the built-in < operator.\nCode size / coding time:\nThe number of lines required to sort using qsort was 1 for the call to qsort and 10 for the comparison function. In comparison, writing my own quicksort routine required 23 lines of code. Calling a special-case library routine only required 1 line of code.\nThe program that used qsort required little coding time and no debugging time. My own quicksort routine on the other hand took some time to code and much time to debug (mostly because it was written from memory, without an algorithms book as a reference). The special-case library, like the general-case library routine qsort, had already been debugged.\nThe special-case library routine is the winner here.\nRunning time:\nThe hand-written sort ran between 2.9 times as fast (for floats) to 7.9 times as fast (for bytes). The special-case library routine ran only slightly faster than the hand-written sort.\nThe special-case library routine and hand-written code are both winners here.\nFlexibility:\nThe qsort routine can be reused for other data types, or for other sorting orders. The hand-written sort only works on one data type and sorting order, but we can just copy the code and make another sort routine that works for another data type or sorting order. (It\u2019s still better not to have to copy and paste.) The special-case library routine does not work on other data types or sorting orders.\nThe general-case library routine is the winner here.\nAs expected, there is a tradeoff here. You can have two of: ease of coding, speed, and flexibility. The general-case library routine was flexible and easy to use, but ran slower; the hand written routine was hard to code and somewhat flexible, but ran faster; the special-case library routine was fast and easy to use, but inflexible.\nAnother Possibility: C++ Templates\nIn C++, the standard template library (STL) provides a sort routine. I tested STL sort as an alternative to the three options available in C.\nCode size / coding time:\nWith STL, there is no need to define a comparison function, since STL can take advantage of C++ operator overloading. There is only 1 line of code to sort an array.\nSTL\u2019s solution matches the best solution (special-case library functions) in C, in terms of coding time.\nRunning time:\nAs I expected, STL\u2019s sort ran faster than C\u2019s qsort, because C++\u2019s templates generate optimized code for a particular data type and a particular comparison function. STL\u2019s sort also ran faster than the hand-coded quicksort routine, and it ran faster than the special-case library routine. (However, this may simply be unique to sorting, and may not extend to other algorithms.)\nSTL\u2019s solution exceeds the best solutions (special-case library functions or my hand-written code) in C, in terms of execution speed.\nFlexibility:\nSTL\u2019s sort works for other data types and other sorting orders. (It works for different data containers as well \u2014 C arrays, C++ vectors, C++ deques, and other containers that can be written by the user. This kind of flexibility is rather difficult to achieve in C.)\nSTL\u2019s solution exceeds the best solution (general-case library functions) in C, in terms of flexibility.\nSTL\u2019s sort retains the advantages of a specialized library routine: it is optimized for a particular data type and comparison function, so it runs fast. At the same time, it retains the advantages of using a general-purpose library routine: it works for any data type, and works with different comparison functions.\nSTL\u2019s solutions can do better than the C library solutions because templates allow C++ code to learn more from their environment than functions do in C code. STL\u2019s sort runs 20% to 50% faster than the hand-coded quicksort or the C special-case library function (and 250% to 1000% faster than the C general-case library function). STL has optimized algorithms that I could write, if I had the time and desire to read research papers in journals about the state of the art in sorting algorithms.* However, I don\u2019t have a lot of time, so it is likely that if I were forced to write a sorting algorithm, I would end up writing insertion sort or (if running time was important) quicksort, and my own quicksort is unlikely to be as fast as the one included with STL.\n* SGI\u2019s STL is using introsort, a combination of quicksort (used when the subarrays are large and the recursion is shallow), heapsort (used when the recursion is deep), and insertion sort (used when the subarrays are small).\nIn another test, between sorting C arrays and instances of the C++ vector class, I found that there was no real difference. Running time (at least for sort) isn\u2019t a factor in the choice between C arrays and C++ vectors.\nTemplate functions save both development time and run time. They retain the flexibility of general-case library routines. No longer do we have to make this tradeoff. We can get better algorithms, a good implementation, less coding time, and fewer bugs.\nAppendix: Running Times\n"}]]}